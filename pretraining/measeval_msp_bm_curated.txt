What is claimed is:
1. A composite transition metal oxide-based precursor represented by the following Chemical Formula 1:
<in-line-formulae>Ni<sub>a</sub>Co<sub>b</sub>M′<sub>c</sub>O<sub>x </sub>(1<<i>x≦</i>1.5)  [Chemical Formula 1]</in-line-formulae>
in the formula,
M′ is one or more selected from the group consisting of an alkali metal, an alkaline earth metal, a Group XIII element, a Group XIV element, a Group XV element, a Group XVI element, a Group XVII element, a transition metal, and a rare earth element, and
<in-line-formulae>0.6≦<i>a<</i>1.0, 0≦<i>b≦</i>0.4, 0≦<i>c≦</i>0.4, <i>a+b+c=</i>1.</in-line-formulae>2. The composite transition metal oxide-based precursor of claim 1, wherein M′ is one or more selected from the group consisting of Al, Mn, Zr, W, Ti, Mg, Sr, Ba, Ce, Hf, F, P, S, La, and Y.3. The composite transition metal oxide-based precursor of claim 1, wherein the precursor is a primary particle or a secondary particle in which a plurality of primary particles is aggregated.4. The composite transition metal oxide-based precursor of claim 3, wherein the primary particle has a flake-like or needle-like shape with an average particle diameter in a range of 0.01 to 0.8 μm, and a plurality of pore structures is present on the surface or inside thereof, and the secondary particle has an average particle diameter (D50) in a range of 3 to 30 μm.5. The composite transition metal oxide-based precursor of claim 1, wherein the precursor has a tap density of 2.0 g/cc or more.6. The composite transition metal oxide-based precursor of claim 1, wherein the precursor has a specific surface area in a range of 5 to 80 m2/g measured according to the nitrogen adsorption BET method.7. The composite transition metal oxide-based precursor of claim 1, wherein in the precursor, a volume of pores in a range of 5 nm to 50 nm is in a range of 10−3to 10−2cm3/g·nm per weight of particles.8. A cathode active material prepared using the composite transition metal oxide-based precursor of claim 1 and a lithium precursor.9. The cathode active material of claim 8, wherein nickel (Ni) content is 60% or more in the overall transition metals.10. A method for preparing the composite transition metal oxide-based precursor of claim 1, the method comprising a step of oxidizing a composite transition metal hydroxide-based precursor represented by the following Chemical Formula 2:
NiaCobM′c(OH)2[Chemical Formula 2]
in the formula,
M′ is one or more selected from the group consisting of an alkali metal, an alkaline earth metal, a Group XIII element, a Group XIV element, a Group XV element, a Group XVI element, a Group XVII element, a transition metal, and a rare earth element, and
<in-line-formulae>0.6≦<i>a<</i>1.0, 0≦<i>b≦</i>0.4, 0≦<i>c≦</i>0.4, <i>a+b+c=</i>1.</in-line-formulae>11. The method of claim 10, wherein in the step of oxidizing,
(i) a heat treatment is performed under an oxygen atmosphere,
(ii) an oxidizing agent is used, or
(iii) both (i) and (ii) are applied.12. The method of claim 10, wherein in the oxidizing, a heat treatment is performed in a range of 200 to 1,000° C. under an oxygen atmosphere at an oxygen concentration of 80% or more for 1 to 12 hours.13. The method of claim 11, wherein the oxidizing agent is one or more selected from the group consisting of KMnO4, H2O2, Na2O2, FeCl3, CuSO4, CuO, PbO2, MnO2, HNO3, KNO3, K2Cr2O7, CrO3, P2O5, H2SO4, K2S2O8, a halogen, and C6H5NO2.1. Li-Ni composite oxide particles having a composition of Li<sub>x</sub>Ni<sub>1-y-a-b</sub>Co<sub>y</sub>M1<sub>a</sub>M2<sub>b</sub>O<sub>2</sub> wherein x, y, a and b represent 1.00 ≤ x ≤ 1.10; 0 < y ≤ 0.25; 0 < a ≤ 0.25; and 0 ≤ b ≤ 0.10, respectively; M1 is at least one element selected from the group consisting of Al and Mn; and M2 is at least one element selected from the group consisting of Zr and Mg, a product of a metal occupancy (%) of lithium sites of the Li-Ni composite oxide as determined by Rietveld analysis of X-ray diffraction thereof and a crystallite size (nm) of the Li-Ni composite oxide as determined by the Rietveld analysis being not less than 700 and not more than 1400.2. The Li-Ni composite oxide particles according to claim 1, wherein the metal occupancy of lithium sites of the Li-Ni composite oxide as determined by the Rietveld analysis is not less than 2% and not more than 7%.3. The Li-Ni composite oxide particles according to claim 1 or 2, wherein the crystallite size of the Li-Ni composite oxide as determined by the Rietveld analysis is not more than 500 nm.4. The Li-Ni composite oxide particles according to any one of claims 1 to 3, wherein the Li-Ni composite oxide particles have an average particle diameter of 1 to 20µm and a BET specific surface area of 0.1 to 1.6 m2/g.5. A process for producing the Li-Ni composite oxide particles as claimed in any one of claims 1 to 4, comprising the steps of mixing lithium compound particles and Ni-Co hydroxide particles, and calcining the resulting mixture, the Ni-Co hydroxide particles being produced by mixing an aqueous solution of a sulfuric acid salt of each metal element, an ammonia aqueous solution and a sodium hydroxide aqueous solution such that an ammonia concentration in a reaction vessel is controlled to not more than 1.4 mol/L, and a ratio of the ammonia concentration in the reaction vessel to a surplus hydroxyl group concentration in the reaction vessel is controlled to not less than 6.6. A process for producing the Li-Ni composite oxide particles as claimed in any one of claims 1 to 4, comprising the steps of mixing lithium compound particles, Ni-Co hydroxide particles, and aluminum compound particles and/or zirconium compound particles, and calcining the resulting mixture, the Ni-Co hydroxide particles being produced by mixing an aqueous solution of a sulfuric acid salt of each metal element, an ammonia aqueous solution and a sodium hydroxide aqueous solution such that an ammonia concentration in a reaction vessel is controlled to not more than 1.4 mol/L, and a ratio of the ammonia concentration in the reaction vessel to a surplus hydroxyl group concentration in the reaction vessel is controlled to not less than 6.7. A non-aqueous electrolyte secondary battery using a positive electrode comprising a positive electrode active substance comprising the Li-Ni composite oxide particles as claimed in any one of claims 1 to 4.Carbon-coated silicon nanotube arrays on carbon cloth as a hybrid anode for lithium-ion batteries
Silicon nanotubes were fabricated by using ZnO nanowire as a sacrificial template [18]. Typically, CC substrates (W0S1002, Phychemi Company Limited) were cut into small pieces (1-2 cm2), and then cleaned by sonication sequentially in acetone and alcohol for 10 min, respectively. The cleaned CC was then soaked in 0.0025 M zinc acetate dihydrate alcohol solution for 5 min and annealed in oven for 20 min at 300 degC to form a ZnO seed layer. A precursor 150 ml solution was prepared with 0.04 M zinc acetate dihydrate, 0.04 M hexamethylenetetramine and 0.009 M poly(ethylene imine). After preheating the precursor solution for 1 h at 95 degC, the seeded CC substrates were dipped into the solution for another 4 h and finally ZnO nanowire arrays were grown on the substrates. Subsequently, the substrates with ZnO nanowires were cleaned by deionized water and transferred into a chemical vapor deposition chamber to deposit a silicon shell. The deposition of silicon shell was achieved at 500 degC for several min with passing 4 sccm SiH4 and 20 sccm Ar at a pressure of 100 Pa. To form a carbon coating on the surface of silicon shell, another deposition process was carried out at 650 degC for 90 min with passing 4 sccm C2H2 and 20 sccm Ar at a pressure of 500 Pa. Finally, the core ZnO nanowires were selectively removed via a reduction process at 600 degC for 12 h with 20% H2 in N2 at 600 Pa, leaving a carbon-coated Si NT. For comparison, the sample of Si NT arrays/CC was prepared without carbon deposition process.
1. A transition metal precursor for preparation of a lithium transition metal oxide, in which a ratio of tap density to average particle diameter D50 of the precursor satisfies a condition represented by Equation 1 below, and wherein transition metal precursor particles are transition metal hydroxide particles represented by Formula 2 below: 2000 ≤ Tap density/Average particle diameter D50 of transition ≤ 3500 (g/cc∙cm) (1) metal precursor M(OH1-x)2 (2) wherein M consists of Ni, Co, and Mn; and 0≤x≤0.5, and wherein the tap density indicates a bulk density of a powder obtained by vibrating a container under a constant conditions when filled with the powder, wherein 50 g of the transition metal precursor was added to a 100cc cylinder for tapping and then tapped 3000 times.2. A lithium transition metal oxide in which a ratio of average particle diameter D50 of the lithium transition metal oxide to average particle diameter D50 of a transition metal precursor as claimed in claim 1 for preparation of the lithium transition metal oxide satisfies the condition represented by Equation 3 below: 0 <Average particle diameter D50 of lithium transition metal oxide/Average particle diameter D50 of transition metal precursor < 1.2 (3)3. The lithium transition metal oxide as claimed in claim 2, wherein the lithium transition metal oxide is a compound represented by Formula 4 below: LiaNixMnyCozO2-tAt (4) wherein 0<a≤1.2, 0<x≤0.9, 0<y≤0.9, 0<z≤0.9, 2≤a+x+y+z≤2.3, and 0≤t<0.2; and A is at least one monovalent or divalent anion.4. The lithium transition metal oxide as claimed in claim 3, wherein, in Formula 4, x>y and x>z.5. A lithium secondary battery in which a unit cell comprising a positive electrode comprising the lithium transition metal oxide as claimed in any one of claims 2 to 4, a negative electrode, and a polymer membrane disposed between the positive electrode and the negative electrode is accommodated in a battery case.6. The lithium secondary battery as claimed in claim 5, wherein the lithium secondary battery is a lithium ion battery.7. The lithium secondary battery as claimed in claim 5, wherein the lithium secondary battery is a lithium ion polymer battery.8. The lithium secondary battery as claimed in claim 5, wherein the lithium secondary battery is a lithium polymer battery.Comparative study of oxidation of benzyl alcohol: Influence of Cu-doped metal cation on nano ZnO catalytic activity
Pure ZnO was prepared through hydrolysis and oxidizing process. Zn(NO3)2 (1 mmol) dissolved in 100 mL of distilled water with continuous stirring. NaOH solution was added into the drop by drop until the pH of the solution became 12. The resulting white particles were washed three times with distilled water and dried for 24 h at 80 degC. At this point, the white product did not show high crystallinity since it consisted of ZnO and Zn(OH)2. Calcining at 500 degC for 1 h was necessary to obtain high crystalline ZnO [30].
Synthesis of CuxZn1-xO (1% Cu-doped) nanopowder was carried out using a modified procedure [23] and [24]. The targets were specifically designed using high purity Zn(NO3)2*6H2O (99.99%) and CuSO4*5H2O (99%) powders. The Cu-doped ZnO catalyst was prepared by a two-step procedure: (1) preparation of the precursor by co-precipitation; (2) formation of the Cu/ZnO nanopowder by thermal decomposition. This method has been considered to be efficient and inexpensive, allowing for the production of high purity, homogeneous, and fine crystalline powders.
Stoichiometric quantities of Zn and Cu salts were dissolved in 100 mL of deionized double distilled water (solution A). Separately, a solution was prepared by dissolving appropriate amounts of NaOH and Na2CO3 in deionized double distilled water (solution B). Solution A was heated to 85 degC and solution B was added dropwise into it with constant stirring. The temperature was maintained at 85 degC and the reaction mixture was stirred for 1 h and refluxed through a water condenser. The resulting solution was cooled to room temperature and the green precipitate that formed was washed with 3 x 20 mL of de-ionized water and dried under vacuum overnight at 50 degC. Finally, the precursors were calcined at 450 degC for 90 min in a muffle furnace under air atmosphere to obtain the nanocrystalline Cu/ZnO powder.Catalytic Performance of Zeolite-Supported Vanadia in the Aerobic Oxidation of 5-hydroxymethylfurfural to 2,5-diformylfuran
5-Hydroxymethylfurfural (HMF; >99 %), acetonitrile (>=99.9 %), toluene (anhydrous, 99.8 %), α,α,α-trifluorotoluene (TFT) (>=99 %), N,N-dimethylformamide (DMF; >=99.9 %), ammonium metavanadate (>=99 %), oxalic acid (>=99 %), sodium nitrate (>=99 %), and anisole (99 %) were acquired from Sigma-Aldrich. Methyl isobutyl ketone (MIBK; >=98 %) and dimethylsulfoxide (DMSO; >=99 %) were purchased from Fluka. 2,5-Diformylfuran (DFF; 98 %) was supplied by ABCR GmbH & Co. Dioxygen (99.5 %) was purchased from Air Liquide, Denmark. All NH4-zeolites were obtained from Zeolyst International, USA. Vanadium oxide (99.8 %) was acquired from Chr. Gerner-Jensen. All chemicals were used as received.

Commercial NH4-ZSM-5 (Si/Al=15), NH4-beta (Si/Al=25), NH4-mordenite (Si/Al=10) and NH4-Y (Si/Al=12) zeolites were initially calcined at 550 degC for 5 h to obtain the H-ZSM-5, H-beta, H-mordenite, and H-Y supports, respectively.

Vanadia supported on zeolite catalysts with 1-10 wt % V2O5 were prepared by wet impregnation of the supports with vanadium oxalate solution, by a procedure adopted from the literature.44 In a typical experiment to prepare 3 wt % zeolite-supported vanadia catalyst, 1.75 mL of NH4VO3/oxalic acid aqueous solution (0.378 M, prepared from ammonium metavanadate and oxalic acid in the molar ratio 1:2 at 70 degC) was added to 1 g of the zeolite using an incipient wetness impregnation technique. Once the impregnation was completed, the solids were dried at 120 degC for 8 h and then calcined at 500 degC for 5 h. The catalysts were named according to the nominal V2O5 loading in wt % and the support.

For the preparation of the Na-beta support, 10 g of NH4-beta were suspended in 300 mL of 1 M aqueous solution of NaNO3. The mixture was heated to 80 degC and stirred for 1 hour. Afterwards, the zeolite was filtered and washed with distilled water. This procedure was repeated twice. Finally, the recovered Na-beta material was dried and then calcined at 500 degC for 5 h.
Catalytic reduction of NO x by CO over a Ni-Ga based oxide catalyst
To synthesize the NaGaO2 powder, 0.03 mol of Na2CO3 and 0.03 mol of Ga2O3 were mixed and ground in an agate mortar for 30 min. After that, the mixture of Na2CO3 and Ga2O3 was heated at 850 degC for 12 h and the furnace was then cooled to room temperature. The preparation procedure for Ni1-xZnxGa2O4 (0 < x < 1) was performed as follows: the as-prepared NaGaO2 powder (0.004 mol) was dispersed in 25 mL of deionized water and magnetically stirred for 10 min to obtain a NaGaO2 colloidal solution. The NaGaO2 colloidal solution was added to 20 mL of an aqueous solution of Zn(CH3COO)2*2H2O and Ni(CH3COO)2*4H2O (Ni + Zn 0.004 mol) and stirred for 30 min at room temperature. The resulting mixture was heated in a 50 mL Teflon-lined hydrothermal autoclave at 200 degC for 5 h to form the crystalline Ni1-xZnxGa2O4 (0 < x < 1). The sediment was separated by centrifugation and dried at 60 degC for 2 h. Using the same hydrothermal procedure, the ZnGa2O4 and NiGa2O4 powders were prepared.Comparison of the effects of phenyl dichlorophosphate modified and unmodified β-iron(III) oxide hydroxide on the thermal, combustion, and mechanical properties of ethylene-vinyl acetate/magnesium hydroxide composites
Phenyl dichlorophosphate (PDCP), iron(III) chloride hexahydrate (FeCl3*6H2O), and tetrahydrofuran were purchased from Sinopharm Chemical Reagent Co., Ltd. These reagents were of analytical grade and were used without further purification. MH was kindly provided by KeYan Co. (Hefei, China). The EVA copolymer, containing 28 wt % VA, was supplied by Hanwha CO., Ltd. (Korea).
β-Fe(O)OH was prepared by a typical experiment,[10, 11] FeCl3 (0.487 g) was dissolved in distilled water (30 mL) under stirring. Then, the mixture was transferred to a 40-mL Teflon-lined autoclave. Hydrothermal synthesis was carried out in an oven at 110degC for 2 h. The products were collected by filtration, washed with distilled water and ethanol several times, and then dried in an oven at 60degC for 6 h.
Surface modification of β-Fe(O)OH was prepared by the mixture of 10 mL of PDCP with 1 g of β-Fe(O)OH. After it was shaken for a short time, the mixture was moved into a three-necked, round-bottom flask. Then, water (20 mL) was slowly added in to the three-necked, round-bottom flask at 10degC for 5 h. Thereafter, the product was filtered and washed with low-density polyethylene on a Soxhlet extractor for over 36 h. Finally, the particles were dried at 60degC in an oven for 24 h. The obtained particles were labeled β-Fe(O)OPDCP.Structure and properties of MgMxCr2-xO4 (M = Li, Mg, Ti, Fe, Cu, Ga) spinels for electrode supports in solid oxide fuel cells
Samples containing Li, Mg, Fe and Cu were prepared by citric acid-nitrate combustion synthesis,34 while the samples containing Ti, Ga were prepared by solid state reaction. After the combustion reaction was complete, the obtained powders were calcined at 1000 degC for 10 hours. For solid state synthesis Cr2O3 (Alfa Aesar (99%)), (MgCO3)4Mg(OH)2*5H2O (Aldrich (99%)), TiO2 (Alfa Aesar (99.5%)), Ga2O3 (Aldrich (99.99%)) were weighed in stoichiometric amounts and mixed using an ultrasonic probe in acetone for 10 minutes, in order to obtain a good dispersion of the oxides and homogenisation. A small amount (1 wt%) of dispersant KD-I (Tape Casting Warehouse, polyester/polyamide copolymer), was added and the dispersion was stirred for 2 hours. The mixtures were dried and the resulting powders were calcined at 1000 degC for 10 hours. After being calcined, the powders were milled for 2 hours in acetone and then dried. The resulting powders were then pressed into pellets (13 mm diameter and 2 +- 0.3 mm thickness) at a pressure of ~200 MPa and sintered at 1400 degC for 12 hours in air.Energy distribution asymmetry of electron precipitation signatures at Mars

Abstract
The different types of asymmetry observed in the energy distributions of electrons and heavy-ions (M/Q=16-44) during signatures of electron precipitation in the Martian ionosphere have been classified.
This has been achieved using the space plasma instrumentation of MEX ASPERA-3 from peri-centre altitude to 2200km.
ASPERA-3 ELS observes signatures of electron precipitation on 43.0% of MEX orbits.
Unaccelerated electrons in the form of sudden electron flux enhancements are the most common type of electron precipitation signature at Mars and account for ∼70% of the events observed in this study.
Electrons that form unaccelerated electron precipitation signatures are either local ionospheric electrons with enhanced density, or electrons transported from another region of ionosphere, solar wind or tail, or a combination of local and transported electrons.
The heating of electrons has a strong influence on the shape of most electron energy spectra from accelerated precipitation signatures.
On most occasions the general flow of heavy-ions away from Mars is unchanged during the precipitation of electrons, which is thought to be the result of the finite gyroradius effect of the heavy-ions on crustal magnetic field lines.
Only ∼17% of events show some form of heavy-ion acceleration that is either concurrent or at the periphery of an electron precipitation signature.
The most common combination of electron and heavy-ion energy distributions for signatures of electron precipitation involves electrons that visually have very little asymmetry or are isotropic and heavy-ions that have a upward net flux, and suggest the upward current associated with aurora.
Due to a lack of reliable measurements of electrons travelling towards Mars, it is likely we miss further evidence of upward currents.
The second most common combination of electron and heavy-ion energy distributions for signatures of electron precipitation, are those distributions of electrons that are asymmetric and have an net upward flux, with distributions of heavy-ions that also have a net upward flux.
Energy distributions of heavy-ions with a net flux towards Mars occur half as often as heavy-ions with an upward net flux.
There is also evidence to suggest we observe downward currents during electron precipitation signatures when we find energy distributions of electrons that are asymmetric and have an upward net flux, combined with energy distributions of heavy-ions that have a downward net flux.
Wave particle interactions and downward parallel electric fields may be responsible for electrons that display a large amount of asymmetry in the upward direction of the energy distribution and have a upward net flux.
Highlights
► We use ASPERA-3 to classify particle asymmetries during electron precipitation at Mars.
► Most common type of energy distribution is of isotropic electrons and upward heavy-ions.
► Distributions suggesting upward currents observed four times more than downward currents.
► Heavy-ion flow at Mars mostly unaffected by electron precipitation due to finite gyro-radius.
► Acceleration of heavy-ions is rarely observed during electron precipitation.

Introduction
Since the discovery that Mars has the remanent magnetisations of an ancient dynamo imprinted on its crust (Acuña et al., 1998), it has been conceivable that the otherwise non-magnetised planet could host mechanisms that energise plasma and accelerate particles, typically related to planets with strong intrinsic magnetic fields.
Indeed, in 2005 the Mars EXpress (MEX) Spectroscopy for the Investigation of the Characteristics of the Atmosphere of Mars (SPICAM) Ultra-Violet (UV) spectrometer (Bertaux et al., 2004) made the first observations of localised auroral emissions from the nightside atmosphere of Mars over regions of strong crustal magnetic fields (Bertaux et al., 2005).
Further processes akin to a planet with a strong dipole magnetic field have been found in observations of magnetic reconnection (Eastwood et al., 2009; Halekas et al., 2009), and flux rope formation at Mars (Brain et al., 2010; Morgan et al., 2011).
The processes that occur at the low altitudes of the Martian ionosphere and also in the region of the crustal magnetic fields are far from being understood.
As a result, little is known about the mechanisms responsible for the aurora that have been observed in the Martian atmosphere.
In the case of planets with global dipole magnetic fields, such as the Earth, Saturn and Jupiter, auroral emissions occur as a result of atoms and molecules in the respective atmospheres undergoing excitation after collisions with precipitating electrons (McIlwain, 1960).
At the Earth auroral emissions appear in either diffuse or discrete forms.
The former results from electrons typically of plasma sheet origin moving along the Earth's magnetic field until contact is made with the upper atmosphere.
The precipitation of electrons from the plasma sheet is thought to occur mainly from pitch angle diffusion as particles interact with electrostatic electron cyclotron harmonic (ECH) waves (Johnstone et al., 1993; Villalón et al., 1995) or the scattering of electrons by whistler mode waves (Lyons et al., 1974; Horne and Thorne, 2000).
The sharp, bright and discrete form of aurora occurs when the electrons responsible for the auroral emissions have been accelerated.
The type of acceleration most explored in research involves a quasi-static field-aligned potential drop that accelerate electrons over a narrow range in energy, thus forming what is known as a mono-energetic peak.
The potential drop facilitates electrons transported over large distances via an upward current to reach the low altitudes where auroral emissions take place (Gurnett and Frank, 1973).
It has been established that this process occurs as part of a larger current system that closes through the Pedersen and Hall currents in the ionospheres and a return parallel current to a dynamo at higher altitudes (Johansson et al., 2006).
These current systems are responsible for transporting large amounts of energy and momentum within the magnetospheres of the magnetised planets and is therefore of great importance.
For Earth and Saturn, the current system is powered by an external dynamo in the solar wind and for Jupiter the source is internal to the planet's magnetosphere.
Acceleration of electrons can also be observed over a wide range or broad band of energies, as associated with dispersive Alfvén waves (DAWs) (Ergun et al., 1998; Chaston et al., 2003).
For the Earth, modelling by Newell et al. (2009), shows that diffuse aurora may account for 70% of the precipitating particle energy flux into the high-latitude ionosphere.
SPICAM observes an auroral signal as a sudden increase in the intensity of the nightglow from the Martian atmosphere.
Leblanc et al. (2006) have calculated this is most probably produced by electrons with a peak energy of a few tens of eV and not by electrons that have been accelerated.
This would suggest the aurora at Mars is more comparable to the Earth's diffuse aurora.
Studies by Haider et al. (1992, 2007) and Seth et al. (2002), support such a possibility by demonstrating nightglow emissions at Mars would occur from the precipitation of unaccelerated solar wind electrons onto the atmosphere.
However, evidence has been mounting that the UV emissions observed by SPICAM at Mars could also be associated with similar processes that occur in regions of the cusp/polar magnetic field of the Earth during discrete aurora.
The first such example was of peaked electron distributions on open magnetic field lines indicative of acceleration by a current system similar to that found during the Earth's discrete aurora (Brain et al., 2006).
This was followed by the detection of an "inverted-V" shape in the energy-time distribution of electrons and ions, with electrons moving downward and ions upward in the high altitude deep shadow of Mars, indicative of acceleration and the current from a parallel electric field (Lundin et al., 2006a).
Further observations then revealed density depletions alongside peaked electron distributions and beams of O+ ions that have a missing cold component, indicative of long-lived active auroral type flux tubes (Dubinin et al., 2009).
Despite the similarities in the properties of accelerated particles found at Mars and the Earth, there is a great amount of uncertainty as to whether the observations at Mars are the result of a direct analogy with the auroral current systems of the Earth.
This is due to the difference of the Martian ionosphere, which has high values of Pedersen conductivity.
Modelling by Dubinin et al. (2008b) shows that the high conductance of the Martian ionosphere would leave the electric currents that couple the Martian ionosphere and the induced magnetosphere prone to short circuit.
Previous attempts were made by Leblanc et al. (2008), to compare UV observations of Martian aurora by SPICAM in its nadir orientation with in situ electron and ion measurements by MEX Analyzer of Space Plasmas and Energetic Atoms (ASPERA-3) Electron Spectrometer (ELS) and Ion Mass Analyzer (IMA) instruments and electron content as measured by the MEX MARSIS Radio Sounder.
The study found a very good correlation between the locations of aurora with regions that were least probable to be on closed field lines from the crust, as well as a simultaneous correlation to increased electron content and precipitating electron flux at these locations.
However, the SPICAM observations of the aurora did not show any corresponding ion signal from IMA measurements.
This was possibly due to the lack of complimentary viewing direction between that of the SPICAM instrument with the aperture plane of IMA.
Previous studies of "inverted-V" signatures using ELS and IMA were restricted by an earlier energy table of IMA before May 2007, which did not adequately resolve ion measurements below 50 eV.
As a result, possible observations of heavy planetary ions as they begin to accelerate from low altitudes were missed.
Without observing ion beams in conjunction with SPICAM UV observations of Martian aurora or in the low altitudes regions associated with the aurora, it is not possible to assess the current systems responsible for the Martian aurora and therefore to know with certainty that a similar mechanism for creating aurora at the Earth is present at Mars.
Hence, this paper will survey accelerated and unaccelerated electron signatures of electron precipitation as could lead to the Martian aurora, to assess the different mechanisms that lead to electron precipitation.
Using the ASPERA-3 ELS and IMA instruments we will compare the energy distributions in differential energy flux (DEF) of electrons and heavy-ions at the times of these signatures.
The study will make use of IMA's updated energy table that allows for increased energy resolution of ion measurements below 50eV.
By comparing the energy distributions of electron and heavy-ions with the new energy table of IMA, we will search for evidence of upward flowing ion beams from peri-centre altitude, ∼275km, up to altitudes of 2200km and further evidence of Earth-like auroral acceleration processes and current systems around Mars.
A study by Nilsson et al. (2012), of average ion distribution functions around Mars, shows that there is a general outflow of ions from the Martian ionosphere, with greater amounts of cold plasma close to the planet.
On average, ions are observed flowing away and towards Mars.
In this study, we will look at the combination of energy distributions of electrons and heavy-ion during signatures of electron precipitation and will attempt to classify different types of particle distribution asymmetries.
Instrumentation
We present data from the ELS and IMA, the two plasma instruments on MEX ASPERA-3 (Barabash et al., 2004), to study the energy distributions of electrons and heavy-ions during signatures of electron precipitation at Mars.
ELS is a compact spherical top-hat electrostatic analyzer and collimator system, and measures electrons in the energy range of 1-20keV with an energy resolution of ΔE/E=8%.
ELS has a time resolution of 4s, which it takes to complete a sweep of 128 energy levels.
The intrinsic field of view for ELS is 4°×360° and is divided in to 16×22.5° sectors.
ELS is located on the ASPERA-3 main unit, which uses a rotating platform to perform scans over different scanner angles.
For the first 2 years of MEX operation, the scanner did not move from its launch configuration, which was at a scanner angle of 90°.
Since the scanner motion and for the purpose of this study, most data is obtained with the scanner offset angle of 10°.
In this position the ELS sector 0 and sectors 12-15 look out over the spacecraft.
The effect is that these anodes give less integrated flux of electrons compared to unobstructed anodes when observing what should be an isotropic feature of electrons.
For the purpose of this study, we have chosen to interpolate across the spacecraft viewing ELS sectors rather than exclude the data.
This is to compensate for the bias that would result from having sectors with no observations of electrons.
IMA measures ions of energies 0.01-40keV/q for the main ion components H+, H2+, He+ and O+, and the group of molecular ions (20-80amu/q).
IMA measures ions using a deflecting electrostatic analyser system to resolve the energy of the incoming ions and then uses a magnetic filter, which deflect and separate ions over 32 mass channels.
The intrinsic field of view for IMA is 4.6°×360°, which is divided into 16, 22.5° sectors.
IMA also uses electrostatic sweeping to take measurements from 16 polar angles that vary in elevation between ±45°.
IMA completes a full 3-D sweep of 96 energy levels in 192s.
Only heavy-ions are examined since IMA does not measure protons at energies below 700eV-1keV in standard instrument modes.
We do not expect the results to be affected by missing lighter-ion measurements.
This is because the electric field suggested by previous studies to be present during electron precipitation will accelerate different mass particles to the same energy.
Therefore, if we do not observed acceleration in the heavy-ions, we do not expect acceleration to occur for the lighter-ions.
Observations
Our study covers two time periods after IMA started measuring below 50eV, from 25th June 2007 to 31st March 2008 and 1st November 2009 to 31st July 2010 (18 months).
The signatures of electron precipitation were visually selected from hour long ELS energy-time DEF spectrograms of electrons, centred on closest approach of the spacecraft.
As such, the altitude coverage for this study ranged from 2200km to the MEX peri-centre altitude of 275-300km.
Fig. 1 shows the accelerated and unaccelerated signatures of electron precipitation used in this study.
Each panel on the left shows an ELS spectrogram centred in time on the example signature.
The colour of each spectrogram gives the DEF of electrons as averaged over all ELS sectors.
Also shown is the altitude of MEX and the energy of the peak in DEF.
The bar at the top of each panel gives the duration for each electron signature.
The right of the figure compares the energy spectrum averaged over the duration of the electron signature with energy spectra from the ionosphere, wake, tail, solar wind and the induced magnetosheath.
Throughout this paper, we will compare the energy spectra of the electron signatures with spectra from the different regions of the Mars solar wind interaction.
In addition, we also compare to Maxwellian distributions that are modelled on the position of the main peak of the energy spectrum of the electron signature.
The first distribution is referred to as the heated model (dotted line) since the accelerating potential is kept at 0eV and the electron temperature (Te) is increased until a curve of best fit is found.
The second distribution is referred to as the heated/accelerated model (dotted and dashed line) as we decrease the electron temperature and increase the accelerating potential to create a best fit curve.
The third distribution is referred to as the accelerated model (dashed line) as the electron temperature is decreased to a minimum and the accelerating potential is increased until the peak DEF of the model is shifted to the same energy as the measured peak DEF.
When plotted in phase space density (PSD) (s3/km6), the lack of a low energy component to the modelled spectra after being shifted in energy by the accelerating potential, causes the model spectra to diverge with greater PSD than the measured spectra at the peak energy.
To reduce this divergence in PSD, a second less accelerated or unaccelerated population of electrons with energy lower than the accelerating potential must be added to the spectra of the accelerated model.
Hence, we have also plotted the DEF from adding together the heated model to the accelerated model (grey line).
Comparing the different Maxwellian distributions should indicate whether the energy spectrum of the electron signature consists of electrons that have been accelerated, only unaccelerated electrons, electrons that have had some acceleration and some heating or the combination of a population of accelerated electrons and a population unaccelerated and possibly heated electrons.
The first panel of Fig. 1 shows an example of a peaked electron distribution.
The time interval of the ELS spectrogram is 12min, revealing a clear "inverted-V" structure over a 2-min interval at the centre of the spectrogram.
The DEF energy spectrum at the right shows a positive gradient below the energy of the peak, which is at approximately 60eV.
This corresponds to a peak of similar energy when the energy spectrum is plotted in PSD, but is not shown here.
By comparing the energy spectrum of the electron feature to those from the different regions around Mars we find the origin is most likely from the solar wind.
Comparing to the modelled Maxwellian distributions show the presence of an accelerated peak as well as an added contribution of heated electrons.
This indicates the electron feature could be some way between penetrating solar wind and magnetosheath electrons.
Evidence of electrons being accelerated and heated has been found in previous analysis of "inverted-V" electrons and is observed as preferably transverse to the magnetic field at low altitudes (Lundin et al., 2006b; Dubinin et al., 2009).
The second panel shows 20min of data and provides an example of an "inverted-V" signature.
The difference to a peaked electron signal is that the signature lasts for a longer time period of almost 10min.
Its energy spectrum peaks in DEF at 300eV and at a similar energy in PSD.
The energy spectrum is comparable with a typical observation from the induced magnetosheath.
The spectrum also contains a population of secondary electrons and a small peak close to the photoelectron peak of the ionospheric spectrum at ∼17-18eV.
When plotted in units of flux instead of DEF we find similar characteristics to the spectra of a low-altitude "inverted-V" as reported by Lundin et al. (2006a,b), where electrons with energy greater than the energy of the peak flux are those of the accelerated primary electrons, below this energy is degraded primary and backscattered electrons and at lower energies are secondary electrons caused by impacting primary electrons.
However, we are unable to reproduce the main peak of the measured distribution using the accelerated or heated model alone and also when using the superposition of these two model distributions.
Therefore, the "inverted-V" primary electrons are not only accelerated but also heated, such that a distribution between the heated and the heated/accelerated model would best represent the measurements.
The third panel down shows another example of an "inverted-V" signature.
However, this subset of "inverted-V" displays a more bursty appearance and a more intense enhancement in DEF of electrons.
The energy of the peak DEF is also highly variable in time and sometimes displays a broader peak in DEF than is found with a typical "inverted-V".
When averaged over the duration of the signature, the energy spectrum on the right shows the combination of spectra with different peak energies since they are changing in time.
As in the first example of an "inverted-V", the energy spectrum shows two populations of electrons with a peak energy of 20eV and at 300eV, however in this case the secondary population is of comparable DEF to the main peak.
In PSD the peak energies show up at ∼20eV and between 300 and 400eV when plotted in PSD.
By comparing to the modelled distributions we see that the high energy peak compares more closely to the heated model than the sharp peak produced by the most accelerated model.
Although some acceleration is needed to better represent the measurements, the extent in comparison to the amount of heating is a lot less than examples reported in previous studies.
Such strong heating is evidence of wave-particle interaction, which in-turns contributes to the level of degraded primaries, backscattered and secondary electrons and may cause the larger intensities of secondary electrons (Lundin et al., 2006b).
Similar bursty events were found on a number of orbits close to each other near the end of 2007.
The orbits all had a similar orientation around the dusk terminator and peri-centre at southern latitudes.
Most of the events were found near the edges of the crustal magnetic field regions.
Some of the events occurred alongside strong beams of heavy planetary ions and ions in the M/Q=2 IMA mass channel.
The ions were accelerated to similar energies as the electrons indicating acceleration by an electric field.
A more detailed description for each of these events is beyond the scope of this paper and will be addressed in future work.
It is important to note that one of the events on 19th November 2007 had been reported by Dubinin et al. (2008c), (Fig. 9 of the paper) as possibly due to the crossing of a plasma sheet or current sheet.
Indeed, the bursty "inverted-V" signature have a similar appearance as electrons observed by the Mars Global Surveyor (MGS), Electron Reflectometer (ER) during current sheet crossings (Halekas et al., 2006).
The bursty appearance also lends itself to comparisons with electron bursts observed at the Earth that form within the Alfvén acceleration region of the aurora (Paschmann et al., 2002) or as a result of the outflow from a reconnection X-line that are responsible for Alfvén jets (Paschmann et al., 1979).
The fourth panel shows a further subset of "inverted-V" events where ELS observes electrons >1000eV, with low-to-medium DEF.
This is illustrated in right hand energy spectrum where there is a peak DEF at ∼1000eV, as well as a secondary population of electrons with a peak DEF close to 10eV.
The shape for this portion of the energy spectrum is closest in appearance to electrons from the tail.
However, the overall shape of the spectrum is also close to that of the bursty "inverted-V" signature shown in the third panel.
Comparing to the modelled spectrums shows that the heated model alone is capable of reproducing the higher energy peak without the need of acceleration to shift the spectrum.
As in the previous example, this suggests wave-particle interactions are involved in the heating of electrons and also in the generation of a secondary electron population.
Electrons characterised by such fluxes and energies sometimes appear as spikes as in this example, or are found as a more extended feature in time and accompany a more typical "inverted-V" event, for example to the left and right of the "inverted-V" shown in the second panel.
A more detailed examination of these events will be left for future work.
The fifth panel down shows our final signature of electron precipitation of two electron spikes characterised by their low-to-medium DEF and close occurrence to the cusps of crustal magnetic fields.
The electron spikes typically peak in DEF at suprathermal energies (few tens of eV) and include a photoelectron peak, which indicates a connection to the ionosphere.
This is demonstrated by the right hand energy spectrum where the peak in DEF occurs at the same energy as the photoelectron peak found in the ionosphere.
Similar electron spectra have already been reported by Frahm et al. (2006).
Comparing to the Maxwellian distribution shows this electron signature is unaccelerated as it was not necessary to use acceleration to reproduce a best fit spectrum.
However, the electron spikes displayed here are characterised by a greater DEF at higher energies than an energy spectrum found in the ionosphere, and is comparable in form to electrons from the wake and tail.
Our study also includes the electron spikes that only involve electrons ≤ 40eV, which are found at the cusps of the crustal magnetic fields between the electron voids contained within closed crustal magnetic fields (Mitchell et al., 2001; Soobiah et al., 2006).
From ∼18 months of peri-centre data, we have found 157 events of peaked electrons, 22 events of "inverted-V" electrons, 15 events of bursty "inverted-V" electrons (subset-1), 17 events of subset-2 "inverted-V" electrons and 478 events of electron spikes.
It is important to note, a number of other accelerated electron features occur in the Martian ionosphere that have not been included in this study.
Each feature is characterised by a different appearance on the ELS spectrogram.
The reason for not including other accelerated electron features in this study is due to those events having more association to the solar wind than processes involving the crustal magnetic fields.
The main example includes the electron intensifications or electron spikes found on the dayside with a broad peak of large DEF in electrons (Lundin et al., 2004; Gurnett et al., 2010).
Observations of this kind have been connected to localised distortions of the solar wind interaction boundaries as caused by Kelvin-Helmholtz instability (Penz et al., 2004; Gurnett et al., 2010).
Fig. 2(b) shows the distributions of the different signatures of precipitating electrons in a Mars Solar Orbital (MSO) reference frame.
Here the x-axis points towards the Sun, the z-axis is perpendicular to the planets velocity and directed towards the northern ecliptic hemisphere and the y-axis completes the right handed set and is in the opposite direction to the Mars velocity vector.
Note, that the dusk terminator is located along the +y-axis and the dawn terminator is along the -y-axis.
The first row of Fig. 2(a) shows the distributions of peaked electrons, "inverted-V" electrons (normal+first subset).
The second row of Fig. 2(a) shows the distributions of "inverted-V" electrons (second subset) and electron spikes.
The third row of Fig. 2(a) shows the orbital coverage given in number of passes for comparison.
Fig. 2(a) shows the distributions of the electron precipitation signatures in the MSO reference frame are strongly dependent on the MEX orbit coverage.
This is especially significant with regard to the dusk/dawn asymmetry with most events found in the dusk hemisphere of Mars.
However, there is a reasonable suggestion of a north/south asymmetry with more signatures observed in the southern hemisphere.
The distributions also suggest a greater concentration of peaked electrons and electron spikes at the dusk terminator.
Also, we have separated the second subset of "inverted-V" in order to highlight the unique appearance to their distribution in the MSO frame, which is distributed more towards the ecliptic plane.
Fig. 2(b) shows the distributions of the electron precipitation signatures over a map of the radial component of crustal magnetic field as measured by the MGS Magnetometer (MAG) instrument at 400km (Connerney et al., 2001).
All signatures show a remarkable correlation with the contours of the radial magnetic field.
This is an indication for the strong influence that the crustal magnetic fields have on the signatures.
The correlation of the electron precipitation signatures with the contours of the radial crustal magnetic field also reflect results by Dubinin et al. (2008c), which showed when total electron fluxes on the nightside of Mars are plotted over regions of strong crustal magnetic field that a spatially well organised pattern of longitudinally stretched bands of electron penetration form over the strong crustal magnetic fields.
However, Fig. 2(c) shows that signatures of enhanced electron precipitation are also arranged by the contours of weaker crustal magnetic fields.
Note, as with the plot in the x-z axis of the MSO coordinate frame, the second subset of "inverted-V" also demonstrates an interesting and unique appearance over the map of the radial crustal magnetic field.
Next we compare the 689 events of electron precipitation signatures found by ELS with heavy-ion data from IMA to assess the possible mechanisms leading to electron precipitation on a case by case basis.
Energy distribution category-1: electrons down/heavy-ions up
The ELS spectrogram of the first panel of Fig. 3(a) shows an example of a bursty "inverted-V" signature (subset-1), given the broad peak in DEF of electrons.
This is compared to the IMA spectrogram of heavy-ions on the lower left panel, as well as the energy spectrum of the electrons in the upper right panel and the energy spectrum of the heavy-ions in the lower right panel.
Orbit parameters (local time, solar zenith angle, latitude, and longitude) for the 11-min spectrograms are common and shown at the bottom of the heavy-ion spectrogram.
The electron spectrogram is created as described previously.
The spectrogram for heavy-ions (M/Q=16-44) is generated after the removal of instrument noise, by averaging DEF values for those IMA sectors which are not blocked by the spacecraft.
Overplotted on the electron spectrogram is the radial component of the crustal magnetic field as predicted by the model of Cain et al. (2003).
Overplotted on the ion spectrogram is the altitude of the MEX spacecraft.
On both spectrogram panels, a solid line indicates the energy of the peak in DEF.
The plots show the bursty "inverted-V" signature occurs at an altitude of ∼450km and on the eastern edge of the strong southern crustal magnetic field region.
The right hand panel shows the electron signature has an energy spectrum that is closest to electrons from the magnetosheath and has a main peak in DEF at ∼75eV (∼100eV in PSD).
There is also a strong element of secondary electrons with a possible photoelectron peak.
Comparing the energy spectrum of the electron signature to the best fit Maxwellian distributions show that a good approximation of the main peak is found using the heated model on its own (Te=32eV).
As in the example of a bursty "inverted-V" of Fig. 1, it is possible wave-particle interaction is responsible for the strong heating and for the large contribution of secondary electrons at lower energies.
The IMA spectrogram of heavy-ions shows a correlation of heavy-ions to regions of ionosphere, where there is a photoelectron peak around ∼17-20eV in the ELS spectrogram.
The DEF of the heavy-ions increases as MEX moves to smaller solar zenith angles and into greater levels of sunlight where the atmosphere would undergo greater levels of ionisation.
The heavy-ions show very little response to the occurrence of the electron signature apart from a slight increase in energy.
The initial format of the data from ELS and IMA is within the instrument frame and there is a corresponding vector direction in the MSO frame at every observation.
Given this information we have resampled the data into 16 angular bins with respect to Mars.
This results in the energy distribution with respect to Mars of electrons measured by ELS as shown in the first panel of Fig. 3(b) and heavy-ions measured by IMA in the second panel.
The parallel direction represents electrons or ions moving towards (negative vertical axis) and away from Mars (positive vertical axis) and the perpendicular direction represents the horizontal above the Mars surface.
Fig. 3(b) contains the data summed and averaged over the time period of the bursty "inverted-V" electron signature from Fig. 3(a), which is also indicated at the top of the figure.
The shaded background indicates the viewing coverage of the energy distribution.
Measurements where the average flux is greater than zero are shown with the colour scale.
The black line at the rim of the electron distribution indicates those Mars angle bins that contains data interpolated over those ELS sectors viewing over the spacecraft.
To determine the direction of electrons and heavy-ions, we first use the method of Fränz et al. (2006), to calculate the integrated moments of density and bulk velocity for both the electrons and heavy-ions.
This involves correcting the data for the spacecraft potential and is determined using the method described by Fränz et al. (2006).
The moment calculation is performed on the averaged electron and heavy-ion DEF data in the Mars angle bins.
This assumes bins of angle around Mars and the instrument energy that contain no sample observations also contain an average particle flux of zero.
Therefore, we recognise our data is biased for energy distributions that have incomplete coverage.
Fig. 3(b) shows the bursty "inverted-V" event of Fig. 3(a) has a rather isotropic distribution.
Note, this is after interpolating across the spacecraft viewing sectors of ELS, which before the interpolation contained a reduction in DEF at energies ≳ 70eV.
Despite the apparent isotropy shown in the figure, there is still some asymmetry since a comparison of the electron flux from each Mars angle bin of the energy distribution in the upward hemisphere to the downward hemisphere show a net flux of electrons is directed towards Mars.
The heavy-ion energy distribution does not show a clear asymmetry apart for ions measured at higher energies.
A comparison between heavy-ions fluxes from each hemisphere indicates the heavy-ions are directed away from Mars.
The net flux of electrons and the heavy-ions in opposite hemispheres of the energy distribution is suggestive of an upward current, with electrons moving towards and the heavy-ions moving away from Mars.
The estimate of the current density using the above method is ≈0.33μAm-2.
Energy distribution category-2: electrons up/heavy-ions down
Fig. 4(a) shows the observation of two electron spikes at 04:24:02 UT and 04:28:39 UT.
The plot of the radial crustal magnetic field obtained using the Cain model shows both electron spikes occur at the boundary between open and closed crustal magnetic field.
The spectrum of the second electron spike shown on the right has a peak DEF from photoelectrons at ∼13eV.
The photoelectron peak is part of a broad peak from ∼10eV to ∼100eV indicative of an increase in density of the ionospheric electrons.
However, the form of the energy spectrum is similar to electron spectra observed in the tail, but with increased density.
Therefore, it is also possible the electron spike is made up of local photoelectrons (indicated by the line of peak DEF at the photoelectron energy) and electrons transported from the tail, which increase in density when focused by converging crustal magnetic fields.
Being located at the terminator and not deeper in the nightside indicates there is a possibility that the electron spikes signatures could occur due to incursions of the magnetosheath as caused by a Kelvin-Helmholtz instability (Penz et al., 2004; Gurnett et al., 2010).
However, this has so far been associated with electron spikes of much higher flux further on the dayside (Gurnett et al., 2010).
The best fit Maxwellian distributions show when the electron temperature is reduced, only a small amount of acceleration can be applied to shift the peak of the modelled spectra to the peak energy of the measured spectrum.
Therefore, this electron spike has not been accelerated.
Both events coincide with sharp boundaries observed in the heavy-ions, as the spacecraft moves through regions of heavy-ions that are significantly structured and energised.
The DEF energy spectrum for the heavy-ions shown on the right displays multiple peaks, where the peak with the largest DEF occurs at ∼11eV and a second main peak occurs at ∼100eV.
Fig. 4(b) shows that at the time of the electron spike signature of Fig. 4(a), the electron energy distribution has a clear asymmetry between the upward and downward hemispheres.
The difference is even larger between energies of 100-200eV.
The moment calculation provides a strong upward net flux of electrons directed at ∼180° to the Mars nadir.
Note, this result is in conflict with the appearance of the energy spectrum, which suggests the electron spike contains tail electrons precipitating along the crustal magnetic field, unless those electrons have already been reflected by converging magnetic fields.
The second panel shows the heavy-ions clearly have a greater DEF for energies up to 100eV in the downward hemisphere of the energy distribution.
The moment calculation for the heavy-ions provides a strong downward net flux directed at ∼15° to the Mars nadir.
The energy distribution also seems rather isotropic in this hemisphere with the exception of heavy-ions found at larger energies at small angles to the nadir.
Such an observation suggests a scattering process is occurring in the region of the electron spike, as might occur in the presence of plasma waves.
This is possible as the results from the electron measurements suggest the spike feature is in a region of magnetic mirroring where wave-particle interaction could take place (Lennartsson, 1976).
The large angle between the net flux of the electrons and heavy-ions suggests a downward current with electrons moving upwards and heavy-ions moving downwards.
The current density is estimated as ≈0.13μAm-2.
Energy distribution category-3: electron precipitation/heavy-ion hole
Highlighted at the centre of the ELS spectrogram of Fig. 5(a) is a signature of electron precipitation in the form of peaked electrons.
The energy spectrum of the electron signature shown in the right hand panel has a peak at ∼140eV (∼140eV in PSD).
When compared to the best fit Maxwellian distributions, we see a strong resemblance between the main peak and the heated/accelerated model.
This indicates the electrons observed during the signature have been accelerated as well as heated.
At the same time of the accelerated electrons, there is a strong depletion of heavy-ions, such that a hole appears in the IMA spectrogram on the second panel.
This event and other peaked distributions over the same region of the crustal magnetic field have been investigated by Dubinin et al. (2009).
The investigation concluded the depletion of the heavy-ions is most likely the effect of the ejection of heated/accelerated oxygen ions from the ionosphere that would occur in an auroral-type magnetic flux tube.
Therefore, these features were described as occurring in auroral-type magnetic flux tubes.
Even though auroral flux tubes would be common aspect to most aurorae at Mars, we use the term here to identify a region that has a hole or is almost completely depleted of heavy-ions that IMA measures and is also limited to the same structure as the electron signature.
In such cases, we are unable to form a reliable energy distribution for the heavy-ions and therefore we are unable to determine the type of mechanism leading to the electron precipitation apart from the above description.
Fig. 5(b) shows that the peaked electron signature at the centre of the spectrogram in Fig. 5(a) has an energy distribution of electrons that is highly isotropic and illustrates results from previous studies (Brain et al., 2006; Dubinin et al., 2009).
Due to the depletion of heavy-ions during this time, the energy distribution only displays a low flux of heavy-ions that is predominantly flowing towards Mars.
Energy distribution category-4: electrons up/heavy-ions up
The first panel of Fig. 6(a) shows the observation of a peaked electron signature by ELS and with lower DEF than is typical, as shown by its energy spectrum on the right.
The peak is observed at ∼135eV (∼135eV in PSD) and occurs close to a cusp of the crustal magnetic field.
The comparison with the best fit Maxwellian distributions shows the main peak of the energy spectrum of the electron signature fits most closely to the plot of the heated/accelerated model.
This demonstrates the electron signature has been accelerated and heated.
Indeed, when looking closely at the main peak of the electron energy spectra, we observed two smaller peaks at the very top, which would occur when the typical energies of the photoelectrons had been shifted upwards as a result of acceleration.
The second panel and the energy spectrum on the right shows the appearance of heavy-ions at ∼400eV with a low DEF.
At the same time, the heavy-ions at the low energy of ∼10eV decreases in DEF with respect to the surrounding region.
Fig. 6(b) shows that the energy distribution of the electrons has a clear structure directed away from Mars, whereas the second panel shows that the heavy-ions at ∼400eV are found in the opposite hemisphere of the energy distribution directed towards Mars.
However, a greater DEF of heavy-ions are found at lower energies (∼10eV) directed away from Mars.
The net flux for both the electrons and heavy-ions are strongly directed away from Mars at an angle of ∼170° from the nadir in both circumstances.
The difference in the net flux of electrons and heavy-ions gives an estimate of the current density as ≈0.18μAm-2.
This is because the net flux of the heavy-ions as measured by IMA is almost always negligible compared to the net flux of electrons as measured by ELS.
Given these results, the electron signature presented above is classified in an energy distribution category where electrons and heavy-ions both have a coincident distribution directed away from Mars.
It is possible to explain such observations in general if we consider MEX to be above a region that has accelerated electrons upwards, in which case heavy-ions could also be accelerated downwards.
This may explain the appearance of the very low DEF of heavy-ions at ∼400eV.
However, due to the finite gyro-radius effect it is also plausible that the general behaviour of the heavy-ions flowing away from Mars may not change, even when passing an acceleration region.
The gyroradius of heavy-ions with energies around ∼10eV at the location of the accelerated electron signature is around 100km, which is of a similar spatial scale to the horizontal size of a closed crustal magnetic field line at 400km.
Therefore, it is possible the heavy-ions do not remain in the acceleration region long enough to experience its effects.
The upward acceleration of ionospheric electrons without an acceleration of heavy-ions can also be expected to occur when most of the potential-drop responsible for accelerating the electrons is below the spacecraft.
If however, some heavy-ions are found to be accelerated upward at the same time as the electrons, it may suggest that the heavy-ions have been heated and reflected by the mirror force.
Another explanation is found with observations of double potential-drop structures in the Earth auroral region, where electrons from the ionosphere that have been accelerated upwards by a potential-drop below the observations, are sometimes observed alongside upflowing ion beams when a potential-drop is also present above the observation (Yoshioka et al., 2000).
In this case, it suggests that the electrons have entered the bottom end of a potential-drop above the observation that is accelerating ions upwards and depending on the size of the potential counter-streaming electrons may be observed when electrons are reflected back to Earth by the potential (Yoshioka et al., 2000).
Energy distribution category-5: electrons down/heavy-ions down
Fig. 7(a) shows the observation of an electron spike at 05:20:33 UT (∼400km) and a peaked electron signature between 05:22:37 and 05:23:10 UT (∼500km).
The same events were reported by Dubinin et al. (2009), as being caused by long-lived auroral flux tubes.
The DEF energy spectrum of the electron spike signature shows a main peak due to the CO2 and O photoelectron peaks as observed in the energy spectrum from the ionosphere.
At higher energies, the spectrum compares well to those found at larger SZA in the wake and tail, however with added multiple peaks.
Such non-Maxwellian spectra are generally unstable and may create waves.
On the other hand, particles that have passed through a region of plasma waves can also develop such an energy spectrum (Janhunen et al., 2001).
Hence, we suggest that this is an example of wave-particle interaction.
The observations of heavy-ions by IMA show there is a slight increase in the energy of peak DEF during the time of the electron spike signature.
During the observation of the peaked signature that occurs afterwards, there is the depletion of heavy-ions as caused by the evacuation of an auroral-flux tube (Dubinin et al., 2009).
Looking at Fig. 7(b) the energy distribution during the electron spike signature shows a strong downward direction towards Mars, with a net flux vector of 15° to the Mars nadir.
There is also a reasonable suggestion that the heavy-ions also have a net flux directed down towards Mars.
The heavy-ion energy distribution has three sectors in the downward hemisphere with DEF 1.5×10-5erg/(cm2·sr·sec·eV) compared to the sectors of the upward hemisphere where the DEF is no greater than 7.0×10-6erg/(cm2.
sr.sec.eV).
This indicates a clear asymmetry in the heavy-ion energy distribution and suggests a net downward flux.
Though the poor coverage for the heavy-ion energy distribution makes it difficult to trust the net flux vector calculation, the clear asymmetry of the heavy-ions is reason to place this event into a category of coincident downward energy distribution for electrons and heavy-ions.
Given the electron spike signature occurs near a cusp of the crustal magnetic field, it is possible that the energy distribution of electrons is the product of being located at the boundary of open and closed crustal magnetic fields.
The presence of electrons below 40eV travelling towards Mars at the dusk terminator suggests an ionospheric population moving along closed field lines.
This could represent an example of downgoing photoelectrons that could be absorbed by the atmosphere and cause auroral emissions, as suggested in the study by Liemohn et al. (2007).
Such a plasma flow from the dayside would also involve heavy-ions moving in the same direction as the electrons.
The dashed line overplotted on Fig. 6(b) shows the multiple narrow peaks found above 40eV in the energy spectrum of the electron spike, are observed in the downward direction and over a broad range of angles.
This suggests we have observed electrons that have passed through a region of plasma waves above the spacecraft.
As well as causing multiple peaks in the energy spectrum, the region of plasma waves could also be responsible for scattering the electrons over a broad range of angles.
Energy distribution category-6: electrons down-up-horizontal/heavy-ions down-up-horizontal
Fig. 8(a) shows an example of a bursty "inverted-V" signature of electron precipitation at ∼450km on the dusk terminator.
The signature is also found at the eastern edge of a moderate region of crustal magnetic fields.
As show by the energy spectrum on the right, the electron signature has a main peak in the DEF at ∼300eV (∼200eV in PSD), and a strong secondary contribution to the spectrum with a peak DEF between 20 and 30eV.
Comparing to the best fit Maxwellian distributions shows that shape of the main peak of the signature is closest to that of the heated/accelerated model.
As this model involves a significant accelerating potential of 160eV, we can conclude the electrons from the signature have been accelerated as well as receiving a moderate amount of heating.
The second panel shows the behaviour of the heavy-ions is largely unaffected at the start and end of the bursty "inverted-V" signature, whereas at the centre of the signature there is a reduction in DEF of the heavy-ions.
This could be a sign of an auroral flux tube as discussed in Section 3.3.
However, this signature is not classified as such since the heavy-ions in this region still form an adequate energy distribution to compare to the electrons.
Fig. 8(b) shows an isotropic energy distribution of electrons.
Despite the isotropic appearance, the net flux vectors of the electrons are directed at 109° to the Mars nadir.
However, when comparing the net flux of heavy-ions from each angle bin from the upward hemisphere with the downward hemisphere, there is an almost equal contribution from each hemisphere.
We consider this to show that the heavy-ions are directed towards as well as away from Mars.
Therefore, this could be the observation of electrons and heavy-ions moving away from Mars similar to energy distribution category-4 described in Section 3.4.
Also possible is that this event could adhere to energy distribution category-2 as described in Section 3.2 where there is the suggestion of a downward current of electrons moving away and heavy-ions moving towards Mars.
The much larger net flux in electrons estimates as a downward current of current density ≈0.46μAm-2.
As the signature presented in Fig. 8(a) can be counted in more than one of the energy distribution categories, it is considered as showing a combination of different energy distribution categories.
The drop out of heavy-ions that is aligned along both directions of the vertical axis of Fig. 8(b) is an interesting feature.
If the vertical axis of Fig. 8(b) was aligned with the magnetic field direction, the feature could be interpreted as the loss cones of a trapped population of heavy-ions.
Alternatively, if the magnetic field direction was horizontal as suggested by the Cain model and perpendicular to the vertical axis, the feature could be interpreted as showing bi-directional field aligned heavy-ions.
Unfortunately, we are prevented from presenting a meaningful interpretation due to a limited field-of-view and lack of magnetometer data.
Distribution of electron precipitation asymmetry categories
Out of a total of 689 events of electron precipitation signatures, 38 were classified into energy distribution category-1, 137 were classified into energy distribution category-2, 40 were classified into energy distribution category-3 or auroral flux tubes, 306 were classified into energy distribution category-4, 4 were classified into energy distribution category-5, 56 were classified into energy distribution category-6, 34 were unidentified and 74 of the events had no heavy-ion data from IMA.
Note that out of the 40 events classified into energy distribution category-3 and indicating an auroral flux tube, 34 events had energy distributions of electrons with net flux of electrons directed upwards.
Most of the events classified into energy distribution category-6 were the result of the energy distribution of the heavy-ions having an almost equal flux directed away and towards Mars.
Out of the 56 events classified into energy distribution category-6, 36 events had energy distributions of electrons with a net flux directed upwards.
This result suggests that the flow of electrons is greatest away and not towards Mars for the majority of both accelerated and unaccelerated signatures of precipitating electrons.
However, this is not expected, as it is counter to observing signatures of electrons that should be precipitating down on Mars.
For those electrons that have been accelerated, it suggests a potential-drop exists below the spacecraft with a downward parallel electric field accelerating electrons upwards.
We do not consider this as a likely explanation for the majority of cases, since previous studies using electron and in situ magnetic field data from the MAG/ER instrument on MGS of peaked electron distributions (Brain et al., 2006) and electron acceleration signatures (Halekas et al., 2008) do not find similar evidence of downward parallel electric fields.
Therefore, we have explored factors that could influence the energy distributions to produce an upward net flux of electrons.
We have found the angle for the net flux of electrons and therefore how the asymmetry of energy distributions are categorised, showed a strong dependence on the positioning of the ELS sectors.
In particular, the angle to the Mars nadir of the electron net flux direction is mostly found in the opposite hemisphere to the flow direction of electrons measured by the ELS sectors that have the spacecraft in their field of view.
Note, this is still the case even after interpolating data across those ELS sectors affected by the spacecraft.
Since the ELS sectors viewing the spacecraft have a look direction that is away from Mars, it causes a bias towards measuring an upward net flux of electrons, as enhanced fluxes of electrons towards Mars would be missed and therefore measured less often at Mars by ELS.
However, studies by Brain et al. (2007) and Halekas et al. (2008) show that even if we were able to measure the downgoing electrons accurately it is not hugely likely we will find downgoing field aligned beams of electrons, and is even less likely when the upgoing part of the energy distribution is isotropic.
We are most likely to miss field aligned beams or other asymmetries in the energy distribution of electrons towards Mars, when the upgoing part of the energy distribution includes field aligned beams or other asymmetries.
The study by Halekas et al. (2008), also showed that most accelerated electron events occur on closed magnetic field lines, which is evident from electron distributions with loss cones in both directions along the magnetic field.
Note, if the loss cones are not in the measuring plane of the instrument, the remaining part of the distribution would be highly isotropic, which is usually the case for peaked electrons around Mars (Brain et al., 2006; Dubinin et al., 2009).
As our study looks at the electron distribution with respect to the Mars nadir, we are quite likely to miss features from a loss cone.
This is because for solar zenith angles between 30° and 135° SZA (14.0- 20.0 LT) as used in our study, the majority of magnetic field lines from either the crustal magnetic field or the solar wind occur at a large enough angles away from the Mars nadir or even horizontal to Mars, that loss cones in either direction along the magnetic field can be missed.
Given the information from the studies by Brain et al. (2007) and Halekas et al. (2008), we can consider any electron energy distributions with an isotropic distribution in the upward direction, as also being isotropic in the downward direction.
Therefore, even if an isotropic distribution is calculated with an upward net flux of electrons, it is still possible electrons also precipitate down on Mars, enough to reach the atmosphere and cause aurora.
The result of making this assumption changes how the events of electron precipitation signatures are distributed in relation to the categories of electron and heavy-ion energy distribution asymmetry.
The greatest change is to energy distribution category-1 and category-4, as 200 events from category-4 are now added into category-1.
Out of a total of 689 events of electron precipitation signatures, 238 are now classified into energy distribution category-1, 137 are classified into energy distribution category-2, there are still 40 events classified into energy distribution category-3, 106 are classified into energy distribution category-4, 82 are classified into energy distribution category-5, there are still 56 events classified into energy distribution category-6 and the number of events that are unidentified or without heavy-ion data from IMA have remained unchanged.
Fig. 9 shows the distribution of each energy distribution category from peri-centre altitudes up to 2200km.
For each energy distribution category, the number of events are largest between 300 and 400km.
The relationship with altitude could be influenced by the unequal amount of observing time with height.
The nature of the elliptical orbit of MEX provides more observing time for equal distances as the altitude of the spacecraft decreases.
However, we do not expect this to change the main feature of the above distribution.
What Fig. 9 does show is that for each energy distribution category, most of the events occur below an altitude of 600km.
The different categories of energy distribution asymmetry during electron precipitation signatures are plotted in Fig. 10(a,b) in the MSO reference frame.
There is not much difference in how events from each energy distribution category are distributed around Mars and show a strong dependence on the MEX orbit coverage, as presented in Fig. 2(a,b).
Unfortunately, the incomplete orbit coverage in local time impacts on our ability to observe any dependence of our classification scheme with local time and location around Mars.
It would also be desirable to create a similar plot in IMF direction to see if this would reveal any relationship.
Since our study started in 2007 we do not have the suitable solar wind proxy data from the MGS MAG instrument to do this, as the MGS mission ended in the same year.
Although, we would expect a similar result to that from Dubinin et al. (2008d), which found peaked and "inverted-V" electrons measured by ELS occurred in the Martian hemisphere pointed by the interplanetary electric field.
Summary and discussions
We use the space plasma instrumentation of MEX ASPERA-3 to classify the different types of asymmetry found in the energy distributions of electrons and heavy-ions during signatures of electron precipitation at Mars.
The study used data from peri-centre altitude up to 2200km.
Our study included signatures of electron precipitation that had been accelerated as found during peaked and "inverted-V" electron signatures and those that only contained unaccelerated electrons as found during electron spike signatures.
Note, the "inverted-V" electron signatures also covered two subset of signatures.
The first subset were of bursty "inverted-V" signatures characterised by their sudden and bursty appearance of intense DEF in electrons over a broad energy range.
Such features have previously been connected to current sheet crossings, but may also be associated with Alfvénic acceleration regions of the Earth aurora (Paschmann et al., 2002) or outflow from a reconnection X-line (Paschmann et al., 1979).
Most of the energy spectra of these electron signatures showed evidence of either electrons being heated as well as accelerated or of a combination of heated and accelerated electron populations.
The heating of electrons, have also been found in previous studies (Lundin et al., 2006b; Dubinin et al., 2009), and suggest the presence of wave-particle interactions.
The second subset of "inverted-V" signatures consisted of electrons with energies up to 1keV, that were of a low-to-medium DEF and included two peaks of comparable DEF, one of secondary electrons at low energy and another peak at higher energies.
The shape of the high energy peak showed little to no sign of acceleration and instead showed the electrons had mostly been heated, and therefore not like what is typically expected from an "inverted-V" signature.
Electron spikes were also studied and represented unaccelerated signatures of electron precipitation.
Similar signatures were first observed with MGS MAG/ER by Mitchell et al. (2001), and then with ELS by Soobiah et al. (2006).
Dubinin et al. (2008a) found that electron fluxes from electron spikes and their enhanced precipitation could explain observed UV emissions.
However, the study by Dubinin et al. (2008a), did not determine the origin of the electron spikes.
Comparing the electron energy spectra for all 478 events of electron spikes with spectra from different regions of the solar wind interaction with Mars shows that the electron spikes have a variety of different sources.
These included local plasma regions of the tail, dusk or dayside ionosphere, sometimes energy spectra indicated electrons transported from the solar wind or the tail and on other occasions a combination of both local and transported electrons were measured.
Energy distribution of the electrons measured by ELS was created with respect to the Mars nadir and compared to the corresponding energy distributions of heavy-ions measured by IMA.
The events of electron precipitation signatures were then categorised according to the combinations of asymmetry as defined by the net flux directions found in the energy distributions of the electrons and heavy-ions.
This methodology resulted in six different combinations of energy distribution asymmetry that are listed in the first column of Table 1.
Table 1 presents the percentage of MEX orbits that we observe the different combinations of energy distribution asymmetries.
In total, ASPERA-3 observes electron precipitation signatures on 43.0% of MEX orbits at Mars.
The signatures of unaccelerated electron precipitation from electron spikes make up ∼70% of this total, occurring on 29.8% of MEX orbits, and are therefore the most common form of electron precipitation signature at Mars.
This compares to signatures of accelerated electron precipitation from peaked electrons and "inverted-V" electrons, which occur on 9.8 and 3.4% of MEX orbits, respectively.
This suggests a similarity to the Earth where the diffuse aurora from the precipitation of unaccelerated electrons accounts for a much larger amount of the precipitating flux of particles onto the atmosphere than discrete aurora (Newell et al., 2009).
Peaked electron, "inverted-V" electron and electron spike signatures are observed most with energy distributions that have a net upward flux of electrons.
However, most of these events have electrons with energy distributions that are visually isotropic or with little asymmetry.
As a result, such distributions are considered to also show electrons precipitating down on Mars.
At the times of the electron precipitation signatures, heavy-ions are observed most with an asymmetrical energy distribution that has a net upward flux of heavy-ions.
Hence, the combination of electron and heavy-ion energy distributions that suggest an upward current as associated with aurora (category-1), are the most common combination during signatures of electron precipitation signatures at Mars and is also associated the greatest with each signature of electron precipitation used in this study.
Note, the combination of energy distributions that form category-1 would be one of the least common if we only used the direction of the net flux of electrons to classify the asymmetry.
Furthermore, as downgoing field aligned beams of electrons are mostly likely missed in the ELS data when upward field aligned beams are observed, the number of electron precipitation signatures that could be connected to aurora and make up category-1 is still underestimated.
Heavy-ions that have an asymmetrical energy distribution with a net downward flux, are also a common feature during electron precipitation signatures and occur half as often as those with a net upward flux.
The combination of electron and heavy-ion energy distributions that make up category-2, suggestive of a downward current and category-4 with both up-going electrons and heavy-ions, is only considered for those electron precipitation signatures that have electron energy distributions with a significant asymmetry.
For these signatures, up-going electrons and heavy-ions are the most common combination, while the combination of up-going electrons and down-going heavy-ions occur almost half as often.
Even after discounting upward net flux of electrons from those signatures with isotropic electron energy distribution, category-4 still make up the second largest group and when added together with category-2 occur on 10% of MEX orbits.
Energy distributions of heavy-ions with an almost equal flux directed towards and away from Mars are observed far less often.
Therefore, the combination of energy distributions as classified by category-6 is one of the least common for electron precipitation signatures at Mars.
The relationship of the different energy distribution categories for electron precipitation signatures in regions over crustal magnetic fields (CF) and in regions where there is almost no crustal magnetic fields (Non-CF) is shown in the sixth and seventh columns of Table 1.
Electron precipitation signatures in regions over crustal magnetic fields and in regions where there is almost no crustal magnetic fields, both follow a similar relationship with the different energy distribution categories as found with the total amount of precipitation signatures.
However, the amount of electron precipitation signatures is over four times greater for MEX orbits over the over crustal magnetic fields and is around ten times greater for those events involving auroral flux tubes.
This further demonstrates the strong association signatures of electron precipitation at Mars have with the crustal magnetic fields, which is also illustrated by Fig. 2(c).
Indeed, studies by the MGS MAG/ER show accelerated electron signatures at Mars have magnetic field pitch angle distributions of electrons that indicate the occurrence on crustal magnetic fields with open, closed and counter streaming magnetic field topology (Halekas et al., 2008).
Although the upward net flux direction of electrons for the majority of electron precipitation signatures is questionable, due to the instrumental bias from ELS and the isotropic nature of the energy distributions, an upward net flux of electrons is more definite and believable for cases of energy distributions that are non-isotropic.
This is shown for example in Figs.
4(b) and 6(b).
In the case of Fig. 4(b) it is possible to explain the net upward flux of electrons by the reflection of unaccelerated precipitating electrons by the converging magnetic field found near the cusps of the crustal magnetic field, and in the case of Fig. 6(b) by a downward parallel electric field below the spacecraft.
The comparison of ELS spectrograms of electrons with IMA spectrograms of the heavy-ions measured by IMA (Figs.
3-8) provided more straightforward results than comparing the energy distributions of electrons and heavy-ions created with respect to the Mars nadir.
In particular, it demonstrated the variation in response of heavy-ions to the signatures of electron precipitation.
Figs.
3, 6 and 7 show examples where there is only a negligible response of the heavy-ions to the signatures of electron precipitation.
As discussed earlier, the general flow of heavy-ions away from Mars is unchanged during most observations of electron precipitation signatures.
This is thought to be the result of the finite gyro-radius effect of the heavy-ions on crustal magnetic field lines.
The heavy-ions measured during a signature of electron precipitation may also show a strong depletion or a hole as presented in Figs.
5 and 8 and is thought to occur as a result of an auroral flux tube (Dubinin et al., 2009).
Fig. 5 also shows an acceleration of heavy-ions between ∼01:25:00 UT and 01:25:40 UT, prior to the first signature of electron precipitation shown in the ELS spectrogram.
Similar acceleration of heavy-ions is found around a number of other events of electron precipitation signatures.
Further analysis of this type of acceleration of heavy-ions will be left for future work.
However, we refer to these events as showing a "peripheral acceleration" of heavy-ions.
We have included the identification of these events in Table 1 to compare with the results of the energy distribution categories.
Fig. 4a shows how a signature of electron precipitation occurs at boundary of heavy-ions that are energetic and structured.
Note, this also coincided with energy distributions of the electrons and heavy-ions suggesting a downward current.
Similar observations feature when a downward parallel electric field is measured at the sharp plasma boundary between the Earth's polar cap and plasma sheet (Johansson et al., 2006).
Under the circumstances that arise at the Earth there is a strong association with monopolar electric fields as characterised by a stepped/S-shaped potential.
Such a potential occurs when only one side of the boundary has a plasma population that supports significant field-aligned currents (FACs) and closure of the current.
Unfortunately, Figs.
3-8 do not show examples of accelerated heavy-ions that are restricted to the same structure of the electron precipitation signature, which we also observe.
Such examples have already been noted in the discussion of the bursty "inverted-V" signature presented in Fig. 1.
As with events showing a "peripheral acceleration" of heavy-ions, we have included the identification of these events in Table 1 to compare with the results of the energy distribution categories and are referred to as a "concurrent acceleration" of heavy-ions.
Hence, the final two columns show the relationship of electron precipitation signatures observed with "concurrent" and "peripheral" acceleration of heavy-ions, respectively with the different energy distribution categories.
The relationship of these events with the different signatures of electron precipitation is presented in the final two rows, respectively.
Out of the total 689 events of electron precipitation signatures, 85 were observed with a concurrent acceleration of heavy-ions.
This accounts for 12% of precipitation signatures occurring on ∼5% of MEX orbits.
Only 37 events of electron precipitation signatures were observed with a peripheral acceleration of heavy-ions.
This makes the peripheral acceleration of heavy-ions less common occurring on just 5% of precipitation signatures and on ∼2% of MEX orbits.
Therefore, it is quite rare to observe accelerated beams of heavy-ions during signatures of electron precipitation at Mars.
The final two rows of Table 1 show an even spread of events with concurrent acceleration of heavy-ions for unaccelerated and accelerated signatures of electron precipitation.
This indicates the concurrent acceleration of heavy-ions has a greater association with signatures of accelerated electron precipitation given as there is a much larger number of unaccelerated electron precipitation signatures observed.
This provides further evidence to previous studies for the association of a quasi-static field-aligned potential drop and its upward current with signatures of accelerated electron precipitation observed at Mars.
Acknowledgments
We wish to acknowledge the Swedish National Space Board for the support of this work and the ASPERA-3 ELS & IMA instruments at Swedish Institute of Space Physics (IRF), Kiruna, Sweden.
We also wish to acknowledge NASA for their support of ELS at the South West Research Institute, San Antonio, TX, USA, on the NASA contract NASW-00003 and STFC, UK, for their support of ELS at the Mullard Space Science Laboratory, UK

1. A lithium-transition metal composite particles, characterized in, comprising:
Lithium transition metal oxide particles;
A metal-doped, doped lithium transition metal oxide particles to form; and
LiF, lithium transition metal oxide is formed on the doped layer comprises metal particles.2. A lithium-transition metal composite particles according to claim 1, characterized in, selected from the metals contained in the metal doped layer comprising Al, Zn, Zr, Ti, W, Sr, B, Mg, Y, Mo, Nb, group Sn Si and the one or mixing of two or more of metal.3. The lithium-transition metal composite particles according to claim 1, characterized in, relative to 100 weight percent of lithium transition metal composite particles, 0.01 weight percent to 3 weight percent of a metal doping region is included.4. A lithium-transition metal composite particles according to claim 2, characterized in, from a lithium transition metal oxide particles having a metal surface to the interior of concentration gradient gradually decreases.5. A lithium-transition metal composite particles according to claim 4, characterized in, of the following chemical formula 1 composite particles comprising a metal-doped:
1 formula
LiaM1-bMebO2,
In the formulae,
M=NixMnyCoz,(0.3 0 ≤ z ≤ 0.6 or ≤ x ≤ 0.9, 0 ≤ y ≤ 0.6),
Selected from Me=Al, Zn, Zr, Ti, W, Sr, B, Mg, Y, Mo, Nb, one of the group consisting of Sn Si and the two or more blending elements,
0.9 ≤ a ≤ 1.3,
0 b ≤ 0.02.6. The lithium-transition metal composite particles according to claim 2, characterized in, further comprising a metal-doped oxide, the oxide comprises a metal.7. A lithium-transition metal composite particles according to claim 2, characterized in, 2 µM % is more than 0 µM % and the doping amount of the metal.8. The lithium-transition metal composite particles according to claim 1, characterized in, fluoride-based polymer is LiF lithium transition metal oxide particles containing at least one of lithium impurities on the portion modified by reaction.9. A lithium-transition metal composite particles according to claim 1, characterized in, with respect to the total weight of the lithium transition metal composite particles, 0.1 weight percent to 0.5 weight percent to LiF comprised.10. A lithium-transition metal composite particles according to claim 8, characterized in, lithium impurity is LiOH, Li2CO3or mixtures thereof.11. A lithium-transition metal composite particles according to claim 8, characterized in, based on the total relative to a lithium-transition metal composite particles, contained in an amount less than 0.3 weight percent lithium impurities.12. A lithium-transition metal composite particles according to claim 8, characterized in, fluoride-based polymer is polyvinylidene fluoride, polyvinylidene fluoride-hexafluoropropylene copolymer or a mixture thereof.13. A lithium-transition metal composite particles according to claim 1, characterized in, lithium cobalt-lithium transition metal oxide particles are selected from oxide, lithium manganese oxide, lithium nickel manganese oxide, lithium manganese cobalt oxide and lithium nickel manganese cobalt oxide of the group consisting of one or a mixture of two or more of.14. A lithium-transition metal composite particles according to claim 13, characterized in, lithium transition metal oxide is selected from the LiCoO2,LiNiO2,LiMnO2,LiMn2O4,Li(NiaCobMnc)O2( here, 0 a 1, 0 b 1, 0 c 1, a + b + c=1), LiNi1-YCoYO2,LiCo1-YMnYO2,LiNi1-YMnYO2( here, 0 ≤ Y 1), Li (NiaCobMnc)O4( here, 0 a 2, 0 b 2, 0 c 2, a + b + c=2) and the LiMn2-zNizO4,LiMn2-zCozO4( here, 0 Z 2) in the group consisting of two or more of the one or their mixture.15. Method for producing a lithium-transition metal composite particles, characterized in, comprising:
A mixed transition metal precursor, lithium compound and metal oxide are mixed and calcined, to obtain lithium transition metal oxide particles include a metal doping step; and
A lithium transition metal oxide particles including a metal doped with a surface modifier and heat treating the mixing step.16. Method for producing a lithium-transition metal composite particles, characterized in, comprising:
The mixed transition metal precursor and a lithium compound mixed and calcined, to obtain a metal-doped lithium transition metal oxide particles comprises the step; and
Lithium transition metal oxide particles including a metal doped layer mixed with a surface modifier and subjected to heat treatment.17. Producing lithium transition metal composite particles according to claim or 16 15, characterized in, 800 °C to 1000 °C calcination is performed at a temperature.18. A method lithium transition metal composite particles prepared according to claim or 16 15, characterized in, heat treatment is performed at a temperature 500 °C to 300 °C.19. Method of preparing lithium transition metal composite particles according to claim or 16 15, characterized in, or having a mixed transition metal precursor M MOOH2(M=NixMnyCoz,0.3 0 ≤ z ≤ 0.6 or ≤ x ≤ 0.9, 0 ≤ y ≤ 0.6, x + y + z=1) of compound.20. Method for producing lithium transition metal composite particles according to claim 15, characterized in, the metal oxide is selected from Al2O3,ZnO,ZrO2,TiO2,WO3,SrO2,B2O3,MgO,Y2O3,MoO3,Nb2O3,Nb2O6,SiO2SnO and the group consisting of two or more of the one or mixture.21. A method lithium transition metal composite particles prepared according to claim 15, characterized in, relative to the total lithium transition metal composite particles, amount to 0.1 weight percent to 1 weight percent of a metal oxide.22. Producing lithium transition metal composite particles according to claim or 16 15, characterized in, the surface modification agent is polyvinylidene fluoride, polyvinylidene fluoride-hexafluoropropylene copolymer or a mixture thereof.23. Method of preparing lithium transition metal composite particles according to claim or 16 15, characterized in, based on the total relative to a lithium-transition metal composite particles, 0.2 weight percent to 0.5 weight percent to used in an amount of surface modification agent.24. A positive electrode active material, lithium transition metal composite particles comprising claim 1.25. A positive electrode, as claim 24 including a positive electrode active material.26. A lithium secondary battery, comprising a positive electrode, negative electrode, a separator between the cathode and interposed between, characterized in, according to claim 25 positive electrode is a cathode.Synthesis and characterization of silver nanoparticles doped reduced graphene oxide

Graphite oxide was first synthesized from the precursor graphite (natural graphite, Sigma Aldrich) by the modified Brodie method as described elsewhere [17] and [18]. The graphite oxide powder of 1 g was mixed with deionized (DI) water of 1 ml and stirred for 12 h for the GO solution. Then, the colloidal silver powder (from Sigma-Aldrich, 75% Ag basis) of 0.002 g were added to the GO solution and sonicated for 1 h followed by dropping hydrazine hydrate of 0.2 μl and stirring for 3 h at 90 degC to reduce GO. The mixture was then filtered several times with DI water and kept in a vacuum oven for overnight. The obtained reduced GO with and without the Ag powder are named as RGO_Ag and RGO, respectively.
1. A positive electrode active material for a lithium secondary battery, characterized by comprising a lithium transition metal composite oxide having a hexagonal crystal structure, wherein Me is Ni, Co and Mn, wherein the molar ratio of Ni in Me is 0.5. ltoreq.
Ni/Me. ltoreq.0.9, the molar ratio of Co is 0.1. ltoreq.
Co/Me. ltoreq.0.3, the molar ratio of Mn is 0.03. ltoreq.
Mn/Me. ltoreq.0.3, and the ratio is relative to Li/Li+The half-width ratio F (003)/F (104) at 4.3V divided by the potential vs.
Li/Li+The half-width ratio F (003)/F (104) at 2.0V is 0.9 to 1.1.2. The positive electrode active material for a lithium secondary battery according to claim 1, wherein the composition formula Li for the lithium transition metal composite oxide<Sub>1+x</Sub>(Ni<Sub>a</Sub>Co<Sub>b</Sub>Mn<Sub>c</Sub>)<Sub>1-x</Sub>O<Sub>2</Sub>Wherein-0.1 < x < 0.1, 0.5 < a < 0.9, 0.1 < b < 0.3, 0.03 < c < 0.3, and a + b + c is 1.3. The positive electrode active material for a lithium secondary battery according to claim 1 or 2, wherein the particle size distribution of the lithium transition metal composite oxide does not have 2 or more maxima.4. An electrode for a lithium secondary battery, comprising the positive electrode active material for a lithium secondary battery according to any one of claims 1 to 3.5. A lithium secondary battery comprising the electrode for a lithium secondary battery according to claim 4.6. An electricity storage device configured by integrating a plurality of lithium secondary batteries according to claim 5.Nanocrystal manganese oxide (Mn3O4, MnO) anchored on graphite nanosheet with improved electrochemical Li-storage properties

In a typical synthesis of Mn3O4/GN, 20 mg of graphite oxide, prepared by a modified Hummer's method [42], was dispersed in deionized (DI) water with sonication for 1 h to form graphene oxide. Then, 1 mmol of Mn(CH3COO)2*4H2O was added into the above solution. When it was dissolved completely, 28 mg of poly(ethylene glycol) was introduced into the solution, followed by adding 20 mL of NaOH aqueous solution (0.2 M) slowly under stirring. The NaOH was used to adjust the pH of the reaction solution. It was found that Mn3O4 is difficult to form by the one-pot route without adding NaOH. The mixed solution was transferred to a Teflon-lined stainless steel autoclave (100 mL) and heated in an electric oven at 180 degC for 12 h. The resultant product was separated by centrifugation, washed with DI water and dried at 60 degC under vacuum for 8 h. MnO/GN was obtained by heating Mn3O4/GN at 400 degC for 2 h in flowing N2/H2 (10% H2). For comparison, bare Mn3O4 and MnO were also prepared using the similar route without adding graphite oxide in the precursors.


Anthracene based organodisulfide positive active materials for lithium secondary battery
The ABTH was prepared following the methods in the literature [10] either from 1,4,5,8-tetrachloroanthracene (route 1 in Scheme 1) or from anthracene (route 2 in Scheme 1), the products were extracted with CS2 in Soxhlet's extractor for 24 h to remove free sulfur then with water-alcohol to remove soluble impurities. PABTH was polymerized simply by oxidative coupling reaction of ABTH in the presence of FeCl3 in chloroform at room temperature for 48 h. After reaction, the chloroform was filtered out, FeCl3 was removed by washing with dilute hydrochloric acid then water till pH 7. The polymer was dried at 60 degC in oven to constant weight.1. A transition metal oxide-based precursor of a composite, characterized in, is a nickel (Ni) and the different type of the transition metal composite oxide of a transition metal,
Of the X-ray diffraction spectrum using CuK α-ray of the at, having a peak at a diffraction angle 2 θ is a 37-39°, 42-44°, 62-64°, 75-77° and a 78-80° 5 occurring when the peak,
And the crystal structure is a cubic system.2. Composite transition metal oxide-based precursor according to claim 1, characterized in,
X-ray diffraction spectrum of the precursor further comprises a diffraction angle 2 θ for at 30-32°, 36-38°, and a 58-60° 3 peaks that occur when,
The group of spinel structure and a cubic system and mixed together.3. Composite transition metal oxide-based precursor according to claim 1, characterized in,
Precursor represented by the following chemical formula 1:
[Formula 1]
NiaCobM'cOx
Wherein,
M'is selected from the alkali metal, alkaline earth metals, Group 13 elements, Group 14 elements, Group 15 elements, Group 16 element, a Group 17 element, a transition metal and one or more of the group consisting of rare earth elements,
1 ≤ x ≤ 1.5,0.6 ≤ a<1.0,0 ≤ b ≤ 0.4,0 ≤ c ≤ 0.4,a + b + c=1.4. Composite transition metal oxide-based precursor according to claim 1, characterized in,
M'is selected from the group consisting of Y and the one of Al,Mn,Zr,W,Ti,Mg,Sr,Ba,Ce,Hf,F,P,S,La or more.5. Composite transition metal oxide-based precursor according to claim 1, characterized in,
The precursor is of the primary particles, or a plurality of primary particles are formed by agglomeration of the secondary particles.6. Composite transition metal oxide-based precursor according to claim 5, characterized in,
Average particle diameter of primary particles of the platy or acicular form 0.01-0.8 µm range,
An average particle size D50 of the secondary particles are 3-30 μm range.7. Composite transition metal oxide-based precursor according to claim 1, characterized in,
Precursor has a tap density less than 2.0 g/cc to.8. An anode active material, which is used according to any one of a transition metal composite oxide claim 1-7 precursor and a lithium precursor to produce.9. The anode active material according to claim 8, characterized in, to all the transition metal group consisting of nickel (Ni) content of not less than 60%.10. Claim 1-7 transition metal oxide-based precursor composite according to any one of the manufacturing method, comprising the following chemical formula 2 in the air atmosphere the transition metal hydroxide represented by the composite oxygen atmosphere at 700-1100 degrees C or a temperature of 1-12 hours of the heat treatment step of:
[Formula 2]
NiaCobM'c(OH)2
Wherein,
M'is selected from the alkali metal, alkaline earth metals, Group 13 elements, Group 14 elements, Group 15 elements, Group 16 element, a Group 17 element, a transition metal and one or more of the group consisting of rare earth elements,
0.6 ≤ a<1.0,0 ≤ b ≤ 0.4,0 ≤ c ≤ 0.4,a + b + c=1.Enhanced thermoelectric performance of Ag0.8Pb18SbTe20 alloyed with Se
The Ag0.8Pb18SbTe20-xSex samples were prepared with tellurium (99.999% purity), lead (99.9%), antimony (99.999%), silver (99.9%), and selenium (99.95%) powders as sources, which were weighed according to the stoichiometric ratio. After being uniformly mixed in an agate mortar under argon protection inside a glove box, the mixtures were pressed into a disk with diameter of 10 mm and height of 4 mm. The pole-shaped samples were assembled for high-pressure synthesis. The Ag0.8Pb18SbTe20-xSex compounds were prepared in a cubic anvil high-pressure apparatus (SPD 6 x 1200T) with a sample chamber of 23 mm on an edge at 1200-1300 K and 4.0 GPa. The temperature and pressure were maintained for 20 min. After that, the electrical power for heating was cut off and the pressure was released. The pressure was estimated by the oil press load, which was calibrated by the pressure-induced phase transitions of bismuth, thallium, and barium metals. The temperature was estimated by the relationship of input heater power and temperature, which was measured by the platinum-rhodium thermocouples.
Valence band study of thermoelectric Zintl-phaseSrZn2Sb2andYbZn2Sb2: X-ray photoelectron spectroscopy and density functional theory


Stoichiometric amounts of high-purity Zn (99.99%), Sb (99.999%), Yb (99.9%), and Sr (99.9%) were combined in a pyrolitic BN crucible (within an argon dry box), which was sealed in a quartz tube evacuated to 1x10-5 torr. Note that the elemental purity implies the metal basis purity. Samples were held at 1223 K for 30 min after a slow heating process. Cooling to 825 K took place over 2.5 h, at which point a 2.5 h anneal was employed. The resulting ingot was ball milled for 15 min in the high-energy SPEX 8000 Series Mixer/Mill utilizing stainless steel vial and balls (all steps under argon). Samples were consolidated via hot pressing at 823 K for ~3 h. A high-density graphite die (POCO) was employed and a force of roughly 1.4 t that was placed on a 12 mm diameter. The consolidated pellets were sliced using nonaqueous lubricant and the resulting wafers were ~1-2 mm thick.


A novel chemical process of Bi2Te2.7Se0.3 nanocompound for effective adjustment in transport properties resulting in remarkable n-type thermoelectric performance
Bismuth shots (Bi, Kojundo Chemical, 99.99%, 2-5 mm) dissolve in diluted nitric acid (2.5 M HNO3(aq)) to subsequently form a transparent solution, which showed signs of bismuth oxidation to generate bismuth ions (Bi3 +) and nitrate (NO3-). Tellurium powders (Te, Kojundo Chemical, 99.99%, 45 μm) do not dissolve in HNO3 but they turn to white precipitates, indicative of tellurium oxidation to form tellurous acid (H2TeO3), which is one of the most stable Te oxoacids. When exposing the reactant elements to HNO3 simultaneously, we observed an entirely different phenomenon; the Te powders gradually dissolved with the Bi shots, resulting in a transparent solution. We assumed that the Te was oxidized by NO3- from the Bi oxidation instead of HNO3, and thus tellurite (TeO32 -) might form rather than H2TeO3 (Eq. (1)) -- a small portion of Te was replaced with selenium powders (Se, Kojundo Chemical, 99.9%, 75 μm) to prepare a ternary compound.Hydrothermal synthesis and electrochemical properties of hexagonal hydrohausmannite plates as supercapacitor electrode material
A 200 mg KMnO4, 100 mg NaOH and 100 mg glucose were dissolved in deionized water with total volume 30 mL. After stirring, the solution was transferred into an autoclave and heated at 200 degC for 72 h, then cooled down to room temperature naturally. The product was repeatedly washed with deionized water and ethanol, then dried at 60 degC for 10 h. The product was characterized by X-ray diffraction (XRD, a Philips X' Pert Pro. Diffractometer), field emission scanning electron microscopy (FESEM, JSM-6701F) and transmission electron microscope (TEM, Tecnai-G2-F30 and Hitachi H-600). Electrochemical tests were performed on an electrochemical workstation (CHI 660E) in a three electrode system in 1 M Na2SO4 solution, with Pt foil and saturated calomel electrode (SCE) as the counter electrode and reference electrode. After mixing the product: acetylene black: polytetrafluorene-ethylene (PTFE) at a mass ratio of 80:15:5, the mixture was pressed onto nickel foam and dried at 60 degC overnight as the working electrode. The mass loading is about 7.6 mg (9.5 mg/cm2).1. A precursor for the preparation of a lithium transition metal oxide that is used for the preparation of a lithium transition metal oxide as a cathode active material for a lithium secondary battery, through a reaction with a lithium-containing compound, wherein the precursor contains two or more transition metals, and the precursor also contains sulfate ion (SO4)-containing salt ions derived from a transition metal salt for the preparation of the precursor at a content of 0.1 to 0.7% by weight based on the total weight of the precursor.2. The precursor according to claim 1, wherein the transition metal is two or more elements selected from Group VB to VIIIB elements of the Periodic Table of the Elements.3. The precursor according to claim 1, wherein the transition metal is two or more elements selected from the group consisting of Ni, Mn and Co.4. The precursor according to claim 1, wherein the transition metal has a composition of NixCOyMn1−(x+y)wherein 0.3≦x≦0.9, 0.1≦y≦0.6, and x+y≦1.5. The precursor according to claim 4, wherein the transition metal is substituted with one or more elements selected from the group consisting of Al, Mg, Cr, Ti and Si within the range of 0.1 mole or less.6. The precursor according to claim 1, wherein the transition metal salt is sulfate.7. The precursor according to claim 6, wherein the sulfate is one or more selected from the group consisting of nickel sulfate, cobalt sulfate and manganese sulfate.8. The precursor according to claim 1, wherein the salt ions include nitrate ions (NO3).9. The precursor according to claim 1, wherein the salt ions have a content of 0.2 to 0.6% by weight, based on the total weight of the precursor.
10-14. (canceled)15. The precursor according to claim 1, wherein the precursor comprising sulfate ion (SO4)-containing salt ions is prepared by a sol-gel, hydrothermal, spray pyrolysis or co-precipitation method.Combined analysis of sMRI and fMRI imaging data provides accurate disease markers for hearing impairment

Abstract
In this research, we developed a robust two-layer classifier that can accurately classify normal hearing (NH) from hearing impaired (HI) infants with congenital sensori-neural hearing loss (SNHL) based on their Magnetic Resonance (MR) images.
Unlike traditional methods that examine the intensity of each single voxel, we extracted high-level features to characterize the structural MR images (sMRI) and functional MR images (fMRI).
The Scale Invariant Feature Transform (SIFT) algorithm was employed to detect and describe the local features in sMRI.
For fMRI, we constructed contrast maps and detected the most activated/de-activated regions in each individual.
Based on those salient regions occurring across individuals, the bag-of-words strategy was introduced to vectorize the contrast maps.
We then used a two-layer model to integrate these two types of features together.
With the leave-one-out cross-validation approach, this integrated model achieved an AUC score of 0.90.
Additionally, our algorithm highlighted several important brain regions that differentiated between NH and HI children.
Some of these regions, e.g.
planum temporale and angular gyrus, were well known auditory and visual language association regions.
Others, e.g.
the anterior cingulate cortex (ACC), were not necessarily expected to play a role in differentiating HI from NH children and provided a new understanding of brain function and of the disorder itself.
These important brain regions provided clues about neuroimaging markers that may be relevant to the future use of functional neuroimaging to guide predictions about speech and language outcomes in HI infants who receive a cochlear implant.
This type of prognostic information could be extremely useful and is currently not available to clinicians by any other means.
Highlights
•
We probe brain structural and functional changes in hearing impaired (HI) infants.
•
We build a robust two-layer classifier that integrates sMRI and fMRI data.
•
This integrated model accurately separates HI from normal infants (AUC 0.9).
•
Our method detects important brain regions different between HI and normal infants.
•
Our method can include diverse types of data and be applied to other diseases.

Introduction
It has been estimated that approximately 1 to 6 infants per 1000 are born with severe to profound congenital sensori-neural hearing loss (SNHL) (Bachmann and Arvedson, 1998; Cunningham and Cox, 2003; Kemper and Downs, 2000; Northern, 1994).
Those children receive little or no benefit from hearing aids and face challenges in developing language abilities due to their inability to detect acoustic-phonetic signals, which are essential for hearing-dependent learning.
Cochlear implantation (CI) is a surgical procedure that inserts an electronic device into the cochlea for direct stimulation of the auditory nerve and has been demonstrated to be effective in restoring hearing in patients suffering from SNHL.
Statistical data from the National Institute on Deafness and Other Communication Disorders (NIDCD) indicate that approximately 28,400 children in the United States have received a cochlear implant as of December 2010.
While many congenitally deaf CI recipients achieve a high degree of accuracy in speech perception and develop near-normal language skills, about 30% of the recipients do not derive any benefit from the CI (Niparko et al., 2010).
A deeper understanding of hearing loss and better characterization of the brain regions affected by hearing loss will help reduce the high variance in CI outcomes and result in a more effective treatment of children with hearing loss.
In recent years, Magnetic Resonance (MR) images have been used to study neurological disorders and brain development in children, such as reading and attention problems, traumatic brain injury, hearing impairment, perinatal stroke and other conditions (Horowitz-Kraus and Holland, 2012; Leach and Holland, 2010; Smith et al., 2011; Tillema et al., 2008; Tlustos et al., 2011).
Brain MRI scans have revealed significant differences between Hearing Impaired (HI) and Normal Hearing (NH) children.
Jonas et al. reviewed a total number of 162 patients' structural MRI scans, and detected 51 abnormalities in 49 patients.
Those abnormalities included white matter changes, structural or anatomical abnormalities, neoplasms, gray matter changes, vasculitis and neuro-metabolic changes (Jonas et al., 2012).
Similar studies have showed consistent results (Lapointe et al., 2006; Smith et al., 2011; Trimble et al., 2007).
Furthermore, functional MRI studies have demonstrated that the activation pattern of HI is different from that of NH during certain scanning tasks (Bilecen et al., 2000; Patel et al., 2007; Propst et al., 2010; Scheffler et al., 1998; Tschopp et al., 2000).
For example, Propst and colleagues studied the activation pattern of HI with narrowband noise and speech-in-noise tasks (Propst et al., 2010).
In the narrowband noise task, they found that HI children had weaker activation in the auditory areas when compared to NH children.
Meanwhile, NH also activated auditory association areas and attention networks, which were not detected in HI children.
In the speech-in-noise task, HI children activated the secondary auditory processing areas only in the left hemisphere, rather than bilaterally as is typical of NH.
Recently, we have tried to use the activation in the primary auditory cortex (A1) to predict CI outcomes.
A strong correlation (linear regression coefficient, R=0.88) was detected between the improvement in post-CI hearing threshold and the amount of activation in the A1 region before CI (Patel et al., 2007).
Despite these recent advances, it remains unclear whether these structural and functional abnormalities are sufficient to distinguish HI from NH individuals.
In this study, we set out to investigate whether we can accurately classify HI from NH individuals based on MR images alone by utilizing machine learning techniques.
We have trained three classifiers, one based on structural MR (sMRI) images, another based on functional MR (fMRI) images, and a third that integrates sMRI and fMRI images.
While traditional methods utilize voxel-based morphometric (VBM) features, in which each single voxel serves as an independent feature, we extracted high-level features to characterize the 3D images.
Specifically, we employed the Scale Invariant Feature Transform (SIFT) algorithm to detect and describe local features in sMRI and extracted region-level features to represent the functional contrast maps.
Based upon the extracted features, SVM classifiers were trained to separate HI from NH.
The SIFT algorithm was first proposed by Lowe for object recognition (Lowe, 1999).
Since then, it has been widely used in the computer vision field.
Basically, the SIFT algorithm detects blob-like image components and calculates a vector to describe each of these components.
Each vector becomes a SIFT feature.
The set of SIFT features extracted from an image contains important characteristics of this image and can be used for subsequent analysis, e.g.
object recognition, gesture recognition etc.
In this study, we employed the SIFT algorithm to extract SIFT features from brain structural MR images, and devised an approach for the automatic classification of NH vs. HI based on the SIFT features.
There are three levels of significance for this study.
First of all, we convincingly demonstrate that hearing loss can be accurately diagnosed based on MR images alone.
Secondly, brain regions identified by the classifiers enable us to better understand hearing loss, and may serve as valuable indicators for the CI outcome and facilitate follow-up treatment post-CI (Jonas et al., 2012).
Finally, our algorithm can be easily extended to assist in diagnosing other disorders affecting children's brains, e.g., speech sound disorders of childhood, leading to a path for improving child health.
The organization of this article is as follows.
In Materials and methods, we describe in sequence the data sources and the preprocessing procedures, the methods of analyzing sMRI and fMRI images, the integrative model that combines these two methods, and the validation of our classifiers.
In Results, we compare the classification performance of the sMRI classifier, the fMRI classifier and the combined classifier, and assess the stability of feature selection as well as the discriminatory power of features.
Finally, in Discussion, we summarize the present work, highlight the significance of our approach, and discuss the limitations and envisioned future improvements.
We also examine the predictive brain regions our classifiers identified and discuss their relevance in the context of hearing loss.
Materials and methods
Data acquisition and preprocessing
Participants
Thirty-nine infants and toddlers participated in a clinically indicated MRI brain study under sedation.
This study was conducted with approval from the Cincinnati Children's Hospital Medical Center Institutional Review Board (IRB).
Eighteen of the participants had SNHL (10 females, average age=14months, range=8-24months).
All hearing impaired participants were referred by the Division of Otolaryngology for MRI as part of the cochlear implant staging process and consented to participate in our adjoining fMRI protocol.
They had documented bilateral severe to profound hearing loss with average hearing thresholds in the range of 90dB or greater.
Nine of these subjects had no measureable hearing response in either ear at the maximum level of our audiometry equipment, at 120dB and can be considered deaf.
The remaining 21 participants were normal hearing controls (15 females, average age=12months, range=8-17months).
These children received clinical MRI scans with sedation for non-hearing related indications.
They were recruited for the control group if they met the inclusion criteria: gestational age of at least 36weeks, normal otoacoustic emissions hearing, and normal neuroanatomy determined by the neuroradiologist.
Informed consent of parent or guardian was obtained prior to the study protocol, and the parent agreed to additional hearing tests at a separate visit.
The child's reason for referral for brain MRI was not related to hearing.
Exclusions included head circumference <5 percentile or >95 percentile, orthodontic or metallic implants that interfere with the MRI, abnormal brain pathology in the central auditory pathways.
Examples of indications for scanning in this group were, "odd body positioning-rule out chiari malformation", "recent onset irritable behavior-rule out brain tumor".
All participants were screened for hearing loss using otoacoustic emission (OAE) prior to the MRI scan.
Failed OAE at the time of scan was also an exclusion criterion for the normal control group.
All of these brain scans of both hearing impaired group and control group were reviewed by a pediatric neuroradiologist and assessed as having no anatomical findings of significance.
One of the challenges of research in pediatric neuroimaging is that it is unethical to expose children to more than minimal risk for the purposes of research.
This principle is dictated by our conscience as well as by the IRB at most institutions.
Consequently, one of the fine points in the design of the present study is that we were required to select our control population among infants who were referred for an MRI scan with sedation because of a clinical indication.
With the precautions described above and other procedures we took to insure normal auditory function and brain anatomy, this is perhaps the best control group that could be obtained for this age group in an ethical fashion.
However, it is important to note that the controls were not randomly sampled from the general population.
MRI/fMRI acquisition
Anatomical images for this study were acquired using a 3.0Tesla Siemens Trio MRI scanner in the clinical Department of Radiology.
Isotropic images of the brain were acquired using an inversion recovery prepared rapid gradient-echo 3D method (MP-RAGE) covering the entire brain at a spatial resolution of 1×1×1mm in an axial orientation.
3D MP-RAGE acquisition parameters were as follows: TI/TR/TE=1100/1900/4.1ms, FOV=25.6×20.8cm, matrix=256×208, scan time=3min and 50s.
These high resolution 3D-T1 weighted images were used for co-registration of fMRI scans which were also acquired during this scheduled MRI.
Functional MRI scans were performed using a silent background fMRI acquisition technique that allowed auditory stimuli to be presented during a silent gradient interval of the scan, followed by an acquisition interval that captured the peak BOLD response of relevant brain regions (Schmithorst and Holland, 2004).
Using the scanner described above we acquired BOLD fMRI scans in an axial plane (4×4mm resolution), using the manufacturer's standard gradient echo, EPI sequence covering the same FOV as the 3D T1 images (see paragraph above), with the following parameters: TR/TE=2000/23msec, flip angle=90°, matrix=64×64 and 25 axial slices with thickness=5mm.
In the present study, all stimulus and control intervals were of equal duration (5s) in a three-phase auditory paradigm consisting of speech, silence, and narrow band noise tones interleaved with acquisition periods of 6s during which 3 image volumes were obtained covering the whole brain.
A timing diagram for the fMRI data acquisition and stimulation paradigm is shown in Fig. 1.
The speech stimulus consisted of sentences read in a female voice.
Altogether 36 sentences were read in 18 segments of 5s duration and comprising 2 sentences each.
This condition was followed by a 6s data acquisition and then a 5s interval of silence as a control condition.
After another 6s control interval acquisition, a second auditory control condition was played.
This condition consisted of Narrow Band Noise (NBN) tones patterned after standard audiology evaluations for detection of hearing thresholds.
Five NBN tones of 1s duration with center frequencies of 250, 500, 1000, 2000 and 4000Hz and bandwidth of 50% were played in random order during this control condition, for a total of 5s during a silent interval of the scanner.
An additional interval of 1s of silence followed each acquisition to provide an acoustic demarcation prior to the stimulus onset of each stimulus condition.
This resulted in the fMRI acquisition time of approximately 11min.
See Fig. 1 for a detailed schematic of the task and timing.
Auditory stimuli were administered through calibrated MR compatible headphones at a sound level of 10-15dB greater than the individual participant's Pure Tone Average (PTA) hearing threshold.
Each hearing impaired participant in the study had a recent audiogram, which was used to determine the sound level for fMRI.
Our MR compatible audio system was modified to allow for an output through the headphones measuring up to 130dB.
Data analysis - preprocessing
fMRI data were initially analyzed on a voxel-by-voxel basis to identify the activated brain regions using a standard pre-processing pipeline implemented in the Cincinnati Children's Hospital Image Processing Software (CCHIPS) (Schmithorst et al., 2010) written in IDL computer language.
In this paper, we use voxel for 3-dimensional images and pixel for 2-dimensional images.
Since the subjects were sedated, we assumed that the anatomical image was naturally aligned with the functional images for each individual.
Therefore, alignments between anatomical images and functional images were not needed in preprocessing.
In this case, it does not matter if we apply the normalization transformation before or after contrast determination.
To generate both normalized contrast maps used in the current study as well as contrast maps in native space for other uses, we first generated contrast maps in each individual's native space and then normalized the contrast maps to standard space.
The raw EPI images were simultaneously corrected for Nyquist ghosting and geometrical distortion (due to B0 field inhomogeneity) (Schmithorst et al., 2001).
EPI functional MR time-series images were corrected on a voxel-by-voxel basis for drift using a quadratic baseline correction.
Motion artifacts were corrected using a pyramid iterative co-registration algorithm (Thevenaz et al., 1998).
During this stage, infant brain images were transformed to the AC-PC plane.
Finally, the individual image volumes (1,2,3) in the event-related fMRI acquisition were separated and submitted to a final pre-processing step using the General Linear Model (Worsley et al., 2002) to construct individual Z-maps for each volume and contrast condition (speech vs. silence, speech vs. tones and tones vs. silence).
Z-maps showing activation for each condition for each participant were then computed by averaging the Z-maps from the individual volumes for each contrast condition (Patel et al., 2007; Schmithorst and Holland, 2004).
These Z-maps, in each individual's native space were used by the radiologists and neurotologists for clinical interpretation of findings.
The neuroradiologist reviewed both functional and anatomical MRI scans for each participant and completed a standardized report indicating whether brain abnormalities or brain activities were detected in primary auditory areas, language areas or other brain regions.
After that, we performed spatial normalization using SPM8 with a T1 template constructed from a control group of age matched subjects selected specifically for this infant cohort (Altaye et al., 2008).
The normalized anatomical images and functional Z-maps were then submitted to the next stages of analysis.
Feature extraction and model learning based on structural MR images
For sMRI images, we used SIFT features to represent the brain images and developed an algorithm to analyze the SIFT features.
We have previously applied this method to Alzheimer's disease, Parkinson's disease and bipolar disease, and it has demonstrated promising classification performance (Chen et al., 2013).
Obtaining 2D slices from 3D brain images
Due to the high density of SIFT features in the brain images and the pair-wise comparison among SIFT features required in a later step, analyzing the 3D brain image as a whole is computationally infeasible.
Thus, the spatially normalized 3D brain (157×189×136) was divided into 560 20×20×20 cubes.
Since the dimensions of brain image were not divisible by 20, the cubes at the end of dimensions only contained the remaining volume of the brain image and therefore had a size smaller than 20×20×20.
The number 20 was determined based on our experience from the application of this algorithm to several other diseases.
The cube size mainly affects the computation speed and accuracy of the likelihood scores as described in the Feature evaluation section below.
A larger size leads to a much longer computation time, while a smaller size decreases the accuracy of likelihood scores and subsequently leads to lower classification accuracy.
According to our experimental results, the cube size 20×20×20 provides a good balance between speed and accuracy.
Every cube was sliced along three different orientations to obtain 3 sets of 20 2D brain images.
We analyzed every cube and every set of 2D brain images individually.
The analysis results were combined together in the last step.
Extracting SIFT features
The SIFT algorithm for analyzing 2D images was implemented in several stable software packages (Lowe; Vedaldi and Fulkerson, 2010).
In this study, we used the SIFT algorithm provided in a publicly available computer vision software package vlFeat (Vedaldi and Fulkerson, 2010).
The SIFT features are described by center locations, scales, orientations and appearance matrices.
An example of SIFT features is shown in Fig. 2.
The SIFT features are shown as circles in Fig. 2(a).
Each circle represents a SIFT feature.
The center and radius of the circle represent the center location and the scale of the SIFT feature.
The existence of a SIFT feature suggests that there is a blob-like image component at the center location of the SIFT feature and the scale of the feature represents the radius of the blob-like component.
The image intensity distribution around the blob-like component is further characterized by an orientation and an appearance matrix.
The orientation, as shown by the line starting from the center of the circle, represents the general direction of change in image intensity.
The appearance matrix represents the detailed change in image intensity.
An example of an appearance matrix is shown in Fig. 2(b).
The square centered at the center location of a SIFT feature is divided into 16 subsquares.
There are 8 lines starting from the center of each subsquare along 8 different directions.
The length of a line represents the number of pixels which have a gradient direction the same as the line, and some of the lines may have a length of zero.
For example, many of the pixels in the lower left corner subsquare, as shown in Fig. 2(b), have a gradient direction pointing to the lower side of the image; therefore the length of the line starting from the center of this subsquare and pointing to the lower side is long.
The center location, scale, direction and appearance matrix of a SIFT feature can be organized as a vector of 133 numbers: the center location includes 3 numbers representing its coordinates in the 3D volume of the brain image; the scale and orientation is represented as one number respectively; the appearance matrix is represented by 128 numbers, 8 numbers for each of the 16 subsquares.
This vector form is used in the computation; while the isomorphic graph representation, as shown in Fig. 2, is used as a user friendly way of representing the SIFT features.
Feature evaluation
The extracted SIFT features were identified as one of the three feature types, namely patient feature, healthy feature and noise feature.
The features were evaluated based on their frequencies of occurrence in patient brains and healthy brains.
There were two steps to evaluate the features, and each SIFT feature was evaluated separately.
The first step was to find all the other features that were similar to the feature that was being analyzed.
The similarity between two features was measured by four criteria: the distance between the center locations Δx(i, j), the scale difference Δσ(i, j), the orientation difference Δo(i, j) and the difference between their appearance matrix Δa(i, j).
They were defined as follows:(1)Δxij=xi-xj2σi(2)Δσij=lnσjσi(3)Δoij=minoi-oj,2π-oi-oj(4)Δaij=ai-aj2where xi was the center location of feature i, σi was the scale of feature i, oi was the orientation angle of feature i and ai was the appearance matrix of feature i.
If all the four differences were less than their corresponding threshold, two features were considered to be similar.
All the features that were similar to feature i constituted the similar feature set for feature i:(5)Si=fj:Δxij<ϵx∧Δσij<ϵσ∧Δoij<ϵo∧Δaij<ϵawhere ϵx, ϵσ, ϵo and ϵa were similarity thresholds for center locations, scales, orientations and appearance matrix, respectively.
According to Toews et al. (2010), the thresholds ϵx and ϵσ were set to 0.5 and 2/3 respectively.
The thresholds ϵo and ϵa were set to π/2 and 0.45 respectively based on a grid search (Chang and Lin, 2011).
Grid search is an efficient way to find the best parameter combinations, when there are multiple parameters in a model and the parameters are continuous variables.
First, we discretized the continuous parameters.
Parameter ϵo was discretized into three discrete values [π/4, 2π/4, 3π/4], and parameter ϵa was discretized into five discrete values [0.3, 0.35, 0.4, 0.45, 0.5].
Then all the combinations of these discrete values, 15 combinations in total, were tried and the parameter combination with the highest classification accuracy was chosen as the best parameter setting.
The second step for feature evaluation was to assign likelihood scores to the SIFT features.
The likelihood score was defined as follows:(6)Li=lnSi∩P/NPSi∩C/NC,Si≥NP+NC0otherwisewhere Si was the similar feature set for SIFT feature i, P was the patient feature set which included all the SIFT features extracted from all patient brains in the training set, C was the healthy feature set including all the SIFT features from all healthy brains in the training set, NP and NC was the number of patient brains and the number of healthy control brains in the training set, respectively.
A SIFT feature was identified as a patient feature if Li was larger than a threshold ϵl; it was a healthy feature if Li was smaller than -ϵl; it was a noise feature otherwise.
Formally, the class labels of the features were determined as follows:(7)Ci=1,Li>ϵl0,Li≤ϵl-1,Li<-ϵlwhere ϵl was the threshold for likelihood scores.
We used grid search to determine the best parameter setting.
For the threshold, the value from 0.1 to 1.2 with a step size of 0.1 was searched.
After the grid search, ϵl was set to be 0.9.
According to the above feature evaluation process, we need to find the similar feature set for every feature (Eq.
(5)), which requires comparing this feature with all other features.
For more than 105 features in 39 brains, it would require 1010 pair-wise distance calculations, which is a very slow process.
Upon those observations, we divided the whole brain volume into small cubes.
For the evaluation of a feature, we only calculated its distance to the other features in the same cube.
In this way, the computation time is significantly reduced, but the classification accuracy may be adversely affected.
For example, a feature close to cube boundaries may have some of its similar features (Eq.
(5)) in adjacent cubes.
Ignoring those similar features in adjacent cubes could lead to an inaccurate likelihood score (Eq.
(6)) for this feature.
This issue is especially serious when the number of training samples is limited as in our project.
On the other hand, a larger cube size would have fewer features close to cube boundaries, and would result in more accurate likelihood scores and hence higher classification accuracy.
According to our previous experience from the application of this algorithm to the classification of several other diseases, such as Parkinson's disease, Alzheimer's disease and bipolar disorder, 20×20×20 was considered to be an appropriate cube size.
This cube size 20×20×20, determined based on adult-sized brains in our previous studies, was used directly for the infant brains in the present study, since our infant brains were normalized using the infant template and the infant template was enlarged to the size very close to that of adult brains (Altaye et al., 2008).
Training SVM classifiers
We trained a linear SVM for every set of 2D slices in every cube to classify the set of SIFT features extracted from this set of 2D slices across subjects into 3 categories.
For a new SIFT feature from a brain image whose class-label is unknown, the corresponding SVM is expected to be able to predict the class label of this new SIFT feature without finding its similar feature set in the huge amount of SIFT features extracted from the brain images used for training.
Predicting new subjects
To predict a new subject to be NH or HI, the subject's sMRI scan was first normalized to the standard space using SPM8 with the infant T1 template (Altaye et al., 2008).
The normalized brain was divided into cubes and sliced along three orientations as described above.
SIFT features were extracted and then classified using the SVM that was trained for the same cube and same slice orientation.
After all the SIFT features were classified, we counted the number of features of the three types.
The total number of noise features was not used in the final decision process.
The new subject was classified according to the following equation:(8)Classlabel=HI,ifCsum>ϵsNH,otherwisewhere Csum=∑iC^i, C^i is the predicted class label of the i-th SIFT feature as shown in Eq.
(7), ϵs is a threshold for the final classification of sMRI and its value is determined based on the method described in section Validation of the classifier.
Feature extraction and model learning based on functional MR images
For fMRI images, we constructed contrast maps using the General Linear Model (GLM) (Worsley et al., 2002) as described in the Data acquisition and preprocessing section.
Contrast values were estimated from the difference in image intensity for each voxel between two conditions.
A positive contrast value indicated that brain activation was higher in the first condition when compared to the second condition, while a negative contrast value suggested a lower activation in the first condition.
We generated region-level features and proposed a novel approach to vectorize the contrast maps utilizing the "bag-of-words" strategy (Sivic and Zisserman, 2009).
Feature generation from contrast maps
Normalized Z-maps were thresholded to select voxels with extreme contrast values for subsequent analysis.
Among the selected voxels, we connected the voxels which were adjacent to each other in a 3D neighborhood, in which each voxel had 26 neighbors if it was not on the border.
As a result, the selected voxels were merged into a set of disjoint regions, each of which was defined as a region of interest (ROI) (Dykstra, 1994; Pokrajac et al., 2005).
To prevent mixing positive voxels and negative voxels in a single ROI, which could negate the signal, we considered these two categories of voxels separately.
Positive voxels were ranked decreasingly whereas negative voxels were ranked increasingly according to their activation magnitudes.
Only the top 5% of each category were selected.
The cutoff of 5% was chosen because it outperformed other cutoffs, 1% and 10%, with respect to the classification performance.
In this way, a number of ROIs were delineated to characterize the pattern of a contrast map.
Due to individual differences and random noise, however, the set of ROIs delineated from different subjects varied significantly.
To address this problem, we delineated a set of ROIs based on each subject, and applied all ROIs derived from all subjects to each single subject to form a long vector for each subject, with each dimension representing the mean contrast value over all voxels within the corresponding ROI.
Finally, we concatenated the vectors from the three contrast maps, and obtained a 1474-dimension vector for each subject.
In other words, each significantly activated/deactivated region was treated as a word, and all words occurring across all subjects constituted the dictionary.
The frequency of each word was measured by the mean contrast value.
An intuitive view of the contrast map vectorization process is shown in Fig. 3.
Since we performed ROI detection on each contrast map and then concatenated all the ROIs together, ROIs that were consistent among subjects were detected more than once.
To merge those similar ROIs into one single feature, we performed a hierarchical clustering with average linkage (Johnson, 1967).
The original space was represented as:(9)S1, 1⋯S1, 1474⋮⋱⋮SN1⋯SN1474where each row represents a training sample and each column represents a ROI, S(i,j) is the mean contrast value of ROI j for subject i, N is the total number of subjects.
The distance between two ROIs was calculated as the Euclidean distance:(10)distROIiROIj=∑k=1NSki-Skj2
We cut the hierarchical tree with the inconsistency coefficient of 0.01, and calculated the mean value of the ROIs that were clustered together as the value of the joint feature.
The cutoff of 0.01 was easily determined since the cluster results did not change in the cutoff range from 0.01 to 0.7.
After hierarchical clustering, the dimensionality was reduced to 969.
Sedation method
Subjects were sedated with three different sedation methods during the MRI scanning.
Different sedation methods were expected to affect the activation pattern differently (DiFrancesco et al., in press).
Therefore, we added sedation method as an additional feature, which was represented as a 3D binary vector(11)100010001.
As shown in the matrix defined in Eq.
(11), each row of the matrix represented one of the three sedation methods.
In this way, we represented each subject as a 972-dimension feature vector, including 969 features from the contrast maps after hierarchical clustering and 3 binary features from sedation method.
Therefore, our dataset was represented as D defined in Eq.
(12):(12)D=x1y1,⋯,xiyi⋯,x39y39|xi∈R972where x(i) and y(i) was the feature vector and group label (NH or HI) for the i-th subject, respectively.
This dataset D was used for subsequent feature selection and model learning.
Feature selection and model learning
The WEKA software package was utilized to select a subset of features that were highly correlated with class labels and uncorrelated with each other (Hall, 1999).
The merit of a subset of features was measured as:(13)MS=krcf¯k+kk-1rff¯where rcf¯ was the mean correlation between class label and selected features, rff¯ was the mean correlation between two features, k was the number of features in subset S. Greedy hill-climbing augmented with a backtracking facility was applied to search through the space of feature subsets (Dechter and Pearl, 1985).
For explanation purposes, we can imagine that there was a rooted tree, which had included all possible feature subsets.
In this tree, each node was a feature subset, which was represented as a 972 dimensional binary vector, with 1(0) indicating that the corresponding feature was (not) selected.
Each node had 972 successors/children, each of which was generated by flipping one of the 972 dimensions of the current node.
Our goal was to step through this tree to find a node with relatively high Ms.
In practice, the whole tree would not be constructed because it was unlimited.
Only the successors were generated whenever needed.
The search started from the root, which was the empty set of features in our project, and repeatedly chose the successor with the highest Ms at each node.
The search terminated when 5 consecutive non-improving steps occurred.
With the selected subset of features, we trained a linear SVM classifier (Chang and Lin, 2011).
Predicting new subjects
Given a new subject, we first normalized the contrast maps to the infant template space (Altaye et al., 2008), so that the given contrast maps were registered with the training contrast maps.
A 972-D feature vector was then constructed with procedures described above, which was subsequently filtered based on the feature selection results obtained from the training set.
Finally, the formatted feature vector was fed to the trained classifier, yielding a decision score (fMRI_score) for the new subject based on the functional MRI data alone.
The rule for classification was formulated as:(14)Classlabel=HI,iffMRI_score≥ϵfNH,otherwise.
Important features
The importance of a feature was measured as follows:(15)If=∑i=1Nσiwifwhere || was the absolute value function, N was the total number of folds of cross-validation as described in the following part, wif was the SVM weight for feature f during i-th fold of cross-validation, σi=1 if the feature f was selected in the i-th fold of cross-validation.
Otherwise, σi=0.
For the ROIs that were merged into a joint feature through the hierarchical clustering, the importance of such an ROI was equal to the importance of the feature, to which this ROI belonged.
Integrated model
To combine the sMRI and fMRI data, we designed a two-layer classification model (Fig. 4).
Given a training set, we trained two classifiers, namely sMRI classifier and fMRI classifier.
Then we applied these two classifiers to the training set.
As a result, we obtained two predicted scores for each training sample.
Thus, the original feature space was transformed into a new two-dimensional feature space through these two classifiers.
Finally, we trained a linear SVM classifier (with parameter C=1) in the new feature space to combine the two scores together.
When predicting new subjects, we first obtained the two predicted scores from the sMRI classifier and fMRI classifier, then fed these two predicted scores into the second layer classifier to yield the final decision score y.
The decision rule was defined as follows:(16)y=fCsum,fMRI_score=w1∗Csum+w2∗fMRI_score+bias(17)Classlabel=HI,ify≥ϵiNH,otherwisewhere w1, w2 and bias were the parameters in the SVM model, which were learnt from the training.
Validation of the classifier
Leave-one-out cross-validation (LOOCV) was employed to validate the three classifiers as follows.
The total number of subjects was denoted as N.
We performed N experiments, each of which was called one fold of cross-validation.
In the n-th (n=1,…,N) fold of cross-validation, the n-th subject was used for testing; while the others were used for training.
Threshold ϵs was determined so that the false positive rate and false negative rate for the training brains were equal, while ϵf and ϵi were set to be 0.
These thresholds were applied to the test images to assign them to be either NH or HI.
The classification accuracy for all the N subjects was reported as accuracy.
Equal error rate (EER) accuracies were also determined based purely on the predicted scores of the testing brain images, e.g.
the threshold ϵs/ϵf/ϵi were chosen so that the false positive rate was equal to false negative rate for the testing brains.
In addition, area under curve (AUC) was also calculated to evaluate the performance of classifiers.
Results
Classifier performance
Performances of the three classifiers are shown in Table 1, and receiver operating curves (ROCs) are plotted in Fig. 5.
While the sMRI classifier and fMRI classifier performed well individually, their combination achieved a significant improvement in performance.
The combined classifier yielded AUC and EER as high as 0.90 and 0.89, respectively.
From the ROC, we can see that the sMRI classifier could not predict some of the positive subjects (HI) correctly even when the decision threshold was set to be very low, because the classifier did not reach 100% true positive rate even when the false positive rate approached 100%.
However, the ROC for fMRI was in an opposite situation.
The ROC did not reach 0% false positive rate even when the true positive rate approached 0%, suggesting that the fMRI classifier had difficulty in classifying some of the negative subjects (NH) correctly.
As sMRI and fMRI classifiers were vulnerable to different types of errors, it was possible to combine them to overcome their individual limitations.
To illustrate the reason why the combination can be successful, we plotted sMRI-fMRI scores in Figs.
6 and S1.
Simply speaking, the fMRI classifier draws a horizontal line to separate the two groups of subjects based on the fMRI data, while the sMRI classifier draws a vertical line to separate the two groups based on the sMRI data.
Obviously, the two groups could not be perfectly separated by either a horizontal or a vertical line in Figs.
6 and S1.
However, by combining the fMRI and sMRI classifiers, the two groups of subjects were separable with a diagonal line as shown in the figures.
Performances of the three classifiers are shown in Table 1, and receiver operating curves (ROCs) are plotted in Fig. 5.
While the sMRI classifier and fMRI classifier performed well individually, their combination achieved a significant improvement in performance.
The combined classifier yielded AUC and EER as high as 0.90 and 0.89, respectively.
From the ROC, we can see that the sMRI classifier could not predict some of the positive subjects (HI) correctly even when the decision threshold was set to be very low, because the classifier did not reach 100% true positive rate even when the false positive rate approached 100%.
However, the ROC for fMRI was in an opposite situation.
The ROC did not reach 0% false positive rate even when the true positive rate approached 0%, suggesting that the fMRI classifier had difficulty in classifying some of the negative subjects (NH) correctly.
As sMRI and fMRI classifiers were vulnerable to different types of errors, it was possible to combine them to overcome their individual limitations.
To illustrate the reason why the combination can be successful, we plotted sMRI-fMRI scores in Fig. 6 and S1.
Simply speaking, the fMRI classifier draws a horizontal line to separate the two groups of subjects based on the fMRI data, while the sMRI classifier draws a vertical line to separate the two groups based on the sMRI data.
Obviously, the two groups could not be perfectly separated by either a horizontal or a vertical line in Fig. 6 and S1.
However, by combining the fMRI and sMRI classifiers, the two groups of subjects were separable with a diagonal line as shown in the figures.
Feature selection in sMRI analysis
In the analysis of sMRI data, image features were selected based on their likelihood scores.
The total number of image features in a brain image ranged from 35,000 to 52,000.
Most of these image features were noise features.
The total number of selected features, i.e., healthy and patient features, ranged from 300 to 1400 for different brains with a likelihood threshold of 0.9.
Different choices of likelihood threshold for the sMRI feature selection resulted in different numbers of selected features and therefore different classification accuracies.
Table 2 shows the relation between classification accuracy and the likelihood threshold.
The classification accuracy did not change for likelihood threshold ranging from 0.7 to 1.1.
The AUC changed within a range of 0.09 with a peak where the likelihood threshold equaled 0.9.
The EER accuracy varied within a range of 0.08.
All three classification performance measures were stable with different likelihood thresholds.
Stability of feature selection in fMRI analysis
We have analyzed the stability of feature selection in the analysis of fMRI data.
There were in total 972 features as the input for feature selection.
Only 6.2% of the features (with a total number of 60) were selected at least once.
For each fold of cross-validation, there were usually about 20 features selected for the training, generally 30% of which were consistently present in all folds of cross-validation.
We calculated a stability index as follows (Kalousis et al., 2007):(18)Simsisj=si∩sjsi∪sj(19)index=2cc-1∑i=1c-1∑j=i+1cSimsisjwhere c was the total number of rounds of feature selection, si and sj were two sets of features selected during two runs, |si∩sj| was the cardinality of the intersection between si and sj, and |si∪sj| was the cardinality of the union of si and sj.
Our feature selection yielded a stability index of 66.2%, which indicated that 66.2% of the selected features, on average, were common between any two runs of feature selection.
Since the Euclidean distance was used in the hierarchical clustering, only very similar ROIs were merged.
There was still considerable redundancy among features.
For example, two ROIs, e.g.
one from the contrast speech vs. silence and the other from the contrast tones vs. silence, were significantly correlated with class labels, and meanwhile they were also highly correlated with each other.
Due to the large Euclidean distance between them, however, they were not merged during the hierarchical clustering.
In feature selection, these two ROIs were treated as different features and selected interchangeably.
This caused the calculated stability index to be lower than the actual value.
In this regard, 66.2% represented very high stability.
Discriminative brain regions
For sMRI, we measured the importance of a SIFT feature with its likelihood score.
In our project, however, the SIFT features usually had a scale of 10mm or even larger, and correspondingly the side length of the appearance matrices was larger than 40mm.
Due to the large size of the SIFT features, it was more difficult and less useful to interpret the medical implications of such large brain regions.
With those considerations, we only focused on the highly predictive brain regions identified by the fMRI classifier.
Fig. 7 shows the top 10 functional features extracted from fMRI data that differentiate the HI and NH groups.
Features are numbered from A to J in order.
ROI A1 and A2 were merged during hierarchical clustering into a joint feature A.
Similar procedures were performed for features C, E, F, I and J.
We can see that ROIs grouped together during hierarchical clustering are always from the same type of contrast maps (Table 3) and encompass adjoining or sometimes overlapping brain regions as designated by Brodmann's Areas in the 4th column of Table 3.
Discussion
In this work, we have built a robust two-layer classifier that can accurately separate HI from NH infants.
We realize that hearing in newborns can be accurately tested using the auditory brainstem response (ABR) evaluations or the otoacoustic emission (OAE) measures, it is thus not our intention to develop a tool for computer-aided diagnosis of hearing loss.
Rather we provide a proof of principle that it is possible to accurately determine the functional, developmental status of the central auditory system in congenitally hearing impaired children based on MR images alone by utilizing machine learning techniques.
Such success has been previously reported in other progressive diseases, such as Alzheimer's disease (Cuingnet et al., 2011).
However, for many progressive diseases, definite diagnosis is often difficult to establish, in which case the LOOCV approach may not be able to estimate the classifier performance accurately.
Therefore, our dataset with solid labels corresponding to diagnostic categories of the participants that have NH or HI enables us to make an objective evaluation of our algorithm, and demonstrate conclusively the feasibility of using machine learning in making automated diagnoses or prognoses based on imaging examinations.
The approach described here may not be limited to a specific disease; essentially, any disease dataset with sMRI and fMRI brain images can be analyzed with our method provided that sufficient training data is available.
A major innovation that makes highly accurate predictions possible in our approach is that we extracted high-level features instead of using each single voxel as a feature as in traditional approaches.
The SIFT features from sMRI images and region-level features from fMRI images are much less sensitive to registration errors when compared to voxel-features.
In addition, utilization of high-level features can considerably reduce the dimensionality of feature space, which not only makes our classification problem easier to handle, but also helps to reduce the problem of over-fitting.
At last, our classification model is more interpretable, because our model involves fewer features consisting of continuous regions instead of scattered voxels.
These features can then be related more easily to disease etiology, diagnosis and prognosis.
Another innovation of our approach is that we employed a bag-of-words strategy to analyze the functional contrast maps.
This technique can characterize the activation pattern for every individual in spite of the great variability in the activation pattern among individuals.
Considering the relatively small sample size, we constructed our feature pool with all available samples, including the one used for testing during the cross-validation.
We implemented a variant version of our algorithm, in which we extracted ROIs based only on the training samples, and subsequently applied those ROIs to the testing sample directly.
As expected, the variant algorithm performed slightly worse (AUC=0.81) than our original algorithm (AUC=0.83).
Adding the ROIs from new samples requires us to retrain the classifier every time when new samples are available.
As the feature pool becomes larger in the future, the retraining is not necessary.
Integration of different types of data, e.g.
data from multiple modalities, has been demonstrated to be more powerful for classification (Fan et al., 2007, 2008; Tosun et al., 2010; Wang et al., 2012).
However, how to implement such integrations in the best way remains to be explored (Orru et al., 2012).
Traditionally, features from different types of data are concatenated and a single classifier is trained (Fan et al., 2007, 2008; Tosun et al., 2010; Wang et al., 2012).
Specifically, the traditional integration method requires the training set to be organized into matrices, with each row representing a training sample and each column representing a feature.
One matrix is constructed for one type of data, and subsequently all the matrices are concatenated into one big matrix, which serves as the input for classifier training.
In our project, the fMRI data can be easily organized in this way.
For sMRI, however, each training sample has a set of SIFT features, which can be treated as a set of words included in an article.
Different articles have different sets of words.
Thus, it is not easy to organize the sMRI data into a matrix as described above, and the traditional integration method is not applicable.
Under such circumstances, we proposed a two-layer model to integrate the sMRI and fMRI data.
Since the traditional approach was not applicable in our project, we did not compare their performances in the present paper.
Additionally, our two-layer model is also applicable when features from different modalities can be concatenated.
In this case, one classifier is trained for one modality, and a second-layer classifier is subsequently used to integrate the multiple classifiers on the first-layer.
This approach is able to combine as many types of data as possible, without worrying about the high dimensionality or overfitting.
One feature that is conspicuously absent from those illustrated in Fig. 7 is the primary auditory cortex (BA41).
We expected that this region would be important in differentiating HI from NH participants and hoped that it could potentially become a biomarker for predicting outcome for hearing and language following cochlear implantation in HI infants as suggested by our earlier work (Patel et al., 2007).
The sedation used in the present study is a likely confounding to primary auditory function and may be partly responsible for the absence of a functional MRI feature in primary auditory cortex that differentiates the groups (DiFrancesco et al., in press).
However, because Fig. 7 highlights differences between the groups that optimally separate them, it is possible that brain regions beyond primary auditory cortex that are responsible for recognizing sounds as speech and for extracting and associating content are more differentially stimulated in a scenario where the hearing impaired brain receives a rare auditory input that is above the threshold it can detect.
Vibrations, loud noise and other stimuli may occasionally stimulate the auditory cortex in a deaf infant so that it is capable of processing sound and responds during our experiment in the same manner as the NH children who are receiving sound stimulation at the same relative SPL.
However, unless the HI infant is participating in a successful hearing aid trial, it is much less likely that they are routinely subjected to an auditory stream of speech that is consistently above their hearing threshold and hence unintelligible.
HI infants in this study were all severe to profoundly hearing-impaired and ultimately received a cochlear implant because they did not derive sufficient benefit from an external hearing aid.
Though this explanation is speculative, it could explain why features B, C, E, F, G, H, and I seem to be more important in separating the HI and NH groups of infants based on brain activation during fMRI.
On the other hand, our analysis on the fMRI data in this study also identified a number of areas that are not necessarily expected to play a role in differentiating HI from NH children.
In particular, several functional features also appear in various portions of the anterior cingulate cortex (ACC, BA 24,32,33): areas associated with attention management, conflict monitoring, and error detection (Weissman et al., 2005).
These features may be related to responses in the HI group to the novel auditory stimulus.
ACC features are present in all three contrasts (C2, E1, E2, and G), suggesting a difference in response to sound input in the HI group who do not typically receive an auditory input at a level above their auditory threshold.
Important features are also present in secondary visual cortex (H) (BA18), associative visual cortex (BA19) and other subcortical regions; differentiating the two groups.
These features provide clues about additional neuroimaging biomarkers that may be relevant to the future use of functional neuroimaging to guide predictions about speech and language outcomes in HI infants who receive a cochlear implant.
This type of prognostic information, currently not available, is obviously of great significance.
For example, it helps to calibrate the expectations and avoid subsequent disappointment, save money of the family and avoid anesthetic risks when it is clear that a child will derive no benefit from the procedure.
In the present study, all infants were sedated for a clinical MRI scan and the fMRI task was appended to the end of the protocol.
Further, there were different agents used for the sedation in the population we sampled, including propofol, Nembutal and sevoflurane.
These drugs may have a different influence on the BOLD signal we detected.
Note that the influence of sedation is to attenuate the auditory and language related brain activity and corresponding BOLD signal relative to what would be detected in awake or even sleeping babies (Difrancesco et al., 2011; DiFrancesco et al., 2013, in press; Wilke et al., 2003).
Therefore, the current approach for automatic classification of NH vs. HI would likely be more effective in a scenario where fMRI data could be recorded from the participants without the influence of sedation.
Demonstrating that our approach can accurately classify infants by hearing status even under the confounding influence of sedation encourages optimism for other applications where confounding disease-related conditions may modify the BOLD signal, such as cerebrovascular diseases.
In the future, we will try image segmentation algorithms to define ROIs instead of thresholding the contrast maps.
Other evidence, such as tissue density maps and functional connectivity networks, may be integrated into our model.
For example, we can train a classifier based on the tissue density maps and then integrate it into our model with the second-layer classifier.
Beyond the MRI data, our model will also permit integration from electrophysiologic imaging modalities such as evoked response potentials (ERP), electroencephalography (EEG), or magnetoencephalography (MEG).
These brain scanning techniques directly record brain activities; however they are limited in their spatial resolution by the algorithms that are used to localize sources of brain activity based on recordings at the surface of the skull.
Combining MR imaging features with electrophysiologic features recorded directly from brain responses to auditory input could leverage the benefits of each imaging modality to produce much more accurate predictions about patient outcomes.
Due to the inherent properties of our two-layer model, integration of other evidences can be easily implemented.
With the improved classifier, the method is likely to have applications to many other diseases.
Conclusion
First, our study demonstrates that HI and NH infants can be differentiated by brain MR images, e.g.
different fMRI contrasts in auditory language network and auditory brain stem nuclei.
Based upon the discriminative features, a classification model can be built to predict whether an individual has normal hearing or impaired hearing.
The discriminative features may also be used as objective biomarkers of hearing loss or used for further disease mechanism studies.
Secondly, our two-layer model integrates sMRI and fMRI in an effective way.
While our sMRI classifier and fMRI classifier work moderately well individually, the combination of the two classifiers gives birth to a much more powerful classifier, which corroborates the hypothesis that integration of multiple modalities improves classification accuracy.
Besides, our integration approach is very flexible, and it can be easily extended to include many diverse types of data.
Future work with this machine learning approach to automated image classification may allow us to make predictions about speech and language outcomes in individual children who receive cochlear implants for remediation of congenital hearing impairment.
The following are the supplementary data related to this articleFig. S1
Distribution of sMRI-fMRI scores for all 39 folds of cross validation.
Each panel is one-fold of cross-validation.
Horizontal axis is the output of the sMRI classifier and vertical axis is the output of the fMRI classifier.
Blue dots are HI training samples, red dots are NH training samples, the black star is the testing sample.
The true label of the testing sample is HI for fold1 to fold18, and NH for fold19 to fold39.
Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.nicl.2013.09.008.
Acknowledgment
LT designed and developed the fMRI classifier.
YC designed and developed the sMRI classifier.
LT and YC developed the integration of the sMRI and fMRI classifiers.
TCM preprocessed the sMRI and fMRI data.
MMC reviewed the anatomical and functional MRI images.
LJL and SKH conceived the project idea and supervised the project.
LT, YC, SKH and LJL are involved in writing and preparing the manuscript.
The project is funded by the CCTST Methodology grant as part of an Institutional Clinical and Translational Science Award (NIH/NCRR 8UL1TR000077-04) and NIH R01-DC07186.



A sulphide lithium super ion conductor is superior to liquid ion conductors for use in rechargeable batteries

The 70P2S5-30Li2S glass was synthesized from reagent-grade chemicals, P2S5 (Aldrich, 99%) and Li2S (Idemitsu, 99%). A mixture of these chemicals was sealed in a carbon-coated quartz tube and heated at 700 degC for 2 h in an electric furnace. The molten sample was rapidly quenched in ice water. The glass-ceramic material for the cold-pressed sample was prepared by heat treatment of the obtained glass after grinding it into a powder. The densified glass-ceramic samples were obtained by compressing the glass powders at 94 MPa and then heating at 280 degC or 300 degC for 2 h.
Synthesis of lanthanide doped CeF3:Gd3+, Sm3+ nanoparticles, exhibiting altered luminescence after hydrothermal post-treatment
The syntheses were carried out to produce 0.75 g of the product. A typical synthesis procedure was performed as follows. NH4F (50% excess) was dissolved in 25 mL of water (solution A). An aqueous solution B was prepared that contained Ln(NO3)3 and CeCl3 which were mixed at the desired molar ratio (0.01 Sm(NO3)3, 0.15 Gd(NO3)3 and 0.84 CeCl3 mol%), and diluted with water to make 100 mL. In the case of the products obtained in the presence of EDTA and citric acid, 0.5 wt.% of the desired polycarboxylic acid was additionally dissolved in solutions A and B. The pH of the systems was adjusted to [?]7, by the use of an aqueous NaOH solution. Solution B was added dropwise to solution A, resulting in the precipitation of the lanthanide doped fluorides (LnF3). The reaction was performed at 343 K, with continuous stirring. The addition was completed in 0.5 h. Afterward the as-prepared products were divided into two parts. Half of the product was purified by centrifugation and washed several times with water. After this the product was dried overnight in the oven (at 358 K). The rest of the colloidal precipitate was transferred into a Teflon vessel and hydrothermally treated for 2 h, at 453 K/40 bar (microwave autoclave - ERTEC, Magnum II, 600 W). When the reaction was finished, the product obtained under hydrothermal conditions was purified by centrifugation and dried in the oven, as well. Fig. 1 presents the scheme of the coprecipitation process and subsequent hydrothermal treatment of the nanoparticles synthesized (recrystallization and growth of the nanocrystals).1. A transition metal composite hydroxide comprising secondary particles formed by aggregates of plate-shaped primary particles,
the transition metal composite hydroxide comprising at least one layer of low-density layer formed from aggregates of fine primary particles having a smaller particle size than the plate-shaped primary particles in a range of 30% with respect to the particle size of the secondary particles from the surface of the secondary particles, and the average ratio of the thickness of the at least one layer of low-density layer with respect to the particle size of the secondary particles being within a range of 3% to 15%.2. The transition metal composite hydroxide according to claim 1, wherein the transition metal composite hydroxide comprises a main portion constructed by the plate-shaped primary particles, a low-density layer formed outside the main portion and constructed by the fine primary particles, and an outer-shell portion formed outside the low-density layer and constructed by the plate-shaped primary particles.3. The transition metal composite hydroxide according to claim 1, wherein the main portion is constructed by the plate-shaped primary particles; the low-density layer is formed outside the main portion and constructed by the fine primary particles; a high density layer is formed outside the first low-density layer and constructed by the plate-shaped primary particles; a second low-density layer is formed outside the high density layer and constructed by the fine primary particles; and the outer-shell portion is formed outside the second low-density layer and constructed by the plate-shaped primary particles.4. The transition metal composite hydroxide according to claim 1, wherein the average ratio of the outer diameter of the main portion with respect to the particle size of the secondary particles is within a range of 65% to 95%, and the average ratio of the thickness of the outer-shell portion or the total thickness of the outer-shell portion and the high density layer with respect to the particle size of the secondary particles is within a range of 2% to 15%.5. The transition metal composite hydroxide according to claim 1, wherein the average particle size of the plate-shaped primary particles is within a range of 0.3 μm to 3 μm, and the average particle size of the fine primary particles is within a range of 0.01 μm to 0.3 μm.6. The transition metal composite hydroxide according to claim 1, wherein the average particle size of the secondary particles is within a range of 1 μm to 15 μm to, and the value of [(d90−d10)/average particle size], which is an index that represents the spread of the particle size distribution of the secondary particles, is 0.65 or less.7. The transition metal composite hydroxide according to claim 1, which has a composition that is represented by a general formula (A): NixMnyCozMt(OH)2+a, where x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, 0≤a≤0.5, and M is one or more additional element selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.8. The transition metal composite hydroxide according to claim 7, wherein the additional element M is uniformly distributed inside the transition metal composite hydroxide, and/or a surface of the transition metal composite hydroxide is coated by a compound that includes the additional element M.9. A method for manufacturing a transition metal composite hydroxide, wherein the method is for manufacturing a transition metal composite hydroxide which is a precursor of the positive electrode active material for a non-aqueous electrolyte secondary battery by mixing a raw material aqueous solution including at least a transition metal element and an aqueous solution including an ammonium ion donor to form a reaction aqueous solution, and performing a crystallization reaction, the method is characterized in comprising:
a nucleation step in which nucleation is performed in a non-oxidizing atmosphere having an oxygen concentration of 5% by volume or less in which the pH value of the reaction aqueous solution at a standard liquid temperature of 25° C. is adjusted to be within a range of 12.0 to 14.0; and
a particle growth step in which at a standard liquid temperature 25° C. the pH value of the reaction aqueous solution including the nuclei obtained in the nucleation step is adjusted to be lower than the pH value of the nucleation step and to be within a range of 10.5 to 12.0 so as to grow the nuclei; wherein
an atmosphere control is performed such that the non-oxidizing atmosphere is maintained in the early period and the middle period of the particle growth step which is in a range of 70% to 90% of time from the initiation of the particle growth step with respect to the entire period of the particle growth step, and in the latter period of the particle growth step, the non-oxidizing atmosphere is switched to the oxidizing atmosphere where the oxygen concentration exceeds 5% by volume, and then the oxidizing atmosphere is switched to the non-oxidizing atmosphere again.10. The method for manufacturing a transition metal composite hydroxide according to claim 9, wherein in the latter period of the particle growth step, after 0.5% to 20% of time with respect to the entire particle growth step passed from the point of switching the non-oxidizing atmosphere to the oxidizing atmosphere, the oxidizing atmosphere is switched to the non-oxidizing atmosphere again, and maintain the non-oxidizing atmosphere from the point of switching the atmosphere again to the termination of the particle growth step, that is for a range of 3% to 20% of time with respect to the entire particle growth step.11. The method for manufacturing a transition metal composite hydroxide according to claim 9, wherein the transition metal composite hydroxide has a composition that is represented by a general formula (A): NixMnyCozMt(OH)2+a, where x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, 0≤a≤0.5, and M is one or more additional element selected from Mg, Ca, Al, Ti, V Cr, Zr, Nb, Mo, Hf, Ta, and W.12. The method for manufacturing a transition metal composite hydroxide according to claim 11, wherein after the particle growth step, a coating step may be further provided for coating the surface of the transition metal composite hydroxide with a compound that includes the additional element M.13. A positive electrode active material for a non-aqueous electrolyte secondary battery which includes secondary particles formed by aggregates of a plurality of primary particles and having a tap density of 1.5 g/cm3or more and a surface roughness index which is a value in which the measured specific surface area of the secondary particles is divided by the geometric surface area of the secondary particles when the secondary particles are assumed to be true sphere is within a range of 3.6 to 10.14. The positive electrode active material for a non-aqueous electrolyte secondary battery according to claim 13, wherein the average particle size of the secondary particles is within a range of 1 μm to 15 μm, and the value of [(d90−d10)/average particle size], which is an index indicating the spread of the particle size distribution of the secondary particles, is 0.7 or less.15. The positive electrode active material for a non-aqueous electrolyte secondary battery according to claim 13, which is constructed by a hexagonal lithium nickel manganese composite oxide represented by a general formula (B): Li1+uNixMnyCozMtO2, where −0.05≤u≤0.50, x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, and M is one or more additional element selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.16. A method for manufacturing a positive electrode active material for a non-aqueous electrolyte secondary battery comprising:
a mixing step to form a lithium mixture by mixing heat-treated particles obtained by heat-treating the transition metal composite hydroxide or the transition metal composite hydroxide according to claim 1 and a lithium compound; and
a firing step in which the lithium mixture is fired at a temperature range of 650° C. to 1000° C. to obtain a positive electrode active material for a non-aqueous electrolyte secondary battery that is constructed by a lithium transition metal-containing composite oxide.17. The method for manufacturing a positive electrode active material for a non-aqueous electrolyte secondary battery according to claim 16, wherein in the mixing step, the mixing amount of the lithium compound is preferably adjusted so that the ratio of the number of atoms of Li that is included in the lithium mixture with respect to the sum of the number of atoms of metal elements other than Li is within a range of 0.95 to 1.5.18. The method for manufacturing a positive electrode active material for a non-aqueous electrolyte secondary battery according to claim 16, which comprises a heat-treating step to heat-treat the transition metal composite hydroxide at a temperature range of 105° C. to 750° C. before the mixing step.19. The method for manufacturing a positive electrode active material for a non-aqueous electrolyte secondary battery according to claim 16, wherein the lithium transition metal-containing composite oxide has a composition represented by a general formula (B): Li1+uNixMnyCozMtO2, where −0.05≤u≤0.50, x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, and M is one or more additional element selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and w).20. A non-aqueous electrolyte secondary battery comprising a positive electrode, a negative electrode, a separator, and a non-aqueous electrolyte, and the positive electrode active material for a non-aqueous electrolyte secondary battery according to claim 13 is used as the positive electrode material of the positive electrode.1. A lithium transition metal composite particle comprising:
a lithium transition metal oxide particle;
a metal-doped layer formed by doping the lithium transition metal oxide particle;
LiF formed on the lithium transition metal oxide particle including the metal-doped layer, and
wherein the metal-doped layer comprises particles of Chemical Formula 1:        Li<sub>a</sub>M<sub>1-b</sub>Me<sub>b</sub>O<sub>2</sub>     <Chemical Formula 1>
where M=Ni<sub>x</sub>Mn<sub>y</sub>Co<sub>z</sub>, (0.3≦x≦0.9, 0<y≦0.6, and 0<z≦0.6),
Me is any one selected from the group consisting of Al, Zn, Zr, Ti, W, Sr, B, Mg, Y, Mo, Nb, Si, and Sn, or a mixed element of two or more thereof, 0.9≦a≦1.3, and
0<b≦0.02.2. The lithium transition metal composite particle of claim 1, wherein the metal-doped layer is included in an amount of 0.01 wt % to 3 wt % based on 100 wt % of the lithium transition metal composite particle.3. The lithium transition metal composite particle of claim 1, wherein the metal has a concentration gradient in which a concentration gradually decreases from a surface of the lithium transition metal oxide particle to inside thereof.4. The lithium transition metal composite particle of claim 1, wherein the metal-doped layer further comprises an oxide including the metal.5. The lithium transition metal composite particle of claim 1, wherein a doping amount of the metal is greater than 0 mol % and equal to or less than 2 mol %.6. The lithium transition metal composite particle of claim 1, wherein the LiF is modified by reacting a fluoride-based polymer with at least a portion of lithium impurities included in the lithium transition metal composite particle.7. The lithium transition metal composite particle of claim 1, wherein the LiF is included in an amount of 0.1 wt % to 0.5 wt % based on a total weight of the lithium transition metal composite particles.8. The lithium transition metal composite particle of claim 6, wherein the lithium impurities comprise LiOH, Li2CO3, or a mixture thereof.9. The lithium transition metal composite particle of claim 6, wherein the lithium impurities are included in an amount of less than 0.3 wt % based on the total weight of the lithium transition metal composite particles.10. The lithium transition metal composite particle of claim 6, wherein the fluoride-based polymer comprises polyvinylidene fluoride (PVdF), a polyvinylidene fluoride-hexafluoropropylene copolymer (PVdF-co-HFP), or a mixture thereof.11. A method of preparing lithium transition metal composite particles according to claim 1 the method comprising:
mixing a mixed transition metal precursor, a lithium compound, and a metal oxide and sintering to obtain lithium transition metal oxide particles including a metal-doped layer; and
mixing the lithium transition metal oxide particles including a metal-doped layer and a surface modifier, and performing a heat treatment to obtain LiF formed on the lithium transition metal oxide particle including the metal-doped layer, and
wherein the metal-doped layer comprises particles of Chemical Formula 1:        Li<sub>a</sub>M<sub>1-b</sub>Me<sub>b</sub>O<sub>2</sub>     <Chemical Formula 1>
where M=Ni<sub>x</sub>Mn<sub>y</sub>Co<sub>z</sub>, (0.3≦x≦0.9, 0<y≦0.6, and 0<z≦0.6),
Me is any one selected from the group consisting of Al, Zn, Zr, Ti, W. Sr, B, Mg, Y, Mo, Nb, Si, and Sn, or a mixed element of two or more thereof,
0.9≦a≦1.3, and
0<b≦0.02.12. A method of preparing lithium transition metal composite particles according to claim 1 the method comprising:
mixing a mixed transition metal precursor doped with a metal and a lithium compound and sintering to obtain lithium transition metal oxide particles including a metal-doped layer; and
mixing the lithium transition metal oxide particles including a metal-doped layer and a surface modifier, and performing a heat treatment to obtain LiF formed on the lithium transition metal oxide particle including the metal-doped layer, and
wherein the metal-doped layer comprises particles of Chemical Formula 1:        Li<sub>a</sub>M<sub>1-b</sub>Me<sub>b</sub>O<sub>2</sub>     <Chemical Formula 1>
where M=Ni<sub>x</sub>Mn<sub>y</sub>Co<sub>z</sub>, (0.3≦x≦0.9, 0<y≦0.6, and 0<z≦0.6),
Me is any one selected from the group consisting of Al, Zn, Zr, Ti, W, Sr, B, Mg, Y.
Mo, Nb, Si, and Sn, or a mixed element of two or more thereof, 0.9≦a≦1.3, and
0<b≦0.02.13. The method of claim 11, wherein the sintering is performed in a temperature range of 800° C. to 1,000° C.14. The method of claim 11, wherein the heat treatment is performed in a temperature range of 300° C. to 500° C.15. The method of claim 11, wherein the mixed transition metal precursor is a compound having a composition of MOOH or M(OH)2(where M=NixMnyCoz, 0,3≦x≦0.9, 0≤y≦0.6, 0≤z≦0.6, and x+y+z=1).16. The method of claim 11, wherein the metal oxide comprises any one selected from the group consisting of Al2O3, ZnO, ZrO2, TiO2, WO3, SrO2, B2O3, MgO, Y2O3, MoO3, Nb2O3, Nb2O6, SiO2, and SnO, or a mixture of two or more thereof.17. The method of claim 11, wherein the metal oxide is used in an amount of 0.1 wt % to 1 wt % based on a total weight of the lithium transition metal composite particles.18. The method of claim 11, wherein the surface modifier comprises polyvinylidene fluoride (PVdF), a polyvinylidene fluoride-hexafluoropropylene copolymer (PVdF-co-HFP), or a mixture thereof.19. The method of claim 11, wherein the surface modifier is used in an amount of 0.2 wt % to 0.5 wt % based on the total weight of the lithium transition metal oxide particles.20. A cathode active material comprising the lithium transition metal composite particles of claim 1.21. A cathode comprising the cathode active material of claim 20.22. A lithium secondary battery comprising:
a cathode;
an anode; and
a separator disposed between the cathode and the anode,
wherein the cathode is the cathode of claim 21.Mutations in mitochondrial ribosomal protein MRPL12 leads to growth retardation, neurological deterioration and mitochondrial translation deficiency

Abstract
Multiple respiratory chain deficiencies represent a common cause of mitochondrial diseases and are associated with a wide range of clinical symptoms.
We report a subject, born to consanguineous parents, with growth retardation and neurological deterioration.
Multiple respiratory chain deficiency was found in muscle and fibroblasts of the subject as well as abnormal assembly of complexes I and IV.
A microsatellite genotyping of the family members detected only one region of homozygosity on chromosome 17q24.2-q25.3 in which we focused our attention to genes involved in mitochondrial translation.
We sequenced MRPL12, encoding the mitochondrial ribosomal protein L12 and identified a c.542C>T transition in exon 5 changing a highly conserved alanine into a valine (p.Ala181Val).
This mutation resulted in a decreased steady-state level of MRPL12 protein, with altered integration into the large ribosomal subunit.
Moreover, an overall mitochondrial translation defect was observed in the subject's fibroblasts with a significant reduction of synthesis of COXI, COXII and COXIII subunits.
Modeling of MRPL12 shows Ala181 positioned in a helix potentially involved in an interface of interaction suggesting that the p.Ala181Val change might be predicted to alter interactions with the elongation factors.
These results contrast with the eubacterial orthologues of human MRPL12, where L7/L12 proteins do not appear to have a selective effect on translation.
Therefore, analysis of the mutated version found in the subject presented here suggests that the mammalian protein does not function in an entirely analogous manner to the eubacterial L7/L12 equivalent.
Highlights
•
MRPL12 function is not entirely analogous to the eubacterial L7/L12 equivalent.
•
Mutations in MRPL12 cause translation defects.
•
Mutations in apparently universal translation factors can affect different OXPHOS complexes.

Introduction
The mitochondrial machinery responsible for oxidative phosphorylation (OXPHOS) comprises five enzyme complexes containing approximately 80 proteins of which only 13 are encoded by the mitochondrial genome (mtDNA) [1].
OXPHOS deficiencies affecting a single or multiple complexes can result from mutations in either mitochondrial or nuclear genes and are associated with a variety of disease mechanisms [2,3].
With the advent of Next Generation Sequencing there is an increasing number of pathogenic mutations being identified that are not solely restricted to the 80 genes encoding OXPHOS components, thus highlighting the importance of mechanisms impacting on mitochondrial gene expression [4,5].
Combined OXPHOS deficiencies can arise from alterations in mtDNA, its maintenance [6], cardiolipin levels [7,8], or where none of these are affected, from direct defects in synthesis of mitochondrially encoded proteins [9].
This last group constitutes a heterogeneous mix of patients suffering from a wide range of clinical symptoms making clinical diagnosis difficult [10].
Genetic diagnosis is yet more elusive in children with mitochondrial disease where unidentified nuclear mutations account for the majority of cases [11].
This diagnostic problem is compounded by our relatively poor understanding of the complex molecular machinery that drives translation in mitochondria.
This machinery comprises over a hundred proteins [12], all of which are putative candidate genes for translation deficiencies in human.
Indeed, translation deficiencies represent a growing cause of multiple OXPHOS deficiencies with several published pathogenic mutations in genes related to the intra-organellar protein synthesis.
Although many mutations associated with impaired mitochondrial translation currently map to tRNA genes [13] and a few ribosomal RNA (rRNA) [14], the list of nuclear gene mutations is steadily growing as mutations in genes encoding mitochondrial translation factors such as GFM1 (OMIM: 606639) [15,16], TSFM (OMIM: 604723) [17] and TUFM (OMIM: 602389) [18]; mitochondrial aminoacyl-tRNA synthetases (RARS2 (OMIM: 611524) [19], DARS2 (OMIM: 610956) [20], YARS2 (OMIM: 610957) [21], SARS2 (OMIM: 612804) [22], HARS2 (OMIM: 600783) [23], AARS2 (OMIM: 612035) [24], MARS2 (OMIM: 609728) [25], EARS2 (OMIM: 612799) [26]), FARS2 (OMIM: 611592) [27]; tRNA-modifying enzymes (PUS1 (OMIM: 608109) [28], TRMU (OMIM: 610230) [29], MTO1 (OMIM: 614667) [30]); other factors (C12orf65 (OMIM: 613541) [31], TACO1 (OMIM: 612958) [32], LRPPRC (OMIM: 607544) [33], C12orf62 (OMIM: 614478) [34]) and mitochondrial ribosomal proteins (MRPS16 (OMIM: 609204) [35], MRPS22 (OMIM: 605810) [36], MRPL3 (OMIM: 607118) [5]) have been successively reported (reviewed in Ref.
[14]).
Relatively few cases of OXPHOS deficiencies associated with mutations in mitochondrial ribosomal proteins (MRPs) have been described so far.
MRPS16 mutations have been described in only one family with agenesis of corpus callosum and dysmorphism.
MRPS22 mutations lead to cardiomyopathy, hypotonia and tubulopathy in a first family and Cornelia de Lange-like dysmorphic features, brain abnormalities and hypertrophic cardiomyopathy in another family.
Finally, we recently identified MRPL3 mutations in four siblings of the same family presenting cardiomyopathy and psychomotor retardation.
Since the mammalian mitoribosome (55S) is ~2megadalton machine consisting of approximately 80 components that make up the 28S small (SSU) and 39S large subunit (LSU), it is likely that more pathogenic mutations in the constituent polypeptides will be uncovered.
One of the substantial differences between the mammalian mitoribosome and those of eubacteria (70S) or the eukaryotic cytosol (80S) is the reversal in the protein to rRNA ratio.
The 70S and 80S particles contain ~70% rRNA, whilst human mitoribosomes contain ~70% protein.
This change in the ratio represents both an acquisition of new MRPs as well as loss of bacterial orthologues [37,38].
MRPL12 does have a bacterial orthologue, which through its interactions with translation factors is important in protein synthesis regulating both speed and accuracy [39-41].
Here we investigate the genetic basis of disease in a subject born to consanguineous parents, who initially presented with growth retardation and then neurological distress, with evidence of compromised mitochondrial protein synthesis.
We have identified the causative mutation to be in MRPL12, encoding a protein of the large subunit of mitochondrial ribosome.
This is an important finding indicating that the function and consequence of dysfunction cannot automatically be extrapolated from an apparent orthologue.
We show that proteins involved in mitochondrial translation, even close orthologues as submitted here, can defy predictions.
Moreover, mutations in such genes that should affect all mitochondrially encoded gene products can exert respiratory chain complex specific defects.
Materials and methods
Analysis of oxidative phosphorylation activities
Spectrophotometric assays of respiratory chain and complex V enzymes were carried out as previously described [42].
Mitochondrial suspension from cultured skin fibroblasts was obtained after suspending 50μl of frozen cells in 1ml of mitochondria extraction medium (20mM Tris-HCl (pH 7.2), 250mM sucrose, 2mM EGTA, 40mM KCl, 1mg/ml BSA) supplemented with 0.01% (w/v) digitonin and 10% Percoll (v/v).
After 10min at 4°C, the sample was pelleted, washed in extraction medium and pelleted before resuspension in 30μl of extraction medium for respiratory chain enzyme measurements.
Microsatellite genotyping and mutation screening
A genome-wide search for homozygosity was undertaken with 382 pairs of fluorescent oligonucleotides from the Genescan Linkage Mapping Set, version II (Perkin-Elmer) under conditions recommended by the manufacturer.
Amplified fragments were electrophoresed and analyzed with an automatic sequencer (ABI 377).
The polymorphic markers had an average spacing of 10cM throughout the genome.
Genes encoding mitochondrial proteins were selected in Mitocarta [43].
The exons and exon-intron boundaries of the MRPL12 gene were amplified using specific primers (sequences available on request) with initial denaturation at 96°C - 5min, followed by 30 cycles of 96°C - 30s, 55°C - 30s, 72°C - 30s, and a last extension at 72°C for 10min.
Amplification products were purified by ExoSapIT (Amersham, Buckinghamshire, UK) and directly sequenced using the PRISM Ready Reaction Sequencing Kit (Perkin-Elmer, Oak Brook, IL) on an automatic sequencer (ABI 3130xl; PE Applied Biosystems, Foster City, CA).
Cell culture
Human skin fibroblasts were cultured in DMEM medium (Dulbecco's modified Eagle's medium, Gibco) supplemented with 10% (v/v) fetal calf serum (FCS), 2mM l-glutamine, 50μg/ml uridine, 110μg/ml pyruvate, 10,000U/ml penicillin G and 10,000μg/ml streptomycin.
Protein analysis
For blue native-polyacrylamide gel electrophoresis (BN-PAGE), mitochondria and OXPHOS complexes were isolated as described [44].
Solubilized OXPHOS proteins (20μg) were loaded on a 4-16% (w/v) polyacrylamide non-denaturing gradient gel (Invitrogen).
SDS-PAGE analysis was performed on either solubilized mitochondrial proteins (40μg) or cell lysate (50μg) extracted from cultured skin fibroblasts.
After electrophoresis, gels were transferred to a PVDF membrane (GE-Healthcare) and processed for immunoblotting.
Metabolic labelling of mitochondrial translation products
In vitro labeling of mitochondrial translation products was a modification from Chomyn et al. [45].
Essentially, cultured skin fibroblasts were preincubated in methionine/cysteine-free DMEM (2×10min) followed by a 10min in the presence of emetine (100μg/ml).
Radiolabel (125μCi/ml EasyTag™ express35S protein labelling mix - NEG772002MC, PerkinElmer) was then added for 1h at 37°C and chased for 1h.
Cells were harvested in cold 1mM EDTA/PBS, washed 3 times in cold PBS and the pellet resuspended in 30μl PBS containing 1× EDTA free protease inhibitors (Roche) and 1mM PMSF.
Samples were treated with 2× dissociation buffer (20% (v/v) glycerol, 4% (w/v) SDS, 250mM Tris-HCl pH 6.8, 100mM DTT) and 12U Benzonase nuclease (Novagen) for 1h and separated on a 15% (w/v) SDS-PAGE.
The gel was fixed overnight (3% (v/v) glycerol, 10% (v/v) acetic acid, 30% (v/v) methanol) and vacuum dried (60°C, 2h).
Radiolabelled proteins were visualized by PhosphorImage and analyzed with Image-Quant software (Molecular Dynamics, GE Healthcare).
Homology modeling of the human MRPL12 protein
The three dimensional structure of the human MRPL12 (residues 64 to 198) was modeled by comparative protein modeling and energy minimization, using the Swiss-Model program (http://swissmodel.expasy.org/) in the automated mode.
The 2Å coordinate set for the ribosomal protein L12 from Thermotoga maritima (PDB code: 1dd3) was used as a template for modeling the human MRPL12 protein.
Swiss-Pdb Viewer 3.7 (http://www.expasy.org/spdbv) was used to analyze the structural insight into MRPL12 mutation and visualize the structures.
Cell lysates, Westerns and isokinetic sucrose gradients
Cell lysates were prepared from fibroblasts by addition of cold lysis buffer (50mM Tris-HCl pH 7.5, 130mM NaCl, 2mM MgCl2, 1mM PMSF, 1% (v/v) NP-40) to cell pellets, which were vortexed for 30s, centrifuged at 600g for 2min (4°C) to remove nuclei and the supernatant retained.
These were used for standard westerns as described [46].
Mitochondria were prepared as above and lyzed in 50mM Tris-HCl pH 7.4, 150mM NaCl, 1mM EDTA, 1% Triton X-100, protease inhibitor mix (EDTA free, Roche), 1mM PMSF, 10mM MgCl2.
Mitochondrial lysates (0.3mg) were loaded on a isokinetic sucrose gradient (1ml 10-30% (v/v)) in 50mM Tris-HCl (pH 7.2), 10mM Mg(OAc)2, 40mM NH4Cl, 0.1M KCl, 1mM PMSF, 50μg/ml chloramphenicol), and centrifuged for 2h 15min at 100,000g at 4°C.
Fractions (100μl) were collected and 10μl aliquots were analyzed directly by western blotting [47].
Immunodetection was performed using the following primary antibodies: anti-CI-Grim19, CII-SDHA 70kDa, CIII-core2, CIV-COXI, CIV-COXII, CV-subunit β, cyt c, NDUFB8 (mouse monoclonal antibodies, MitoSciences); anti-MRPL3 goat polyclonal, MRPS18B rabbit polyclonal, MRPS25 rabbit polyclonal and ICT1 rabbit polyclonal antibodies (Protein Tech Group, Inc., Chicago); anti-DAP3 mouse monoclonal, POLRMT rabbit polyclonal (Abcam); and anti-Porin, mouse monoclonal antibodies (Invitrogen).
Anti-MRPL12 rabbit polyclonal antibody was custom made (Eurogentec).
Secondary antibody detection was performed using peroxidase-conjugated anti-rabbit, anti-goat or anti-mouse IgG (Abcam or Dako).
The signal was generated using ECL+ (Pierce, Rockford, USA) and visualised by phosphorimaging and analyzed by ImageQuant software.
RNA and Northern blotting
RNA was isolated from fibroblasts using Trizol following manufacturer's protocol (Invitrogen).
Northern blots were performed as described [48].
Briefly, aliquots of RNA (5μg) were electrophoresed through 1.2% (w/v) agarose under denaturing conditions and transferred to GenescreenPlus membrane (NEN duPont) following the manufacturer's protocol.
Radiolabelled probes were generated using random hexamers on PCR-generated templates corresponding to internal regions of the relevant genes.
Results and discussion
Clinical report
The subject, a boy, was born to first cousin Roma/Gypsy parents by cesarean section, at 41weeks of pregnancy with severe general hypotrophy (birth weight 2250g, height 48cm, Occipitofrontal Circumference 34cm).
His clinical examination at birth was normal and initial investigations failed to identify the cause of his severe hypotrophy (normal blood caryotype, heart ultrasound, bone age, CT scan and metabolic workup).
He failed to thrive thereafter and was repeatedly admitted in the first 12months for gradual worsening of his condition (weight and height: -4SD, OFC: -2SD).
Clinical examination at 10months showed severe denutrition, muscle weakness but detectable deep tendon reflexes and no major trunk hypotonia.
He started walking with aid at 12months.
Basal growth hormone (GH) levels in plasma were normal (0.5ng/ml) but plasma IGF1 was very low (0.07U/ml, normal 0.2).
Stimulation by ornithine triggered an adequate elevation of plasma GH with correction of plasma IGF1 (GH: 46.9ng/ml, IGF1: 0.2U/ml, and 0.45U/ml following GH administration).
These results ruled out a dwarfism of endocrine origin.
Yet, high plasma lactate (3.5 to 4.4mmol/l, normal below 2.2) prompted skeletal muscle, liver and skin biopsies that revealed a multiple respiratory chain enzyme deficiency in muscle and liver homogenate and cultured skin fibroblasts (Table 1) [42].
He had a first episode of generalized tonic seizure aged 2years and his neurological condition rapidly worsened following an episode of acute fever (40°C) caused by a respiratory infection.
At that age, he had severe denutrition, flat weight curve, no weight and height gain (-5 SD) and a mildly enlarged liver.
Neurological examination revealed overt psychomotor retardation, severe trunk hypotonia, inability to sit and stand unaided and no speech.
Intermittent horizontal nystagmus, with cerebellar ataxia and tremor were noted.
He had a mild facial dysmorphism, with round face, epicanthic folds, arched palate, short neck, low-set ears and a unique bilateral median palmar crease.
Brain MRI showed T1 hyposignal and T2 hypersignal of white matter and basal ganglia.
Rapid aggravation of respiratory conditions required endotracheal intubation and assisted ventilation.
Plasma lactates rose to 3.3-4.2mmol/l, he gradually developed abnormal limb movements then fell into a deep coma, and died following a cardiac arrest.
During the next pregnancy, the mother expected dizygotic twin fetuses.
A prenatal diagnosis based on assessment of respiratory chain enzyme activities in cultured skin fibroblasts on amniotic fluid was offered.
Hemoglobin contamination of the samples rendered the very low respiratory chain activities; despite this the activity ratios clearly showed a severe complex IV deficiency in cells from the fetus twins leading to termination of the second pregnancy.
The same test was normal in the third pregnancy and a normal baby girl was born, now 14years old (Table 1).
Blue native-PAGE analysis
Whilst skin fibroblasts were propagated for enzyme analysis it was clear that those from the subject harboring the mutation had a reduced doubling time on glucose medium compared to control lines (data available on request).
Cell extracts were subsequently prepared and SDS-PAGE/western blot analysis revealed reduced steady-state level of COXII subunit, consistent with the decreased CIV activity (Fig. 1A).
Moreover, we also observed a decreased amount of nuclear encoded protein NDUFB8 (Fig. 1A), which joins late in complex I assembly [44] and is consistent with the low CI activity in the biopsies.
These decreases were consistent with the BN-PAGE data from fibroblast mitoplasts (Fig. 1B, method as described [49]).
Both complexes I and IV, detected by anti-GRIM19 antibody or mitochondrially-encoded COXI, were severely decreased in the subject, which in contrast exhibited control levels of complex II (70kDa SDHA subunit) and complex III (Core 2 subunit) (Fig. 1B).
No partial complexes were observed.
Identifying the causative mutation by Microsatellite genotyping and mutation screening
In order to identify the causative mutation a genome-wide search for homozygosity was undertaken using 382 polymorphic microsatellite markers (Genescan Linkage Mapping Set, version II (Perkin-Elmer).
We obtained evidence for homozygosity in all affected individuals at loci D17S785, D17S784, and D17S928 on chromosome 17q24.2-q25.3 (Fig. 2A).
No other homozygous region was found with other markers.
This 32.7cM region was then refined by additional microsatellite markers reducing the critical region to the 25cM interval, defined by loci D17S1352 and D17S928.
This corresponds to a 8.4Mb physical region containing more than 170 genes, 10 of which encode mitochondrial proteins or proteins predicted to be mitochondrially targeted (FDXR (MIM 103270), ICT1 (MIM 603000), ATP5H (MIM 607196), MRPS7 (MIM 611974), SLC25A19 (MIM 606521), MRPL38 (MIM 611844), PGS1 (NM_024419), MRPL12 (MIM 602375), SLC25A10 (MIM 606794), FASN (MIM 600212)).
Considering the multiple RC deficiency observed in patient muscle, liver and fibroblasts and the abnormal BN-PAGE pattern reminiscent of translation deficiencies, we focused on genes involved in mitochondrial translation, excluding mutations in MRPS7, MRPL38 and ICT1 by direct sequencing.
Sequencing of MRPL12 exons and exon-intron boundaries on genomic DNA (primer sequences available on request) from the affected child identified a homozygous c.542C>T transition in exon 5 (RefSeq accession number NM_002949.3, Fig. 2B).
This mutation changed a highly conserved alanine into a valine (p.Ala181Val, Fig. 2C) and was predicted by Polyphen2 (http://genetics.bwh.harvard.edu/pph2/) and SIFT (http://sift.jcvi.org/) software to be "probably damaging" and "deleterious" respectively.
The parents were heterozygous for the mutation, both twin fetuses were homozygous and the healthy girl was wild-type homozygous.
This mutation was absent from 100 controls of the same ethnic origin and from all SNP databases.
Further, no additional MRPL12 mutation could be identified in two other unrelated subjects with similar clinical presentation and biochemical defect.
To demonstrate the deleterious nature of the p.Ala181Val MRPL12 substitution we used, overexpression of wild-type or mutant human MRPL12 cDNA in the SV40-immortalized fibroblasts but rather recapitulating the respiratory phenotype of the patient fibroblasts, this was found to be lethal.
Cells stopped growing, became polynucleated and progressively died.
In silico analysis of the putative impact of A181V substitution
Ribosome assembly
The steady state level of MRPL12 in the subject's fibroblasts was reduced to 30% of control value (Fig. 4A and B).
The mt-LSU protein ICT1 was also decreased (~30% of control values) as was MRPL3 (by 37%) suggesting that a consequence of the MRPL12 mutation is a global defect in assembly of the large ribosomal subunit (mt-LSU).
In order to estimate the effect of the MRPL12 mutation on assembly of the whole ribosome, we also tested three proteins of the small ribosomal subunit (SSU), MRPS18B, MRPS25 and DAP3.
These were modestly decreased with levels of ~60-80% of control (Fig. 4A and B).
Correspondingly, 16S and 12S rRNA levels were decreased by 35% and 22% respectively (Fig. 7A).
Since porin indicated that there was no compensatory mitochondrial biogenesis and staining of the mitochondrial network with tetramethylrhodamine methyl ester showed no significant alteration in amount or distribution of mitochondria (AR and ZCL unpublished observation) we conclude that the MRPL12 mutation destabilizes the protein resulting in less mt-LSU and to a lesser extent of the small subunit.
In order to determine whether the MRPL12 mutation also induced changes in composition and assembly of the mitochondrial ribosomal large and small subunits, mitochondrial lysates from cultured fibroblasts (subject and control) were fractionated on isokinetic sucrose gradients (10-30%, as in Ref.
[47]).
If assembly of either the large subunit or the entire ribosome was affected then the distribution of individual ribosomal proteins would change within the gradient profile.
On analysis MRPL12 from the patient was substantially decreased in all fractions but detectable in the fractions consistent with mt-LSU; however it was noticeably absent from the free pool (fractions 1 and 2, Fig. 5).
This was in contrast to the control that exhibited a pool of free MRPL12, which has been reported to interact with POLRMT [56].
MRPL3 was also slightly reduced in subject cells but remained in fractions consistent with the large subunit.
The MRPL12 mutation impacted more modestly on the small ribosomal subunit, with DAP3 apparently unaffected and MRPS18B found in lower amounts only in fractions 4 and 5 but otherwise with similar steady state levels and distribution profile compared to control.
Since POLRMT and MRPL12 have been published as interactors, we analyzed both the steady state level and gradient distribution of POLRMT to see if these were affected by the MRPL12 mutation.
Overall levels in the subject sample were decreased to 63% of control value (Fig. 5B) but distribution in the gradient appeared largely unaffected with the exception of fraction 11, where levels were lower than control (Fig. 5A bottom panels).
In vitro translation
To identify any effect on global mitochondrial protein synthesis, we studied de novo mitochondrial translation in cultured skin fibroblasts, as described in Ref.
[45].
Although there was an overall decrease in mitochondrial translation compared to control, densitometric profiles showed that certain polypeptides were more affected than others (Fig. 6).
In particular, there was a significant reduction of synthesis of COXI, COXII and COXIII subunits.
Consistent with the respiratory chain activities, complex I polypeptides were affected to a lesser extent and cytochrome b from complex III appears to be spared.
Despite the potential role of MRPL12 in translational accuracy, no aberrant translation product could be detected.
Steady-state level of mitochondrial transcripts
MRPL12 has been shown to interact with the mitochondrial RNA polymerase (POLRMT) and to stimulate mitochondrial transcription [56,57].
Since the steady-state level of POLRMT in subject fibroblasts was modestly decreased, we analyzed the steady-state level of mitochondrial transcripts to see if these were similarly affected.
Northern blots on control and subject fibroblast RNA did not show a decrease that paralleled the reduced levels of POLRMT.
In fact there was a slight increase of MTND1 in subject cells compared to control, whilst MTCOI and MTCYB transcripts appeared unaffected (Fig. 7A).
Conversely, as mentioned earlier, the levels of 16S and 12S rRNA were modestly decreased (Fig. 7A) in a proportion that was consistent with the loss of MRPs with which they would associate.
Analysis of the distribution on 10-30% (w/v) sucrose gradients (as in Ref.
[58]) of two mt-mRNAs, MTCOI and MTND1, showed a relatively similar pattern for subject and control but in each case a smaller proportion of the subject transcript sedimented to the final fraction (data available on request).
This was also true for the 16S and 12S rRNA.
As no major redistribution in the sucrose gradient was observed, it is unlikely that the MRPL12 mutation causes a global defect in assembly of the mitoribosome.
Since the levels of POLRMT were slightly decreased with normal or slightly elevated levels of mt-mRNA, we assessed the stability of transcripts to see if the half-lives were extended to compensate for reduced synthesis in order to maintain normal steady state levels.
Mitochondrial transcription was poisoned by addition of low levels of ethidium bromide and RNA prepared at numerous time points thereafter (0-16h).
Densitometry of the subsequent Northern demonstrated that half-lives of MTCOI and MTND1 were extended, the latter more so, consistent with the modest increase in steady state levels (data available on request).
MRPL12 dimerization and interaction with the mitoribosome
MRPL12 is the orthologue of eubacterial L7/L12, where L7 is identical to L12 except that it is N-terminal acetylated.
L7/L12 is also phosphorylated, which can affect both conformation and binding to partner ribosomal proteins (reviewed in Ref.
[37]).
This modification has now been confirmed to be present in mammalian MRPL12 [59].
In eubacteria, association of L7/12 to the large subunit takes place via the L10 protein such that two L7/L12 heterodimers normally associate per LSU [60].
Interestingly these dimers are actively exchanged on the 70S molecule without disruption of the ribosomal particle [54,61].
The dimerization status and number of dimers attached to the human 55S has not been clarified.
In order to identify if the stoichiometry of MRPL12 per mt-LSU was altered as a consequence of the mutation, we performed immunoprecipitation (IP) analysis on subject and control fibroblasts using antibodies to MRPL12.
Analysis of the immunoprecipitate demonstrated similar levels small subunit polypeptides including DAP3 and MRPS18B in subject and control samples.
In contrast, the total amount of MRPL12 was reduced (Fig. 7B).
In the patient the IP is restricted to MRPL12 in the large subunit or the fully assembled 55S with the total amount of MRPL12 being reduced as it lacks the "free" population.
The densitometric measurements indicate that the patient IP has ~49% MRPL12 compared to control, in accordance with the gradient and steady state data.
ICT1 appears to be sensitive to the MRPL12 levels and so is reduced in both the IP (~58% of control) and in the steady state westerns (Fig. 4).
The lower levels of MRPL12 could reflect loss of multimerization but since the region in the bacterial protein involved in multimerization is towards the N-terminus [62], this mutation is unlikely to have an impact on dimer/multimer formation.
The translation factor bound dimer has been suggested to have an increased affinity for the ribosome [54,61].
Thus a possible explanation is that the mutation affects translation factor binding, thereby reducing the affinity of the mutant MRPL12 for the ribosome.
If this were the case, however, then we would expect an increase in the pool of free MRPL12 whereas the subject exhibits a reduced pool of free MRPL12, which interacts with POLRMT [57].
The immunoprecipitation was performed using an MRPL12 specific antibody and so should contain all free MRPL12, MRPL12 associated with uncomplexed mt-LSU and MRPL12 as part of the fully assembled 55S.
Since the levels of small and large subunit proteins appeared to be similar in subject and control, these data suggest that the mt-LSU and 55S assembly are unaffected by the mutation consistent with the gradient data for the protein and RNA components.
Thus the reduced levels of mutant MRPL12 in this subject correspond to i) loss of stability, ii) a decrease in the free pool that is believed to interact with the mitochondrial RNA polymerase and iii) reduced translation potentially resulting from decreased interactions with translation factors, but with no detectable increase in aberrant translation products.
In conclusion, we report a mutation in human MRPL12 that results in growth retardation and neurological distress.
It is interesting to note that whereas the eubacterial orthologue is not essential for in vitro translation assays, this MRPL12 mutation induces a mitochondrial translation defect in human.
This lack of predictability between orthologous proteins makes it important to examine and not assume what impact mutant forms or loss of MRPs may have on mitochondrial homeostasis and the resulting clinical manifestation.
The data presented here provide another example, amongst a growing list of translation factors, which despite their apparent universal contribution to the synthesis of all mitochondrially encoded proteins, has a selective effect on the different oxidative phosphorylation complexes.
Acknowledgments
This research was supported in part by the Association Française contre les Myopathies (A.F.M.) and the French Agence Nationale pour la Recherche (A.N.R.).
Z.M.A.C.-L. would like to thank The Wellcome Trust [096919/Z/11/Z], and the BBSRC [BB/F011520/1] for continuing support.
We thank Alexander Tzagoloff for studies on ΔMRPL12 yeast strain.

Facile synthesis of a MoO2-Mo2C-C composite and its application as favorable anode material for lithium-ion batteries

All chemical regents employed in this work were of analytical grade without further purification. A pretreated macroporous cinnamic anion-exchange resin (10 g) was first added into 100 mL of an aqueous solution containing 6.18 g ammonium molybdate tetrahydrate (AHM) and stirred in an ambient atmosphere at room temperature for 10 h. After the adsorption, the resin was filtered, washed several times with distilled water, and dried in air at 60 degC for 24 h. The resulting product was carbonized at 750 degC for 3 h under an argon atmosphere at a heating rate of 5 degC min-1. Finally, the black-colored product was further ground into tiny particles by ball milling and collected for further characterization. For comparison, pure carbon was prepared using the same method to calcine the bare resin without the addition of AHM. Mo2C was bought from Hunan Guangyuan Company and used without any further treatment.
The escape of heavy atoms from the ionosphere of HD209458b.
II.
Interpretation of the observations

Highlights
► We compare UV transits of HD209458b with empirical and hydrodynamic models.
► We constrain the mean temperature, densities and escape rates of different species.
► The detection of atomic oxygen implies a minimum mass loss rate of 6×106kgs-1.
► The results constrain the temperature, chemistry, and ionization of the atmosphere.
► Detection of Si2+ indicates that clouds of forsterite and enstatite do not form.
Abstract
Transits in the H I 1216Å (Lyman α), O I 1334Å, C II 1335Å, and Si III 1206.5Å lines constrain the properties of the upper atmosphere of HD209458b.
In addition to probing the temperature and density profiles in the thermosphere, they have implications for the properties of the lower atmosphere.
Fits to the observations with a simple empirical model and a direct comparison with a more complex hydrodynamic model constrain the mean temperature and ionization state of the atmosphere, and imply that the optical depth of the extended thermosphere of the planet in the atomic resonance lines is significant.
In particular, it is sufficient to explain the observed transit depths in the H I 1216Å line.
The detection of O at high altitudes implies that the minimum mass loss rate from the planet is approximately 6×106kgs-1.
The mass loss rate based on our hydrodynamic model is higher than this and implies that diffusive separation is prevented for neutral species with a mass lower than about 130amu by the escape of H.
Heavy ions are transported to the upper atmosphere by Coulomb collisions with H+ and their presence does not provide as strong constraints on the mass loss rate as the detection of heavy neutral atoms.
Models of the upper atmosphere with solar composition and heating based on the average solar X-ray and EUV flux agree broadly with the observations but tend to underestimate the transit depths in the O I, C II, and Si III lines.
This suggests that the temperature and/or elemental abundances in the thermosphere may be higher than expected from such models.
Observations of the escaping atmosphere can potentially be used to constrain the strength of the planetary magnetic field.
We find that a magnetic moment of m≲0.04mJ, where mJ is the jovian magnetic moment, allows the ions to escape globally rather than only along open field lines.
The detection of Si2+ in the thermosphere indicates that clouds of forsterite and enstatite do not form in the lower atmosphere.
This has implications for the temperature and dynamics of the atmosphere that also affect the interpretation of transit and secondary eclipse observations in the visible and infrared wavelengths.

Introduction
The detection of H, O, C+, and Si2+ in the upper atmosphere of HD209458b (Vidal-Madjar et al., 2003, 2004; Linsky et al., 2010), and the tentative detection of H in the upper atmosphere of HD189733b (Lecavelier des Etangs et al., 2010) and Mg+ in the upper atmosphere of WASP-12b (Fossati et al., 2010) are among the most exciting recent discoveries related to the atmospheres of extrasolar giant planets (EGPs).
The observations demonstrate that the upper atmospheres of close-in EGPs such as HD209458b differ significantly from the thermospheres of the giant planets in the Solar System.
They are much hotter, they extend to several planetary radii and instead of molecular hydrogen, they are primarily composed of atoms and atomic ions.
The detection of heavy atoms and ions such as O, C+, Si2+, and Mg+ implies that the atmospheres of close-in EGPs are not always affected by diffusive separation.
A likely explanation is that diffusive separation of the heavy atoms and ions is prevented by momentum transfer collisions with the rapidly escaping light atoms and ions.
Mass fractionation during hydrodynamic escape is believed to have played an important role in the early evolution of the atmospheres of the terrestrial planets (e.g., Zahnle and Kasting, 1986; Hunten et al., 1987) but it cannot be observed in action anywhere in the Solar System.
Observations of EGP atmospheres thus provide a unique opportunity to study this phenomenon that should lead to a better understanding of evolutionary processes in our own Solar System.
Extended thermospheres give rise to large transit depths in UV transmission spectra.
However, they are also potentially detectable in optical and infrared spectra.
For instance, Coustenis et al. (1997, 1998) searched for an exosphere around 51 Peg b in the near-IR.
In line with the current understanding, they suggested that the exospheres of close-in EGPs such as 51 Peg b are hot and composed primarily of atoms and ions.
They also argued that hydrodynamic escape can lead to the escape of heavier species, and give rise to large in-transit absorption by such species in optical and near-IR spectra.
However, 51 Peg b is not a transiting planet and the search for an exosphere around it was not successful.
On the other hand, once the transit of HD209458b was first detected (Charbonneau et al., 2000; Henry et al., 2000), similar searches on this planet were also undertaken.
Moutou et al. (2001) looked for absorption by species such as Na,H,He,CH+,CO+,N2+, and H2O+ in the upper atmosphere of HD209458b.
These observations were followed by Moutou et al. (2003) who attempted to measure the transit depth in the He 1083nm line that was predicted to be significant by Seager and Sasselov (2000).
The most recent searches were reported by Winn et al. (2004) and Narita et al. (2005) who looked for transits in the Na D, Li, Hα, Hβ, Hγ, Fe, and Ca absorption lines.
So far none of the ground-based searches have led to a detection of the upper atmosphere.
However, the non-detection is based on only a few observations that have proven difficult to analyze, and the search should continue.
Visible and infrared observations have been able to probe the atmosphere of HD209458b at lower altitudes.
In fact, HD209458b was the first EGP to have its atmosphere detected by transmission spectroscopy.
The first detection was achieved by Charbonneau et al. (2002) who observed a deeper in-transit absorption in the Na D 589.3nm resonance doublet compared to the adjacent wavelength bands.
This detection was based on four transits observed with the Space Telescope Imaging Spectrograph (STIS) onboard the Hubble Space Telescope (HST) (Brown et al., 2001).
The same data were later reanalyzed by Sing et al. (2008a,b) who combined them with other observations (Knutson et al., 2007) and created a transmission spectrum of HD209458b at wavelengths of 300-800nm.
They argued that the abundance of sodium in the atmosphere is depleted above the 3mbar level either by condensation into Na2S clouds or ionization.
We note that the detection of Si2+ in the thermosphere (Linsky et al., 2010) constrains cloud formation mechanisms in the upper atmosphere and implies that the depletion of Na at low pressures is probably due to ionization (see Section 3.3).
The atmosphere of HD209458b has also been observed several times in the infrared with the Spitzer space telescope.
Deming et al. (2005) detected the secondary eclipse of the planet at 24μm by using the Multiband Imaging Photometer (MIPS).
Together with similar observations of TrES-1 at 4.5 and 8.0μm obtained by Charbonneau et al. (2005) who used the Infrared Array Camera (IRAC), these observations constitute the first detections of infrared emission from extrasolar planets.
They were followed by Richardson et al. (2007) who observed the infrared emission spectrum of HD209458b between 7.5 and 13.2μm with the Infrared Spectrograph (IRS).
This spectrum was reanalyzed by Swain et al. (2008) who noted that it is largely featureless with some evidence for an unidentified spectral feature between 7.5 and 8.5μm.
Knutson et al. (2008) used IRAC to observe the secondary eclipse of HD209458b in the 3.6, 4.5, 5.8, and 8.0μm bands.
They observed a higher than expected flux in the 4.5 and 5.8μm bands and interpreted this as evidence for the presence of a stratospheric temperature inversion that gives rise to strong water emission at these wavelengths.
Beaulieu et al. (2010) observed the transit of the planet in the same wavelength bands and also found evidence for the presence of water vapor in the atmosphere.
The detection of water vapor is also supported by Swain et al. (2009) who used the Near Infrared Camera and Multi-Object Spectrometer (NICMOS) on HST to observe the secondary eclipse of HD209458b between 1.5 and 2.5μm.
In addition to water vapor, the NICMOS observations revealed the presence of methane and carbon dioxide in the emission spectrum.
These detections provide valuable clues to the overall composition of the atmosphere but the uncertainties in the data and degeneracies between temperature and abundances in the forward model prevent a more quantitative characterization of the density and temperature profiles.
In general, difficulties associated with reducing the data and the need to describe a few uncertain data points with models of growing complexity has led to disagreements on the analysis and interpretation of transmission and secondary eclipse data on different exoplanets.
The same is true of the FUV transit observations of HD209458b.
Vidal-Madjar et al. (2003) used the STIS G140M medium resolution grating to detect a 15±4% transit depth in the wings of the stellar H Lyman α emission line.1
The core of the H Lyman α line is entirely absorbed by the interstellar medium (ISM).
1 Based on the data, they argued that the planet is followed by a cometary tail of escaping hydrogen atoms that are accelerated to velocities in excess of 100kms-1 by stellar radiation pressure (see also Schneider et al., 1998).
Later, Vidal-Madjar et al. (2004) used the STIS G140L low resolution grating to detect absorption by H, O and C+ in the upper atmosphere, and argued that the atmosphere of HD209458b escapes hydrodynamically.
Linsky et al. (2010) used the Cosmic Origins Spectrograph (COS) on HST to confirm the detection of C+ and reported on the detection of Si2+ around the planet.
They also argued that the atmosphere escapes hydrodynamically.
Ben-Jaffel (2007, 2008) disagreed with the interpretation of the G140M observations.
He reanalyzed the G140M data and argued that the 15% H Lyman α transit depth was exaggerated because the data were partly corrupted by short-term variability of the host star and geocoronal Lyman α emissions.
He also showed that there is no evidence for a cometary tail in the data, and that absorption by H in the extended thermosphere of the planet can explain the observations.
Ben-Jaffel and Hosseini (2010) reanalyzed the G140L data and reached a similar conclusion regarding H (see also Koskinen et al., 2010).
However, Ben-Jaffel and Hosseini (2010) fitted the H Lyman α observations by scaling the density profiles from the model of Garcia Munoz (2007), and argued that suprathermal O and C+ are required to fit the transit depths in the O I and C II lines.
They did not explain how the suprathermal atoms form and simply chose their properties to fit the observations.
Holström et al. (2008) offered yet another explanation for the H Lyman α observations.
They argued that hydrogen atoms cannot be sufficiently accelerated by stellar radiation pressure before they are ionized by stellar X-rays and EUV (XUV) radiation.
Instead, they suggested that the observed absorption arises from a cloud of energetic neutral atoms (ENAs) that form by charge exchange between the protons in the stellar wind and the escaping hydrogen.
In their model, the observed absorption reflects the velocity of the stellar wind, and the data can potentially be used to characterize the magnetosphere of the planet.
Ekenbäck et al. (2010) recently updated the model to include a more realistic description of the escaping atmosphere and stellar wind properties.
However, both studies ignored absorption by H in the thermosphere, which is significant, and did not attempt to explain the presence of heavier atoms and ions such as O, C+, and Si2+ in the escaping atmosphere.
All of the interpretations of the UV transit data require that HD209458b has a hot and extended thermosphere.
In fact, Koskinen et al. (2010) (hereafter K10) also showed that absorption by thermal H and O in such a thermosphere explains both the H Lyman α and O I 1304Å transit depths.
Further, their model related the observations to a few physically motivated characteristics such as the mean temperature and composition of the upper atmosphere.
They also used the observations to constrain the pressure level where H2 dissociates and estimated that the H2/H transition should occur at 0.1-1μbar.
However, these results are based on an empirical model that was simply designed to fit the observations.
One of the aims of the current paper is to show that the results are also supported by more complex physical models.
We have also attempted to establish a more comprehensive description of the thermosphere that treats it as an integral part of the whole atmosphere rather than a separate entity.
In order to do so, we developed a new one-dimensional escape model for the upper atmosphere of HD209458b that includes the photochemistry of heavy atoms and ions, and a more realistic description of heating efficiencies.
This model is described in detail by Koskinen et al. (2012) (hereafter, Paper I).
We also used results from a state-of-the-art photochemical model (Lavvas et al., in preparation) to constrain the density profiles of the observed species in the lower atmosphere.
We discuss the implications of our results in the context of different observations, and show that observations of the upper atmosphere also yield valuable constraints on the properties of the lower atmosphere.
Methods
Stellar emission lines
The interpretation of the FUV transit measurements relies on accurate characterization of the stellar emission lines and, provided that the ISM is optically thick over parts of the line profile, their absorption by the ISM.
The observed transit depths also depend on stellar variability.
In this section we discuss the properties of the line profiles, stellar variability, and absorption by the ISM.
The properties of the H Lyman α and the O I 1304Å triplet lines (hereafter, the O I lines) have been discussed in detail before (e.g., Ben-Jaffel and Hosseini, 2010; Koskinen et al., 2010), and we present only a brief summary of them here.
However, the COS observations of the C II 1335Å multiplet (hereafter, the C II lines), and the Si III 1206.5Å line (hereafter, the Si III line) (Linsky et al., 2010) have not been modeled before, and thus we constructed emission line models for these lines based on the new data.
These, and the previous models for the H Lyman α and O I lines, were used to calculate the predicted transit depths in Section 3.
The observed line-integrated transit depths for HD209458b are listed in Table 2.
H Lyman α and O I 1304Å lines
Wood et al. (2005) measured the H Lyman α emission line of HD209458 by using the high resolution echelle E140M grating on STIS, and used the details of the line profile to constrain the column density of H in the ISM along the line of sight (LOS) to HD209458.
Following K10, we adopted the reconstructed line profile and ISM fit parameters from Wood et al. (2005) for the transit depth calculations in this paper.
The short-term variability of the H Lyman α emissions from HD209458 was estimated by Ben-Jaffel (2007) who used the G140M data to derive a magnitude of ∼8.6±5.6% for this variability - although the large uncertainty of the observations makes it difficult to separate variability from noise.
There are no observations to constrain the long-term variability or center-to-limb variations of the Lyman α line on HD209458.
However, typical solar characteristics provide some guidance on these properties.
Woods et al. (2000) studied the variability of solar Lyman α emissions based on satellite observations spanning four and a half solar cycles between 1947 and 1999.
They found that the variability ranges between 1% and 37% during one period of solar rotation (27days), and the average variability during one solar rotation was found to be 9±6%.
This result agrees well with the estimated variability of the Lyman α emissions from HD209458.
The rotation period of HD209458 is estimated to be ∼10-11days (Silva-Valio, 2008).
The G140M observations covered three different transits and took place within a month and a half.
Each observation covered approximately 2h in time.
Thus the data can be affected by short-term variability and it is essential that such variability be properly accounted for.
For this reason, we compare our models with the results of Ben-Jaffel (2007, 2008) and Ben-Jaffel and Hosseini (2010) who analyzed the data in the time tag mode and accounted for variability before calculating transit depths.
Short-term variability in the H Lyman α line is mostly related to plage activity that is modulated by solar rotation while long-term variability depends on variations in both plage and active network regions (Woods et al., 2000).
In fact, it is misleading to imagine the transits in any of the FUV emission lines as the planet crossing a smooth, uniformly emitting disk.
Rather, one has to imagine the planet crossing a relatively dark disk with scattered bright regions.
For instance, during solar maximum the plages2
We include enhanced network in the definition of a plage here.
2 cover approximately 23% of the solar disk while active network covers about 8.5%.
The brightness contrasts of the plages and active network are 6.7 and 3.2, respectively, when compared to the quiet Sun (Worden et al., 1998; Woods et al., 2000).
This means that the transit depth in the H Lyman α can vary by factors of 0.4-3 as a function of time during maximum activity.
Similar variability can be expected in the other FUV emission lines.
The transit depth depends mostly on the path of the planet across the stellar disk although the plage and active network coverage can also change slightly with stellar rotation during the transit.
This highlights the importance of light curve analysis to work out transit depths - limited 'snapshots' during transit may be corrupted by the planet either covering or not covering an active region.
However, it is also possible for the transit depth to be altered even if the variations are not immediately evident in the light curves.
This complicates the analysis of the observations, and underlines the need for repeated observations at different times.
We note that the plage and active network coverage is significantly smaller during solar minimum than it is during solar maximum.
This means that the transit depth is likely to be more stable and closer to the true transit depth during stellar minimum.
Unfortunately, the activity cycle of HD209458 has not been studied in detail, and thus we have no information about it.
As we noted above, the properties of the O I lines were discussed extensively by K10 who fitted parameterized solar line profiles (Gladstone, 1992) to the O I lines of HD209458 that were observed with the STIS E140M grating (Vidal-Madjar et al., 2004).
We adopted the O I line profile and ISM parameters from K10 for this study, and do not discuss these properties further here.
We note that the plage and active network contrasts of the O I lines are 4.2 and 1.7, respectively (Woods et al., 2000).
These values are slightly smaller than the corresponding values for H Lyman α.
We note that limb darkening or brightening can also affect the observed transit depths (K10, Schlawin et al., 2010).
However, center-to-limb variations are not particularly significant in the solar H Lyman α and O I lines (Curdt et al., 2008; Roussel-Dupre, 1985).
C II 1335Å and Si III 1206.5Å lines
Linsky et al. (2010) used the medium resolution (R∼17,500) G130M grating of the COS instrument to observe four transits of HD209458b between September 19 and October 18, 2009 at wavelengths of 1140-1450Å.
During each HST visit, they observed the star during transit, at first quadrature, secondary eclipse and second quadrature.
In order to obtain the out-of-transit reference spectra, they co-added the secondary eclipse and quadrature observations from all four visits.
They also co-added the in-transit spectra from all visits to create a single spectrum.
The detections of the transits in the C II 1335Å and Si III 1206.5Å lines were compared with observations of other lines such as Si IV 1395Å in which the transit was not detected.
We note that the transit in the Si III line was not detected earlier by Vidal-Madjar et al. (2004).
However, these authors use a wider wavelength interval to calculate the line-integrated transit depth and yet report a 2σ upper limit of 5.9%, implying that the 3σ upper limit includes the transit depth observed by Linsky et al. (2010).
As we have seen, stellar variability can also cause changes in the perceived transit depth.
Therefore we are not convinced of the reality of a non-detection in the earlier STIS G140L observations.
In order to calculate transit depths in the C II and Si III lines, we created models for the stellar emission line profiles.
We note that Ben-Jaffel and Hosseini (2010) also modeled the C II lines in order to fit the transit depth in the low resolution G140L data (Vidal-Madjar et al., 2004) where the main components of the C II multiplet are unresolved.
Their model line profiles were also constrained by the high resolution STIS E140M observations of Vidal-Madjar et al. (2003) that resolve the components, although this data has low S/N and it was not used for transit observations.
Here we use the higher S/N COS observations that also resolve the main components of the C II lines to constrain the line profile models.
The atomic line parameters for the C II and Si III lines are listed in Table 1.
The C II multiplet consists of three separate emission lines.
The two lines at 1335.66Å and 1335.71Å (hereafter, the C II 1335.7Å line) are unresolved in the COS (and most solar) observations.
The core of the C II 1334.5Å line is strongly absorbed by the ISM whereas the C II 1335.7Å line is not similarly affected.
The ground state 2P1/2,3/2 of C+ is split into two fine structure levels.
Interstellar absorption of the C II 1335.7Å emissions depends on the population of the 2P3/2 level in the ISM.
In line with a similar assumption regarding the excited states of O (K10), we assumed that this population is negligible.
To estimate the emission line profile from HD209458, we fitted the solar C II 1335.7Å line from the SUMER spectral atlas (Curdt et al., 2001) with a Voigt function and adjusted the result to agree with observations (Linsky et al., 2010).
Fig. 1 shows that the model line profile agrees reasonably well with the observed line profile.
The fit parameters for this and other relevant emission lines are listed in Table 1.
Strong absorption by the ISM makes fitting the C II 1334.5Å line more complicated than fitting the C II 1335.7Å line.
Following Ben-Jaffel and Hosseini (2010), we estimated that the column density of ground state C+ in the ISM is 2.23×1019m-2 by scaling the column density of C+ measured along the LOS to Capella (Wood and Linsky, 1997) to the distance of HD209458.
We fitted the solar C II 1334.5Å line from the SUMER spectral atlas with a Voigt profile, and used the estimated column density and the results of Wood et al. (2005) to calculate absorption by the ISM.
We then varied the total flux within the line profile until the results agreed with the observations of Linsky et al. (2010).
As a result, we obtained a pre-ISM flux ratio of [C II 1334.5Å]/[C II 1335.7Å]∼0.7, which agrees well with the solar value (Curdt et al., 2001).
The model and observed line profiles are again shown in Fig. 1.
We note that the ISM is optically thick at wavelengths between 1334.54Å and 1335.58Å, which correspond to Doppler shifts of 2.2 and 11.2kms-1, respectively.
However, the observed flux in this region is not zero because of spectral line broadening in the COS instrument.
The C II lines are formed in the upper chromosphere and lower transition region of the solar atmosphere.
Similarly with H Lyman α, the brightest emissions are associated with plage activity (e.g., Athay and Dere, 1989).
The plage and active network contrasts for the C II lines are 5.9 and 1.5, respectively (Woods et al., 2000).
The large spatial variability makes it difficult to characterize center-to-limb variations.
However, different solar observations point to approximately 40% limb brightening in the 1335.7Å line and probably a similar variation in the 1334.5Å line, with the intensity rising steadily at μ>0.6 (Lites et al., 1978; Judge et al., 2003).
This brightening effect is due to the broadening of the emission line in the limb.
The Si III line arises from a transition between the 1S ground state and the 1P excited state.
Again, we fitted the Si III line profile from the SUMER spectral atlas (Curdt et al., 2001) with a Voigt profile and adjusted the resulting line profile to agree with observations of HD209458 (Linsky et al., 2010).
Fig. 2 shows the observed and model line profiles, and the fit parameters are listed in Table 1.
We assumed that the abundance of Si2+ in the ISM is negligible.
This is supported by a lack of detectable absorption by the ISM in the observed line profile.
We note that absorption by the ISM affects the interpretation of the measured transit depths only if parts of the line profile are entirely absorbed or if the properties of the ISM change between observations.
The solar Si III line has not been studied to the same degree as the C II lines.
However, some constraints on the variability and center-to-limb variations of the Si III line have been obtained.
For instance, Nicolas et al. (1977) used SKYLAB observations to study the center-to-limb variations of the Si III emissions from the quiet chromosphere.
They found that the line is strongly limb-brightened.
The total line intensity increased by a factor of 2.4 between the disk center and μ=0.73, and then again by a factor of 3 towards the edge of the limb.
This means that the total intensity in the limb is a factor of ∼7 higher than at the disk center, and the line profile is also significantly broader at μ>0.73 with a self-reversal that does not appear at the disk center.
The Si III emissions from the Sun also exhibit strong spatial and temporal variability (e.g., Nicolas et al., 1982).
Limb brightening makes the transit depth appear smaller when the planet is covering the stellar disk while a steeper transit is seen during ingress and egress (e.g., Schlawin et al., 2010).
On the other hand, if the planet covers active regions on the disk, the transit depth can appear significantly deeper (K10).
Spatial variability and limb brightening of the emission lines on HD209458 can potentially be studied through a careful analysis of the transit light curves.
Ideally, the observations should be analyzed in the time tag mode (e.g., Ben-Jaffel, 2007) to identify variations.
This type of reanalysis of the COS data is beyond the scope of this paper.
Instead, we use idealized transit depths based on a uniformly emitting stellar disk in Section 3 to show that the optical depth of the extended thermosphere in the FUV lines is significant and that the transit in the O I, C II, and Si III lines is comparable to the transit in the H I line.
We consider this sufficient for the present purposes.
Empirical model
K10 developed an empirical model to fit the UV transit observations of HD209458b and other extrasolar planets.
They argued that the H I transits can be explained in terms of three simple parameters that describe the distribution of H in the thermosphere.
These parameters are the pressure p0 where H2 dissociates (the bottom of the H layer), the mean temperature T¯ within the H layer and an upper cutoff level r∞ based on the ionization of H.
Transits in other emission lines can be explained by fitting the abundance of the heavier absorbers with respect to H.
The details of this model are discussed by K10 and are not repeated here.
Basically it calculates the transit depth observed at Earth orbit by assuming that the planet and its atmosphere constitute a spherically symmetric obstacle with a density profile in hydrostatic equilibrium up to the sonic point (when a sonic point exists).
Absorption by the ISM and spectral line broadening within the observing instrument (STIS or COS) are taken into account.
In Section 3, we compare the transit depths calculated by the empirical model with results based on the hydrodynamic model (see below) to show how the empirical model can be used to fit the data and guide the development of more complex models of the upper atmosphere.
Hydrodynamic model
We developed a one-dimensional, non-hydrostatic escape model for HD209458b to constrain the mean temperature and ionization in the upper atmosphere (Paper I).
Results from this model demonstrate that the empirical model is physically meaningful.
It solves the vertical equations of motion for an escaping atmosphere containing H, H+, He, He+, C, C+, O, O+, N, N+, Si, Si+, Si2+, and electrons.
The model includes photoionization, thermal ionization, and charge exchange between atoms and ions.
It calculates the temperature profile based on the average solar X-ray and EUV (XUV) flux and up to date estimates of the photoelectron heating efficiencies (Cecchi-Pestellini et al., 2009, Paper I).
The lower boundary of the model is at 1μbar and thus the model does not include molecular chemistry.
This is justified because molecules are dissociated by photochemical reaction networks near the 1μbar level (e.g., Garcia Munoz, 2007; Moses et al., 2011)).
The upper boundary of the model is typically at 16Rp.
We placed the upper boundary at a relatively high altitude above the region of interest in this study, which is below 5Rp.
However, we do not consider the results to be necessarily accurate above 5Rp (see Paper I for further details).
Results
Transit depths
In this section we constrain the temperature and composition of the upper atmosphere of HD209458b through a combined analysis of transit observations in the FUV emission lines.
We also compare results from a hydrodynamic model (see Section 2.3 and Paper I) with the empirical model of K10, and confirm that the latter can be used to constrain the basic properties of the density profiles in the thermosphere.
Neutral atoms
Here we show from simple arguments that the M7 model agrees in principle with the observed H Lyman α transit depth.
A similar procedure can also be used to obtain crude estimates of the transit depth for other systems, provided that the properties of the line profile and the ISM are known.
Later in this section we compare the results of the empirical model with results from the hydrodynamic model, and show that they are consistent.
To start with, the line-integrated H Lyman α transit depth of 6.6±2.3% (Ben-Jaffel and Hosseini, 2010) is consistent with a 6% transit depth at 1215.2Å (a Doppler shift of -120kms-1 from the line core) i.e., the blue peak of the observed line profile (Fig. 5 of K10).
Assuming that the extended atmosphere is spherically symmetric, a 6% transit depth measured in the blue wing (bw) of the line profile implies an optical depth of τbw≈1 at the impact parameter of 2.1Rp from the center of the planet.
Fig. 3 shows the absorption cross section of H in the Lyman α line at a temperature of 8250K.
The cross section at 1215.2Å is σbw=2×10-23m2 and thus a LOS column density of NH=5×1022m-2 at 2.1Rp is required to explain the observed absorption.
Ekenbäck et al. (2010) and Lammer et al. (2011) argued that the optical depth of H in the thermosphere of HD209458b is not significant and instead a large cloud of energetic neutral atoms (ENAs) is required to explain the H Lyman α observations.
In particular, Lammer et al. (2011) claimed that a column density of NH≈1031m-2 in the thermosphere is required for strongly visible absorption.
It is easy to see that this estimate is not correct because the wings of the line profile become optically thick with column densities much smaller than this.
It also disagrees with Garcia Munoz (2007) and Ben-Jaffel (2007, 2008) who were the first to suggest that H in the thermosphere may be sufficiently abundant to explain the observations.
This basic result has also been confirmed by more recent calculations by Trammell et al. (2011).
All of these calculations show that the optical depth of the thermosphere below 3Rp is not negligible.
The empirical model of K10 is simplified by the use of the hydrostatic approximation.
This is well justified even if the atmosphere is escaping.
In general, the density profile of the escaping gas in the thermosphere can be estimated from (Parker, 1964):(1)n(ξ)c2(ξ)=n0c02exp-∫1ξduu2W2c2exp-∫1ξduc2vdvduwhere ξ=r/r0, c2(ξ)=kT(ξ)/m, W=G Mp/r0, m is the molecular weight, and v is the vertical flow speed.
For convenience, we retained Parker's original notation.
The first integral on the right hand side of Eq.
(1) applies in hydrostatic equilibrium.
The second integral is negligible below the sonic point for transonic escape and always negligible for subsonic escape (i.e., evaporation).
The sonic point on HD209458b is always above 3Rp (Paper I) and thus the density profile of H is approximately hydrostatic at least up to 2.1Rp.
Assuming a hydrostatic atmosphere, the LOS column density of H at 2.1Rp can be estimated from:(2)NH(r=2.1Rp)≈n0expGMpmkT¯1r-1r02πrH(r)where T¯ and H are the mean temperature and scale height, respectively.
We note that the mean thermal escape parameter is:(3)X¯(r)=GMpmkT¯rAssuming that T¯=8250K and using the planetary parameters of HD209458b (Mp=0.7MJ, Rp=1.3RJ), we obtain values of X¯0=13.8 and X¯(2.1Rp)=6.6.
Thus, according to Eq.
(2), the column density of NH=5×1022m-2 that is required to explain the observations implies that n0=3.5×1017m-3.
This in turn means that p0=0.4μbar i.e., close to the lower boundary of the M7 model (here the agreement with the M7 model is obviously not exact because the transit depths in K10 are based on a complete forward model of the observed transit within the whole line profile).
The parameters of the empirical model can be compared with corresponding values derived from the hydrodynamic models presented in Paper I (see Section 2.3 here for a brief summary).
For instance, the mean temperature of the empirical model corresponds roughly to the pressure-averaged temperature of the thermosphere, which is given by (e.g., Holton, 2004):(4)Tp¯=∫p1p2T(p)d(lnp)ln(p2/p1)The hydrodynamic calculations show that the pressure averaged (mean) temperature below 3Rp based on the average solar flux varies between 6000K and 8000K for net heating efficiencies ηnet between 0.1 and 1.
This temperature is relatively insensitive to different assumptions about heating efficiencies or the upper boundary conditions.
In the reference C2 model of Paper I the mean temperature is 7200K.
This model is based on our best estimate of the heating efficiencies that are appropriate in the strongly ionized upper atmosphere of HD209458b.
With a cutoff level at 2.7Rp, we obtained a line-integrated H Lyman α transit depth of 4.7% based on the density of H in the C2 model.
This value agrees with the observations to within 1σ (Vidal-Madjar et al., 2004; Ben-Jaffel and Hosseini, 2010), but it is smaller than the transit depth of 6.6% predicted by the M7 model.
One reason for this is the lower mean temperature of the C2 model.
In order to facilitate a direct comparison between the hydrodynamic model and the empirical model, we calculated the empirical transit depth based on the mean temperature of 7200K (hereafter, the M7b model).
The line-integrated transit depth based on this model is 5.8%, which is still higher than the transit depth based on the C2 model.
Fig. 4 shows the density profiles of H, H+, O, and O+ from the C2 model, and the density profile of H from the M7b model.
The difference between the transit depths based on the empirical and hydrodynamic models arises because the C2 model has large temperature gradients (Paper I) and a gradual H/H+ transition rather than a sharp cutoff.
The difference does not arise because the density profile of the C2 model deviates from hydrostatic equilibrium.
Given the temperature gradient in the model, the density profile is almost exactly in hydrostatic equilibrium below 3Rp.
In fact, the neutral density profile of the C2 model is better represented by a mean temperature of 6300K (not shown).
This implies that the correspondence of the mean temperature of the empirical model and the pressure averaged temperature of the hydrodynamic model is relatively good but not exact.
A better agreement between the transit depths based on the C2 and M7b models is obtained with the cutoff level of the C2 model at 5Rp (see Table 2).
Fig. 5 shows the line-integrated transit depth within the H Lyman α line profile as a function of the cutoff level for the C2 model.
This figure indicates that the transit depth increases less steeply with altitude above 4Rp than it does below this cutoff level, and saturates near 5Rp.
This is a natural consequence of the fact that the LOS column density decreases approximately exponentially with altitude in the lower thermosphere and the wings of the line profile become optically thin at high altitudes.
We note that K10 already compared the results based on the M7 model with in-transit transmission as a function of wavelength within the H Lyman α line profile and the light curve derived from the G140M data (Ben-Jaffel, 2007, 2008).
It is not necessary to repeat a similar comparison here because spherically symmetric models that predict line-integrated transit depths that agree with the measured values are generally compatible with both the G140L and G140M data.
This is partly because the uncertainty of the individual data points within the line profile and the light curve is large, and thus they do not strongly constrain the properties of the atmosphere.
According to Fig. 4, the H/H+ transition in the C2 model occurs near 3.1Rp.
The exact location depends on photochemistry and vertical velocity, and generally the transition occurs near or above 3Rp (Paper I).
With a fixed pressure at the lower boundary, a faster velocity leads to a transition at a higher altitude.
These results disagree with Yelle (2004) and Murray-Clay et al. (2009) who predicted a lower transition altitude, but they agree qualitatively with the solar composition model of Garcia Munoz (2007).
The density profiles of O and O+ are strongly coupled to H and H+ by charge exchange.
As a result, the O/O+ transition occurs generally near the H/H+ transition.
For instance, in the C2 model it is located near 3.4Rp.
We note that significant ionization of H and O above 3Rp is anticipated by K10 and there is good agreement on this between the empirical and hydrodynamic models.
The detection of O at high altitudes constrains the mass loss rate and the ionization state of the upper atmosphere (see Section 3.2).
However, the large uncertainty in the observations means that repeated observations are required to confirm the transit depth.
The M7 model of K10 with a solar O/H ratio (Lodders, 2003) yields an O I transit depth of 3.9%, which is within 1.5σ of the observed value and therefore a satisfactory fit to the data.
This value agrees well with the O I transit depth based on the C2 model (see Table 2).
K10 argued that a higher transit depth is possible if the mean temperature is higher and/or if the O/H ratio is enhanced with respect to solar.
The hydrodynamic calculations indicate that the latter option is more favorable because higher temperatures lead to stronger ionization of O and may not help to significantly enhance the transit depth.
Indeed, the predicted transit depth agrees with the observations to better than 1σ if the O/H ratio is enhanced by a factor of 5 relative to solar (see the MSOL2 model in Table 2).
We have now verified that the empirical model can be used to constrain the mean temperature and extent of the absorbing layer in the thermosphere of HD209458b.
In particular, the comparison of the empirical model with the hydrodynamic model shows that the results of K10 were not affected by the simplifying assumption of hydrostatic equilibrium.
We note that the purpose of the empirical model is to identify physical processes that might otherwise be missed in more complex models that are often based on a large number of uncertain assumptions.
The results from any model can now be compared with the observations by identifying the limits of the absorbing layer and calculating the global pressure averaged temperature within that layer.
The values can then be compared with the parameters of the best-fit empirical model.
Ions
Fig. 6 shows the density profiles of the carbon and silicon ions in the thermosphere of HD209458b based on the C2 hydrodynamic model (Paper I).
The major carbon and oxygen-bearing species in the lower atmosphere, CO and H2O, dissociate near the 1μbar level (e.g., Moses et al., 2011, or Lavvas et al., in preparation).
Thermochemical calculations indicate that SiO is the dominant silicon-bearing gas on HD209458b (Visscher et al., 2010).
The detection of Si2+ in the upper atmosphere implies that the formation of silicon clouds in the lower atmosphere is suppressed (see Section 3.3), and SiO is also dissociated near 1μbar either thermally or by photochemistry.
Thus we assumed that only atomic carbon and silicon are present in the thermosphere, initially with solar abundances (Lodders, 2003).
According to Fig. 6, the C/C+ transition occurs at a much lower altitude of 1.2Rp than the H/H+ and O/O+ transitions.
Silicon is also almost fully ionized at the lower boundary of the model, and the Si2+/Si+ ratio is about 10% below 3Rp.
These results are in qualitative agreement with the observations, because they show that H and O are mostly neutral below 3Rp whereas C and Si are mostly ionized, and a significant abundance of Si2+ is possible.
However, it is also useful to explore if the results from the models are in quantitative agreement with the observations and if not, to adjust the model parameters as necessary to explain the data.
With a cutoff level at 2.7Rp, we obtained line-integrated transit depths of 2.3%, 3.6%, and 2.5% in the C II 1334.5Å, C II 1335.7Å and Si III lines, respectively, based on the C2 model.
Here we assumed that the population of the 2P levels of C+ are in LTE.
We also calculated empirical transit depths based on the M7 model.
In order to do this we assumed that both C and Si are ionized at the lower boundary, and that 10% of silicon is Si2+.
With these assumptions we obtained C II 1334.5Å and 1335.7Å transit depths of 2.7% and 4.2%, respectively, and a Si III transit depth of 3%.
These values agree well with the C2 model, and further demonstrate the consistency of the empirical and hydrodynamic models.
However, they deviate from the observed values by more than 2σ.
The cutoff level of the empirical model is somewhat arbitrary.
For neutral species it is partly based on ionization (K10), but this criterion obviously does not apply to ions.
With a cutoff level at 5Rp for the ions only, the M7 model yields line-integrated transit depths of 3.9%, 8%, and 5.8% in the C II 1334.5Å, C II 1335.7Å, and Si III lines, respectively, if 40% of silicon is Si2+.
These values agree with the observed values to better than 2σ.
Similarly, by extending the cutoff level of the C2 model to 5Rp, we obtained transit depths of 3.2%, 6.7%, and 4.6% in the C II 1334.5Å, C II 1335.7Å, and Si III lines, respectively (see Table 2).
These values deviate from the observed values by 2σ, 0.9σ, and 2.6σ, respectively.
The transit depths predicted by the M7 model are higher partly because the mean temperature of 8250K is higher than the corresponding temperature in the C2 model (Paper I).
This also leads to the higher Si2+/Si+ ratio that we used here.
It is not clear if the apparent disagreement between the models and some of the observations needs to be taken seriously.
Stellar activity and other uncertainties mean that the true transit depth can differ from measured values by a significant factor (see Section 2.1).
Further, the C2 model agrees with the line-integrated H Lyman α and C II 1335.7Å transit depths to within 1σ, and with the O I and C II 1334.5Å lines to within 2σ.
Thus we could argue that the present observations are roughly consistent with solar abundances and heating based on the average solar XUV flux.
Nevertheless, we explore the apparent disagreement between the C2 model and the observations further below.
This disagreement is limited to the O I, C II 1334.5Å, and Si III lines.
Fig. 7 shows the observed in-transit flux differences in the C II and Si III lines as a function of wavelength together with different model predictions.
The observations indicate that the transit depths based on the C2 model fall short of the observed values because the model underestimates the width of the absorption lines.
Linsky et al. (2010) argued that there is velocity structure within the line profiles near Doppler shifts of -10kms-1 and 15kms-1 that accounts for broad absorption.
However, the uncertainty of the individual data points is too large to constrain the shape of the absorption lines in detail.
We agree with Linsky et al. (2010) that the presence of velocity structure needs to be confirmed by future observations.
Thus the additional absorption could also arise from spectral line broadening.
A higher mean temperature leads to higher transit depths.
Therefore we generated a new hydrodynamic model by multiplying the average solar flux by a factor of 2 and assumed a net heating efficiency of ηnet=0.5 (hereafter, the SOL2 model).
This model agrees to better than 1σ with all of the observed transit depths apart from the O I and C II 1334.5Å lines (see Fig. 7 and Table 2).
The mean temperature of the model below 3Rp is 7400K, which is lower than the mean temperature in the M7 model.
However, absorption by the SOL2 model is strengthened by velocity dispersion within the escaping plasma that is not included in the M7 model.
In general, the outflow velocity of the SOL2 model is significantly higher than the velocity in the C2 model.
The model also predicts a mass loss rate of 108kgs-1, which is twice as high as the mass loss rate based on the C2 model.
This proves that a higher stellar XUV flux or a corresponding alternative energy source can explain the observations.
An enhancement of the average solar flux by a factor of 2 is not unreasonable, and would roughly correspond to solar maximum conditions.
In Paper I we noted that the stellar XUV flux, or the corresponding alternative heat source, would have to be 5-10 stronger than the average solar flux to produce a mean temperature between 8000 and 9000K.
Under such circumstances, the predicted transit depths in the C II and Si III lines would obviously be even higher than the values predicted by the SOL2 model.
Indeed, higher temperatures broaden absorption in the wings of the line profiles and may help to explain the in-transit flux differences better (see Fig. 7).
However, the energy input and temperature in the model cannot be increased without bound.
Higher temperatures and flux lead to more efficient ionization of the neutral species, and as a result the transit depths in the H Lyman α and O I lines begin to decrease.
Also, mass loss rates of 109-1010kgs-1 lead to the loss of 10-100% of the planet's mass over the estimated lifetime of the system, and this probably limits reasonable energy inputs to less than ∼10 times the solar average on HD209458b.
In addition to higher temperature and velocity, supersolar abundances of O, C, and Si can also lead to higher transit depths.
This option is interesting because it also allows for a higher transit depth in the O I lines.
As an example, we generated the MSOL2 model by enhancing the solar O/H, C/H, and Si/H abundances in the hydrodynamic model by a factor of 5.
As a result, we obtained transit depths that agree with nearly all of the observed line-integrated transit depths to better than 1σ (see Table 2).
However, the MSOL2 model overestimates the line-integrated C II 1335.7Å transit depth, and generally overestimates absorption within the cores of the C II and Si III lines.
This could imply that a higher temperature or some other source of additional broadening is a better explanation of the Si III and C II transit depths while a supersolar O/H ratio is required to match the measured O I transit depth.
However, the data points in Fig. 7 and the observed O I transit depth are too uncertain and do not allow for firm constraints on this.
Enrichment of heavy elements is a common feature on the gas and ice giants in the Solar System.
For instance, the C/H, N/H, S/H, Ar/H, Kr/H, and Xe/H ratios in the atmosphere of Jupiter are all enriched by factors of 2-3 with respect to solar abundances (e.g., Mahaffy et al., 2000; Wong et al., 2004).
On Saturn, on the other hand, the C/H ratio is enriched by a factor of 10 (Flasar et al., 2005; Fletcher et al., 2009).
Enrichment by factors of 4-20 is expected in the N/H and S/H ratios, although condensation of NH3 and H2S in the deep atmosphere of Saturn makes it difficult to constrain the abundances precisely (see Fouchet et al. (2009) for a review).
On Neptune and Uranus the C/H ratio is believed to be enriched by factors of 30-50 (e.g., Owen and Encrenaz, 2003; Guillot and Gautier, 2007)), and similar enrichments are possible in the abundance ratios of some of the other heavy elements.
Substantial enrichment of heavy elements with respect to solar abundances is therefore also feasible in EGP systems even if the metallicity of the star is close to solar.
Unfortunately, we cannot use the current observations to constrain the elemental abundances of the atmosphere with accuracy.
In this regard, the large uncertainty of the observations is unfortunate, because similar observations can potentially be used to estimate them.
The dissociation of molecules at the relatively high pressure of 1μbar and the lack of diffusive separation mean that the abundances of the heavy atoms and ions are simply dependent on the elemental abundances and ionization rates.
Observations of the neutral species can therefore be used to constrain the temperature and ionization state, and thus the elemental abundances of the heavy species, but the S/N of the current data does not allow for strong constraints.
It is interesting to note that while the velocity structure of the escaping plasma can lead to broader absorption that helps to explain the transit depths, it is not necessarily detectable in the data.
For instance, Fig. 7 shows the transit depths based on the SOL2 model that has a relatively high radial velocity reaching 11kms-1 by 5Rp.
The velocity structure is not detectable because the optical depth of the high velocity material is not sufficient, the LOS velocity at the limb of the planet is slower than the radial velocities in general, and because spectral line broadening within the COS instrument smooths the structure out of the line profiles.
If the presence of velocity structure is confirmed in the data (Linsky et al., 2010), it probably implies that there is detached, optically thick plasma moving at large velocities around the planet.
If this turns out to be the case, interaction with the stellar wind probably plays a role in giving rise to the observed absorption.
Such interaction may also produce turbulence that can broaden the absorption further (e.g., Tian et al., 2005).
However, we note that non-thermal broadening such as that proposed by Ben-Jaffel and Hosseini (2010) does not appear to be necessary to explain the current observations.
Ionospheric escape
The escape of heavy atoms and ions has interesting consequences for the nature of the upper atmosphere.
Here we discuss these consequences based on simple analytic arguments, and without explicit use of any complex models.
The detection of heavy neutral species can be used to constrain the mass loss rate while the detection of heavy ions outside the atmosphere of the planet potentially constrains the strength of the planetary magnetic field.
For instance, Hunten et al. (1987) derived an expression for the crossover mass limit mc for a neutral species s to be dragged along by an escaping neutral species t of mass mt<ms:(5)mc=mt+kTFtnDstxtg0r02where Ft is the flux (s-1sr-1) of species t, xt is the volume mixing ratio, g0 is gravity at the lower boundary of the model region, and the mutual diffusion coefficient can be roughly estimated from:(6)nDst=1.52×10181Ms+1Mt1/2Tcm-1s-1where the masses M are in amu.
We used Eq.
(5) to estimate the mass loss rate that is required to drag neutral O to the exosphere of HD209458b.
Assuming that xt∼1, g0=10ms-1, r0=Rp, T=7200K, andnDst=1.3×1022m-1s-1, we obtain Ft≈2.8×1032s-1sr-1.
This implies a minimum mass loss rate of 6×106kgs-1.
The ionosphere of HD209458b is mostly neutral below 3Rp but even weak ionization can lead to frequent Coulomb or ion-neutral collisions that enable heavy ions or atoms to escape more efficiently.
In order to illustrate the role of different collisions in transporting O and Si+, Fig. 8 shows the collision frequencies for these species with H and H+ as a function of altitude based on the C2 model (Paper I).
We used approximate expressions for resonant and non-resonant ion-neutral collisions, and Coulomb collisions from Schunk and Nagy (2000) to calculate the momentum transfer collision frequencies.
The collision frequency between two neutral species, on the other hand, was estimated from the mutual diffusion coefficient as:(7)νst=5.47×10-111Ms+1Mt-1/2ntTMswhere the number density nt is in cm-3.
The results indicate that the transport of O depends on collisions with H below 3.5Rp whereas the transport of Si+ depends on collisions with H+ at all altitudes.
Oxygen is the heaviest neutral species detected in the escaping atmosphere, and this implies that the mass loss rate from HD209458b is Ṁ>6×106kgs-1.
This result agrees with Vidal-Madjar et al. (2003) although it is less model-dependent and based on different criteria.
The dominance of Coulomb collisions means that the heavy ions can escape even if the mass loss rate is lower than this.
Our models predict mass loss rates of Ṁ≈5×107kgs-1 (Table 2) and thus diffusive separation does not take place in the thermosphere of HD209458b for neutral species with masses less than ∼130amu.
We note that this is the case even if escape is subsonic.
In fact, Eq.
(5) was originally derived for subsonic escape under the diffusion approximation although it is also valid for supersonic escape (Zahnle and Kasting, 1986).
We used the collision frequencies to derive expressions for the ion fractions fi=nH+/nH at which ion-neutral and Coulomb collisions become important.
The ratio of the non-resonant neutral-ion to neutral-neutral collisions exceeds 10% when(8)fi≈10-24TMsiγse2where i denotes H+, s denotes the colliding species, γs is the neutral polarizability and all units are in cgs.
However, the collision of O with H+ is resonant and in this case the required ratio differs slightly from the above expression.
The ratio of the Coulomb to non-resonant ion-neutral collisions, on the other hand, exceeds 10% when(9)fi≈4.24×1011T3/2Zs2Zi2Msnγne2Msiwhere i denotes H+ (or the dominant ion) and n is H (or the dominant neutral).
For Si+ this fraction is fi≈10-4 (with γH=6.7×10-25cm3).
These equations can be used to determine if Eq.
(5) is valid, or if more complex plasma models are required.
Trammell et al. (2011) argued that HD209458b could have a strong planetary magnetic field that can impede the escape of ions from equatorial regions and restrict it to the polar regions.
Although the magnetic field does not directly interfere with the escape of the neutral atoms, the trapped ions can stop them from escaping if the neutral-ion collision frequency is sufficiently high.
Unfortunately, transit observations are not spatially resolved and they cannot be used directly to determine if escape is limited to the poles, or if the atmosphere is also escaping over the equator.
However, the transit depths depend on the size of the optically thick obstacle covering the star.
If mass loss is suppressed at low and mid-latitudes, the heavy species are no longer mixed into the upper atmosphere other than at the poles where they are allowed to escape.
This means that the cross-sectional area covered by the ions shrinks and may become insufficient to explain the observations.
Even if the plasma spreads to cover a larger area after being ejected from the poles, it is diluted in the process and thus it is not clear if the resulting cloud would have sufficient optical depth to be detectable.
Assuming that the charged particles escape the Roche lobe of the planet unimpeded at the equator, we estimated an upper limit for the planetary magnetic field by evaluating the magnetic moment that allows them to do so.
In order to estimate this limit, we calculated the plasma β and the inverse of the second Cowling number (Co-1) below 5Rp in the C2 model from:β=2μ0pB2,Co-1=μ0ρv2B2where v is the vertical velocity.
Assuming a dipolar magnetic field, we obtained a limiting magnetic moment of 3.2×1025Am2 or 0.04mJ (mJ=1.5×1027Am2 is the magnetic moment of Jupiter).
This magnetic moment ensures that β>10 below 5Rp and that Co-1 reaches 10 by 5Rp.
Magnetic moments of mp≲0.04mJ agree quite well with the scaling laws discussed by Griesmeier et al. (2004).
We note that the limiting moment produces an equatorial surface field that is approximately four times lower than the surface field of the Earth.
Cloud formation on HD209458b
We have shown that a substantial abundance of silicon in the upper atmosphere is required to produce a detectable transit in the Si III line.
If the silicon ions originate from the atmosphere of the planet, at least a solar Si/H ratio is necessary.
According to thermochemical equilibrium models, silicon should condense into clouds of forsterite (Mg2SiO4) and enstatite (MgSiO3) in the lower atmosphere of HD209458b (Visscher et al., 2010).
If the formation of enstatite is suppressed, silicon should condense to form quartz (SiO2) instead.
In any case, condensation is expected to remove almost all of the silicon from the upper atmosphere.
The detection of Si2+ implies that the abundance of silicon in the thermosphere is substantial, and thus the condensation of silicon does not take place in the atmosphere of HD209458b.
This has significant implications for the structure and dynamics of the atmosphere.
Sing et al. (2008a,b) analyzed the absorption line profile of Na in the atmosphere of HD209458b in detail and argued that the abundance of Na is depleted above the 1mbar level.
They suggested that this is due to the condensation of sodium into Na2S, although ionization could not be ruled out decisively.
Based on the condensation temperature of Na, they argued that the temperature in the upper atmosphere of HD209458b near 1mbar is 420±190K.
This temperature is significantly lower than the outcome of typical radiative transfer models for HD209458b (e.g., Showman et al., 2009).
We note that the condensation temperature of forsterite and enstatite is higher than 1300K at 1mbar (Visscher et al., 2010).
Because silicon clouds do not form, the Na2S clouds cannot form either.
Further, the formation of Na2S relies on H2S, which is dissociated above the 1mbar level (Zhanle et al., 2009).
This implies that any depletion of Na at high altitudes is most likely due to photoionization and/or thermal ionization.
Fig. 9 shows a dayside temperature (T-P) profile for HD209458b based on Showman et al. (2009).
This profile is similar to the dayside temperature profile adopted by Moses et al. (2011).
The figure also shows the condensation curves for forsterite and enstatite.
The T-P profile crosses the condensation curve for forsterite below the 100bar level.
However, the temperature profile in the deep atmosphere is uncertain, and the current profile only barely crosses the condensation curve.
Also, the formation of forsterite ties only a fraction of the total abundance of silicon into clouds (Visscher et al., 2010).
However, the T-P profile crosses the condensation curves for both forsterite and enstatite in a 'cold trap' near 10mbar.
To prevent this, the temperature in the cold trap would have to be T≳1600K.
This is not totally unbelievable but probably unlikely.
In any case, the temperature is close enough to the condensation curves so that moderate vertical transport might be able to preserve silicon above the cold trap.
Spiegel et al. (2009) explored a range of turbulent diffusion coefficients Kzz that would be required to prevent the condensation of TiO and VO in the atmospheres of different EGPs, including HD209458b.
In fact, they assumed that condensates form in the cold trap but are then transported to higher altitudes where they evaporate.
Their work ignores the detailed chemistry of condensation, and thus the results are simply based on the ratio of Kzz to the diffusion coefficient estimated fromDp≈vpHwhere vp is the particle settling velocity and H is the pressure scale height.
The formation of condensates is probably too complicated for such a simplistic treatment, but the results provide some guidance on the value of Kzz that is required to lift the condensates from the cold trap.
We calculated Dp for forsterite grains with a radius of 0.1μm and density3
This is the density of the material in the particles, not the density of the particles in the atmosphere.
3 of 3200kgm-3 (Fortney et al., 2003; Cooper et al., 2003).
The settling velocity for such particles in the cold trap is vp≈3×10-3ms-1 and Dp≈2×103m2s-1.
Given that the upper edge of the cold trap is near the 1mbar level where Dp is higher, Kzz≳105m2s-1 is sufficient to prevent the settling of the cloud particles in the lower atmosphere.
We note that mass loss does not help to enhance the mixing of the particles at low altitudes significantly.
The vertical velocity based on the mass loss rate of 107kgs-1 is only 4.8×10-7ms-1 at 10mbar and 8×10-3ms-1 at 1μbar.
Estimating Kzz on extrasolar planets is very difficult, partly because there is no agreement on exactly what physical processes this parameter describes even in much more sophisticated Solar System applications.
The most recent estimates for HD209458b are based on assuming that Kzz∼v¯H, where v¯ is the rms vertical velocity from circulation models (e.g., Showman et al., 2009).
Based on the GCMs of Showman et al. (2009) and an assumed density dependence with altitude, Moses et al. (2011) estimated that the high pressure value of Kzz=106m2s-1, which implies that Kzz≳107m2s-1 at p≲10mbar.
If such high values are realistic, turbulent mixing is almost certain to prevent the settling of the condensates and to preserve gaseous silicon in the upper atmosphere.
We note that the above requirements on the value of Kzz may in fact be overestimated because they are based on the assumption that clouds particles form in the cold trap.
Cloud formation has to be studied in the context of a photochemical model that includes the chemistry of condensation.
If the chemical timescale is longer than the transport timescale, the cloud droplets may not form in the first place before the gas escapes from the cold trap.
Also, the temperature structure near the cold trap needs to be constrained in greater detail.
The formation of condensates is a complex problem that will be studied in future work (Lavvas et al., in preparation) in order to better constrain the required values of Kzz.
For our purposes it is sufficient to note that the current estimates of Kzz imply, in agreement with the observations, that silicon clouds do not form in the upper atmosphere of HD209458b.
Discussion and conclusions
We have used multiple observational constraints and theoretical models to characterize the upper atmosphere of HD209458b.
Contrary to many of the earlier studies, we did not treat the thermosphere independently of the rest of the atmosphere.
In fact, we have shown that observations of the upper atmosphere can be used to obtain useful constraints on the characteristics of the lower atmosphere.
This is important because the extended upper atmospheres of close-in EGPs produce much larger transit depths than those arising from the lower atmosphere.
In this work, we concentrated mostly on the FUV transit measurements.
Future work should explore the possibility of extending the range of possible observations to other wavelength regions, as well as obtaining repeated observations in the FUV lines.
Theoretical models should be developed to support new observations and to clarify the interpretation of the existing data.
In agreement with K10, we showed that the H Lyman α transit observations (Vidal-Madjar et al., 2003, 2004; Ben-Jaffel, 2007, 2008; Ben-Jaffel and Hosseini, 2010) can be fitted with a layer of H in the thermosphere that is described by three simple parameters.
These are the pressure at the bottom of the H layer, the mean temperature in the thermosphere, and a cutoff level due to ionization.
The most important parameters are the pressure at the lower boundary and the mean temperature.
Because H is the dominant species in the thermosphere, the data can be used to estimate the temperature of the thermosphere.
Choosing a lower boundary pressure of 1μbar based on the location of the H2/H dissociation front in recent photochemical models (Moses et al., 2011) and observational constraints (France et al., 2010), we estimated a mean temperature of about 8250K in the thermosphere below 3Rp.
However, the uncertainty of the observations is large, and the 1σ upper and lower limits on this temperature are approximately 6000K and 11,000K, respectively.4
This uncertainty does not include the possible uncertainties in the other parameters of the fit.
4
We used a hydrodynamic model that treats the heating of the upper atmosphere self-consistently and the average solar XUV spectrum (Paper I) to show that a mean temperature of 8250K in the upper atmosphere below 3Rp is higher than the maximum temperature allowed by stellar heating.
Given that a net heating efficiency equal to or higher than 100% is unrealistic, this temperature requires either a non-radiative heat source, additional opacity, or it implies that the XUV flux of HD209458 is higher than the corresponding solar flux.
Interestingly, this would also imply that the mass loss rate could be higher by a factor of 2 or more than previously anticipated (e.g., Paper I).
However, the uncertainty in the H Lyman α observations also allows for a slightly lower temperature of 7200K that is typical of stellar heating based on the average solar XUV flux and our best estimate of the net heating efficiency.
Therefore the temperature implied by the H Lyman α observations and the mean temperature of the basic hydrodynamic models are in good agreement.
We note that the temperature in the lower thermosphere near the 1μbar region has been constrained previously by Vidal-Madjar et al. (2011a,b) who used the Na D lines to constrain the density and temperature profiles in the atmosphere of HD209458b.
Their results point to a temperature of ∼3600K that is actually higher than the temperature at the lower boundary of our hydrodynamic model and consistent with a high mean temperature at lower pressures in the thermosphere.
However, we caution the reader that the temperature profile based on the Na D lines may not be accurate.
This is because Vidal-Madjar et al. (2011a) used a simple expression for the optical depth due to Na that is based on the scale height of the atmosphere [their Eq.
(1)].
This expression is only valid if Na is uniformly mixed with H2.
Since the authors also argue that Na is depleted (i.e., its mixing ratio changes with altitude) above ∼10mbar, its density scale height cannot be used to estimate temperatures accurately.
The detection of O in the thermosphere allowed us to constrain the mass loss rate from HD209458b based on the crossover mass concept formulated by Hunten et al. (1987).
This is because O is transported to high altitudes in the upper atmosphere primarily by collisions with H.
We found that a minimum mass loss rate of 6×106kgs-1 is required to prevent the diffusive separation of O.
Our hydrodynamic calculation based on the average solar XUV flux predicts a mass loss rate close to 5×107kgs-1.
This implies that species with a mass up to ∼130amu are uniformly mixed in the thermosphere.
Similar constraints do not apply to heavy ions.
They are transported to high altitudes by Coulomb collisions with H+ that are much more efficient in preventing diffusive separation compared to collisions of neutral atoms with H.
In agreement with K10, we found that the presence of O with a solar abundance and a temperature based on the H Lyman α measurements explains the transits observed in the O I lines.
Our models predict a line-integrated transit depth of approximately 4% that deviates from the measured transit depth by 1.5σ and thus agrees with the uncertainty of the observations.
However, the predicted transit depths fall systematically short of the measured value.
K10 suggested that a higher transit depth is possible if the O/H ratio is supersolar, the temperature of the thermosphere is higher than expected, and/or the observations probe escaping gas outside the Roche lobe of the planet, and we agree with their conclusions.
As we explain below, we found that the same is true for the other heavy species.
In order to calculate predicted transits in the C II and Si III lines, we created model emission line profiles for HD209458 based on SUMER observations of the Sun (Curdt et al., 2001) adjusted to the observations of HD209458 (Linsky et al., 2010).
We did not find evidence for significant absorption by the ISM in the C II 1335.7Å or the Si III line.
Parts of the C II 1334.5Å line, on the other hand, are optically thick in the ISM and we took this into account in our models.
We note that resolved observations of the emission line profiles can be used to characterize the composition of the ISM and the activity of the host star.
This information is a valuable byproduct of the FUV transit observations that typically need to be repeated several times.
With solar abundances, the same models that agree with the H Lyman α transit observations tend to underestimate the transit depths in the C II and Si III lines.
Similarly with the O I lines, higher transit depths in the C II and Si III lines are possible if the mean temperature of the absorbers is higher than expected and/or the C/H and Si/H ratios are supersolar.
With solar abundances, a 1σ agreement with the observed line-integrated transit depths is possible if the stellar XUV flux (or the stellar flux combined with an additional heat source) is higher than or equal to two times the average solar XUV flux.
This corresponds to typical solar maximum conditions, and it is quite interesting that a similar enhancement may be required to explain the relatively high mean temperature in the thermosphere that we estimated earlier.
Alternatively, with heating based on the average solar flux, an agreement with the observations is possible with the O/H, C/H and Si/H enhanced by a factor of ∼3-5 relative to solar abundances (Lodders, 2003).
In any case, the atmosphere is escaping with a minimum mass loss rate given above.
This is evident because of both the high temperature of the thermosphere and the detection of heavy species at high altitudes.
We note that the transit observations are affected by stellar variability that can lead to significant changes in the observed transit depths.
Spatial variations of intensity on the stellar disk during maximum activity or limb brightening may render the transit occasionally undetectable, or enhance it by a significant factor.
Generally, observations obtained during periods of minimum activity are more reliable.
Ben-Jaffel (2007) characterized the short-term variability of HD209458 in the H Lyman α line, but the variability of the O I, C II, and Si III are poorly characterized.
This introduces an additional element of uncertainty into the transit depths that needs to be constrained by repeated observations of the star and the transits in the FUV lines.
It also means that the qualitative explanation of the present observations that is based on heating by the average solar XUV flux and solar abundances (Paper I) may be sufficient even if the predicted transit depths in the O I, C II, and Si III lines do not exactly match the current measurements.
On the other hand, if higher transit depths are confirmed, they can be used to further constrain the mean temperature and abundances as we have shown.
The detection of heavy ions escaping the atmosphere can potentially be used to constrain the magnetic field strength of the planet.
We estimated an upper limit of 0.04mJ for the magnetic moment that allows the heavy ions to escape unimpeded at the equator.
The estimated magnetic moment agrees with the scaling laws for the magnetic field strengths of tidally locked close-in EGPs by Griesmeier et al. (2004).
On the other hand, a strong magnetic field inhibits the flow of ions from the equator and may only allow for escape at the poles (e.g., Trammell et al., 2011).
If the neutral-ion collision frequencies are sufficient, the trapped ions may also prevent the neutral atoms from escaping.
We note that a uniform upward flux is required to mix the atmosphere, and escape at the poles may not be sufficient to create a large enough obstacle to explain the transits in the O I, C II, and Si III lines.
Detailed models of the magnetosphere that include the heavy species are required to assess if this is the case or not.
The detection of Si2+ in the upper atmosphere means that silicon cannot condense to form enstatite, forsterite, or other condensates in the lower atmosphere.
This is clear because at least a solar abundance of silicon in the thermosphere is required to explain the large transit depth in the Si III line.
According to the calculated temperature profiles for HD209458b (e.g., Showman et al., 2009), condensation is expected in a cold trap near the 10mbar level.
Provided that the temperature is not much higher than expected near the cold trap, efficient mixing is required to prevent condensation.
Following an argument similar to that of Spiegel et al. (2009), we estimated that an eddy mixing coefficient of Kzz≳105m2s-1 below 1mbar is sufficient to prevent the condensation of forsterite and enstatite in the cold trap.
We note that much higher values than 105m2s-1 were assumed by recent photochemical models by Garcia Munoz (2007) and Moses et al. (2011).
A stratospheric temperature inversion may also be necessary to suppress condensation.
This is because the cold trap cannot extend to much lower pressure than 1mbar or the required values of Kzz become too high.
Also, the lack of condensation implies that the temperature of the lower atmosphere should be relatively high.
The detection of silicon in the upper atmosphere thus provides further evidence for a stratosphere on HD209458b that was first proposed by Knutson et al. (2008).
We note that existing radiative transfer models do not account for molecular opacity at UV wavelengths or visible absorbers potentially generated by photochemistry (e.g., Zhanle et al., 2009).
Our results provide motivation for more detailed models of thermal structure below the thermosphere that can constrain the chemistry of the lower atmosphere.
Once this is achieved, better constraints on the dynamics can be derived.
We also address an old problem related to the atmosphere of HD209458b.
Based on the observed transits in the Na D lines, Charbonneau et al. (2002) argued that Na is depleted in the atmosphere because their cloudless solar composition model predicted significantly deeper absorption in the D lines.
In addition to photoionization, molecular chemistry, and low primordial abundance of Na, they suggested that the formation of high altitude clouds can explain the observed depletion.
Later Sing et al. (2008a,b) found further evidence for the depletion of Na above the 3mbar level, and argued that condensation of Na2S is the most likely explanation.
The fact that silicon does not condense implies that condensation of Na2S is also unlikely.
The observed depletion is therefore most likely due to photoionization and/or thermal ionization.
However, the density profile and ionization state of Na should be studied in the context of molecular and ion chemistry below the 0.1μbar level to verify that this is the case.
Acknowledgments
We thank H.
Menager, M. Barthelemy, N.
Lewis, and D.S.
Snowden for useful discussions and correspondence, and A.
Showman and N.
Lewis for sharing some of their temperature profiles.
We also acknowledge the "Modeling atmospheric escape" workshop at the University of Virginia and the International Space Science Institute (ISSI) workshop organized by the team "Characterizing stellar and exoplanetary environments" for interesting discussions and an opportunity to present our work.
The calculations for this paper relied on the High Performance Astrophysics Simulator (HiPAS) at the University of Arizona, and the University College London Legion High Performance Computing Facility, which is part of the DiRAC Facility jointly funded by STFC and the Large Facilities Capital Fund of BIS.
SOLAR2000 Professional Grade V2.28 irradiances are provided by Space Environment Technologies.
This research was supported by the National Science Foundation (NSF) through grant AST 1211514.

The escape of heavy atoms from the ionosphere of HD209458b.
II.
Interpretation of the observations

Highlights
► We compare UV transits of HD209458b with empirical and hydrodynamic models.
► We constrain the mean temperature, densities and escape rates of different species.
► The detection of atomic oxygen implies a minimum mass loss rate of 6×106kgs-1.
► The results constrain the temperature, chemistry, and ionization of the atmosphere.
► Detection of Si2+ indicates that clouds of forsterite and enstatite do not form.
Abstract
Transits in the H I 1216Å (Lyman α), O I 1334Å, C II 1335Å, and Si III 1206.5Å lines constrain the properties of the upper atmosphere of HD209458b.
In addition to probing the temperature and density profiles in the thermosphere, they have implications for the properties of the lower atmosphere.
Fits to the observations with a simple empirical model and a direct comparison with a more complex hydrodynamic model constrain the mean temperature and ionization state of the atmosphere, and imply that the optical depth of the extended thermosphere of the planet in the atomic resonance lines is significant.
In particular, it is sufficient to explain the observed transit depths in the H I 1216Å line.
The detection of O at high altitudes implies that the minimum mass loss rate from the planet is approximately 6×106kgs-1.
The mass loss rate based on our hydrodynamic model is higher than this and implies that diffusive separation is prevented for neutral species with a mass lower than about 130amu by the escape of H.
Heavy ions are transported to the upper atmosphere by Coulomb collisions with H+ and their presence does not provide as strong constraints on the mass loss rate as the detection of heavy neutral atoms.
Models of the upper atmosphere with solar composition and heating based on the average solar X-ray and EUV flux agree broadly with the observations but tend to underestimate the transit depths in the O I, C II, and Si III lines.
This suggests that the temperature and/or elemental abundances in the thermosphere may be higher than expected from such models.
Observations of the escaping atmosphere can potentially be used to constrain the strength of the planetary magnetic field.
We find that a magnetic moment of m≲0.04mJ, where mJ is the jovian magnetic moment, allows the ions to escape globally rather than only along open field lines.
The detection of Si2+ in the thermosphere indicates that clouds of forsterite and enstatite do not form in the lower atmosphere.
This has implications for the temperature and dynamics of the atmosphere that also affect the interpretation of transit and secondary eclipse observations in the visible and infrared wavelengths.

Introduction
The detection of H, O, C+, and Si2+ in the upper atmosphere of HD209458b (Vidal-Madjar et al., 2003, 2004; Linsky et al., 2010), and the tentative detection of H in the upper atmosphere of HD189733b (Lecavelier des Etangs et al., 2010) and Mg+ in the upper atmosphere of WASP-12b (Fossati et al., 2010) are among the most exciting recent discoveries related to the atmospheres of extrasolar giant planets (EGPs).
The observations demonstrate that the upper atmospheres of close-in EGPs such as HD209458b differ significantly from the thermospheres of the giant planets in the Solar System.
They are much hotter, they extend to several planetary radii and instead of molecular hydrogen, they are primarily composed of atoms and atomic ions.
The detection of heavy atoms and ions such as O, C+, Si2+, and Mg+ implies that the atmospheres of close-in EGPs are not always affected by diffusive separation.
A likely explanation is that diffusive separation of the heavy atoms and ions is prevented by momentum transfer collisions with the rapidly escaping light atoms and ions.
Mass fractionation during hydrodynamic escape is believed to have played an important role in the early evolution of the atmospheres of the terrestrial planets (e.g., Zahnle and Kasting, 1986; Hunten et al., 1987) but it cannot be observed in action anywhere in the Solar System.
Observations of EGP atmospheres thus provide a unique opportunity to study this phenomenon that should lead to a better understanding of evolutionary processes in our own Solar System.
Extended thermospheres give rise to large transit depths in UV transmission spectra.
However, they are also potentially detectable in optical and infrared spectra.
For instance, Coustenis et al. (1997, 1998) searched for an exosphere around 51 Peg b in the near-IR.
In line with the current understanding, they suggested that the exospheres of close-in EGPs such as 51 Peg b are hot and composed primarily of atoms and ions.
They also argued that hydrodynamic escape can lead to the escape of heavier species, and give rise to large in-transit absorption by such species in optical and near-IR spectra.
However, 51 Peg b is not a transiting planet and the search for an exosphere around it was not successful.
On the other hand, once the transit of HD209458b was first detected (Charbonneau et al., 2000; Henry et al., 2000), similar searches on this planet were also undertaken.
Moutou et al. (2001) looked for absorption by species such as Na,H,He,CH+,CO+,N2+, and H2O+ in the upper atmosphere of HD209458b.
These observations were followed by Moutou et al. (2003) who attempted to measure the transit depth in the He 1083nm line that was predicted to be significant by Seager and Sasselov (2000).
The most recent searches were reported by Winn et al. (2004) and Narita et al. (2005) who looked for transits in the Na D, Li, Hα, Hβ, Hγ, Fe, and Ca absorption lines.
So far none of the ground-based searches have led to a detection of the upper atmosphere.
However, the non-detection is based on only a few observations that have proven difficult to analyze, and the search should continue.
The atmosphere of HD209458b has also been observed several times in the infrared with the Spitzer space telescope.
Deming et al. (2005) detected the secondary eclipse of the planet at 24μm by using the Multiband Imaging Photometer (MIPS).
Together with similar observations of TrES-1 at 4.5 and 8.0μm obtained by Charbonneau et al. (2005) who used the Infrared Array Camera (IRAC), these observations constitute the first detections of infrared emission from extrasolar planets.
They were followed by Richardson et al. (2007) who observed the infrared emission spectrum of HD209458b between 7.5 and 13.2μm with the Infrared Spectrograph (IRS).
This spectrum was reanalyzed by Swain et al. (2008) who noted that it is largely featureless with some evidence for an unidentified spectral feature between 7.5 and 8.5μm.
Knutson et al. (2008) used IRAC to observe the secondary eclipse of HD209458b in the 3.6, 4.5, 5.8, and 8.0μm bands.
They observed a higher than expected flux in the 4.5 and 5.8μm bands and interpreted this as evidence for the presence of a stratospheric temperature inversion that gives rise to strong water emission at these wavelengths.
Beaulieu et al. (2010) observed the transit of the planet in the same wavelength bands and also found evidence for the presence of water vapor in the atmosphere.
The detection of water vapor is also supported by Swain et al. (2009) who used the Near Infrared Camera and Multi-Object Spectrometer (NICMOS) on HST to observe the secondary eclipse of HD209458b between 1.5 and 2.5μm.
In addition to water vapor, the NICMOS observations revealed the presence of methane and carbon dioxide in the emission spectrum.
These detections provide valuable clues to the overall composition of the atmosphere but the uncertainties in the data and degeneracies between temperature and abundances in the forward model prevent a more quantitative characterization of the density and temperature profiles.
In general, difficulties associated with reducing the data and the need to describe a few uncertain data points with models of growing complexity has led to disagreements on the analysis and interpretation of transmission and secondary eclipse data on different exoplanets.
The same is true of the FUV transit observations of HD209458b.
Vidal-Madjar et al. (2003) used the STIS G140M medium resolution grating to detect a 15±4% transit depth in the wings of the stellar H Lyman α emission line.1
The core of the H Lyman α line is entirely absorbed by the interstellar medium (ISM).
1 Based on the data, they argued that the planet is followed by a cometary tail of escaping hydrogen atoms that are accelerated to velocities in excess of 100kms-1 by stellar radiation pressure (see also Schneider et al., 1998).
Later, Vidal-Madjar et al. (2004) used the STIS G140L low resolution grating to detect absorption by H, O and C+ in the upper atmosphere, and argued that the atmosphere of HD209458b escapes hydrodynamically.
Linsky et al. (2010) used the Cosmic Origins Spectrograph (COS) on HST to confirm the detection of C+ and reported on the detection of Si2+ around the planet.
They also argued that the atmosphere escapes hydrodynamically.
Ben-Jaffel (2007, 2008) disagreed with the interpretation of the G140M observations.
He reanalyzed the G140M data and argued that the 15% H Lyman α transit depth was exaggerated because the data were partly corrupted by short-term variability of the host star and geocoronal Lyman α emissions.
He also showed that there is no evidence for a cometary tail in the data, and that absorption by H in the extended thermosphere of the planet can explain the observations.
Ben-Jaffel and Hosseini (2010) reanalyzed the G140L data and reached a similar conclusion regarding H (see also Koskinen et al., 2010).
However, Ben-Jaffel and Hosseini (2010) fitted the H Lyman α observations by scaling the density profiles from the model of Garcia Munoz (2007), and argued that suprathermal O and C+ are required to fit the transit depths in the O I and C II lines.
They did not explain how the suprathermal atoms form and simply chose their properties to fit the observations.
Holström et al. (2008) offered yet another explanation for the H Lyman α observations.
They argued that hydrogen atoms cannot be sufficiently accelerated by stellar radiation pressure before they are ionized by stellar X-rays and EUV (XUV) radiation.
Instead, they suggested that the observed absorption arises from a cloud of energetic neutral atoms (ENAs) that form by charge exchange between the protons in the stellar wind and the escaping hydrogen.
In their model, the observed absorption reflects the velocity of the stellar wind, and the data can potentially be used to characterize the magnetosphere of the planet.
Ekenbäck et al. (2010) recently updated the model to include a more realistic description of the escaping atmosphere and stellar wind properties.
However, both studies ignored absorption by H in the thermosphere, which is significant, and did not attempt to explain the presence of heavier atoms and ions such as O, C+, and Si2+ in the escaping atmosphere.
All of the interpretations of the UV transit data require that HD209458b has a hot and extended thermosphere.
In fact, Koskinen et al. (2010) (hereafter K10) also showed that absorption by thermal H and O in such a thermosphere explains both the H Lyman α and O I 1304Å transit depths.
Further, their model related the observations to a few physically motivated characteristics such as the mean temperature and composition of the upper atmosphere.
They also used the observations to constrain the pressure level where H2 dissociates and estimated that the H2/H transition should occur at 0.1-1μbar.
However, these results are based on an empirical model that was simply designed to fit the observations.
One of the aims of the current paper is to show that the results are also supported by more complex physical models.
We have also attempted to establish a more comprehensive description of the thermosphere that treats it as an integral part of the whole atmosphere rather than a separate entity.
In order to do so, we developed a new one-dimensional escape model for the upper atmosphere of HD209458b that includes the photochemistry of heavy atoms and ions, and a more realistic description of heating efficiencies.
This model is described in detail by Koskinen et al. (2012) (hereafter, Paper I).
We also used results from a state-of-the-art photochemical model (Lavvas et al., in preparation) to constrain the density profiles of the observed species in the lower atmosphere.
We discuss the implications of our results in the context of different observations, and show that observations of the upper atmosphere also yield valuable constraints on the properties of the lower atmosphere.
Methods
Stellar emission lines
The interpretation of the FUV transit measurements relies on accurate characterization of the stellar emission lines and, provided that the ISM is optically thick over parts of the line profile, their absorption by the ISM.
The observed transit depths also depend on stellar variability.
In this section we discuss the properties of the line profiles, stellar variability, and absorption by the ISM.
The properties of the H Lyman α and the O I 1304Å triplet lines (hereafter, the O I lines) have been discussed in detail before (e.g., Ben-Jaffel and Hosseini, 2010; Koskinen et al., 2010), and we present only a brief summary of them here.
However, the COS observations of the C II 1335Å multiplet (hereafter, the C II lines), and the Si III 1206.5Å line (hereafter, the Si III line) (Linsky et al., 2010) have not been modeled before, and thus we constructed emission line models for these lines based on the new data.
These, and the previous models for the H Lyman α and O I lines, were used to calculate the predicted transit depths in Section 3.
The observed line-integrated transit depths for HD209458b are listed in Table 2.
H Lyman α and O I 1304Å lines
Wood et al. (2005) measured the H Lyman α emission line of HD209458 by using the high resolution echelle E140M grating on STIS, and used the details of the line profile to constrain the column density of H in the ISM along the line of sight (LOS) to HD209458.
Following K10, we adopted the reconstructed line profile and ISM fit parameters from Wood et al. (2005) for the transit depth calculations in this paper.
The short-term variability of the H Lyman α emissions from HD209458 was estimated by Ben-Jaffel (2007) who used the G140M data to derive a magnitude of ∼8.6±5.6% for this variability - although the large uncertainty of the observations makes it difficult to separate variability from noise.
There are no observations to constrain the long-term variability or center-to-limb variations of the Lyman α line on HD209458.
However, typical solar characteristics provide some guidance on these properties.
Woods et al. (2000) studied the variability of solar Lyman α emissions based on satellite observations spanning four and a half solar cycles between 1947 and 1999.
They found that the variability ranges between 1% and 37% during one period of solar rotation (27days), and the average variability during one solar rotation was found to be 9±6%.
This result agrees well with the estimated variability of the Lyman α emissions from HD209458.
The rotation period of HD209458 is estimated to be ∼10-11days (Silva-Valio, 2008).
The G140M observations covered three different transits and took place within a month and a half.
Each observation covered approximately 2h in time.
Thus the data can be affected by short-term variability and it is essential that such variability be properly accounted for.
For this reason, we compare our models with the results of Ben-Jaffel (2007, 2008) and Ben-Jaffel and Hosseini (2010) who analyzed the data in the time tag mode and accounted for variability before calculating transit depths.
Short-term variability in the H Lyman α line is mostly related to plage activity that is modulated by solar rotation while long-term variability depends on variations in both plage and active network regions (Woods et al., 2000).
In fact, it is misleading to imagine the transits in any of the FUV emission lines as the planet crossing a smooth, uniformly emitting disk.
Rather, one has to imagine the planet crossing a relatively dark disk with scattered bright regions.
For instance, during solar maximum the plages2
We include enhanced network in the definition of a plage here.
2 cover approximately 23% of the solar disk while active network covers about 8.5%.
The brightness contrasts of the plages and active network are 6.7 and 3.2, respectively, when compared to the quiet Sun (Worden et al., 1998; Woods et al., 2000).
This means that the transit depth in the H Lyman α can vary by factors of 0.4-3 as a function of time during maximum activity.
Similar variability can be expected in the other FUV emission lines.
The transit depth depends mostly on the path of the planet across the stellar disk although the plage and active network coverage can also change slightly with stellar rotation during the transit.
This highlights the importance of light curve analysis to work out transit depths - limited 'snapshots' during transit may be corrupted by the planet either covering or not covering an active region.
However, it is also possible for the transit depth to be altered even if the variations are not immediately evident in the light curves.
This complicates the analysis of the observations, and underlines the need for repeated observations at different times.
We note that the plage and active network coverage is significantly smaller during solar minimum than it is during solar maximum.
This means that the transit depth is likely to be more stable and closer to the true transit depth during stellar minimum.
Unfortunately, the activity cycle of HD209458 has not been studied in detail, and thus we have no information about it.
As we noted above, the properties of the O I lines were discussed extensively by K10 who fitted parameterized solar line profiles (Gladstone, 1992) to the O I lines of HD209458 that were observed with the STIS E140M grating (Vidal-Madjar et al., 2004).
We adopted the O I line profile and ISM parameters from K10 for this study, and do not discuss these properties further here.
We note that the plage and active network contrasts of the O I lines are 4.2 and 1.7, respectively (Woods et al., 2000).
These values are slightly smaller than the corresponding values for H Lyman α.
We note that limb darkening or brightening can also affect the observed transit depths (K10, Schlawin et al., 2010).
However, center-to-limb variations are not particularly significant in the solar H Lyman α and O I lines (Curdt et al., 2008; Roussel-Dupre, 1985).
C II 1335Å and Si III 1206.5Å lines
Linsky et al. (2010) used the medium resolution (R∼17,500) G130M grating of the COS instrument to observe four transits of HD209458b between September 19 and October 18, 2009 at wavelengths of 1140-1450Å.
During each HST visit, they observed the star during transit, at first quadrature, secondary eclipse and second quadrature.
In order to obtain the out-of-transit reference spectra, they co-added the secondary eclipse and quadrature observations from all four visits.
They also co-added the in-transit spectra from all visits to create a single spectrum.
The detections of the transits in the C II 1335Å and Si III 1206.5Å lines were compared with observations of other lines such as Si IV 1395Å in which the transit was not detected.
We note that the transit in the Si III line was not detected earlier by Vidal-Madjar et al. (2004).
However, these authors use a wider wavelength interval to calculate the line-integrated transit depth and yet report a 2σ upper limit of 5.9%, implying that the 3σ upper limit includes the transit depth observed by Linsky et al. (2010).
As we have seen, stellar variability can also cause changes in the perceived transit depth.
Therefore we are not convinced of the reality of a non-detection in the earlier STIS G140L observations.
In order to calculate transit depths in the C II and Si III lines, we created models for the stellar emission line profiles.
We note that Ben-Jaffel and Hosseini (2010) also modeled the C II lines in order to fit the transit depth in the low resolution G140L data (Vidal-Madjar et al., 2004) where the main components of the C II multiplet are unresolved.
Their model line profiles were also constrained by the high resolution STIS E140M observations of Vidal-Madjar et al. (2003) that resolve the components, although this data has low S/N and it was not used for transit observations.
Here we use the higher S/N COS observations that also resolve the main components of the C II lines to constrain the line profile models.
The atomic line parameters for the C II and Si III lines are listed in Table 1.
The C II multiplet consists of three separate emission lines.
The two lines at 1335.66Å and 1335.71Å (hereafter, the C II 1335.7Å line) are unresolved in the COS (and most solar) observations.
The core of the C II 1334.5Å line is strongly absorbed by the ISM whereas the C II 1335.7Å line is not similarly affected.
The ground state 2P1/2,3/2 of C+ is split into two fine structure levels.
Interstellar absorption of the C II 1335.7Å emissions depends on the population of the 2P3/2 level in the ISM.
In line with a similar assumption regarding the excited states of O (K10), we assumed that this population is negligible.
To estimate the emission line profile from HD209458, we fitted the solar C II 1335.7Å line from the SUMER spectral atlas (Curdt et al., 2001) with a Voigt function and adjusted the result to agree with observations (Linsky et al., 2010).
Fig. 1 shows that the model line profile agrees reasonably well with the observed line profile.
The fit parameters for this and other relevant emission lines are listed in Table 1.
Strong absorption by the ISM makes fitting the C II 1334.5Å line more complicated than fitting the C II 1335.7Å line.
Following Ben-Jaffel and Hosseini (2010), we estimated that the column density of ground state C+ in the ISM is 2.23×1019m-2 by scaling the column density of C+ measured along the LOS to Capella (Wood and Linsky, 1997) to the distance of HD209458.
We fitted the solar C II 1334.5Å line from the SUMER spectral atlas with a Voigt profile, and used the estimated column density and the results of Wood et al. (2005) to calculate absorption by the ISM.
We then varied the total flux within the line profile until the results agreed with the observations of Linsky et al. (2010).
As a result, we obtained a pre-ISM flux ratio of [C II 1334.5Å]/[C II 1335.7Å]∼0.7, which agrees well with the solar value (Curdt et al., 2001).
The model and observed line profiles are again shown in Fig. 1.
We note that the ISM is optically thick at wavelengths between 1334.54Å and 1335.58Å, which correspond to Doppler shifts of 2.2 and 11.2kms-1, respectively.
However, the observed flux in this region is not zero because of spectral line broadening in the COS instrument.
The C II lines are formed in the upper chromosphere and lower transition region of the solar atmosphere.
Similarly with H Lyman α, the brightest emissions are associated with plage activity (e.g., Athay and Dere, 1989).
The plage and active network contrasts for the C II lines are 5.9 and 1.5, respectively (Woods et al., 2000).
The large spatial variability makes it difficult to characterize center-to-limb variations.
However, different solar observations point to approximately 40% limb brightening in the 1335.7Å line and probably a similar variation in the 1334.5Å line, with the intensity rising steadily at μ>0.6 (Lites et al., 1978; Judge et al., 2003).
This brightening effect is due to the broadening of the emission line in the limb.
The Si III line arises from a transition between the 1S ground state and the 1P excited state.
Again, we fitted the Si III line profile from the SUMER spectral atlas (Curdt et al., 2001) with a Voigt profile and adjusted the resulting line profile to agree with observations of HD209458 (Linsky et al., 2010).
Fig. 2 shows the observed and model line profiles, and the fit parameters are listed in Table 1.
We assumed that the abundance of Si2+ in the ISM is negligible.
This is supported by a lack of detectable absorption by the ISM in the observed line profile.
We note that absorption by the ISM affects the interpretation of the measured transit depths only if parts of the line profile are entirely absorbed or if the properties of the ISM change between observations.
The solar Si III line has not been studied to the same degree as the C II lines.
However, some constraints on the variability and center-to-limb variations of the Si III line have been obtained.
For instance, Nicolas et al. (1977) used SKYLAB observations to study the center-to-limb variations of the Si III emissions from the quiet chromosphere.
They found that the line is strongly limb-brightened.
The total line intensity increased by a factor of 2.4 between the disk center and μ=0.73, and then again by a factor of 3 towards the edge of the limb.
This means that the total intensity in the limb is a factor of ∼7 higher than at the disk center, and the line profile is also significantly broader at μ>0.73 with a self-reversal that does not appear at the disk center.
The Si III emissions from the Sun also exhibit strong spatial and temporal variability (e.g., Nicolas et al., 1982).
Limb brightening makes the transit depth appear smaller when the planet is covering the stellar disk while a steeper transit is seen during ingress and egress (e.g., Schlawin et al., 2010).
On the other hand, if the planet covers active regions on the disk, the transit depth can appear significantly deeper (K10).
Spatial variability and limb brightening of the emission lines on HD209458 can potentially be studied through a careful analysis of the transit light curves.
Ideally, the observations should be analyzed in the time tag mode (e.g., Ben-Jaffel, 2007) to identify variations.
This type of reanalysis of the COS data is beyond the scope of this paper.
Instead, we use idealized transit depths based on a uniformly emitting stellar disk in Section 3 to show that the optical depth of the extended thermosphere in the FUV lines is significant and that the transit in the O I, C II, and Si III lines is comparable to the transit in the H I line.
We consider this sufficient for the present purposes.
Empirical model
K10 developed an empirical model to fit the UV transit observations of HD209458b and other extrasolar planets.
They argued that the H I transits can be explained in terms of three simple parameters that describe the distribution of H in the thermosphere.
These parameters are the pressure p0 where H2 dissociates (the bottom of the H layer), the mean temperature T¯ within the H layer and an upper cutoff level r∞ based on the ionization of H.
Transits in other emission lines can be explained by fitting the abundance of the heavier absorbers with respect to H.
The details of this model are discussed by K10 and are not repeated here.
Basically it calculates the transit depth observed at Earth orbit by assuming that the planet and its atmosphere constitute a spherically symmetric obstacle with a density profile in hydrostatic equilibrium up to the sonic point (when a sonic point exists).
Absorption by the ISM and spectral line broadening within the observing instrument (STIS or COS) are taken into account.
In Section 3, we compare the transit depths calculated by the empirical model with results based on the hydrodynamic model (see below) to show how the empirical model can be used to fit the data and guide the development of more complex models of the upper atmosphere.
Hydrodynamic model
We developed a one-dimensional, non-hydrostatic escape model for HD209458b to constrain the mean temperature and ionization in the upper atmosphere (Paper I).
Results from this model demonstrate that the empirical model is physically meaningful.
It solves the vertical equations of motion for an escaping atmosphere containing H, H+, He, He+, C, C+, O, O+, N, N+, Si, Si+, Si2+, and electrons.
The model includes photoionization, thermal ionization, and charge exchange between atoms and ions.
It calculates the temperature profile based on the average solar X-ray and EUV (XUV) flux and up to date estimates of the photoelectron heating efficiencies (Cecchi-Pestellini et al., 2009, Paper I).
The lower boundary of the model is at 1μbar and thus the model does not include molecular chemistry.
This is justified because molecules are dissociated by photochemical reaction networks near the 1μbar level (e.g., Garcia Munoz, 2007; Moses et al., 2011)).
The upper boundary of the model is typically at 16Rp.
We placed the upper boundary at a relatively high altitude above the region of interest in this study, which is below 5Rp.
However, we do not consider the results to be necessarily accurate above 5Rp (see Paper I for further details).
Results
Transit depths
In this section we constrain the temperature and composition of the upper atmosphere of HD209458b through a combined analysis of transit observations in the FUV emission lines.
We also compare results from a hydrodynamic model (see Section 2.3 and Paper I) with the empirical model of K10, and confirm that the latter can be used to constrain the basic properties of the density profiles in the thermosphere.
Neutral atoms
Here we show from simple arguments that the M7 model agrees in principle with the observed H Lyman α transit depth.
A similar procedure can also be used to obtain crude estimates of the transit depth for other systems, provided that the properties of the line profile and the ISM are known.
Later in this section we compare the results of the empirical model with results from the hydrodynamic model, and show that they are consistent.
To start with, the line-integrated H Lyman α transit depth of 6.6±2.3% (Ben-Jaffel and Hosseini, 2010) is consistent with a 6% transit depth at 1215.2Å (a Doppler shift of -120kms-1 from the line core) i.e., the blue peak of the observed line profile (Fig. 5 of K10).
Assuming that the extended atmosphere is spherically symmetric, a 6% transit depth measured in the blue wing (bw) of the line profile implies an optical depth of τbw≈1 at the impact parameter of 2.1Rp from the center of the planet.
Fig. 3 shows the absorption cross section of H in the Lyman α line at a temperature of 8250K.
The cross section at 1215.2Å is σbw=2×10-23m2 and thus a LOS column density of NH=5×1022m-2 at 2.1Rp is required to explain the observed absorption.
Ekenbäck et al. (2010) and Lammer et al. (2011) argued that the optical depth of H in the thermosphere of HD209458b is not significant and instead a large cloud of energetic neutral atoms (ENAs) is required to explain the H Lyman α observations.
In particular, Lammer et al. (2011) claimed that a column density of NH≈1031m-2 in the thermosphere is required for strongly visible absorption.
It is easy to see that this estimate is not correct because the wings of the line profile become optically thick with column densities much smaller than this.
It also disagrees with Garcia Munoz (2007) and Ben-Jaffel (2007, 2008) who were the first to suggest that H in the thermosphere may be sufficiently abundant to explain the observations.
This basic result has also been confirmed by more recent calculations by Trammell et al. (2011).
All of these calculations show that the optical depth of the thermosphere below 3Rp is not negligible.
The empirical model of K10 is simplified by the use of the hydrostatic approximation.
This is well justified even if the atmosphere is escaping.
In general, the density profile of the escaping gas in the thermosphere can be estimated from (Parker, 1964):(1)n(ξ)c2(ξ)=n0c02exp-∫1ξduu2W2c2exp-∫1ξduc2vdvduwhere ξ=r/r0, c2(ξ)=kT(ξ)/m, W=G Mp/r0, m is the molecular weight, and v is the vertical flow speed.
For convenience, we retained Parker's original notation.
The first integral on the right hand side of Eq.
(1) applies in hydrostatic equilibrium.
The second integral is negligible below the sonic point for transonic escape and always negligible for subsonic escape (i.e., evaporation).
The sonic point on HD209458b is always above 3Rp (Paper I) and thus the density profile of H is approximately hydrostatic at least up to 2.1Rp.
Assuming a hydrostatic atmosphere, the LOS column density of H at 2.1Rp can be estimated from:(2)NH(r=2.1Rp)≈n0expGMpmkT¯1r-1r02πrH(r)where T¯ and H are the mean temperature and scale height, respectively.
We note that the mean thermal escape parameter is:(3)X¯(r)=GMpmkT¯rAssuming that T¯=8250K and using the planetary parameters of HD209458b (Mp=0.7MJ, Rp=1.3RJ), we obtain values of X¯0=13.8 and X¯(2.1Rp)=6.6.
Thus, according to Eq.
(2), the column density of NH=5×1022m-2 that is required to explain the observations implies that n0=3.5×1017m-3.
This in turn means that p0=0.4μbar i.e., close to the lower boundary of the M7 model (here the agreement with the M7 model is obviously not exact because the transit depths in K10 are based on a complete forward model of the observed transit within the whole line profile).
The parameters of the empirical model can be compared with corresponding values derived from the hydrodynamic models presented in Paper I (see Section 2.3 here for a brief summary).
For instance, the mean temperature of the empirical model corresponds roughly to the pressure-averaged temperature of the thermosphere, which is given by (e.g., Holton, 2004):(4)Tp¯=∫p1p2T(p)d(lnp)ln(p2/p1)The hydrodynamic calculations show that the pressure averaged (mean) temperature below 3Rp based on the average solar flux varies between 6000K and 8000K for net heating efficiencies ηnet between 0.1 and 1.
This temperature is relatively insensitive to different assumptions about heating efficiencies or the upper boundary conditions.
In the reference C2 model of Paper I the mean temperature is 7200K.
This model is based on our best estimate of the heating efficiencies that are appropriate in the strongly ionized upper atmosphere of HD209458b.
With a cutoff level at 2.7Rp, we obtained a line-integrated H Lyman α transit depth of 4.7% based on the density of H in the C2 model.
This value agrees with the observations to within 1σ (Vidal-Madjar et al., 2004; Ben-Jaffel and Hosseini, 2010), but it is smaller than the transit depth of 6.6% predicted by the M7 model.
One reason for this is the lower mean temperature of the C2 model.
In order to facilitate a direct comparison between the hydrodynamic model and the empirical model, we calculated the empirical transit depth based on the mean temperature of 7200K (hereafter, the M7b model).
The line-integrated transit depth based on this model is 5.8%, which is still higher than the transit depth based on the C2 model.
Fig. 4 shows the density profiles of H, H+, O, and O+ from the C2 model, and the density profile of H from the M7b model.
The difference between the transit depths based on the empirical and hydrodynamic models arises because the C2 model has large temperature gradients (Paper I) and a gradual H/H+ transition rather than a sharp cutoff.
The difference does not arise because the density profile of the C2 model deviates from hydrostatic equilibrium.
Given the temperature gradient in the model, the density profile is almost exactly in hydrostatic equilibrium below 3Rp.
In fact, the neutral density profile of the C2 model is better represented by a mean temperature of 6300K (not shown).
This implies that the correspondence of the mean temperature of the empirical model and the pressure averaged temperature of the hydrodynamic model is relatively good but not exact.
A better agreement between the transit depths based on the C2 and M7b models is obtained with the cutoff level of the C2 model at 5Rp (see Table 2).
Fig. 5 shows the line-integrated transit depth within the H Lyman α line profile as a function of the cutoff level for the C2 model.
This figure indicates that the transit depth increases less steeply with altitude above 4Rp than it does below this cutoff level, and saturates near 5Rp.
This is a natural consequence of the fact that the LOS column density decreases approximately exponentially with altitude in the lower thermosphere and the wings of the line profile become optically thin at high altitudes.
We note that K10 already compared the results based on the M7 model with in-transit transmission as a function of wavelength within the H Lyman α line profile and the light curve derived from the G140M data (Ben-Jaffel, 2007, 2008).
It is not necessary to repeat a similar comparison here because spherically symmetric models that predict line-integrated transit depths that agree with the measured values are generally compatible with both the G140L and G140M data.
This is partly because the uncertainty of the individual data points within the line profile and the light curve is large, and thus they do not strongly constrain the properties of the atmosphere.
According to Fig. 4, the H/H+ transition in the C2 model occurs near 3.1Rp.
The exact location depends on photochemistry and vertical velocity, and generally the transition occurs near or above 3Rp (Paper I).
With a fixed pressure at the lower boundary, a faster velocity leads to a transition at a higher altitude.
These results disagree with Yelle (2004) and Murray-Clay et al. (2009) who predicted a lower transition altitude, but they agree qualitatively with the solar composition model of Garcia Munoz (2007).
The density profiles of O and O+ are strongly coupled to H and H+ by charge exchange.
As a result, the O/O+ transition occurs generally near the H/H+ transition.
For instance, in the C2 model it is located near 3.4Rp.
We note that significant ionization of H and O above 3Rp is anticipated by K10 and there is good agreement on this between the empirical and hydrodynamic models.
The detection of O at high altitudes constrains the mass loss rate and the ionization state of the upper atmosphere (see Section 3.2).
However, the large uncertainty in the observations means that repeated observations are required to confirm the transit depth.
The M7 model of K10 with a solar O/H ratio (Lodders, 2003) yields an O I transit depth of 3.9%, which is within 1.5σ of the observed value and therefore a satisfactory fit to the data.
This value agrees well with the O I transit depth based on the C2 model (see Table 2).
K10 argued that a higher transit depth is possible if the mean temperature is higher and/or if the O/H ratio is enhanced with respect to solar.
The hydrodynamic calculations indicate that the latter option is more favorable because higher temperatures lead to stronger ionization of O and may not help to significantly enhance the transit depth.
Indeed, the predicted transit depth agrees with the observations to better than 1σ if the O/H ratio is enhanced by a factor of 5 relative to solar (see the MSOL2 model in Table 2).
We have now verified that the empirical model can be used to constrain the mean temperature and extent of the absorbing layer in the thermosphere of HD209458b.
In particular, the comparison of the empirical model with the hydrodynamic model shows that the results of K10 were not affected by the simplifying assumption of hydrostatic equilibrium.
We note that the purpose of the empirical model is to identify physical processes that might otherwise be missed in more complex models that are often based on a large number of uncertain assumptions.
The results from any model can now be compared with the observations by identifying the limits of the absorbing layer and calculating the global pressure averaged temperature within that layer.
The values can then be compared with the parameters of the best-fit empirical model.
Ions
Fig. 6 shows the density profiles of the carbon and silicon ions in the thermosphere of HD209458b based on the C2 hydrodynamic model (Paper I).
The major carbon and oxygen-bearing species in the lower atmosphere, CO and H2O, dissociate near the 1μbar level (e.g., Moses et al., 2011, or Lavvas et al., in preparation).
Thermochemical calculations indicate that SiO is the dominant silicon-bearing gas on HD209458b (Visscher et al., 2010).
The detection of Si2+ in the upper atmosphere implies that the formation of silicon clouds in the lower atmosphere is suppressed (see Section 3.3), and SiO is also dissociated near 1μbar either thermally or by photochemistry.
Thus we assumed that only atomic carbon and silicon are present in the thermosphere, initially with solar abundances (Lodders, 2003).
According to Fig. 6, the C/C+ transition occurs at a much lower altitude of 1.2Rp than the H/H+ and O/O+ transitions.
Silicon is also almost fully ionized at the lower boundary of the model, and the Si2+/Si+ ratio is about 10% below 3Rp.
These results are in qualitative agreement with the observations, because they show that H and O are mostly neutral below 3Rp whereas C and Si are mostly ionized, and a significant abundance of Si2+ is possible.
However, it is also useful to explore if the results from the models are in quantitative agreement with the observations and if not, to adjust the model parameters as necessary to explain the data.
With a cutoff level at 2.7Rp, we obtained line-integrated transit depths of 2.3%, 3.6%, and 2.5% in the C II 1334.5Å, C II 1335.7Å and Si III lines, respectively, based on the C2 model.
Here we assumed that the population of the 2P levels of C+ are in LTE.
We also calculated empirical transit depths based on the M7 model.
In order to do this we assumed that both C and Si are ionized at the lower boundary, and that 10% of silicon is Si2+.
With these assumptions we obtained C II 1334.5Å and 1335.7Å transit depths of 2.7% and 4.2%, respectively, and a Si III transit depth of 3%.
These values agree well with the C2 model, and further demonstrate the consistency of the empirical and hydrodynamic models.
However, they deviate from the observed values by more than 2σ.
The cutoff level of the empirical model is somewhat arbitrary.
For neutral species it is partly based on ionization (K10), but this criterion obviously does not apply to ions.
With a cutoff level at 5Rp for the ions only, the M7 model yields line-integrated transit depths of 3.9%, 8%, and 5.8% in the C II 1334.5Å, C II 1335.7Å, and Si III lines, respectively, if 40% of silicon is Si2+.
These values agree with the observed values to better than 2σ.
Similarly, by extending the cutoff level of the C2 model to 5Rp, we obtained transit depths of 3.2%, 6.7%, and 4.6% in the C II 1334.5Å, C II 1335.7Å, and Si III lines, respectively (see Table 2).
These values deviate from the observed values by 2σ, 0.9σ, and 2.6σ, respectively.
The transit depths predicted by the M7 model are higher partly because the mean temperature of 8250K is higher than the corresponding temperature in the C2 model (Paper I).
This also leads to the higher Si2+/Si+ ratio that we used here.
It is not clear if the apparent disagreement between the models and some of the observations needs to be taken seriously.
Stellar activity and other uncertainties mean that the true transit depth can differ from measured values by a significant factor (see Section 2.1).
Further, the C2 model agrees with the line-integrated H Lyman α and C II 1335.7Å transit depths to within 1σ, and with the O I and C II 1334.5Å lines to within 2σ.
Thus we could argue that the present observations are roughly consistent with solar abundances and heating based on the average solar XUV flux.
Nevertheless, we explore the apparent disagreement between the C2 model and the observations further below.
This disagreement is limited to the O I, C II 1334.5Å, and Si III lines.
Fig. 7 shows the observed in-transit flux differences in the C II and Si III lines as a function of wavelength together with different model predictions.
The observations indicate that the transit depths based on the C2 model fall short of the observed values because the model underestimates the width of the absorption lines.
Linsky et al. (2010) argued that there is velocity structure within the line profiles near Doppler shifts of -10kms-1 and 15kms-1 that accounts for broad absorption.
However, the uncertainty of the individual data points is too large to constrain the shape of the absorption lines in detail.
We agree with Linsky et al. (2010) that the presence of velocity structure needs to be confirmed by future observations.
Thus the additional absorption could also arise from spectral line broadening.
A higher mean temperature leads to higher transit depths.
Therefore we generated a new hydrodynamic model by multiplying the average solar flux by a factor of 2 and assumed a net heating efficiency of ηnet=0.5 (hereafter, the SOL2 model).
This model agrees to better than 1σ with all of the observed transit depths apart from the O I and C II 1334.5Å lines (see Fig. 7 and Table 2).
The mean temperature of the model below 3Rp is 7400K, which is lower than the mean temperature in the M7 model.
However, absorption by the SOL2 model is strengthened by velocity dispersion within the escaping plasma that is not included in the M7 model.
In general, the outflow velocity of the SOL2 model is significantly higher than the velocity in the C2 model.
The model also predicts a mass loss rate of 108kgs-1, which is twice as high as the mass loss rate based on the C2 model.
This proves that a higher stellar XUV flux or a corresponding alternative energy source can explain the observations.
An enhancement of the average solar flux by a factor of 2 is not unreasonable, and would roughly correspond to solar maximum conditions.
In Paper I we noted that the stellar XUV flux, or the corresponding alternative heat source, would have to be 5-10 stronger than the average solar flux to produce a mean temperature between 8000 and 9000K.
Under such circumstances, the predicted transit depths in the C II and Si III lines would obviously be even higher than the values predicted by the SOL2 model.
Indeed, higher temperatures broaden absorption in the wings of the line profiles and may help to explain the in-transit flux differences better (see Fig. 7).
However, the energy input and temperature in the model cannot be increased without bound.
Higher temperatures and flux lead to more efficient ionization of the neutral species, and as a result the transit depths in the H Lyman α and O I lines begin to decrease.
Also, mass loss rates of 109-1010kgs-1 lead to the loss of 10-100% of the planet's mass over the estimated lifetime of the system, and this probably limits reasonable energy inputs to less than ∼10 times the solar average on HD209458b.
In addition to higher temperature and velocity, supersolar abundances of O, C, and Si can also lead to higher transit depths.
This option is interesting because it also allows for a higher transit depth in the O I lines.
As an example, we generated the MSOL2 model by enhancing the solar O/H, C/H, and Si/H abundances in the hydrodynamic model by a factor of 5.
As a result, we obtained transit depths that agree with nearly all of the observed line-integrated transit depths to better than 1σ (see Table 2).
However, the MSOL2 model overestimates the line-integrated C II 1335.7Å transit depth, and generally overestimates absorption within the cores of the C II and Si III lines.
This could imply that a higher temperature or some other source of additional broadening is a better explanation of the Si III and C II transit depths while a supersolar O/H ratio is required to match the measured O I transit depth.
However, the data points in Fig. 7 and the observed O I transit depth are too uncertain and do not allow for firm constraints on this.
Enrichment of heavy elements is a common feature on the gas and ice giants in the Solar System.
For instance, the C/H, N/H, S/H, Ar/H, Kr/H, and Xe/H ratios in the atmosphere of Jupiter are all enriched by factors of 2-3 with respect to solar abundances (e.g., Mahaffy et al., 2000; Wong et al., 2004).
On Saturn, on the other hand, the C/H ratio is enriched by a factor of 10 (Flasar et al., 2005; Fletcher et al., 2009).
Enrichment by factors of 4-20 is expected in the N/H and S/H ratios, although condensation of NH3 and H2S in the deep atmosphere of Saturn makes it difficult to constrain the abundances precisely (see Fouchet et al. (2009) for a review).
On Neptune and Uranus the C/H ratio is believed to be enriched by factors of 30-50 (e.g., Owen and Encrenaz, 2003; Guillot and Gautier, 2007)), and similar enrichments are possible in the abundance ratios of some of the other heavy elements.
Substantial enrichment of heavy elements with respect to solar abundances is therefore also feasible in EGP systems even if the metallicity of the star is close to solar.
Unfortunately, we cannot use the current observations to constrain the elemental abundances of the atmosphere with accuracy.
In this regard, the large uncertainty of the observations is unfortunate, because similar observations can potentially be used to estimate them.
The dissociation of molecules at the relatively high pressure of 1μbar and the lack of diffusive separation mean that the abundances of the heavy atoms and ions are simply dependent on the elemental abundances and ionization rates.
Observations of the neutral species can therefore be used to constrain the temperature and ionization state, and thus the elemental abundances of the heavy species, but the S/N of the current data does not allow for strong constraints.
It is interesting to note that while the velocity structure of the escaping plasma can lead to broader absorption that helps to explain the transit depths, it is not necessarily detectable in the data.
For instance, Fig. 7 shows the transit depths based on the SOL2 model that has a relatively high radial velocity reaching 11kms-1 by 5Rp.
The velocity structure is not detectable because the optical depth of the high velocity material is not sufficient, the LOS velocity at the limb of the planet is slower than the radial velocities in general, and because spectral line broadening within the COS instrument smooths the structure out of the line profiles.
If the presence of velocity structure is confirmed in the data (Linsky et al., 2010), it probably implies that there is detached, optically thick plasma moving at large velocities around the planet.
If this turns out to be the case, interaction with the stellar wind probably plays a role in giving rise to the observed absorption.
Such interaction may also produce turbulence that can broaden the absorption further (e.g., Tian et al., 2005).
However, we note that non-thermal broadening such as that proposed by Ben-Jaffel and Hosseini (2010) does not appear to be necessary to explain the current observations.
Ionospheric escape
The escape of heavy atoms and ions has interesting consequences for the nature of the upper atmosphere.
Here we discuss these consequences based on simple analytic arguments, and without explicit use of any complex models.
The detection of heavy neutral species can be used to constrain the mass loss rate while the detection of heavy ions outside the atmosphere of the planet potentially constrains the strength of the planetary magnetic field.
For instance, Hunten et al. (1987) derived an expression for the crossover mass limit mc for a neutral species s to be dragged along by an escaping neutral species t of mass mt<ms:(5)mc=mt+kTFtnDstxtg0r02where Ft is the flux (s-1sr-1) of species t, xt is the volume mixing ratio, g0 is gravity at the lower boundary of the model region, and the mutual diffusion coefficient can be roughly estimated from:(6)nDst=1.52×10181Ms+1Mt1/2Tcm-1s-1where the masses M are in amu.
We used Eq.
(5) to estimate the mass loss rate that is required to drag neutral O to the exosphere of HD209458b.
Assuming that xt∼1, g0=10ms-1, r0=Rp, T=7200K, andnDst=1.3×1022m-1s-1, we obtain Ft≈2.8×1032s-1sr-1.
This implies a minimum mass loss rate of 6×106kgs-1.
The ionosphere of HD209458b is mostly neutral below 3Rp but even weak ionization can lead to frequent Coulomb or ion-neutral collisions that enable heavy ions or atoms to escape more efficiently.
In order to illustrate the role of different collisions in transporting O and Si+, Fig. 8 shows the collision frequencies for these species with H and H+ as a function of altitude based on the C2 model (Paper I).
We used approximate expressions for resonant and non-resonant ion-neutral collisions, and Coulomb collisions from Schunk and Nagy (2000) to calculate the momentum transfer collision frequencies.
The collision frequency between two neutral species, on the other hand, was estimated from the mutual diffusion coefficient as:(7)νst=5.47×10-111Ms+1Mt-1/2ntTMswhere the number density nt is in cm-3.
The results indicate that the transport of O depends on collisions with H below 3.5Rp whereas the transport of Si+ depends on collisions with H+ at all altitudes.
Oxygen is the heaviest neutral species detected in the escaping atmosphere, and this implies that the mass loss rate from HD209458b is Ṁ>6×106kgs-1.
This result agrees with Vidal-Madjar et al. (2003) although it is less model-dependent and based on different criteria.
The dominance of Coulomb collisions means that the heavy ions can escape even if the mass loss rate is lower than this.
Our models predict mass loss rates of Ṁ≈5×107kgs-1 (Table 2) and thus diffusive separation does not take place in the thermosphere of HD209458b for neutral species with masses less than ∼130amu.
We note that this is the case even if escape is subsonic.
In fact, Eq.
(5) was originally derived for subsonic escape under the diffusion approximation although it is also valid for supersonic escape (Zahnle and Kasting, 1986).
We used the collision frequencies to derive expressions for the ion fractions fi=nH+/nH at which ion-neutral and Coulomb collisions become important.
The ratio of the non-resonant neutral-ion to neutral-neutral collisions exceeds 10% when(8)fi≈10-24TMsiγse2where i denotes H+, s denotes the colliding species, γs is the neutral polarizability and all units are in cgs.
However, the collision of O with H+ is resonant and in this case the required ratio differs slightly from the above expression.
The ratio of the Coulomb to non-resonant ion-neutral collisions, on the other hand, exceeds 10% when(9)fi≈4.24×1011T3/2Zs2Zi2Msnγne2Msiwhere i denotes H+ (or the dominant ion) and n is H (or the dominant neutral).
For Si+ this fraction is fi≈10-4 (with γH=6.7×10-25cm3).
These equations can be used to determine if Eq.
(5) is valid, or if more complex plasma models are required.
Trammell et al. (2011) argued that HD209458b could have a strong planetary magnetic field that can impede the escape of ions from equatorial regions and restrict it to the polar regions.
Although the magnetic field does not directly interfere with the escape of the neutral atoms, the trapped ions can stop them from escaping if the neutral-ion collision frequency is sufficiently high.
Unfortunately, transit observations are not spatially resolved and they cannot be used directly to determine if escape is limited to the poles, or if the atmosphere is also escaping over the equator.
However, the transit depths depend on the size of the optically thick obstacle covering the star.
If mass loss is suppressed at low and mid-latitudes, the heavy species are no longer mixed into the upper atmosphere other than at the poles where they are allowed to escape.
This means that the cross-sectional area covered by the ions shrinks and may become insufficient to explain the observations.
Even if the plasma spreads to cover a larger area after being ejected from the poles, it is diluted in the process and thus it is not clear if the resulting cloud would have sufficient optical depth to be detectable.
Assuming that the charged particles escape the Roche lobe of the planet unimpeded at the equator, we estimated an upper limit for the planetary magnetic field by evaluating the magnetic moment that allows them to do so.
In order to estimate this limit, we calculated the plasma β and the inverse of the second Cowling number (Co-1) below 5Rp in the C2 model from:β=2μ0pB2,Co-1=μ0ρv2B2where v is the vertical velocity.
Assuming a dipolar magnetic field, we obtained a limiting magnetic moment of 3.2×1025Am2 or 0.04mJ (mJ=1.5×1027Am2 is the magnetic moment of Jupiter).
This magnetic moment ensures that β>10 below 5Rp and that Co-1 reaches 10 by 5Rp.
Magnetic moments of mp≲0.04mJ agree quite well with the scaling laws discussed by Griesmeier et al. (2004).
We note that the limiting moment produces an equatorial surface field that is approximately four times lower than the surface field of the Earth.
Cloud formation on HD209458b
We have shown that a substantial abundance of silicon in the upper atmosphere is required to produce a detectable transit in the Si III line.
If the silicon ions originate from the atmosphere of the planet, at least a solar Si/H ratio is necessary.
According to thermochemical equilibrium models, silicon should condense into clouds of forsterite (Mg2SiO4) and enstatite (MgSiO3) in the lower atmosphere of HD209458b (Visscher et al., 2010).
If the formation of enstatite is suppressed, silicon should condense to form quartz (SiO2) instead.
In any case, condensation is expected to remove almost all of the silicon from the upper atmosphere.
The detection of Si2+ implies that the abundance of silicon in the thermosphere is substantial, and thus the condensation of silicon does not take place in the atmosphere of HD209458b.
This has significant implications for the structure and dynamics of the atmosphere.
Sing et al. (2008a,b) analyzed the absorption line profile of Na in the atmosphere of HD209458b in detail and argued that the abundance of Na is depleted above the 1mbar level.
They suggested that this is due to the condensation of sodium into Na2S, although ionization could not be ruled out decisively.
Based on the condensation temperature of Na, they argued that the temperature in the upper atmosphere of HD209458b near 1mbar is 420±190K.
This temperature is significantly lower than the outcome of typical radiative transfer models for HD209458b (e.g., Showman et al., 2009).
We note that the condensation temperature of forsterite and enstatite is higher than 1300K at 1mbar (Visscher et al., 2010).
Because silicon clouds do not form, the Na2S clouds cannot form either.
Further, the formation of Na2S relies on H2S, which is dissociated above the 1mbar level (Zhanle et al., 2009).
This implies that any depletion of Na at high altitudes is most likely due to photoionization and/or thermal ionization.
Fig. 9 shows a dayside temperature (T-P) profile for HD209458b based on Showman et al. (2009).
This profile is similar to the dayside temperature profile adopted by Moses et al. (2011).
The figure also shows the condensation curves for forsterite and enstatite.
The T-P profile crosses the condensation curve for forsterite below the 100bar level.
However, the temperature profile in the deep atmosphere is uncertain, and the current profile only barely crosses the condensation curve.
Also, the formation of forsterite ties only a fraction of the total abundance of silicon into clouds (Visscher et al., 2010).
However, the T-P profile crosses the condensation curves for both forsterite and enstatite in a 'cold trap' near 10mbar.
To prevent this, the temperature in the cold trap would have to be T≳1600K.
This is not totally unbelievable but probably unlikely.
In any case, the temperature is close enough to the condensation curves so that moderate vertical transport might be able to preserve silicon above the cold trap.
Spiegel et al. (2009) explored a range of turbulent diffusion coefficients Kzz that would be required to prevent the condensation of TiO and VO in the atmospheres of different EGPs, including HD209458b.
In fact, they assumed that condensates form in the cold trap but are then transported to higher altitudes where they evaporate.
Their work ignores the detailed chemistry of condensation, and thus the results are simply based on the ratio of Kzz to the diffusion coefficient estimated fromDp≈vpHwhere vp is the particle settling velocity and H is the pressure scale height.
The formation of condensates is probably too complicated for such a simplistic treatment, but the results provide some guidance on the value of Kzz that is required to lift the condensates from the cold trap.
We calculated Dp for forsterite grains with a radius of 0.1μm and density3
This is the density of the material in the particles, not the density of the particles in the atmosphere.
3 of 3200kgm-3 (Fortney et al., 2003; Cooper et al., 2003).
The settling velocity for such particles in the cold trap is vp≈3×10-3ms-1 and Dp≈2×103m2s-1.
Given that the upper edge of the cold trap is near the 1mbar level where Dp is higher, Kzz≳105m2s-1 is sufficient to prevent the settling of the cloud particles in the lower atmosphere.
We note that mass loss does not help to enhance the mixing of the particles at low altitudes significantly.
The vertical velocity based on the mass loss rate of 107kgs-1 is only 4.8×10-7ms-1 at 10mbar and 8×10-3ms-1 at 1μbar.
Estimating Kzz on extrasolar planets is very difficult, partly because there is no agreement on exactly what physical processes this parameter describes even in much more sophisticated Solar System applications.
The most recent estimates for HD209458b are based on assuming that Kzz∼v¯H, where v¯ is the rms vertical velocity from circulation models (e.g., Showman et al., 2009).
Based on the GCMs of Showman et al. (2009) and an assumed density dependence with altitude, Moses et al. (2011) estimated that the high pressure value of Kzz=106m2s-1, which implies that Kzz≳107m2s-1 at p≲10mbar.
If such high values are realistic, turbulent mixing is almost certain to prevent the settling of the condensates and to preserve gaseous silicon in the upper atmosphere.
We note that the above requirements on the value of Kzz may in fact be overestimated because they are based on the assumption that clouds particles form in the cold trap.
Cloud formation has to be studied in the context of a photochemical model that includes the chemistry of condensation.
If the chemical timescale is longer than the transport timescale, the cloud droplets may not form in the first place before the gas escapes from the cold trap.
Also, the temperature structure near the cold trap needs to be constrained in greater detail.
The formation of condensates is a complex problem that will be studied in future work (Lavvas et al., in preparation) in order to better constrain the required values of Kzz.
For our purposes it is sufficient to note that the current estimates of Kzz imply, in agreement with the observations, that silicon clouds do not form in the upper atmosphere of HD209458b.
Discussion and conclusions
We have used multiple observational constraints and theoretical models to characterize the upper atmosphere of HD209458b.
Contrary to many of the earlier studies, we did not treat the thermosphere independently of the rest of the atmosphere.
In fact, we have shown that observations of the upper atmosphere can be used to obtain useful constraints on the characteristics of the lower atmosphere.
This is important because the extended upper atmospheres of close-in EGPs produce much larger transit depths than those arising from the lower atmosphere.
In this work, we concentrated mostly on the FUV transit measurements.
Future work should explore the possibility of extending the range of possible observations to other wavelength regions, as well as obtaining repeated observations in the FUV lines.
Theoretical models should be developed to support new observations and to clarify the interpretation of the existing data.
In agreement with K10, we showed that the H Lyman α transit observations (Vidal-Madjar et al., 2003, 2004; Ben-Jaffel, 2007, 2008; Ben-Jaffel and Hosseini, 2010) can be fitted with a layer of H in the thermosphere that is described by three simple parameters.
These are the pressure at the bottom of the H layer, the mean temperature in the thermosphere, and a cutoff level due to ionization.
The most important parameters are the pressure at the lower boundary and the mean temperature.
Because H is the dominant species in the thermosphere, the data can be used to estimate the temperature of the thermosphere.
Choosing a lower boundary pressure of 1μbar based on the location of the H2/H dissociation front in recent photochemical models (Moses et al., 2011) and observational constraints (France et al., 2010), we estimated a mean temperature of about 8250K in the thermosphere below 3Rp.
However, the uncertainty of the observations is large, and the 1σ upper and lower limits on this temperature are approximately 6000K and 11,000K, respectively.4
This uncertainty does not include the possible uncertainties in the other parameters of the fit.
4
We used a hydrodynamic model that treats the heating of the upper atmosphere self-consistently and the average solar XUV spectrum (Paper I) to show that a mean temperature of 8250K in the upper atmosphere below 3Rp is higher than the maximum temperature allowed by stellar heating.
Given that a net heating efficiency equal to or higher than 100% is unrealistic, this temperature requires either a non-radiative heat source, additional opacity, or it implies that the XUV flux of HD209458 is higher than the corresponding solar flux.
Interestingly, this would also imply that the mass loss rate could be higher by a factor of 2 or more than previously anticipated (e.g., Paper I).
However, the uncertainty in the H Lyman α observations also allows for a slightly lower temperature of 7200K that is typical of stellar heating based on the average solar XUV flux and our best estimate of the net heating efficiency.
Therefore the temperature implied by the H Lyman α observations and the mean temperature of the basic hydrodynamic models are in good agreement.
We note that the temperature in the lower thermosphere near the 1μbar region has been constrained previously by Vidal-Madjar et al. (2011a,b) who used the Na D lines to constrain the density and temperature profiles in the atmosphere of HD209458b.
Their results point to a temperature of ∼3600K that is actually higher than the temperature at the lower boundary of our hydrodynamic model and consistent with a high mean temperature at lower pressures in the thermosphere.
However, we caution the reader that the temperature profile based on the Na D lines may not be accurate.
This is because Vidal-Madjar et al. (2011a) used a simple expression for the optical depth due to Na that is based on the scale height of the atmosphere [their Eq.
(1)].
This expression is only valid if Na is uniformly mixed with H2.
Since the authors also argue that Na is depleted (i.e., its mixing ratio changes with altitude) above ∼10mbar, its density scale height cannot be used to estimate temperatures accurately.
The detection of O in the thermosphere allowed us to constrain the mass loss rate from HD209458b based on the crossover mass concept formulated by Hunten et al. (1987).
This is because O is transported to high altitudes in the upper atmosphere primarily by collisions with H.
We found that a minimum mass loss rate of 6×106kgs-1 is required to prevent the diffusive separation of O.
Our hydrodynamic calculation based on the average solar XUV flux predicts a mass loss rate close to 5×107kgs-1.
This implies that species with a mass up to ∼130amu are uniformly mixed in the thermosphere.
Similar constraints do not apply to heavy ions.
They are transported to high altitudes by Coulomb collisions with H+ that are much more efficient in preventing diffusive separation compared to collisions of neutral atoms with H.
In agreement with K10, we found that the presence of O with a solar abundance and a temperature based on the H Lyman α measurements explains the transits observed in the O I lines.
Our models predict a line-integrated transit depth of approximately 4% that deviates from the measured transit depth by 1.5σ and thus agrees with the uncertainty of the observations.
However, the predicted transit depths fall systematically short of the measured value.
K10 suggested that a higher transit depth is possible if the O/H ratio is supersolar, the temperature of the thermosphere is higher than expected, and/or the observations probe escaping gas outside the Roche lobe of the planet, and we agree with their conclusions.
As we explain below, we found that the same is true for the other heavy species.
In order to calculate predicted transits in the C II and Si III lines, we created model emission line profiles for HD209458 based on SUMER observations of the Sun (Curdt et al., 2001) adjusted to the observations of HD209458 (Linsky et al., 2010).
We did not find evidence for significant absorption by the ISM in the C II 1335.7Å or the Si III line.
Parts of the C II 1334.5Å line, on the other hand, are optically thick in the ISM and we took this into account in our models.
We note that resolved observations of the emission line profiles can be used to characterize the composition of the ISM and the activity of the host star.
This information is a valuable byproduct of the FUV transit observations that typically need to be repeated several times.
With solar abundances, the same models that agree with the H Lyman α transit observations tend to underestimate the transit depths in the C II and Si III lines.
Similarly with the O I lines, higher transit depths in the C II and Si III lines are possible if the mean temperature of the absorbers is higher than expected and/or the C/H and Si/H ratios are supersolar.
With solar abundances, a 1σ agreement with the observed line-integrated transit depths is possible if the stellar XUV flux (or the stellar flux combined with an additional heat source) is higher than or equal to two times the average solar XUV flux.
This corresponds to typical solar maximum conditions, and it is quite interesting that a similar enhancement may be required to explain the relatively high mean temperature in the thermosphere that we estimated earlier.
Alternatively, with heating based on the average solar flux, an agreement with the observations is possible with the O/H, C/H and Si/H enhanced by a factor of ∼3-5 relative to solar abundances (Lodders, 2003).
In any case, the atmosphere is escaping with a minimum mass loss rate given above.
This is evident because of both the high temperature of the thermosphere and the detection of heavy species at high altitudes.
We note that the transit observations are affected by stellar variability that can lead to significant changes in the observed transit depths.
Spatial variations of intensity on the stellar disk during maximum activity or limb brightening may render the transit occasionally undetectable, or enhance it by a significant factor.
Generally, observations obtained during periods of minimum activity are more reliable.
Ben-Jaffel (2007) characterized the short-term variability of HD209458 in the H Lyman α line, but the variability of the O I, C II, and Si III are poorly characterized.
This introduces an additional element of uncertainty into the transit depths that needs to be constrained by repeated observations of the star and the transits in the FUV lines.
It also means that the qualitative explanation of the present observations that is based on heating by the average solar XUV flux and solar abundances (Paper I) may be sufficient even if the predicted transit depths in the O I, C II, and Si III lines do not exactly match the current measurements.
On the other hand, if higher transit depths are confirmed, they can be used to further constrain the mean temperature and abundances as we have shown.
The detection of heavy ions escaping the atmosphere can potentially be used to constrain the magnetic field strength of the planet.
We estimated an upper limit of 0.04mJ for the magnetic moment that allows the heavy ions to escape unimpeded at the equator.
The estimated magnetic moment agrees with the scaling laws for the magnetic field strengths of tidally locked close-in EGPs by Griesmeier et al. (2004).
On the other hand, a strong magnetic field inhibits the flow of ions from the equator and may only allow for escape at the poles (e.g., Trammell et al., 2011).
If the neutral-ion collision frequencies are sufficient, the trapped ions may also prevent the neutral atoms from escaping.
We note that a uniform upward flux is required to mix the atmosphere, and escape at the poles may not be sufficient to create a large enough obstacle to explain the transits in the O I, C II, and Si III lines.
Detailed models of the magnetosphere that include the heavy species are required to assess if this is the case or not.
The detection of Si2+ in the upper atmosphere means that silicon cannot condense to form enstatite, forsterite, or other condensates in the lower atmosphere.
This is clear because at least a solar abundance of silicon in the thermosphere is required to explain the large transit depth in the Si III line.
According to the calculated temperature profiles for HD209458b (e.g., Showman et al., 2009), condensation is expected in a cold trap near the 10mbar level.
Provided that the temperature is not much higher than expected near the cold trap, efficient mixing is required to prevent condensation.
Following an argument similar to that of Spiegel et al. (2009), we estimated that an eddy mixing coefficient of Kzz≳105m2s-1 below 1mbar is sufficient to prevent the condensation of forsterite and enstatite in the cold trap.
We note that much higher values than 105m2s-1 were assumed by recent photochemical models by Garcia Munoz (2007) and Moses et al. (2011).
A stratospheric temperature inversion may also be necessary to suppress condensation.
This is because the cold trap cannot extend to much lower pressure than 1mbar or the required values of Kzz become too high.
Also, the lack of condensation implies that the temperature of the lower atmosphere should be relatively high.
The detection of silicon in the upper atmosphere thus provides further evidence for a stratosphere on HD209458b that was first proposed by Knutson et al. (2008).
We note that existing radiative transfer models do not account for molecular opacity at UV wavelengths or visible absorbers potentially generated by photochemistry (e.g., Zhanle et al., 2009).
Our results provide motivation for more detailed models of thermal structure below the thermosphere that can constrain the chemistry of the lower atmosphere.
Once this is achieved, better constraints on the dynamics can be derived.
We also address an old problem related to the atmosphere of HD209458b.
Based on the observed transits in the Na D lines, Charbonneau et al. (2002) argued that Na is depleted in the atmosphere because their cloudless solar composition model predicted significantly deeper absorption in the D lines.
In addition to photoionization, molecular chemistry, and low primordial abundance of Na, they suggested that the formation of high altitude clouds can explain the observed depletion.
Later Sing et al. (2008a,b) found further evidence for the depletion of Na above the 3mbar level, and argued that condensation of Na2S is the most likely explanation.
The fact that silicon does not condense implies that condensation of Na2S is also unlikely.
The observed depletion is therefore most likely due to photoionization and/or thermal ionization.
However, the density profile and ionization state of Na should be studied in the context of molecular and ion chemistry below the 0.1μbar level to verify that this is the case.
Acknowledgments
We thank H.
Menager, M. Barthelemy, N.
Lewis, and D.S.
Snowden for useful discussions and correspondence, and A.
Showman and N.
Lewis for sharing some of their temperature profiles.
We also acknowledge the "Modeling atmospheric escape" workshop at the University of Virginia and the International Space Science Institute (ISSI) workshop organized by the team "Characterizing stellar and exoplanetary environments" for interesting discussions and an opportunity to present our work.
The calculations for this paper relied on the High Performance Astrophysics Simulator (HiPAS) at the University of Arizona, and the University College London Legion High Performance Computing Facility, which is part of the DiRAC Facility jointly funded by STFC and the Large Facilities Capital Fund of BIS.
SOLAR2000 Professional Grade V2.28 irradiances are provided by Space Environment Technologies.
This research was supported by the National Science Foundation (NSF) through grant AST 1211514.

Effect of oxygen partial pressure on the electrochemical impedance of La0.8Sr0.2MnO3-δ/Zr0.92Y0.08O2 porous composite anodes in solid oxide electrolysis cell

The La0.8Sr0.2MnO3-δ was synthesized by a solid-state reaction method. Firstly, the appropriate ratios of Lanthanum nitrate (La(NO3)3*6H2O, AR), strontium carbonate (SrCO3, AR), manganous carbonate (MnCO3, AR) were thoroughly mixed with alcohol using a ball mill. Subsequently, the alcohol was removed by heating the slurry at 120 degC in air for 12 h. Next, the resulting dry powder was then calcined at 1000 degC for 6 h in air. The obtained powders were ground, milled and finally sifted through 300 mesh. Finally the LSM powders were mixed with the YSZ (TZ-8Y, Tosoh Corporation, Japan) powders at a mass ratio of LSM/YSZ = 1.0 by ball milling for 10 h and then the composite powders were obtained.


Sr0.7Ho0.3CoO3-δ as a potential cathode material for intermediate-temperature solid oxide fuel cells

Polycrystalline Sr0.7Ho0.3CoO3-δ (SHCO) was prepared from a stoichiometric mixture of SrCO3, Co3O4 and Ho2O3[8]. The mixtures were ground and sintered at 1100 degC for 12 h followed by re-grinding and another firing at 1100 degC for 48 h in air. The obtained product was characterized by powder X-ray diffraction (XRD) for phase identification with a Philips X'pert diffractometer (Cu Kα radiation, λ = 1.5418 Å) in Bragg-Brentano reflection geometry. The XRD data were recorded in the 2θ range of 20-90deg with an increment of 0.02deg and a dwell time of 10 s.

Scalable expansion of human induced pluripotent stem cells in the defined xeno-free E8 medium under adherent and suspension culture conditions

Abstract
Large-scale production of human induced pluripotent stem cells (hiPSCs) by robust and economic methods has been one of the major challenges for translational realization of hiPSC technology.
Here we demonstrate a scalable culture system for hiPSC expansion using the E8 chemically defined and xeno-free medium under either adherent or suspension conditions.
To optimize suspension conditions guided by a computational simulation, we developed a method to efficiently expand hiPSCs as undifferentiated aggregates in spinner flasks.
Serial passaging of two different hiPSC lines in the spinner flasks using the E8 medium preserved their normal karyotype and expression of undifferentiated state markers of TRA-1-60, SSEA4, OCT4, and NANOG.
The hiPSCs cultured in spinner flasks for more than 10 passages not only could be remained pluripotent as indicated by in vitro and in vivo assays, but also could be efficiently induced toward mesodermal and hematopoietic differentiation.
Furthermore, we established a xeno-free protocol of single-cell cryopreservation and recovery for the scalable production of hiPSCs in spinner flasks.
This system is the first to enable an efficient scale-up bioprocess in completely xeno-free condition for the expansion and cryopreservation of hiPSCs with the quantity and quality compliant for clinical applications.
Highlights
•
We report the first suspension culture of hiPSCs using E8 medium.
•
We report the first hydrodynamic model for glass-ball type spinner flasks.
•
We establish a scalable bioprocess in completely defined xeno-free conditions.
•
We optimized a single-cell cryopreservation for hiPSCs in suspension.
•
Human iPSCs maintained pluripotency and normal karyotype along the bioprocess.

Introduction
Human pluripotent stem cells (hPSCs), including human induced pluripotent stem cells (hiPSCs) and human embryonic stem cells (hESCs) that can differentiate into any mature cell type of the body, hold great promise for revolutionizing regenerative medicine.
Specifically, the integration-free reprogramming technologies, such as ones using plasmids, provide a feasible method to generate autologous and clinical-grade hiPSC lines for therapeutic applications under current good manufacture practice (cGMP) conditions.
Patient-specific hiPSC lines derived from postnatal somatic cells (Chou et al., 2011; Dowey et al., 2012; Ye et al., 2009) exhibit vast potential not only in disease modeling for pathological studies but also in practical cellular therapies.
These clinical applications require a large number of hiPSCs or their progenies.
For example, an optimized dose was suggested to contain 4.2×108 to 5.6×108 CD34+ cells for hematopoietic stem cell (HSC) transplantation for a 70-kg adult patient (Mehta et al., 2009).
Production of a clinically relevant quantity of hiPSCs and/or their progenies for specific applications, sometimes considered as ~1 to 2billion (Kehoe et al., 2010), in a chemically defined condition by robust, reproducible and economic methods remains a major challenge for advancing hiPSC technology from the bench to the clinic.
Conventionally, hiPSCs are induced and expanded on feeder cells as adherent colonies in media containing sera or serum replacement containing human or animal serum albumin (Okita et al., 2007; Yu et al., 2007).
The involvement of animal products or sera impedes these culture conditions to meet the strict requirement of clinical or pre-clinical utilization because of the uncertainty of complex components and the quality variance from batch to batch.
Since the first isolation of hiPSCs, significant improvements in feeder- and serum-free chemically defined culture medium and substrates for adherent hiPSC culture have been developed (Chen et al., 2011; Li et al., 2005; Ludwig et al., 2006; Vallier et al., 2005; Wang et al., 2007).
However, these approaches involving adherent culture of hiPSCs in Petri dishes still raise a major hurdle of large scale and well-controlled expansion for clinical use.
Suspension culture for hiPSC expansion provides a feasible solution for its scale-up capacity.
After a Rho-associated-coiled-coil kinase (ROCK) inhibitor Y27632 was reported to permit the survival of dissociated hESCs when supplemented in the medium only on the first day of seeding (Watanabe et al., 2007), detailed protocols were established for the single-cell inoculation and suspension culture of hPSCs as cell aggregates in a variety of vessel types (Amit et al., 2011; Olmer et al., 2010; Zweigerdt et al., 2011).
Other studies have also reported successful suspension culture in spinner flasks in 100-ml vessels (Abbasalizadeh et al., 2012; Chen et al., 2012; Fluri et al., 2012; Krawetz et al., 2010; Olmer et al., 2012; Singh et al., 2010; Steiner et al., 2010).
Despite the rapid development of hPSC suspension culture in these studies, most of the reproducible systems are based on commercially available serum-free media, StemPro or mTeSR, which are complex and expensive.
The unknown composition (such as StemPro) and high cost of these media pose a major concern for developing reproducible methods for large-scale expansion of hiPSCs.
Chen et al. recently reported the development of a significantly improved hiPSC culture medium, E8, which contains only seven other completely defined and xeno-free components supplementing the standard DMEM/F-12 medium (Chen et al., 2011).
We did confirm that this significantly improved medium without the need to add bovine serum albumin (BSA) Fraction V or human albumin supported the growth of multiple hiPSC lines under feeder-free conditions in adhesion.
Based on this, we sought to test whether the significantly simplified E8 medium could support a robust and economic suspension culture system in a stirred bioreactor for large-scale expansion and cryopreservation of hiPSCs.
Here, we used two integration-free hiPSC lines, BC1 and TNC1, which were derived from leukocytes of either a healthy donor or a sickle cell disease patient using plasmid-based episomal vectors (Chou et al., 2011).
We began by evaluating the capacity of E8 medium to support the expansion of hiPSCs in static suspension cultures.
We then optimized the operating protocol for direct adaptation and expansion of the hiPSCs in spinner flasks using computational simulation of the hydrodynamic properties and experimental tests.
Furthermore, we tested serial passaging and differentiation potential of hiPSCs expanded in spinner flasks.
Finally, xeno-free cryopreservation and recovery methods were established for hiPSCs cultured in the bioreactor.
Materials and methods
Maintenance of hiPSCs in adhesion
Human iPSC lines BC1 and TNC1 were initially maintained on mouse embryonic fibroblast (MEF) feeders in standard ESC medium as previously described (Chou et al., 2011).
At passage 30-32, BC1 cells were dissociated using 0.5mM EDTA in calcium- and magnesium-free PBS (Beers et al., 2012), and directly adapted onto tissue culture plates coated with Matrigel (MG, 1.67μg/cm2, BD) or truncated recombinant human vitronectin (VNT-N, 5μg/cm2, Invitrogen) in E8 medium (home-made as described by Chen et al. (2011) or Essential 8 from Invitrogen) supplemented with 10μM ROCK inhibitor Y27632 (Stemgent) for 24h.
Medium was changed daily.
Cells were routinely passaged as small clumps using EDTA method (Beers et al., 2012) or as single cells using Accutase (Sigma-Aldrich) with the split ratio of 1:8 to 1:12 every 2 to 3days after reaching 60% to 80% confluence.
Suspension culture of hiPSCs
BC1 and TNC1 were cultured in feeder-free adhesion culture for at least 5 passages before transfer in suspension culture.
Prior to the passage, hiPSCs were treated with Y27632 for 1h and dissociated with pre-warmed Accutase for 2 to 3min in 37°C.
After gentle pipetting, the single cell solution was diluted 1:10 with DMEM/F-12, and centrifuged at 200g for 5min.
Cells were resuspended by adding E8 medium with 10μM Y27632 and plated at a density of 1.5×105 to 2.0×105cells/ml in ultra-low attachment plates or seeded at 4×105 to 5×105cells/ml.
Daily medium change was performed by replacing 70% of the medium with fresh E8 medium without ROCK inhibitor.
Cells were cultured in static condition or in spinner flasks (CELLSPIN system of 100ml capacity with a single glass-ball stirring pendulum, Integra Bio-sciences) in standard 5% CO2 incubator at 37°C and passaged every 3 to 4days using Accutase as aforementioned.
Analysis for aggregates and metabolism
Samples of cell aggregates were analyzed for cell number, viability, and metabolic products.
Briefly, 0.5 to 1.5ml of samples were taken and placed still for 5min.
Supernatants were filtered, stored in -20°C, and tested for glucose, lactate, and pH in a blood gas analyzer (Radiometer, ABL 710 at Department of Radiology, Johns Hopkins Medical Institute).
Light microscopy images were captured using Axiovert phase contrast microscope (Zeiss).
The total number of cell aggregates Nagg in a 1-ml sample was analyzed by using the "Embryoid body analysis" module in Celigo imaging cytometer (Brooks).
The average equivalent diameter of aggregates d and the size of each aggregate in the sample were also measured in this analysis.
For counting the cell number in a 1-ml sample Ncell and the cell viability, cell aggregates were dissociated by Accutase, stained with 0.4% Trypan blue, and counted by Countess automated cell counter (Life Technologies).
The average cell number per aggregate N=Ncell/Nagg was calculated in 8 samples containing aggregates in different average diameters d from three independent culture cycles.
In the plot of N-d (Fig. 3c), the data points fit best to a 3rd-order polynomial, defined by having the smallest R2 value comparing to other fitting functions.
Dissolved oxygen (DO) level in the suspension medium was monitored real-time using a non-invasive optical fiber probe and PSt3 sensor spots via Oxy-4 mini transmitter (PreSens).
Flow cytometry
See Supplementary methods.
Immunofluorescent staining of hiPSCs in adhesion
See Supplementary methods.
Immunofluorescent staining of cell aggregates in suspension culture
Whole aggregate staining was performed for immunofluorescence microscopy as described before (Chen et al., 2012).
Fluorescent images of whole cell aggregates were captured and analyzed by LSM 510 META confocal microscope (Zeiss).
Karyotyping
Karyotype of hiPSCs was examined by a certified cytogeneticist using G-banding (300-500bands) method as previously described (Chou et al., 2011).
At least 20 metaphases were checked for each sample.
For example, the passage number of BC1 p30+21+21 (see Fig. 4d) means that the examined BC1 cells have been cultured for 30 passages on MEF, followed by 21 passages on VNT-N and 21 passages in bioreactor.
For TNC1 p30+17+19, there have been 30 passages on MEF, 17 passages on Matrigel, and 19 passages in bioreactor.
In vitro hematopoietic differentiation
Human iPSCs were differentiated into hematopoietic stem cells (HSCs, CD34+CD45+ cells) using spin-embryoid body (spin-EB) method in feeder- and serum-free conditions as described before (Ng et al., 2008, 2005; Yu et al., 2008).
The EBs were harvested on day 14 of differentiation.
Single cells were isolated and analyzed by flow cytometry for the presence of hematopoietic markers and hematopoietic CFU assay as previously described (Yu et al., 2008; Zhan et al., 2004).
In vitro spontaneous differentiation of germ cells
Single cells were seeded in ultra-low-attachment plates in ESC medium supplemented with 10% fetal bovine serum (FBS, HyClone) at 3.3×105cells/ml to initiate EB formation.
For iPSCs maintained in suspension in spinner flasks, cell aggregates were washed, and the entire medium was changed from E8 to differentiation medium to induce transformation from cell aggregates to EBs on day 3 after passaging.
After 8days, EBs were transferred onto gelatin-coated plates and attached.
Differentiated cells on day 12 were stained for fluorescence microscopy as described.
Teratoma formation assay
The use of immunocompromised mice for teratoma formation assay was approved by the Animal Care and Use Committee in Johns Hopkins University.
The assay was performed as described before (Chou et al., 2011).
See more details in the Supplementary methods.
Cryopreservation and recovery of hiPSCs in suspension culture
Cell aggregates on day 2 or day 3 were pre-treated with 10μM Y27632 for 1h and dissociated into a single cell suspension with Accutase.
After washing with PBS once, cells were suspended in E8+10μM Y27632 and distributed into Cryovial aliquots.
The same amount of E8+Y27632+20% (in volume) DMSO cryopreservation medium was added dropwise.
Cells were immediately frozen at -80°C in isopropanol freezing containers overnight, and transferred to liquid nitrogen for storage.
For recovery from frozen stock, the cells were thawed by following standard cell culture protocol.
Single cells were seeded at 5×104cells/cm2 in either Matrigel- or VNT-N-coated 150-cm2 flasks or ultra-low attachment plates, or at 5×105cells/ml directly in spinner flasks at agitating speed of 60rpm.
The aforementioned culture and passaging protocols were followed afterwards.
Computational fluid dynamics (CFD) analysis
See Supplementary methods.
Data presentation and statistics
For flow cytometry analysis, 20,000 events were collected and analyzed.
Selected cell populations were shown as a percentage among the total gated live cells, based on the comparison with background staining shown when isotype controls of corresponding antibodies were used.
Data of cell number, viability, pH of the medium, and glucose and lactate concentrations, were collected from at least three independent repeats (n≥3), and all measurements were done in duplicates or triplicates to avoid sampling error.
Results were reported as mean±s.d.
if not specified.
The two-tailed Student's t-test or two-way analysis of variance (two-way ANOVA) was performed with GraphPad Prism 5 for comparison between two groups or multiple groups, respectively.
Significance level was set as *P<0.05, **P<0.01, and ***P<0.001.
Results
Adaptation and robust growth in feeder-free adhesion culture
To prevent contamination of feeder cells in suspension culture, we first acclimated hiPSCs expanded in conventional mouse embryonic fibroblast (MEF) culture conditions (Chou et al., 2011) into feeder-free condition.
BC1 and TNC1 were transferred directly to E8 medium (Chen et al., 2011) on culture plates coated with diluted Matrigel or on purified recombinant vitronectin (VNT-N) substrate (Chen et al., 2011).
We found that a step-by-step adaptation process, which is typically associated with other feeder-free conditions (Bigdeli et al., 2008; Desbordes and Studer, 2013; Stover and Schwartz, 2011), is not required when we switched to E8 feeder-free conditions.
Culture on either Matrigel or VNT-N supports robust and long-term culture of adherent, undifferentiated hiPSCs as a monolayer in adhesion (Supplementary Fig.
S1a-e).
The hiPSCs were routinely passaged in single cells (using Accutase) or in small clumps (by EDTA) (Chen et al., 2011) at a split ratio of 1:8 to 1:12 every 3 to 4days.
They expressed TRA-1-60, SSEA4, OCT4, and NANOG, which are markers for undifferentiated hPSCs (Supplementary Fig.
S1c-e).
After more than 30 passages cultured in E8 medium and feeder-free conditions, hiPSCs maintained normal karyotype (Supplementary Fig.
S1f).
These hiPSCs were able to form embryoid bodies (EBs) in vitro containing cells from all three germ layers (Supplementary Fig.
S2a), confirming that the hiPSCs retained their pluripotency.
We also examined directed hematopoietic differentiation of BC1 and TNC1 cultured under the E8 feeder-free conditions for at least 10 passages.
Using the spin-EB method (Ng et al., 2008, 2005; Yu et al., 2008) in a feeder- and serum-free condition, we found that on day 14, ~25%-80% of the differentiating hiPSCs were CD34+CD45+ characteristic of hematopoietic progenitor cells (HPCs) (Supplementary Fig.
S2b).
Moreover, hiPSCs expanded in E8 feeder-free condition showed comparable potential to generate multiple hematopoietic lineages as hiPSCs expanded in MEF culture conditions in the standard colony-forming unit (CFU) assay (Yu et al., 2008; Zhan et al., 2004) (Supplementary Fig. S2c).
Serial passaging and expansion in static suspension culture
It has not been reported whether the significantly simplified E8 medium is sufficient to support the survival and proliferation of undifferentiated hPSCs in suspension culture.
Thus our studies began by establishing a static suspension culture condition in E8 medium for serial passaging and expansion of hiPSCs.
BC1 cells cultured on E8 feeder-free conditions (E8-Matrigel or E8-VNT-N) for at least 3 passages were seeded as a single cell suspension in E8 medium supplemented with Y27632 on the first day (Watanabe et al., 2007).
The cells survived and formed convex dish-shaped aggregates within 24h due to the gravity force (Fig. 1a).
The size of the cell aggregates gradually increased corresponding to the cell number along the culture period.
BC1 cells expanded in static suspension in E8 medium with an average rate of 3.7±0.9 fold per passage, 3.7×106 fold increase in total with >99% of the cells being TRA-1-60+ after 13 passages (Fig. 1b,c).
We also examined the differentiation potential of expanded cells, using the spin-EB method (Ng et al., 2008, 2005; Yu et al., 2008).
We found that under the hematopoiesis-inducing condition, leukocyte-like cells emerged around day 10 (Fig. 1d) with 46.8%±1.6% of the cells becoming CD34+CD45+ HPCs (Fig. 1e).
These results indicate that undifferentiated hiPSCs can be expanded in suspension in E8 medium supplemented with one-day treatment of Y27632, and retain their differentiation potential.
Initiation of dynamic suspension culture of hiPSCs
Computational fluid dynamics (CFD) analysis of glass-ball spinner flask
Previous studies suggest that an adaptation of hPSCs expanded in adherent culture to suspension culture in static conditions is needed prior to inoculation in dynamic suspension culture (Amit et al., 2011; Zweigerdt et al., 2011; Abbasalizadeh et al., 2012).
To support a robust production of hiPSCs from the bench to a clinical setting, we sought to adapt the cells directly from feeder-free adhesion culture to dynamic suspension culture in spinner flasks.
A key parameter of spinner flasks to control is the agitation forces, which should be sufficient to ensure homogeneous distribution of cell aggregates, nutrients and oxygen, but not too severe to harm the cells.
Human PSCs are sensitive to high shear stress that may cause unexpected cell death (Kehoe et al., 2010; Abbasalizadeh et al., 2012) and differentiation (Adamo et al., 2009; Yamamoto et al., 2005).
Thus the velocity field profile and the shear stress distribution in the vessel are major considerations in the bioreactor design and setting for suspension culture of hPSCs.
Spinner flasks equipped with pendulum-shaped (glass-ball) impellers were advantageous compared to other impeller types in previous studies (Zweigerdt et al., 2011; Yirme et al., 2008) that might be due to mild shear stress generated by the smooth surface.
Therefore, we chose the CELLSPIN system with a 100-ml spinner flask equipped with a single glass-ball stirring pendulum as the platform for hiPSC suspension culture.
To predict the hydrodynamic property, we performed computational fluid dynamics (CFD) analysis for a 3-D model of the glass-ball spinner flask (see Supplementary methods).
The physical properties of 37°C pure water were applied to approximate the properties of a single cell suspension in E8 medium during the first 12h after inoculation when the cells are most vulnerable to high shear stress.
The simulation of the steady-state CFD at agitating speeds of 40rpm, 60rpm and 75rpm (maximum spinning speed available of the system) was performed.
A stable flow profile was found in all agitating speeds examined without inadequate mixing or large disturbing turbulence (representatively showing 60rpm, Fig. 2a).
Highest local Reynolds number, Re, was found at the small area tracking the rotating pendulum, which increased from laminar flow regime (Re<1000) to laminar-turbulent transition regime (1000<Re<2000) (O'Connor and Terry Papoutsakis, 1992; Sucosky et al., 2004) as the agitating speed increased (Supplementary Fig.
S3a).
The profile of the z-direction velocity and the streamline indicated that even at the lowest agitation speed of 40rpm, the rotating pendulum provides appropriate convective flow in the vertical direction that enhances the mass transfer (Fig. 2b).
At the maximum spinning speed, 75rpm, the highest shear stress was determined as 0.152N/m2 (1.52dyn/cm2) at the point near the farthest reach of the glass ball from the spinning axis (Fig. 2c).
This shear stress level is lower than spinner flasks equipped with other types of impellers, such as pitched-blade impeller (2-5.2dyn/cm2, 50-100rpm) (Cormier et al., 2006) and paddled impeller (4.5-7.8dyn/cm2, 80-120rpm) (Kehoe et al., 2010; Li et al., 2009), indicating mild hydrodynamic impact on the cells.
Optimization of agitating speed and cell split interval
BC1 cells cultured in E8-Matrigel condition were used for the optimization of agitating speed and cell split interval.
Consistent with previous studies (Zweigerdt et al., 2011; Singh et al., 2010), preliminary experiments using seeding density lower than 4×105cells/ml showed inconsistent cell survival and proliferation (data not shown).
Thus, we used 4×105 to 5×105cells/ml as the seeding density for all following experiments.
After treatment with ROCK inhibitor and dissociation, a single cell suspension of BC1 hiPSCs from feeder-free adhesion culture was directly inoculated into the spinner flask with 45ml of E8+Y27632.
Cell growth at agitating speeds of 40rpm, 60rpm, and 75rpm was compared, and three independent repeats were performed in each condition (n=3).
Within 24h, hiPSCs formed spherical aggregates with diameters of 104.2±44.8μm, 80.6±20.3μm, and 60.8±16.5μm on average in 40rpm, 60rpm and 75rpm, respectively (Fig. 2d-e).
The cell aggregates gradually increased in size along the culture period reaching 120-300μm on day 5.
This is partially due to aggregate agglomeration, but mostly due to the expansion of the cells with 2-3 fold increase after 5-day culture (Fig. 2f-g).
These results further demonstrated significantly higher cell density (Fig. 2f, P=0.0188) without compromising cell viability (Fig. 2g) in hiPSCs cultured in agitation rate of 60rpm.
Moreover, hiPSCs formed more homogeneous aggregates at 60rpm compared to hiPSCs cultured in 40rpm and 75rpm (Fig. 2e and Supplementary Fig.
S3a).
The cell expansion rate decreased from day 3 to day 4 (Fig. 2f) while the cell viability varied on day 5 (Fig. 2g).
Therefore, we decided to culture the cells at 60rpm and split them every 3 to 4days.
Characterization of hiPSCs in dynamic suspension culture
We continued to characterize BC1 cells cultured in the spinner flasks for pluripotency associated marker expression, apoptosis level, and metabolism.
We found that >98% of the cells are positive for TRA-1-60, SSEA4, NANOG, and OCT4 after 4days of culture by flow cytometry analysis (Fig. 3a).
The fluorescent images by confocal microscopy showed the nuclear expression of OCT4 with membrane localization of SSEA4 in BC1 cell aggregates sampled from suspension culture on day 3 (Fig. 3b), indicating the maintenance of the undifferentiated state inside the cell aggregates.
We further found that the cell number per aggregate increased in the order of the cubic diameter - the volume (Fig. 3c).
Together with the confocal z-stack images of cell aggregates, our data suggested an even distribution of cells throughout the entire aggregate without compact center or hollow cavity.
The rate of apoptosis in the cell aggregates was found to be 1.56%±0.1% (n=3, Fig. 3d), indicating no severe programmed cell death due to the aggregate formation and the dynamic culture.
Cell metabolism was monitored by measuring the consumption of glucose, the accumulation of lactate, the change of pH and the oxygen partial pressure (pO2) in the culture medium.
We found that by replacing 2/3 of the medium daily with pre-warmed fresh E8, the consumption of glucose did not exceed 40%.
Also, the concentration of lactate was restricted under 10mM (Fig. 3e), and the pH dropped to 6.75 by day 5 (Fig. 3f).
For real-time monitoring of pO2 in the medium, a non-invasive O2 sensor and optical fiber probe were equipped to the bioreactor (Abaci et al., 2012).
We show that the pO2 was kept above 15% (Fig. 3g), indicating sufficient oxygen supply merely by diffusion through the gas-liquid interface and convective transport in the medium.
These data are consistent with the exponential growth on the first three days followed by a reduced expansion rate after day 4 when the cell density reached ~1.5×106cells/ml (Fig. 2f).
Serial passaging and expansion of hiPSCs in suspension culture
To meet the requirement of scale-up processes for hiPSC production, serial passaging and extended expansion of BC1 and TNC1 cells cultured in our bioreactor system were tested.
Human iPSCs were inoculated into spinner flasks as mentioned above, cultured for 3 to 4days, passaged as a single cell suspension, and re-inoculated at the same conditions.
Both BC1 and TNC1 cells were maintained in spinner flasks for at least 2.5months with 25 passages.
During the culture period, BC1 showed an average expansion rate of 2.4±0.3 per passage, while TNC1 expanded 3.5±0.5 folds per passage on average (Fig. 4a), indicating variation of growth rate in suspension culture between the two different cell lines.
The cells maintained high viability during the serial passaging in suspension.
The overall viability was 92.9%±2.1% for BC1 and 95.0%±1.6% for TNC1 (Fig. 4b).
Both cell lines were also maintained in their undifferentiated state, as indicated by 95-100% positive stain of TRA-1-60, SSEA4, OCT4, and NANOG during the expansion period (Fig. 4c).
Furthermore, BC1 and TNC1 cultured for up to 20 passages in spinner flask (plus at least 20 passages in feeder-free adhesion culture before transfer to suspension culture) retained normal karyotypes (Fig. 4d).
Differentiation potential of hiPSCs in suspension culture
BC1 and TNC1 hiPSCs cultured in spinner flasks for more than 10 passages were tested for their pluripotency by in vitro and in vivo differentiation methods.
For in vitro assay, by simply replacing the entire E8 medium with differentiation medium of day-2 spinner flask suspension culture, BC1 and TNC1 aggregates transformed into EBs and started spontaneous differentiation under the stimulation of 10% serum (Supplementary Fig.
S4).
After 8days in spinner flask followed by 4days on gelatin-coated tissue culture plates in differentiation medium, cells of all three germ layers could be detected by immunofluorescent staining of specific markers (Fig. 5a).
In contrast, undifferentiated hiPSCs on day 0 showed negative staining of all germ markers (data not shown).
For in vivo assay, BC1 and TNC1 harvested from suspension culture in spinner flasks were injected into immune-deficient mice and were able to form teratomas containing cells of all germ layers, including glandular epithelium, chondrocytes, and neural rosettes (Fig. 5b).
Directed hematopoietic differentiation potential was also tested as previously described (Fig. 5c).
Differentiating BC1 and TNC1 cells contained 43.57%±4.35% (n=3) and 43.22%±7.13% (n=3) CD34+CD45+ HPCs on day 14 of differentiation, respectively.
The CFU assay measuring hematopoietic progenitors showed that hiPSCs cultured in spinner flasks were able to generate colonies of different hematopoietic cell lineages.
The total number of CFUs was comparable or significantly larger (P=0.0307, n=3) than cells cultured in parallel in adhesion cultures (Fig. 5d).
Cryopreservation and recovery of hiPSCs in suspension culture
It has been reported that E8 medium with 10% DMSO can be used as cryopreservation medium for hPSCs frozen in small clumps (Beers et al., 2012).
However, whether E8-based freezing medium can support single-cell cryopreservation has not been reported.
As the inclusion of Y27632 in freezing medium was proven to enhance single cell survival at thaw (Li et al., 2009), we examined cryopreservation of hiPSCs harvested from expansion in bioreactors in E8 medium supplemented with 10% (in volume) DMSO and 10μM Y27632.
Following at least two weeks of storage in liquid nitrogen, we thawed the frozen cells in both adhesion and suspension culture conditions.
The viability of the frozen cells upon thaw was measured as 89%±4% for BC1 (n=3) and 93%±2% for TNC1 (n=3).
Moreover, both BC1 and TNC1 showed good attachment and typical morphology of undifferentiated hiPSCs on Matrigel or VNT-N (Fig. 6a).
The cells reached 80% confluence after 3 to 4days in culture and were passaged routinely hereafter, indicating a rapid recovery and good adaptation back in adhesion culture.
Thawed cells were also able to survive and form aggregates in both static and dynamic suspension culture (Fig. 6a).
Three protocols for recovery were tested for the purpose of further expansion in suspension.
Cells were thawed in adhesion (AD), ultra-low attachment plate (UL), or spinner flask (SF) for the first passage, and then transferred directly to spinner flasks on the next splitting (AD-SF, UL-SF, and SF, respectively, Fig. 6b).
We note that the first passage after thawing in static or dynamic suspension culture conditions usually required longer culture period (5 to 6days) to allow enough cell growth for splitting (Fig. 6b).
Cells regained typical expansion rate at the third passage after thawing (Fig. 6b) and contained >98% of the population with positive staining of TRA-1-60, SSEA4, OCT4, and NANOG after 5 passages (Fig. 6c).
Cells in AD-SF and UL-SF conditions showed 2-3-fold higher expansion rate in the first passage than hiPSCs that were directly thawed in spinner flasks (Fig. 6b).
The spin-EB hematopoietic differentiation of hiPSCs recovered from UL-SF protocol demonstrated an efficient differentiation into hematopoietic progenitor cells (Fig. 6d).
Furthermore, hiPSCs were tested for karyotype stability after sequential freeze-thaw cycles using single-cell cryopreservation.
After more than 5 times of sequential freeze-thaw process after adapted to completely xeno-free conditions, including at least 4 times of cryopreservation following UL-SF protocol, both BC1 and TNC1 after ~70 passages maintained normal karyotype (Figs.
6e and 4d), indicating a reliable cryopreservation protocol for hiPSCs throughout their useful lifespan.
This xeno-free cryopreservation and recovery protocol completed our bioreactor system, offering opportunities for large-scale cell growth and banking of hiPSC lines.
Discussion
Following the demonstration that a ROCK inhibitor such as Y27632 permits the survival of single-cell hESCs (Watanabe et al., 2007), several groups utilized it to establish suspension culture systems for the expansion of hPSCs based on single-cell inoculation (Amit et al., 2011; Olmer et al., 2010; Zweigerdt et al., 2011; Abbasalizadeh et al., 2012; Chen et al., 2012; Fluri et al., 2012; Krawetz et al., 2010; Olmer et al., 2012; Singh et al., 2010; Steiner et al., 2010).
Zweigerdt et al. and Amit et al. established detailed protocols for the adaptation and suspension culture of hPSCs in a variety of vessel types (Amit et al., 2011; Zweigerdt et al., 2011).
Recently, Olmer et al. reported a high yield culture in a monitored spinner flask with 100-ml working volume.
Up to 2×108 of hiPSCs were obtained in a single run of 7days (Olmer et al., 2012).
Chen et al. showed long-term culture of hESCs in suspension for at least 20 passages with an average 4.3-fold expansion rate within 3 to 4days of interval, and demonstrated a complete strategy for hESC banking under cGMP or cGLP condition (Chen et al., 2012).
Most of these approaches were based on serum-free media that contain BSA Fraction V or human serum albumin as the major protein component.
The batch-to-batch variance of this animal-source product led to compromised defined medium (Chen et al., 2011).
Toward this, we first transferred hPSCs into the newly developed E8 medium on human vitronectin substrate, to avoid BSA and have a completely xeno-free condition with low concentration of protein components.
We then demonstrated that E8 medium (with Y27632 on the first day of seeding) is sufficient to support hiPSC aggregation and survival, and serial passaging in the undifferentiated state using static suspension culture.
We next aimed to optimize the protocol for dynamic suspension culture in spinner flasks.
While there are many experimental and computational studies for assessing the flow dynamics in spinner flasks equipped with various types of paddle-shaped impellers (Bilgen and Barabino, 2007; Collignon et al., 2010; Papoutsakis, 1991; Venkat et al., 1996; Youn et al., 2006), the relevant analysis for glass-ball spinner flasks has not been reported.
We simulated the velocity field and shear stress of the flow in the glass-ball type spinner flask, providing reasonable prediction of the effect of flow on single cells during the first several hours after inoculation.
Some rational approximations were applied to simplify the model, such as using the properties of 37°C water instead of the unknown data of the medium, omitting gravity due to its negligible effects on single cells compared to the agitating force, and ignoring the cell aggregation due to the complexity it would bring to the model.
Although the model could not estimate the shear stress on the surface of cell aggregates after emergence, the method provides an efficient tool for the design and analysis of the bioreactors with complex vessel structures or non-standardized impeller designs (Bilgen and Barabino, 2007; Venkat et al., 1996).
Using CFD analysis, we found a steady flow in all agitation speeds (i.e. 40rpm, 60rpm and 75rpm).
Although small turbulent flow may occur at high agitating speeds according to the calculation of the local Reynolds number, the shear stress generated by the glass-ball impeller is considerably lower than by other types of impellers.
Experimentally, we further found that an agitation speed of 60rpm supports the formation of homogeneous cell aggregates, showing significantly higher expansion rates and relatively better cell viability than the other agitation speeds.
As a comparison, hiPSCs formed relatively large aggregates on day 1 at 40rpm, which reduced the potential of further proliferation due to the diffusion limitation.
Moreover, occasions of undesired aggregate agglomeration, resulting in aggregates larger than 800μm, were observed on later days due to insufficient agitating force (from Supplementary Fig.
S3b).
At 75rpm, cell viability dropped significantly on day 1, leading to slower recovery and expansion.
Based on these results, the optimal split interval at 60rpm was next determined.
We noticed a reduction in cell expansion after day 3 or day 4, which might be due to the diffusion limitation as cell aggregates grow.
It is widely accepted that the diffusion limitation of oxygen in human tissue is 100 to 200μm (Hoeben et al., 2004).
Therefore, we determined to split the cells on day 3 or day 4 when most of the cell aggregate diameters reached ~150-200μm.
The concentration of glucose and lactate in the culture media also suggested a reduced growth after day 3, when the pH of the media drop to ~6.75 as cell density increased, which might be harmful for the hiPSC lines that are more sensitive to changes in pH and influence the expansion.
This issue may be solved by gradually increasing the culture volume or the media change frequency.
The inclusion of a non-invasive O2 probe-patch made it possible to monitor pO2 levels real-time without disturbing the flow and the rotation of the impeller in the glass-ball bioreactor, which are normally difficult to equip with a standard large O2 probe (Zweigerdt et al., 2011; Niebruegge et al., 2009).
Undifferentiated hiPSCs expanded in bioreactors retained their potential of spontaneous and directed hematopoietic differentiation comparable to cells cultured via adherent culture.
The transformation from day 2 or day 3 cell aggregates to EBs for spontaneous differentiation in the spinner flask demonstrates the possibility to use this system for a continuous bioprocess that begins with a large-scale expansion and directed differentiation in one single batch.
Interestingly, although comparable amount of HPCs and CFU was derived from hiPSCs expanded in bioreactor as in adhesion, HPCs derived from TNC1 in bioreactor generated a considerably higher proportion of cells in erythroid lineage rather than in myelocyte lineage.
Further studies are needed to reveal the significance and the mechanism underlying the contribution of hydrodynamic forces to hPSC differentiation in suspension.
Studies of hematopoietic differentiation in spinner flasks are ongoing.
The development of a single-cell cryopreservation protocol concluded the establishment of our chemically defined xeno-free suspension culture system.
For continuous expansion, UL-SF protocol was the preferred method for recovery with small number of hiPSCs from cryopreservation.
The hiPSCs can be directly thawed in a spinner flask; cells cultured in a spinner flask can be transferred back to VNT-N-coated surface for the applications that require adhesion culture, indicating good flexibility of the system.
The single-cell cryopreservation protocol and the suspension culture system can significantly reduce the workload when handling cells in the scale required for translational uses.
Furthermore, this xeno-free protocol of cryopreservation and recovery was theoretically proved to be compliant to a scale-up strategy for hiPSCs in suspension and under cGMP condition, allowing a yield of ~1×109 cells within 20days after thawing of 1×106 frozen cells (Fig. 7).
The scale-up strategy, which includes multiple xeno-free processes, employs 100-ml and 1-l spinner flasks, enabling the transition from lab scale to pilot scale of hiPSC expansion.
The CFD simulation of the 1-l spinner flask (Supplementary Fig.
S5) equipped with two glass-ball stirring pendulums (such as one from Integra Bio-sciences) indicated that it could be scalable using 1-l spinner flasks toward clinically relevant quantities of hiPSCs from a single batch.
The simulation revealed a steady flow at the highest agitating speed of 75rpm and an even lower maximum shear stress (0.047N/m2) compared to the 100-ml spinner flask in steady state.
On one hand, the distance between the farthest points of the pendulums and the spin axis (r=38.9mm in Supplementary Fig.
S5, equivalent to impeller radius of paddle impellers) in the 1-l spinner flask is not much longer than the 100-ml spinner flask (r=28.2mm); thus, the highest velocity in 1-l spinner flasks is not much higher than that in 100-ml spinner flasks (vmax,1l=0.309m/s vs. vmax,100ml=0.224m/s).
On the other hand, there is much more space between the pendulums to the flask wall in the radial direction (l in Supplementary Fig.
S5, l1l=29.1mm vs. l100ml=3.5mm).
This causes a more moderate drop of velocity from the maximum at the far reach of the pendulums to zero at the flask wall in 1-l spinner flask compared to the 100-ml spinner flask, leading to a significant decrease of the gradient of velocity, dvz/dx, which contributes to the majority of the shear stress.
Before reaching steady state, stepping acceleration could be used to protect the cells from sudden exposure to high-speed flow.
Also, the liquid level of 600-ml medium in the 1-l spinner flask is similar to that of 60-ml medium in the 100-ml spinner flask, which was proven in this study not to cause any reduced O2 transport (from Fig. 3g).
While optimization of the agitation speed should be performed for different hiPSC lines to support appropriate size aggregate formation (for example, diameter of 50-80μm after 24h), this direct scale-up process in 1-l spinner flasks with the same well-designed system should be attainable.
Another practical advantage of the system is its relative low cost.
The high price of current commercially available serum-free media such as StemPro and mTeSR always hinders the universal application of the scale-up technologies for hiPSC expansion, especially for xeno-free media when albumin is required in the medium (Chen et al., 2011).
The simplicity of the medium dramatically cuts down the price (~30%-60%) and makes it possible for bulk production in academic laboratories and cGMP-compliant institutions.
In conclusion, we established a reproducible approach for rapid, economic, and scalable expansion of hiPSCs in xeno-free condition to meet the demand of practical research and clinical applications.
The complete elimination of components from animal sources and remarkably reduced cost of this system provide a reliable technology for scale-up of hiPSC expansion and take a significant step toward the realization of stem cell therapies.
Author contributions
Y.W., S.G. and L.C. developed the concepts and designed the experiments; Y.W., B.C., C.H. and S.D. performed the experiments; Y.W., S.G. and L.C. analyzed the results and wrote the paper.
Competing financial interests
The authors declare no competing financial interests.
Acknowledgment
We thank members in the laboratories of Drs.
Gerecht and Cheng for discussions and support.
We thank Sravanti Kusuma for helping with edits of this manuscript.
This study is supported in part by Edythe Harris Lucas and Clara Lucas Lynn Chair in Hematology to L.C., and grants from Maryland Stem Cell Research Fund (2011-MSCRFII-0088 to L.C.), NIH (U01-HL107446 and 2R01-HL073781 to L.C.) and NSF (1054415 to S.G.).
Supplementary data
Supplementary figures and methods.
Supplementary data
Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.scr.2013.07.011.

Synthesis and property evaluation of CuFeS2-x as earth-abundant and environmentally-friendly thermoelectric materials

The commercial elemental powders of Cu (99.99%), Fe (99.99%) and S (99.9%) were used as starting materials. These powders were mixed according to the compositions of CuFeS2-x (x = 0, 0.05, 0.15, 0.20, 0.25), and the mixtures were subjected to MA in a planetary ball mill (QM-2L) using a hardened stainless steel vial and ball, which was performed at 450 rpm for different times. The weight ratio of balls to powders was kept at about 20:1, and the mill vial was filled with Ar (99.9%) atmosphere to prevent the powders from oxidation during the milling process. The MA-derived powders were densified by SPS at different temperatures ranging from 873 to 923 K for 5 min under an axial pressure of 50 MPa. Graphite dies were used for SPS, which was conducted in vacuum. In this paper, 10 h/873 K means that the sample was synthesized by milling for 10 h and then sintering at 873 K for 5 min.

Effect of addition of rhenium to Pt-based anode catalysts in electro-oxidation of ethanol in direct ethanol PEM fuel cell

The precursors used for the preparation of electrocatalysts were H2PtCl6.6H2O (Alfa Aesar, USA), SnCl2.2H2O (Merck, Germany) and ReCl3 (Alfa Aesar, USA). Vulcan XC-72 (Carbot, USA) was used as support for the catalysts. Carbon paper (90T, Toray, USA) was used as substrate for the catalyst powder to prepare the electrodes. Nafion(r) (DE 521, DuPont, USA) dispersion was used to make the catalyst slurry. Ethanol (Merck, Germany) and H2SO4 (Merck, Germany) were used as fuel and as electrolyte for electrochemical analysis, respectively. Nafion 117(r) (DuPont, USA) proton exchange membrane was used to prepare membrane electrode assembly for DEFC. Pt/C (40%wt) (Johnson Matthey) was used as cathode catalyst.

The precursors were first suspended in propanol and then ultrasonicated for 3 h. High-surface-area carbon black, e.g., Vulcan XC-72, was preheated to 110 degC and suspended in propanol separately and ultrasonicated for 3 h. Precursor suspension is then added drop wise to carbon suspension [17]. The weight ratio of Pt-X/C (X = Sn, Re, Re-Sn) was controlled according to the targeted metal loading. Ultrasonic blending for 3 h, of precursor and carbon suspension was carried out to ensure the proper impregnation of metal precursors on carbon support. The suspension is then kept at 70 degC for 12 h and all the propanol is evaporated. The precursor mixtures were reduced in hydrogen atmosphere at different temperatures to obtain desired bimetallic and tri-metallic catalysts by impregnation method. Table 1 shows different catalysts and their loadings. Abbreviation used to show the metal composition in the catalyst, e.g., Pt-Re-Sn/C (20:5:15) represents 20% Pt, 5% Re, 15% Sn by wt and rest 60 %wt is carbon.
Anthraquinone-ferrocene film electrodes: Utility in pH and oxygen sensing
All chemicals were supplied by Aldrich Chemical Co. and used as received. The buffered solutions were prepared as follows: pH 7 disodium hydrogen phosphate (0.025 M) and potassium dihydrogen phosphate (0.025 M), pH 9 (0.05 M sodium tetraborate), all other pH solutions were prepared using a Britton-Robinson buffer (0.04 M boric acid, phosphoric acid, acetic acid) and adjusted to the appropriate pH with NaOH solution.
The synthesis of this compound and corresponding spectral data has been reported in the literature [14]. Briefly, to a solution of Ferrocene (1 g, 4.23 mmol) in dry diethyl ether (30 ml) was added portion wise Fast Red Al Salt (1.2 g, 4.43 mmol). The solution was stirred under N2 atm. for 48 h. Water (50 ml) was added to this solution and the organic phase was separated and dried over dry MgSO4. The solvent was then evaporated under vacuum and the residue was purified through basic alumina chromatography column using a gradient of petroleum ether (40-60 degC) and diethyl ether as eluent to give the desired compound in 30% yield.1. A cathode active material precursor for a non-aqueous electrolyte secondary battery that is a complex metal hydroxide having a flow factor of 10 or greater to 20 or smaller.2. The cathode active material precursor for the non-aqueous electrolyte secondary battery according to claim 1,
wherein the internal friction angle is 30° or greater and 32° or smaller.3. The cathode active material precursor for the non-aqueous electrolyte secondary battery according to claim 1 or 2, the cathode active material precursor comprising nickel and cobalt.4. The cathode active material precursor for the non-aqueous electrolyte secondary battery according to claim 3, the cathode active material precursor further comprising an element M (M is at least one selected from Mg, Al, Ca, Ti, V, Cr, Mn, Zr, Nb, Mo, Sr, and W).Influences on the energy delivery of thin film photovoltaic modules

Abstract
The energy yield delivered by different types of photovoltaic device is a key consideration in the selection of appropriate technologies for cheap photovoltaic electricity.
The different technologies currently on the market, each have certain strengths and weaknesses when it comes to operating in different environments.
There is a plethora of comparative tests on-going with sometimes contradictory results.
This paper investigates device behaviour of contrasting thin film technologies, specifically a-Si and CIGS derivatives, and places this analysis into context with results reported by others.
Specific consideration is given to the accuracy of module inter-comparisons, as most outdoor monitoring at this scale is conducted to compare devices against one another.
It is shown that there are five main contributors to differences in energy delivery and the magnitude of these depends on the environments in which the devices are operated.
The paper shows that two effects, typically not considered in inter-comparisons, dominate the reported energy delivery.
Environmental influences such as light intensity, spectrum and operating temperature introduce performance variations typically in the range of 2-7% in the course of a year.
However, most comparative tests are carried out only for short periods of time, in the order of months.
Here, the power rating is a key factor and adds uncertainty for new technologies such as thin films often in the range of 10-15%.
This dominates inter-comparisons looking at as-new, first-year energy yields, yet considering the life-time energy yield it is found that ageing causes up to 25% variation between different devices.
The durability of devices and performance-maintenance is thus the most significant factor affecting energy delivery, a major determinant of electricity cost.
The discussion is based on long-term measurements carried out in Loughborough, UK by the Centre for Renewable Energy Systems Technology (CREST) at Loughborough University.
Highlights
•
The paper investigates the energy delivery of thin film photovoltaic devices in a maritime climate for amorphous silicon based modules and Copper -Indium, Gallium -DiSelenide based devices Effects on the energy delivery of thin film photovoltaics are separated and quantified.
•
Key contributors are measurement accuracy and changes in device behaviour with time rather than environment.
•
Effects of temperature, irradiance and spectrum are investigated and quantified.
•
The differences in long term ageing of different devices is quantified.

Introduction
The most critical factor determining the suitability of deploying photovoltaics is the cost of energy, or service, delivered and not the power rating of the devices.
Energy is a commodity and thus the aim is to generate electrical energy, or services, as cheaply as possible.
There are two major contributors to the final cost of electricity produced by a system: its specific energy yield and the costs of purchase, operation and maintenance.
This paper concentrates on the first, the specific energy yield.
The focus is on thin film technologies, namely different modules produced from amorphous silicon (a-Si) and Copper Indium Gallium Diselenide (CIGS), in particular on the energy yields of these devices which are susceptible to variations in the operating environment, have a wider design window and less availability of field experience data than conventional wafer-based crystalline silicon (c-Si) devices.
There are a large number of performance studies reported, some with the aim of understanding the behaviour of a single type of device and some to compare the energy yields of different devices.
This paper focuses on behaviour at module level, which may be built up to include system effects such as mismatch, interconnection and power conversion components.
PV modules are normally labelled with a power rating, which means the power measured at standard test conditions, STC, as defined in [1].
This is called peak-power, denoted as Wp.
STC represent rather favourable operating conditions for most PV technologies as it is an unrealistic combination of a cold module temperature (25°C) at a high irradiance (1000W/m2).
Different modules, even of the same technology, generally have different rated powers and the energy yields must be made comparable in any inter-comparison study.
This is achieved by using the specific yield (kWh/kWp).
The specific yield is a key property of PV modules at a particular location and can be a major sales argument for competing PV module suppliers.
Many manufacturers use energy yield measurements to show the quality of their modules against those of competitors, see e.g.
[2].
Some organisations, e.g.
Photon, provide purportedly independent advice via the comparison of modules.
There are also many independent investigations carried out by research institutes, e.g.
[3-10], which also provide data sources to compare the energy yield of different technologies.
The discussion is then often focussed on the determination of the 'best' technology and generalised claims on technologies are made as e.g.
in [11].
The aim in the following is to show technology-specific differences, but does not claim to identify the 'best' technology or superior devices, since this tends to be specific to each installation.
It is also shown that the differences between different devices within any thin-film technology group are so significant that it is virtually impossible to make a decision of which technology to use in a system purely on the basis of material.
The following demonstrates the differences in long term performance of different PV technologies, where 7 modules have been operated for more than 5 years in the current measurement system, with some having been operated for several years previously on another measurement system.
The analysis is of the key influences on energy delivery of these specific devices and is not meant as a generalisation for any of the technologies in the test.
In the case of amorphous silicon devices, the device structure, number of junctions, material of the junctions as well as manufacturing can impact on the energy yield significantly [12,13].
Similar differences are seen in the case of polycrystalline thin film devices [14].
These issues can be due to different manufacturing techniques or different device structures, where in the case of CdTe, for example, different window layers can result in significantly different quantum efficiencies [15].
The number of design parameters of thin film devices is larger and the production processes are less standardised than for c-Si, resulting in wide variation in the specific energy yield of modules of the same material technology.
One of the aims of this paper is to demonstrate the difference between optimisation for high module power and high specific yield.
Optimisation for energy yield may not coincide with optimisation for STC rated power.
As an example, the performance of a crystalline (c-Si) module is shown in Fig. 1 for a number of locations.
A c-Si example is chosen because the performance of these devices is generally more familiar.
The data used here is a matrix measurement as specified in [16] and the energy yield is calculated utilising an implementation of the proposed energy rating standard [17].
The effects of doubling the series resistance or halving the temperature coefficient are shown.
The modifications are applied to the power measurement matrix and the annual energy yield for a number of locations is calculated by drawing on local meteorological data sets, as indicated on the map in Fig. 1.
The overlay boxes indicate the specific yields and performance ratios for the different modules (described in the box in the centre of the graph entitled 'key').
The effects of the modifications in series resistance and temperature coefficients on the specific energy yield depend on the particular location environments and the device responses relative to STC.
A practical example of such a modification is the number of front contact bus bars on wafer based technologies, e.g.
changing from a two-bus bar design to a three-bus bar design for larger cells.
The sub-optimal design, i.e. the one with two bus bars which causes a higher series resistance and thus high ohmic losses, typically has a lower power at STC but may have a higher specific energy yield than those with three bus bars due to relatively higher efficiency at lower irradiances.
This is illustrated in Fig. 1 where the high resistance case delivers more energy for all sites but those with the highest annual irradiation.
Thus the 'worse' device delivers a higher specific energy in the majority of locations.
Similarly, an elevated temperature coefficient will have different effects in different locations, with relatively modest effects in cool to temperate climates.
It can be seen that in certain environments the ranking in terms of performance ratio or specific energy yield changes for the different series resistances assumed or temperature coefficients.
The analogous situation to the variation in series resistance for thin films would be for e.g.
the width of a cell (see e.g.
[18]) or the change in the thickness of the transparent conducting oxide or any other factor that affects the series resistance in the cell.
The cell geometry is crucial for thin film devices as it can change the series resistance and fill factor significantly [19].
The thickness (depth) of the device affects optical absorption but also degradation.
Different window layers modify the spectral response and some devices undergo a shunt busting treatment, where all shunt paths are burnt out.
Similarly, the temperature coefficient of more tuneable technology families such as CIGS can be influenced by the composition ratio of indium and gallium, i.e. a change of band gap, or simply material quality.
This demonstrates the earlier point that devices of nominally the same material can have very different energy yields, which also needs to be considered in the context of the operating environment, and that one should not over-generalise for technologies.
The aim in the following is thus to derive some general factors, which excludes device specifics and gives an estimated ranking of the importance of the different effects on the performance, thus identifying the optimisation potential for future devices.
Accuracy of measurement inter-comparisons
The accuracy of a measurement campaign depends strongly on the purpose of the experiment.
It was already pointed out that many studies are carried out to quantify environmental effects for specific devices.
In these studies only relative changes are required, i.e. device behaviour under different conditions is normalised to its own performance at STC.
This has a different accuracy as compared to an inter-comparison between different devices.
In these studies, any difference in performance between devices must be larger than the uncertainty of the measurement in order to be statistically meaningful and thus these boundaries are explored before presenting the actual performance data.
Inter-comparison studies typically report kWh/kWp, sometimes with differences resolved to several decimal points, which is on figures typically in the range of 1000kWh/kWp.
This is not significant, yet it is rare to find any consideration of measurement uncertainty which would clearly show this level of reporting.
The uncertainty in the specific energy yield is split into two parts, the actual measurement of the energy yield and the determination of the power rating used for the normalisation.
The former are typically in the range 1-2% [3,20,21], depending on whether the calibrations of electronic measurements were carried out using the same traceability chain or not.
Not many uncertainty calculations to date take low signal strength into account, which increases electrical measurement uncertainty significantly [22] and thus should be included.
The determination of kWp is typically carried out indoors using solar simulator and often involves a much higher uncertainty.
This depends on the quality of the measurement system used for the characterisation, the PV material and device structure, appropriate reference devices and, significantly, on the competence of the measurement agent.
A summary of the different characterisation issues is given in Table 1.
Good review articles on general measurement issues have been recently published covering most of these in depth [23,24].
The achievable uncertainty in Table 1 is estimated based on the review articles as well as an analysis of published round robin measurements [25,26].
These considerations mean that the specific energy yields of thin film devices have 5-7% uncertainty attached to them, if carried out with the same traceability chain.
However, many inter-comparisons are conducted without measuring the power ratings of the specific modules within the test.
Then the module selection becomes crucial [21] as well as the variability of the modules within the supplier power bin.
This can be ∼10% for thin film modules, although some manufacturers have tighter binning in their production.
In the case of unmeasured modules it would be a reasonable approximation to attribute a 10-15% uncertainty in the yield inter-comparison of modules.
The measurement uncertainty is such a key factor in any inter-comparison [21] on system level are dominated by it [27].
In conclusion, it should be kept in mind that the overall experimental design and the power rating dominate the reliability of any inter-comparison [11,21].
Data schedule
The effects of the environment on device operation are investigated in this paper based on long term measurements carried out at Loughborough University's Centre for Renewable Energy Systems Technology (CREST).
The measurements were taken using the COMS-3 (generation 3 of the CREST Outdoor Measurement System) [28], which are based on the roof of the Sir David Davies Building of Loughborough University.
The system is depicted in Fig. 2.
Some of the modules used for this report were new and some had been operated previously for several years on the COMS-2 measurement system located on another roof.
The orientation of the test plane was altered in the relocation and so only data from the newer site is included here; however the point is made that these modules had already undergone outdoor exposure.
The modules are all fully commercial, albeit some are of relatively small size.
There is not, however, any influence of this on the long term device performance as can be seen later in the analysis.
In the following, all data are normalised to the mean performance in 7-18 months of the measurement span used (i.e. the time in the new measurement system and allowing for initial stabilisation of the new modules).
The data presented contains a wafer-based poly-crystalline silicon (c-Si) device as a reference, one single junction amorphous silicon device (a-Si-1), one double junction amorphous silicon device (a-Si-2), one triple junction amorphous silicon device (a-Si-3) and two Copper-Indium, Gallium-Diselenide devices (CIGS).
Data availability for module characterisation over 5.5 years is very high, with only short down-times in January and May 2008.
The environmental conditions seen by the modules are summarised in Fig. 3, which also demonstrate the availability of data.
The rather unusually low irradiation in February 2010 is due to the rare occurrence of the pyranometer being covered by snow.
There is an idiosyncratic month in June 2012, when very low irradiance has been recorded.
This month was actually one of the worst Junes on record and thus is not a measurement issue.
The measurement plane experiences maximum short-term irradiances above 1200W/m2 but much more frequently low-light conditions.
Overall, a significant amount of energy is delivered at low irradiances: approximately 30% of annual electrical energy is generated at irradiances below 250W/m2 at this location.
Observed module temperatures vary between -5°C and 65°C, indicating the cool climate of the UK.
The irradiance-weighted module temperature varies, as shown in Fig. 3, between 15°C and 35°C, which is a rather narrow variation compared to other climatic zones.
In the following, data concerning device performance is normalised to the second year of their operation on the current PV monitoring system.
In the case of a-Si-1 this is after a low number of months, but does not affect the results shown here: comparing the performance of a-Si-1 and a-Si-2 shows similar trends over several years and demonstrates consistent observations as have been shown in e.g.
[12] where similar samples were investigated.
Unless otherwise stated, the analysis is carried out in terms of specific yield, as described in the introduction, and the performance ratio, PR, which is defined as the operating efficiency divided by the STC efficiency.
The STC efficiency has been determined using the data collected in operating months 7-18 inclusively, using the 'southern' method described in [29].
These 'harmonised procedures' were developed during the EU FP6 Programme PV Performance to reconcile the approaches taken by different laboratories to a unified method of extracting key performance parameters at STC-like conditions from long-term outdoor data sets.
The data in the current paper has been prepared precisely following the European Harmonised Procedures protocol, with very minor additional data filtering accounting for local system knowledge (e.g. snow on the irradiance sensor or modules).
The normalisation to months 7-18 is carried out nevertheless as one of intentions of this paper is to show the importance of long term behaviour of devices.
Effects of the environment
The initial question is whether or not there are significant differences in how the environment affects device performance for different technologies.
The effects of irradiance and temperature are typically seen as the main drivers, with spectral effects being relevant for some wide band-gap materials such as amorphous silicon and particularly for multi-junction devices.
The effects of environmental parameters are seen in the seasonal variation of the performance ratio, as plotted in Fig. 4, taking 2009 as an example.
The technologies have distinct patterns in their seasonal behaviour.
The c-Si device in this paper shows non-typical behaviour with no specific dip for the summer months as demonstrated in e.g.
[30-32].
This is due to the irradiance-weighted average operating temperature not being significantly elevated in Summer over STC temperatures (around 30°C, see Fig. 3), whereas in other locations or years (e.g. 2010 in Fig. 3) the temperature is higher.
Thus, the relatively high temperature coefficient of c-Si does not affect the device performance as much as it would in other locations.
CIGS-1 has a very high performance in the months of January and February in this particular year.
This behaviour is recurrent for all years but not in the same magnitude.
It seems to be a device specific feature and is investigated in more detail in the following analysis.
Investigating the short-circuit current, as done in Section 4.2, strongly suggests that it is due to the spectrum and the behaviour may be attributed to a strongly blue-absorbing window layer and thus the device benefits from a relative shift towards the red in Winter time.
The device tends to work better in low irradiance months and thus a reduction in operating efficiency with Summer irradiance is also a contribution to the prominence of these spikes.
The effect was more pronounced in 2009 than in other years, thus a number of effects seem to have coincided.
All amorphous silicon devices have a strong performance maximum in June-July-August, and a distinct minimum in December.
This is unlikely to be an effect of temperature, as the lowest temperatures in Loughborough are typically in January and February and the highest temperatures are in August.
As shown in Fig. 3, there is no significant change in operating temperature observable that would allow such a swing in performance.
There is an active, on-going debate regarding the drivers for these seasonal improvements.
The possible drivers are seasonal annealing (and degradation in winter time) and changes in the spectrum.
Both effects will impact on the seasonal behaviour but their magnitude will be site dependent with the overall effect being comparable in most sites [33].
It will be shown in Section 4.2 that for the UK this is most likely due to spectrum, with only small contributions that could be attributed to annealing.
The other devices have a much more balanced behaviour in the course of the year, where different performance effects appear to balance.
Irradiance and temperature
The effect of low irradiance depends largely on the shunt resistance of the device and is a manufacturer-specific number.
The crystalline silicon module included in this test does not boast an exceptionally high shunt resistance and thus low light behaviour is also not as good as in other devices of this type.
However, given that this is a relatively old device, purchased at some time in 2000, this may have been more common at that time.
In the case of thin film devices, there are ways to deal with shunt resistance and thus improve low light efficiency significantly.
Some amorphous silicon devices are 'shunt busted', where an electrical current is pushed through the device to burn out all shunts in the bulk of the material, which essentially isolates the areas affected and increases shunt resistance.
The effects of temperature have some dependence on the absorber as well as on the material quality.
The effect of operating temperature can be understood by separating the effects into contributions in different device parameters:(1)Pmpp=ISCVOCFFwhere Pmpp is the maximum power point, ISC is the short circuit current, VOC is the open circuit voltage and FF is the fill factor of the device.
In the case of short-circuit current and FF there is only a small variation with temperature for thin film and crystalline silicon devices, thus the majority of the temperature dependence occurs in the voltage, which can be described through using the open circuit voltage as derived from the one diode model.
This is given as(2)ddTVOC≈ddT(nVTlnISCI0)where T is the device temperature, VT is the thermal voltage described as kT/q by the Boltzmann constant k and elemental charge q, I0 is the diode saturation current, and n is the ideality factor.
This simplification overlooks the impact of varying values of the parasitic resistances, which also may have significant thermal changes [34-36], and any thermal influence on the voltage dependence of the photocurrent as exhibited e.g.
by amorphous silicon [37-39].
Taken in isolation, Eq.
(2) seems to indicate that the behaviour of the open circuit voltage is positive with temperature, but it is in fact dominated by the temperature dependence of the diode saturation current, which is given as(3)I0=BTγexp(EGkT)where B is an empirical factor which is temperature independent and dominated by material quality, γ is an empirical factor depending on the specific loss mechanism that is dominating in the cell and EG is the band gap of the cell.
The impact of irradiance and temperature can be investigated by binning the data according to these influences.
The resulting matrix can then be used for modelling the annual yield for different technologies in various locations [16,40-42].
The key uncertainties in this case become the normalisation to kWp and the input irradiance [14].
To demonstrate the effects in device behaviour the outdoor data for 2009 was binned according to irradiance and temperature, as shown in Fig. 5 for two contrasting examples.
The irradiance used was an effective irradiance calculated from the ISC of the module in order to separate intensity from the effects of spectrum and angle of incidence.
This self-referencing approach is very effective in case of single-junction devices but less so in the case of the multi-junction devices, where the relationship between ISC and irradiance is no longer linear (and changes abruptly if the spectral conditions force a switchover in the current-limiting sub-cell).
The CIGS-2 sample is the sample with the strongest irradiance dependence in the field of the modules tested here.
It is a commercial module but has a rather 'round' I-V characteristic.
This strong irradiance behaviour essentially contradicts the 'common knowledge' that thin film devices have excellent low light behaviour, as e.g.
stated in [11].
The CIGS-1 sample would in this plot look closer to the a-Si sample shown here without the low light spike, again demonstrating that it is not possible to generalise from one device to the entire material-classes final energy yield.
CIGS-2 exhibits a slight decrease of efficiency with increasing temperature and more so with decreasing irradiance.
The device has seen several years of outdoor performance beforehand, and newer devices may have better low light behaviour, but samples with similar behaviour are still in the marketplace.
The amorphous silicon double-junction sample, on the other hand, shows a significant increase of efficiency towards very low irradiances.
This may not enormously affect the annual yield, as a low proportion of annual irradiation is delivered at conditions below 100W/m2 irradiance.
It has been shown elsewhere that this low light behaviour is largely due to changes in the incident spectrum [43], causing better current matching between the junctions under higher diffuse fraction conditions which are more likely at low irradiances.
It has already been shown in Fig. 1 that different effects will impact with variable magnitude in different locations.
The matrices shown in Fig. 5 were extracted for all modules and integrated with meteorological datasets generated using Meteonorm for the contrasting sites of Loughborough, UK (low irradiation and low ambient temperature) and Seville, Spain (high irradiance and temperature).
Several simulations were carried out for different materials, one with the efficiency always kept at STC conditions (to normalise), one each with irradiance and temperature being kept at STC conditions with the other parameter varying and finally, one full simulation.
Simulations were normalised to the STC simulation, i.e. the performance ratio variation between the different simulations is plotted in Fig. 6.
It is noticeable that the PR for the majority of the devices is higher in the UK than in Spain.
This agrees with other reported simulations, e.g.
[41].
The only device disagreeing is the CIGS-2 device, where the low irradiance behaviour seems to dominate the annual PR.
The temperature coefficient of a-Si devices tends to be smaller than that of CIGS and c-Si devices as a-Si has the highest band gap and thus will be affected the least according to Eq.
(3).
The extraction of the temperature coefficient from outdoor data is rather complicated and may even appear positive [6,44,45].
The temperature coefficient of amorphous silicon devices extracted from outdoor data is typically problematic as most groups measuring outdoors report a positive coefficient, e.g.
[7], which contradicts the negative temperature coefficient reported in indoor measurements.
This occurs typically when long term datasets are taken and the temperature coefficients are extracted from these.
Short term measurement sub-sets tend to produce negative temperature coefficients.
The difference is that long term measurements are affected by seasonal spectral changes and thus groups utilising spectral information for correction of even long term measurements tend to still report negative temperature coefficients [46,47].
The complication of the temperature coefficient of a-Si devices makes the clear separation of these effects difficult and spectral and seasonal annealing effects be convolved also with the temperature effects.
However, it shows that irradiance effect is not predictable for material classes, which is due to the different design options of thin film devices discussed in Section 1.
The overall losses due to temperature and irradiance are, in dependence of location, between 5% - 15%.
Spectrum
It has been pointed out in the previous section that the spectrum can have a significant influence for amorphous silicon devices.
The behaviour of devices will depend on the band gap of the absorber material and potentially on the window layer, where this is present.
It is apparent from Fig. 7 that there is only a small spectral dependence for c-Si and CIGS devices.
The absolute level depends on the normalisation, but in terms of relative changes in the generated short-circuit current, the effect accounts for about ±3% for CIGS and c-Si and +10% to -20% for a-Si devices.
The overall impact on annual energy yield is slightly positive for a-Si devices and virtually zero for the other technologies.
The detailed influence of the spectrum is obviously site dependent.
Higher AM, which goes together with higher latitudes, result in redder spectra.
Higher cloudiness results in bluer spectra.
The air mass tends to have a larger effect than the cloudiness.
It has been reported, however, that for sites such as Loughborough, amorphous silicon modules gain about 4.5% annually in energy production [45], while CIGS and c-Si do not gain significantly.
However, seasonal variations, including the spectral impact, are the main source of uncertainty when it comes to the modelling of a-Si devices [14].
The influence of the spectral effects is shown in Fig. 8 where the ratio of short circuit current over irradiance is plotted.
This should be a constant for linear devices where the principle of superposition is applicable.
Any deviations of this should be a spectral effect.
It is clearly seen that both air-mass and clearness affect this ratio.
Devices with different structures and/or different materials are affected differently as shown in Fig. 8 for a wide band gap a-Si device and a much narrower band gap material of CIGS.
Change of material parameters
There are several time-scales involved in performance variations of photovoltaic devices.
In the short term, there is the direct influence of the environment as discussed in Section 4.
In the mid-term there is the Staebler-Wronski effect for a-Si [48,49] or pre-conditioning for CIGS, which affects the annual energy prediction by less than 5% typically and are not further explored here.
The effect for CIGS is slightly contradictory as there is no clarity of the time scales involved yet [4,50].
There are also long-term gradual degradation effects which affect all device technologies [51].
In many published discussions there are generalisations of technologies.
It is shown below that such a general behaviour was not observed at CREST.
The maximum power point data of all devices within an irradiance range of 650-750W/m2 have been extracted and corrected to 700W/m2 and 25°C using bi-linear interpolation.
These power values are annotated as P700.
This particular irradiance level was chosen as there is a statistically significant number of data points in each month of the year and thus fewer outliers to affect the analysis.
The result is shown in Fig. 9 (top graph).
To contrast power and energy, the monthly performance ratios are also plotted (bottom graph)Table 2.
It is visually apparent that the long term behaviour of P700 and PR degradation are different.
The initial year for the c-Si device shows an unusually low performance, which is due to an early problem with the monitoring system, rectified in February 2008.
The effect of spectrum and annealing is clearly visible for the a-Si devices, while the behaviour of CIGS and c-Si devices is less affected by the seasons.
It is also clear that within the device categories some real differences exist.
A linear line has been fitted through each of the curves and the percentage rate of change over the long term has been calculated.
The comparison of P700 and PR degradation is given in Table 3.
CIGS-1 on the other hand is one of the oldest devices and exhibits unexpected over-performance in January/February which will affect the degradation to a small extent.
It should be noted, however, that CIGS-1 is not representative for CIGS technologies in general as CIGS-2 behaves more stably, even if the overall performance ratio appears to be rather low.
This is a feature of this particular device as it has an unusual low light behaviour (Fig. 5), which is the main contributor to the annual environmental losses (Fig. 6).
This difference in behaviour confirms the earlier point that the degrees of freedom in the design of thin film devices result in very different energy delivery of devices of the same technology.
The behaviour of the different a-Si devices is also well within typical warranty limits.
There is however for most devices a trend that the PR degrades more than the power, which indicates that the energy yield is affected more than would be expected from a simple consideration of Pmpp degradation at STC or high irradiances, e.g. P700.
The changes reported above depend strongly on the accuracy of the extraction of the parameters.
An analysis of this is shown in Fig. 10, where the distribution of the annual P700 rating is shown.
This power distribution analysis, i.e. probability density function (PDF) approach is further discussed in [52,53] and can be used for analysing the performance of outdoor modules.
The shape, mathematical mean and deviation of the distributions give indications of any changes of devices as well as a clear indication of the quality of the power extraction.
One would expect a normal distribution of the PDF if the extraction is carried out reliably.
A widening of the PDF indicates that devices become more susceptible to changes in the environmental conditions and measurement uncertainties or changes between detector-response and that of the device become increasingly significant.
The distributions obtained for the different modules and using entire years for the extraction of P700 are shown in Fig. 10.
The distribution obtained for the crystalline device is much narrower than those of the thin film devices.
This depends partially on the measurement of the irradiance: the reference sensor is a pyranometer (which has a slower response than the samples and is spectrally less selective with different angle of incidence effects).
The distributions of the CIGS devices are on an average narrower than those of the a-Si devices but still wider than c-Si, which would also be largely due to the mismatch to the detectors.
This gives a ranking in the ease of the characterisation of the devices.
The width of the distributions in the a-Si case is determined by the magnitude of seasonal material changes and the change in the spectral match.
a-Si multi-junction devices have the widest of all distributions.
Device PDFs tend to be more variable with increasing age.
The oldest device, a-Si-2, has by far the widest distribution.
This seems to be indicating that especially in the case of a-Si, there is an increasing influence of seasonal material changes affecting device performance, i.e. the magnitude of the performance swing due to seasonal annealing and degradation is increasing and the matching of the junctions might be closer, thus the changes affect the short circuit current more.
The extremely wide distributions of the multi-junction devices are due to different junction matching in the wide spectral ranges observed at Loughborough [54].
The ageing of devices affects the energy yield significantly.
Assuming that the degradation rate is maintained at the given rate, the overall power degradation of the devices with the minimum/maximum degradation over a 20-year module lifetime is 3%/35% respectively.
The losses in annual energy generation are even higher for the majority of devices, leading to difference between the lifetime specific energy yield of different devices as much as 25%, assuming 25 year warranties, the current standard.
In terms of the impact on energy yield, long-term degradation appears to dominate all other influences.
Conclusions
The energy generation of a number of different photovoltaic materials has been reviewed based on long term measurements carried out at CREST.
Six modules are reviewed.
It is demonstrated that it is not realistic to make global statements for specific technologies as different devices tend to have different behaviours in terms of response to the environment as well as in terms of energy delivery and stability.
It is shown that the effect of irradiance on device performance is largely due to device design, while the temperature and spectral response are dominated by the band gap and material quality of the absorber layer.
It is shown that the different environmental effects do not contribute more than 15% losses to the annual yield even in different locations.
The precise allocation of effects varies in different climates, with maritime climates at mid- to high-latitude such as the UK having high performance ratios.
It is demonstrated that when the energy yield of a module should be investigated either through modelling or through measurements, the long term behaviour can have a much more significant influence on the device productivity than the effect of the environment.
A variation of 25% in life-time energy yield variations is possible.
Currently much less is known about the impact of the environment on ageing than directly on the instantaneous performance.
In terms of bankability of the modules, this appears to be a dangerous oversight as this may have a much higher effect on the profitability of a system than, for example, the response to operating temperature.
It is shown that as soon as more than a single module is to be considered, one needs to include the measurement accuracy of the device characterisation into consideration to assure that any inter-comparisons result in reliable and useful numbers.
It is also argued that sub-percentage deviations in the energy yield are not meaningful in the context of this uncertainty.
In the case of inter-comparisons the magnitude of uncertainty hidden in the module measurement is larger than any single environmental effect.
Acknowledgements
The authors would like to acknowledge funding of several projects of the Research Councils UK (RCUK) through its Energy Programme as well as funding of the European Commission through various FP6 and FP7 programmes during which the underlying data has been funded.
The analysis of the data has been funded through the a joint UK-India initiative in solar energy through a joint project "Stability and Performance of Photovoltaics (STAPP)" funded by Research Councils UK (RCUK) Energy Programme in UK (contract no: EP/H040331/1) and by Department of Science and Technology (DST) in India.
Without this financial support, this contribution would not have been possible.

Synthesis of porous Co3O4 nanoflake array and its temperature behavior as pseudo-capacitor electrode

Cobalt nitrate [Co(NO3)2*6(H2O)], hexamethylenetetramine (C6H12N4) and potassium hydroxide were of analytical grade and used without further purification. All aqueous solutions were freshly prepared with deionized water. The nickel foam substrate with a size of 2 cm x 3 cm was cleaned ultrasonically in ethanol for 10 min. Its top side was protected from solution contamination by uniformly coating with a polytetrafluoroethylene tape. The reaction bath for the deposition of Co(OH)2 precursor contained 0.2 M Co(NO3)2*6(H2O), and 0.1 M C6H12N4. The cleaned nickel foam substrates were immersed vertically in the deposition bath. Deposition was carried out at 100 degC for 5 h. After the deposition, the precursor films were washed by deionized water and alcohol, dried in oven at 60 degC, and then heated in a tube furnace at 250 degC for 2 h in flowing argon to obtain final product Co3O4. The average loading of Co3O4 is determined to be 3.4 mg cm-2, which is calculated by measuring the nickel foam substrate before and after the deposition via a DENVER TB-25 analytical balance.

A simple method to synthesize V 2 O 5 nanostructures with controllable morphology for high performance Li-ion batteries
V2O5 nanomaterials were synthesized using solid-state chemical reaction methods. All the reagents were analytically pure from commercial sources and used without further purification. Typical synthesis for V2O5 nanostructures was as follows. Solid ammonium metavanadate (NH4VO3, AR, >99%) and tartaric acid (C4H6O6, AR, >99%) with a molar ratio of 1:2, and polyethylene glycol 4000 (PEG-4000, AR, >99%) were apart weighed and ground in an agate mortar for about 5 min to ensure the evenness of starting materials, then mixed. Then the reactants were mixed, and the color of mixture changed from white to yellow. After grind for 30 min at room temperature, the color of mixture changed from yellow to grey. The obtained sample was washed with absolute ethanol, and then dried at 60 degC for 10 h. Finally pure grey precursor (NH4)2[(VO)(C4H4O6)2] was got. Then the precursor was placed in muffle furnace at different calcination temperature of 400 degC, 500 degC, 600 degC for 4 h (1 degC min-1) to obtain final products.Synthesis, characterization, magnetic measurements and liquefied petroleum gas sensing properties of nanostructured cobalt ferrite and ferric oxide
All reagents such as ferric chloride, cobaltous acetate and ammonium hydroxide were of analytical grade and used without further purification. The stoichiometric amount of starting materials, such as cobaltous acetate and ferric chloride were taken in 1:1, 1:2, 1:3 and 1:4 M ratios, respectively, and dissolved into required amount of distilled water to form 1 M solution. First of all, we have taken cobaltous acetate and ferric chloride in 1:1 M ratios and dissolved into respective required amount of distilled water to form precursors. The above precursors were refluxed at 60 degC for 4 h to get homogeneous solution. After that both were mixed with each other. The obtained mixed solution was heated at 70-80 degC and magnetically stirred for 4 h. 10 ml of poly-ethylene glycol (PEG) was added to the mixed solution which acts as an encapsulating agent. The resulting solution was precipitated by ammonium hydroxide solution, which was added drop by drop to the above mixed solution and a black colored precipitate was obtained. The pH of the solution was constantly monitored as the NH4OH solution was added until it reached to 12. Then the mixture was mechanically stirred at room temperature for 6 h and the precipitate was washed several times with distilled water until the pH of the filtrate became 7. This whole procedure was repeated for synthesis of CoFe2O4 in 1:2, 1:3 as well as 1:4 M ratios, respectively. Here, ammonium hydroxide was used for smooth liberation of the hydroxide ions in place of strong alkali (potassium hydroxide and sodium hydroxide). Strong alkaline solutions are prone to result in the conversion of Fe3+ and Co2+ into CoFe2O4 immediately, usually leading to the formation of severely agglomerated nanoparticles with irregular shapes. Similar procedure was used for the synthesis of nanostructured ferric oxide.
Initially in aqueous solution, metal ions exist as Co ( H 2 O ) 6 2 + and Fe ( H 2 O ) 6 3 + . As pH was increased, the predominant species existing in solution became Co ( OH ) 6 2 - x and Fe ( OH ) y 3 - y , respectively. At the solubility minima, the predominant species in solution were Co(OH)2 and Fe(OH)3. Further, when the concentration of ammonia solution was increased, cobalt and iron became more soluble as Co ( OH ) 3 - and Fe ( OH ) 4 - species. The synthesis temperature was maintained at 70-80 degC for 4-5 h under vigorous magnetic stirring. The precipitate of CoFe2O4 was obtained through centrifugal settling for 15 min at 3000 rpm. The precipitate was dried overnight at 100 degC. The dried material was grinded into fine powder. The fine powder was then annealed at 450 degC inside a tubular furnace for 2 h with heating and cooling rate of 2 degC per minute. Sensing pellets of the synthesized powder were made by hydraulic press (MB instruments, Delhi) under a uniaxial pressure of 616 MPa. Hereafter, the cobalt ferrite pellets in 1:4, 1:3, 1:2 and 1:1 M ratios were named as P-1, P-2, P-3 and P-4, respectively.Mechanical properties of organic-inorganic halide perovskites, CH3NH3PbX3 (X = I, Br and Cl), by nanoindentation

Single crystals of CH3NH3PbX3 (X = I, Br and Cl) were prepared by precipitation from hydrochloric, hydrobromic and hydroiodic acid solutions after the method of Poglitsch et al.25 In this procedure, 2.5 g lead(II) acetate (Chemical Reagents, Sigma-Aldrich) was dissolved in 10 ml of concentrated aqueous HCl (37 wt%)/HBr (48 wt%)/HI (57 wt%) and heated to above 90 degC. To this solution, 2 ml of concentrated HX (X = Cl, Br and I) was added together with 0.597 g of CH3NH2 (40 wt% aqueous solution, Merck). Crystals about 2 mm in dimension were obtained by cooling the solutions from ~90 degC to 25 degC over 72 hours. The products were colourless for the chloride, orange for the bromide and black for the iodide (Fig. 1). For CH3NH3PbI3, the crystals were separated at above 40 degC to avoid the formation of yellow crystalline (CH3NH3)4PbI6*2H2O.26

5. The production method for producing transition metal composite hydroxide particles according to claim 1, wherein the transition metal composite hydroxide particles are transition metal composite hydroxide particles that are expressed by the general expression (A): NixMnyCozMt(OH)2+a, where x+y+z+t=1, 0.3≦x≦0.95, 0.05≦y≦0.55, 0≦z≦0.4, 0≦t≦0.1, 0≦a≦0.5, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W.7. Transition metal composite hydroxide particles that are the precursor for cathode active material for a non-aqueous electrolyte rechargeable battery, comprising secondary particles that are formed by an aggregation of plate-shaped primary particles and fine primary particles that are smaller than the plate-shaped primary particles;
the secondary particles having a center section that is formed by an aggregation of the plate-shaped primary particles, and one layered structure of a low-density section that is formed by an aggregation of the fine primary particles and a high-density section that is formed by an aggregation of the plate-shaped primary particles on the outside of the center section;
the average value of the ratio of the center section outer diameter with respect to the particle size of the secondary particles being 30% to 80%, and the average value of the high-density section radial direction thickness with respect to the particle size of the secondary particles being 5% to 25%; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90−d10)/average particle size] that indicates the extent of the particle size distribution of 0.65 or less.8. Transition metal composite hydroxide particles that are the precursor for cathode active material for a non-aqueous electrolyte rechargeable battery, comprising secondary particles that are formed by an aggregation of plate-shaped primary particles and fine primary particles that are smaller than the plate-shaped primary particles;
the secondary particles having a center section that is formed by an aggregation of plate-shaped primary particles, and two or more layered structure of a low-density section that is formed by an aggregation of the fine primary particles and a high-density section that is formed by an aggregation of the plate-shaped primary particles on the outside of the center section; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90−d10)/average particle size] that indicates the extent of the particle size distribution of 0.65 or less.10. The transition metal composite hydroxide particles according to claim 7, wherein the transition metal composite hydroxide particles are transition metal composite hydroxide particles that are expressed by the general expression (A): NixMnyCozMt≦(OH)2+a, where, x+y+z+t=1, 0.3≦x≦0.95, 0.05≦y≦0.55, 0≦z≦0.4, 0≦t≦0.1, 0≦a≦0.5, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W).15. The production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 12, wherein the cathode active material comprises layered hexagonal crystal lithium nickel manganese composite oxide particles that are expressed by the general expression (B): Li1+uNixMnyCozMtO2, where −0.05≦u≦0.50, x+y+z+t=1, 0.3≦x≦0.95, 0.05≦y≦0.55, 0≦z≦0.4, 0≦t≦0.1, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W.16. Cathode active material for a non-aqueous electrolyte rechargeable battery comprising secondary particles that are formed by an aggregation of plural primary particles,
the cathode active material comprising layered hexagonal crystal lithium nickel manganese composite oxide particles that are expressed by the general expression (B): Li1+uNixMnyCozMtO2, where −0.05≦u≦0.50, x+y+z+t=1, 0.3≦x≦0.95, 0.05≦y≦0.55, 0≦z≦0.4, 0≦t≦0.1, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W,
the secondary particles having a center section having solid or hollow structure, and at least a hollow section where there are no primary particles and an outer-shell section that is electrically connected to the center section on the outside of the center section;
the average value of the ratio of the center section outer diameter with respect to the particle size of the secondary particles being 30% to 80%, and the average value of the ratio of the outer-shell section radial direction thickness being 5% to 25%; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90−d10)/average particle size] that indicates the extent of the particle size distribution of 0.7 or less.17. Cathode active material for a non-aqueous electrolyte rechargeable battery comprising secondary particles that are formed by an aggregation of plural primary particles,
the cathode active material comprising layered hexagonal crystal lithium nickel manganese composite oxide particles that are expressed by the general expression (B): Li1+uNixMnyCozMtO2, where, −0.05≦u≦0.50, x+y+z+t=1, 0.3≦x≦0.95, 0.05≦y≦0.55, 0≦z≦0.4, 0≦t≦0.1, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W,
the secondary particles comprising a center section, and a hollow section where there are no primary particles, an outer-shell section that is electrically connected to the center section, and at least one inner-shell section between the hollow section and the outer-shell section on the outside of the center section; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90−d10)/average particle size] that indicates the extent of the particle size distribution of 0.7 or less.1. Transition metal composite hydroxide particles which are precursors of positive electrode active materials for nonaqueous electrolyte secondary batteries, wherein,
the transition metal composite hydroxide particles are composed of a plurality of plate-like primary particles and secondary particles formed by aggregating fine primary particles smaller than the plate-like primary particles,
the secondary particles have a central portion formed by the aggregation of the plate-like primary particles, and have a layered structure formed by the lamination of a low-density portion formed by the aggregation of the fine primary particles and a high-density portion formed by the aggregation of the plate-like primary particles outside the central portion,
the average value of the ratio of the outer diameter of the central portion to the particle diameter of the secondary particles is 30 to 80%, and the average value of the ratio of the thickness of the high-density portion in the diameter direction to the particle diameter of the secondary particles is 5 to 25%,
the secondary particles have an average particle diameter of 1 to 15 [ mu ] m, and the [ (d90-d 10)/average particle diameter ] as an index indicating the width of the particle size distribution is 0.65 or less.2. The transition metal composite hydroxide particles according to claim 1, wherein the transition metal composite hydroxide particles are represented by the general formula (A): niXMnyCozMt(OH)2+aIn the transition metal composite hydroxide particles represented by general formula (A), x + y + z + t is 1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, 0. ltoreq. a.ltoreq.0.5, and M is one or more additive elements selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.3. The transition metal composite hydroxide particles according to claim 2, wherein the additive element M is uniformly distributed in the interior of the secondary particles and/or uniformly coats the surfaces of the secondary particles.4. Transition metal composite hydroxide particles which are precursors of positive electrode active materials for nonaqueous electrolyte secondary batteries, wherein,
the transition metal composite hydroxide particles are composed of a plurality of plate-like primary particles and secondary particles formed by aggregating fine primary particles smaller than the plate-like primary particles,
the secondary particles have a central portion formed by the aggregation of the plate-like primary particles, and have a laminated structure in which a low-density portion formed by the aggregation of the fine primary particles and a high-density portion formed by the aggregation of the plate-like primary particles are laminated outside the central portion,
the secondary particles have an average particle diameter of 1 to 15 [ mu ] m, and the [ (d90-d 10)/average particle diameter ] as an index indicating the width of the particle size distribution is 0.65 or less.5. The transition metal composite hydroxide particles according to claim 4, wherein the average value of the ratio of the outer diameter of the central portion to the particle diameter of the secondary particles is 20 to 70%, and the average value of the ratio of the thickness in the diameter direction of the high-density portion of each layer to the particle diameter of the secondary particles is 5 to 25%.6. The transition metal composite hydroxide particles according to claim 4 or 5, wherein the transition metal composite hydroxide particles are represented by the general formula (A): niXMnyCozMt(OH)2+aIn the transition metal composite hydroxide particles represented by general formula (A), x + y + z + t is 1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, 0. ltoreq. a.ltoreq.0.5, and M is one or more additive elements selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.7. The transition metal composite hydroxide particles according to claim 6, wherein the additive element M is uniformly distributed in the secondary particles and/or uniformly coats the surfaces of the secondary particles.8. A method for producing transition metal composite hydroxide particles serving as a precursor of a positive electrode active material for a nonaqueous electrolyte secondary battery by a crystallization reaction, the method comprising:
a nucleus-forming step of controlling an aqueous solution for nucleus formation containing at least a transition metal-containing metal compound and an ammonium ion donor so that the aqueous solution for nucleus formation has a pH of 12.0 to 14.0 at a liquid temperature of 25 ℃ and forming nuclei,
a particle growth step of controlling the aqueous solution for particle growth containing the nuclei obtained in the nucleus production step so that the pH of the aqueous solution for particle growth becomes lower than that in the nucleus production step at a liquid temperature of 25 ℃ and 10.5 to 12.0 to grow the nuclei,
and the reaction environment at the initial stage of the nucleus generation step and the particle growth step is a non-oxidizing environment having an oxygen concentration of 5% by volume or less,
in the particle growth step, an environmental control is performed once in a range of 5% to 35% of the total time of the particle growth step from the start of the particle growth step, the environmental control being: the reaction environment is switched from the non-oxidizing environment to an oxidizing environment having an oxygen concentration of more than 5% by volume, and then from the oxidizing environment to a non-oxidizing environment having an oxygen concentration of 5% by volume or less, so that the crystallization reaction time in the oxidizing environment is 3% to 20% of the total time of the particle growth step.9. The method for producing transition metal composite hydroxide particles according to claim 8, wherein the transition metal composite hydroxide particles are represented by the general formula (A): niXMnyCozMt(OH)2+aIn the transition metal composite hydroxide particles represented by general formula (A), x + y + z + t is 1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, 0. ltoreq. a.ltoreq.0.5, and M is one or more additive elements selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.10. The method for producing transition metal composite hydroxide particles according to claim 9, further comprising a coating step of coating the transition metal composite hydroxide particles with a compound containing the additive element M after the particle growth step.11. A method for producing transition metal composite hydroxide particles serving as a precursor of a positive electrode active material for a nonaqueous electrolyte secondary battery by a crystallization reaction, the method comprising:
a nucleus-forming step of controlling an aqueous solution for nucleus formation containing at least a transition metal-containing metal compound and an ammonium ion donor so that the aqueous solution for nucleus formation has a pH of 12.0 to 14.0 at a liquid temperature of 25 ℃ and forming nuclei,
a particle growth step of controlling the aqueous solution for particle growth containing the nuclei obtained in the nucleus production step so that the pH of the aqueous solution for particle growth becomes lower than that in the nucleus production step at a liquid temperature of 25 ℃ and 10.5 to 12.0 to grow the nuclei,
and the reaction environment at the initial stage of the nucleus generation step and the particle growth step is a non-oxidizing environment having an oxygen concentration of 5% by volume or less,
in the particle growth step, the environmental control is performed 2 or more times, and the environmental control is performed by: the reaction environment is switched from the non-oxidizing environment to an oxidizing environment in which the oxygen concentration exceeds 5% by volume, and then from the oxidizing environment to a non-oxidizing environment in which the oxygen concentration is 5% by volume or less,
the total crystal reaction time in the oxidizing environment in the particle growth step is 3% to 30% of the total time in the particle growth step, and the crystal reaction time in each oxidizing environment is 1% or more of the total time in the particle growth step.12. The method for producing transition metal composite hydroxide particles according to claim 11, wherein in the particle growth step, the non-oxidizing atmosphere is switched to the oxidizing atmosphere within a range of 5% to 35% of the total time of the particle growth step from the start of the particle growth step.13. The method for producing transition metal composite hydroxide particles according to claim 11 or 12, wherein the transition metal composite hydroxide particles are represented by general formula (A): niXMnyCozMt(OH)2+aIn the transition metal composite hydroxide particles represented by general formula (A), x + y + z + t is 1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, 0. ltoreq. a.ltoreq.0.5, and M is one or more additive elements selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.14. The method for producing transition metal composite hydroxide particles according to claim 13, further comprising a coating step of coating the transition metal composite hydroxide particles with a compound containing the additive element M after the particle growth step.15. A method for producing a positive electrode active material for a nonaqueous electrolyte secondary battery, comprising:
a mixing step of mixing the transition metal composite hydroxide particles according to any one of claims 1 to 7 with a lithium compound to form a lithium mixture, and,
and a firing step of firing the lithium mixture formed in the mixing step in an oxidizing atmosphere at a temperature of 650 to 980 ℃.16. The method for producing a positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 15, wherein in the mixing step, the lithium mixture is adjusted so that the ratio of the sum of the numbers of atoms of the metals other than lithium contained in the lithium mixture to the number of lithium atoms is 1: 0.95 to 1.5.17. The method for producing a positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 16, further comprising a heat treatment step of heat-treating the transition metal composite hydroxide particles at a temperature of 105 to 750 ℃ before the mixing step.18. The method for producing a positive electrode active material for a nonaqueous electrolyte secondary battery according to any one of claims 15 to 17, wherein the positive electrode active material for a nonaqueous electrolyte secondary battery is represented by general formula (B): li1+uNixMnyCozMtO2The positive electrode active material for a nonaqueous electrolyte secondary battery is composed of hexagonal lithium nickel manganese composite oxide particles having a layered structure, and in the general formula (B), -0.05. ltoreq. u.ltoreq.0.50, x + y + z + t-1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, and M is at least one additive element selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.19. A positive electrode active material for a nonaqueous electrolyte secondary battery, which is represented by the general formula (B): li1+uNixMnyCozMtO2The positive electrode active material is composed of lithium transition metal composite oxide particles having a hexagonal crystal structure and a layered structure, and in the general formula (B), -0.05. ltoreq. u.ltoreq.0.50, x + y + z + t 1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, M is one or more additive elements selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, W,
the positive electrode active material for a nonaqueous electrolyte secondary battery is composed of secondary particles formed by aggregating a plurality of primary particles,
the secondary particles have a central portion having a hollow structure or a hollow structure, and a space portion having no primary particles and an outer shell portion electrically connected to the central portion outside the central portion,
and the average value of the ratio of the outer diameter of the central portion to the particle diameter of the secondary particles is 30 to 80%, the average value of the ratio of the thickness of the outer shell portion in the diameter direction to the particle diameter of the secondary particles is 5 to 25%,
the secondary particles have an average particle diameter of 1 to 15 [ mu ] m, and the [ (d90-d 10)/average particle diameter ] as an index indicating the width of the particle size distribution is 0.7 or less.20. The positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 19, wherein the specific surface area is 0.7m2/g～3.0m2/g。21. A positive electrode active material for a nonaqueous electrolyte secondary battery, which is represented by the general formula (B): li1+uNixMnyCozMtO2The positive electrode active material is composed of lithium transition metal composite oxide particles having a hexagonal crystal structure and a layered structure, and in the general formula (B), -0.05. ltoreq. u.ltoreq.0.50, x + y + z + t 1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, M is one or more additive elements selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, W,
the positive electrode active material for a nonaqueous electrolyte secondary battery is composed of secondary particles formed by aggregating a plurality of primary particles,
the secondary particles have a central portion of a hollow structure or a hollow structure, and have at least a space portion where the primary particles are not present and an outer shell portion electrically connected to the central portion outside the central portion, and at least one inner shell portion between the space portion and the outer shell portion,
the secondary particles have an average particle diameter of 1 to 15 [ mu ] m, and the [ (d90-d 10)/average particle diameter ] as an index indicating the width of the particle size distribution is 0.7 or less.22. The positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 21, wherein an average value of a ratio of an outer diameter of the central portion to a particle diameter of the secondary particles is 20% to 70%, and an average value of a ratio of a total thickness of the inner case portion and the outer case portion in a diameter direction to the particle diameter of the secondary particles is 10% to 35%.23. The positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 21 or 22, wherein the specific surface area is 0.7m2/g～3.0m2/g。24. A nonaqueous electrolyte secondary battery comprising a positive electrode, a negative electrode, a separator, and a nonaqueous electrolyte, wherein the positive electrode active material for nonaqueous electrolyte secondary batteries according to any one of claims 19 to 23 is used as a positive electrode material of the positive electrode.Significant influence of insufficient lithium on electrochemical performance of lithium-rich layered oxide cathodes for lithium ion batteries

Li[Li1/3-2x/3Mn2/3-x/3Nix]O2 (x = 0.3) was prepared in two steps: formation of transition-metal hydroxide (Ni0.35Mn0.65(OH)2) and subsequent solid phase reaction of Ni0.35Mn0.75(OH)2 and Li2CO3. Specially, 8.74 g MnSO4*H2O (99%) and 7.20 g NiSO4*6H2O (98%) were dissolved into 400 ml H2O under magnetic stirring, and 10 ml ammonia hydroxide was then added as chelating agent. After being kept at 60 degC for 10 min, 100 ml aqueous solution containing 6.4 g KOH (98%) was slowly dropped, and the precipitation reaction was kept for 24 h. The resulting precipitation was mixed completely with lithium carbonate (98%). In this mixing, the precursors with 10 wt% excess lithium (E10), 5 wt% excess lithium (E5), stoichiometric lithium (ST), and 5 wt% deficient lithium (D5) were used. Four samples were obtained by calcining the mixture at 900 degC for 12 h, denoted as 900E10, 900E5, 900ST, and 900D5, respectively.


1. A positive electrode active material precursor comprising:
a transition metal hydroxide particle represented by Formula 1 below; and
a cobalt oxide particle and a manganese oxide particle attached to a surface of the transition metal hydroxide particle:
[Formula 1]     [NiaCObM1cM2d] (OH)2
in Formula 1, 0.8≤a<1, 0<b<0.2, 0≤c≤0.1, and 0≤d≤0.1, M<sup>1</sup> is one or more selected from the group consisting of Mn and Al, and M<sup>2</sup> is one or more selected from the group consisting of Ba, Ca, Zr, Ti, Mg, Ta, Nb, and Mo.2. The positive electrode active material precursor of claim 1, wherein the positive electrode active material precursor contains cobalt in an amount of 10-18000 ppm based on a total weight of the positive electrode active material precursor.3. The positive electrode active material precursor of claim 1, wherein the positive electrode active material precursor contains manganese in an amount of 10-12000 ppm based on the total weight of the positive electrode active material precursor.4. The positive electrode active material precursor of claim 1, wherein a weight ratio of cobalt:manganese in the positive electrode active material precursor is 6:4 to 8:2.5. The positive electrode active material precursor of claim 1, wherein a molar ratio of nickel and cobalt elements in the transition metal hydroxide particle are constant.6. A method for preparing a positive electrode active material precursor, the method comprising:
forming a transition metal hydroxide particle represented by Formula 1 below by adding an ammonium-containing complex forming agent and a basic compound to a metal solution containing a nickel raw material and a cobalt raw material to cause a co-precipitation reaction; and
adding a cobalt oxide particle and a manganese oxide particle into a reaction solution, in which the transition metal hydroxide particle is formed, to attach the cobalt oxide particle and the manganese oxide particle to the surface of the transition metal hydroxide particle:
[Formula 1]     [NiaCObM1cM2d] (OH)2
in Formula 1, 0.8≤a<1, 0<b<0.2, 0≤c≤0.1, and 0≤d≤0.1, M<sup>1</sup> is one or more selected from the group consisting of Mn and Al, and M<sup>2</sup> is one or more selected from the group consisting of Ba, Ca, Zr, Ti, Mg, Ta, Nb, and Mo.7. The method of claim 6, wherein the metal solution further comprises at least one among an M1-containing raw material and an M2-containing raw material.8. The method of claim 6, wherein, when the average particle diameter (D50) of a positive electrode active material precursor to be finally obtained is 100%, the cobalt oxide particle and the manganese oxide particle are added when an average particle diameter (D50) of the transition metal hydroxide particle formed in the reaction solution is 60% or greater.9. The method of claim 6, wherein the cobalt oxide particle and the manganese oxide particle each have an average particle diameter (D50) of 10-500 nm.10. The method of claim 6, wherein the cobalt oxide particle and the manganese oxide particle are added such that a molar ratio of the cobalt:manganese in the positive electrode active material precursor is 6:4 to 8:2.11. A positive electrode active material comprising:
a core portion composed of a lithium composite transition metal oxide particle represented by Formula 2 below; and
a shell portion formed on the core portion,
wherein the molar fractions of nickel, manganese, and cobalt elements in the core portion are constant, and the molar fractions of cobalt and manganese in the shell portion are higher than the molar fractions of cobalt and manganese in the core portion:
[Formula 2]     Lix[Nia'COb'M1c'M2d'] O2-yXy
in Formula 2, 0.8≤a'<1, 0<b'<0.2, 0<c'≤0.1, 0≤d'≤0.1, 1.0≤x≤1.5, and 0≤y≤0.2, M<sup>1</sup> is one or more selected from the group consisting of Mn and Al, M<sup>2</sup> is one or more selected from the group consisting of Ba, Ca, Zr, Ti, Mg, Ta, Nb, and Mo, and X is P or F.12. A positive electrode for a secondary battery comprising the positive electrode active material of claim 11.13. A secondary battery comprising the positive electrode for a secondary battery of claim 12.Three-dimensional aluminum foam/carbon nanotube scaffolds as long- and short-range electron pathways with improved sulfur loading for high energy density lithium-sulfur batteries
The CNTs were mass-produced on Fe based catalysts in a fluidized bed reactor [33]. The as-grown carbon products were purified by sodium hydroxide (12.0 mol L-1) aqueous solution at 160 degC for 4.0 h and hydrochloric acid (5.0 mol L-1) aqueous solution at 70 degC for 4.0 h, subsequently. The high purity CNT bundles were available after filtering, washing, and freeze-drying. Then the CNTs were mixed with sulfur powder in the anticipated mass ratio of 1:1 and ball-milled for 3.0 h to form CNT/sulfur composites.Low temperature electrochemical performance of β-LixV2O5 cathode for lithium-ion batteries

The LixV2O5 sample was prepared through a hydrothermal reaction followed by calcination treatment. All chemicals were used directly without further purification. In a typical procedure, 0.1 g V2O5 and 3.0 g LiNO3 were added into 33 mL de-ionized water with vigorously stirring at room temperature. The resultant orange-yellow suspension was then transferred into a 42 mL Teflon-lined stainless autoclave and kept in an oven at 220 degC for 24 h. After cooling down naturally, the precursor was collected by centrifugation and washed with absolute ethanol for several times before drying. The final product of LixV2O5 was obtained by calcining the precursor hydrate compound at 600 degC for 3 h under Ar atmosphere with a ramping rate of 5 degC /min.

A Microsized Cagelike Sulfur/Carbon Composite for a Lithium/Sulfur Battery with Excellent Performance
Soluble phenolic resin was prepared by following the methods reported by Zhao et al.22 To synthesize MWCNT@mesoporous carbon, the ratio of the carbon source/template/surfactant was optimized, according to Nazar's group.10 In summary, F127 (1.6 g), ethanol (8.0 g), and 0.2 M HCl (1.0 g) were stirred for 2 h at 40 degC to afford a clear solution. Next, TEOS (2.08 g), MWCNTs (1.0 g), and a 20 wt % solution of resols in ethanol were added in sequence and stirred for another 5 h. The mixture was evaporated at room temperature and held for 24 h at 100 degC in an oven for thermo-polymerization. The as-synthesized product was carbonized in a furnace under an argon atmosphere at 900 degC for 300 min. The obtained sample was then washed with 10 wt % HF to remove silica.
At a weight ratio of 1:1, the MWCNT@MPC composite was mixed with sublimed sulfur under ball-milling for 6 h and then heated at 155 degC for 12 h in an argon atmosphere, resulting in the SC composite. For comparison, pristine S-MPC composite was also prepared by the same method as that described above. For both samples, the sulfur content was 50 wt %.Highly permeable zeolite imidazolate framework composite membranes fabricated via a chelation-assisted interfacial reaction
Polyacrylonitrile (PAN) UF membranes (MWCO: 200 kDa) were purchased from Beijing Ande Technology Corporation (China). Sodium hydroxide (NaOH), sodium chloride (NaCl), and magnesium chloride (MgCl2) were supplied from Beijing Chemical Works (China). Methyl blue (MB), Congo red (CR), acid fuchsin (AF), magnesium sulfate (MgSO4), and sodium sulfate (Na2SO4) were obtained from Tianjin Guangfu Fine Chemical Institute (China) and used for the NF performance test. The precursors of ZIF-8, including zinc nitrate hexahydrate (Zn(NO3)2*6H2O) and 2-methylimidazole (Hmim, 99.5%), were purchased from Aladdin Industrial Corporation (China). Polyethyleneimine (PEI, Mw: 750000 Da) was supplied by TCI Chemicals (Japan). Bovine serum albumin (BSA, Mw: 67000 g mol-1) and humic acid (HA) were purchased from Beijing Aobox Biotechnology (China) and Macklin Inc. (China), respectively. Ultrapure water was produced by a lab water purification system (HYP-QX, China). All the chemicals and materials were used as received.

The preparation process of the ZIF-8/PEI-HPAN composite membrane is illustrated in Scheme 2, in which the structure of ZIF-8 has been cited from the literature.29 To obtain a negatively charged surface, the PAN substrate was first hydrolyzed in an NaOH aqueous solution (2 mol L-1) at 65 degC for 60 min, followed by rinsing with DI-water (step 1).30 Then, the hydrolyzed PAN (HPAN) substrate was soaked in a 0.2-0.6 mol L-1 Zn(NO3)2 aqueous solution for 5 h at 25 +- 1 degC to coordinate Zn2+ on the HPAN substrate surface (step 2).16,22 Next, the membrane was introduced into a mixed PEI-Zn(II) complex solution containing 0.1-0.3 mol L-1 zinc nitrate and 0.4 wt% PEI aqueous solution for 0.5 h at a constant temperature (20-50 degC, step 3). After this, the residual aqueous solution was poured off, and the residual water droplets on the membrane surface were gently wiped off using a filter paper. Finally, the PEI-Zn(II)-saturated membrane was immersed in a 0.20 mol L-1 Hmim in a hexane solution for 5-20 min at 25 +- 2 degC to synthesize ZIF-8 crystals in situ (step 4).14 The obtained membrane was then rinsed three times with ethanol and dried at room temperature (25 +- 2 degC) for 2 h before further use.Catalytic oxidation of benzene by ozone over manganese oxides supported on USY zeolite

Manganese oxide catalysts supported on ultrastable zeolite Y (Mn/USY) (Zeolyst CBV390H, SiO2/Al2O3 = 180, surface area 875 m2/g) were prepared by an impregnation method. An aqueous solution containing the appropriate amount of Mn(CH3COO)2*4H2O (Wako Pure Chemical, Japan, >99.9%) was added to the powders of USY and evaporated to dryness at 343 K. The catalyst samples were dried at 383 K for 24 h and then calcined at 773 K for 3 h. The Mn loading was varied from 5 to 15 wt% by changing the precursor amounts. SiO2-supported manganese oxide catalysts (Mn/SiO2) were prepared by the identical technique. Reference samples - MnO, Mn3O4, Mn2O3 and MnO2 - were obtained from Sigma-Aldrich Co. LLC.

Induced Pluripotent Stem Cell Models of Progranulin-Deficient Frontotemporal Dementia Uncover Specific Reversible Neuronal Defects

Summary
The pathogenic mechanisms of frontotemporal dementia (FTD) remain poorly understood.
Here we generated multiple induced pluripotent stem cell lines from a control subject, a patient with sporadic FTD, and an FTD patient with a novel heterozygous GRN mutation (progranulin [PGRN] S116X).
In neurons and microglia differentiated from PGRN S116X induced pluripotent stem cells, the levels of intracellular and secreted PGRN were reduced, establishing patient-specific cellular models of PGRN haploinsufficiency.
Through a systematic screen of inducers of cellular stress, we found that PGRN S116X neurons, but not sporadic FTD neurons, exhibited increased sensitivity to staurosporine and other kinase inhibitors.
Moreover, the serine/threonine kinase S6K2, a component of the phosphatidylinositol 3-kinase and mitogen-activated protein kinase pathways, was specifically downregulated in PGRN S116X neurons.
Both increased sensitivity to kinase inhibitors and reduced S6K2 were rescued by PGRN expression.
Our findings identify cell-autonomous, reversible defects in patient neurons with PGRN deficiency, and provide a compelling model for studying PGRN-dependent pathogenic mechanisms and testing potential therapies.
Graphical Abstract
Highlights
► A human neuron model of progranulin (PGRN) haploinsufficiency is established ► Sporadic and PGRN-deficient frontotemporal dementia patient iPSCs are made ► PGRN S116X mutant neurons are sensitive to stress by kinase inhibitors ► S6K2 is downregulated in patient neurons in a PGRN-dependent manner
Progranulin (PGRN) mutations are a common cause of frontotemporal dementia, but the underlying molecular mechanism is unknown.
Gao and colleagues now generate multiple frontotemporal dementia patient-specific induced pluripotent stem cell lines and establish a human neuronal model of PGRN haploinsufficiency.
Studies on human postmitotic neurons derived from these lines reveal cell-autonomous and reversible defects in specific signaling pathways that are compromised in PGRN-deficient neurons and establish a model system for studying PGRN-dependent pathogenic mechanisms and testing potential therapies.

Introduction
Frontotemporal dementia (FTD), the second most common form of presenile dementia before the age of 65, is associated with focal atrophy of the frontal or temporal lobes and deficits in cognition, behavior, and language (Boxer and Miller, 2005).
Mutations that cause FTD have been identified in several genes, including those encoding valosin-containing protein (VCP; Watts et al., 2004), charged multivesicular body protein 2B (CHMP2B; Skibinski et al., 2005), progranulin (PGRN; Baker et al., 2006; Cruts et al., 2006), and chromosome 9 open reading frame 72 (C9ORF72; DeJesus-Hernandez et al., 2011; Renton et al., 2011).
It is not known how these diverse mutations cause similar clinical manifestations, and no effective treatment is available.
The secreted glycoprotein PGRN has been implicated in cell growth and survival, inflammation, synaptic functions, and other cellular functions (He and Bateman, 2003; Yin et al., 2010; Tapia et al., 2011).
Although most (if not all) pathogenic mutations in GRN lead to pathological changes in FTD due to PGRN haploinsufficiency (Baker et al., 2006; Cruts et al., 2006), the underlying molecular mechanism is unknown.
PGRN mutations are a common cause of FTD.
However, no robust pathological phenotype has been found in Grn+/- mice, and selective neuronal cell loss is limited even in Grn knockout mice (Ahmed et al., 2010; Ghoshal et al., 2012; Petkau et al., 2012; Yin et al., 2010).
Thus, a more suitable model for dissecting the pathogenic mechanisms that underlie PGRN haploinsufficiency is needed.
The ability to generate human induced pluripotent stem cells (iPSCs) offers an unprecedented opportunity to analyze the molecular consequences of pathogenic mutations in the context of the unique genetic background of individual patients (Yamanaka, 2007).
Indeed, iPSCs have been generated from patients with different neurodegenerative diseases (e.g., Dimos et al., 2008; Ebert et al., 2009; Soldner et al., 2009; Nguyen et al., 2011; Israel et al., 2012).
In this study, we generated multiple FTD-patient-specific iPSC lines and established a human neuronal model of PGRN haploinsufficiency.
From studies of human postmitotic neurons derived from these lines, we identify cell-autonomous and reversible defects in specific signaling pathways that are compromised in PGRN-deficient neurons.
Results
Generation and Characterization of FTD-Patient-Specific iPSCs
The two FTD patients under investigation in this study were part of a longitudinal dementia research program at the Memory and Aging Center, University of California, San Francisco.
Both had an 8-year history of behavioral changes and memory impairment at the time of tissue collection for this study.
One patient, a 67-year-old male with sporadic FTD, tested negative for mutations in GRN, MAPT, and C9ORF72.
The other patient, a 64-year-old male with a significant family history of dementia, had behavioral variant FTD.
MRI in this patient demonstrated severe bifrontal and temporal atrophy associated with gliosis in the frontal lobes (greater on the right).
One year later, MRI scans showed progression of atrophy and gliosis.
Genetic testing revealed a novel nonsense mutation in GRN, p.S116X (g.4627C > A, c.347C > A), which is predicted to result in a premature stop codon.
Both FTD patients had parkinsonism, which is typical of all FTD patients with PGRN mutations.
An age-matched subject, a clinically normal 64-year-old male with no mutations in GRN, MAPT, or C9ORF72, served as a control.
Skin biopsies from the upper right thigh were obtained from all three subjects, and primary fibroblasts were derived.
After expansion, the fibroblasts were reprogrammed with four transcription factors (OCT3/4, SOX2, KLF4, and CMYC) into putative pluripotent stem cells as previously described (Takahashi et al., 2007).
Approximately 5 weeks after viral transduction, >50 iPSC colonies per subject were selected on the basis of their embryonic stem cell (ESC)-like morphology and expanded further on feeder cells.
To identify lines in which the exogenous reprogramming factors were completely silenced, we characterized 10-15 putative iPSC lines from each subject by quantitative RT-PCR (qRT-PCR).
Complete transgene silencing was achieved when the total expression of each reprogramming factor did not differ from that of the endogenous gene (Figures 1A-1C).
This assay allows us to detect transgene expression levels of at least 20-fold above than the endogenous levels in ESC H9 cells.
Based on this analysis, we selected three lines per subject for further characterization: control lines 16, 17, and 20 (Figure 1A); sporadic FTD lines 9, 12, and 23 (Figure 1B); and PGRN S116X lines 1, 14, and 26 (Figure 1C).
The total expression of each reprogramming factor was not different from that of the endogenous gene (Figures 1A-1C), indicating complete transgene silencing.
All lines expressed marker genes of undifferentiated ESCs, such as OCT3/4, SOX2, and NANOG, at levels comparable to those in ESC line H9.
Two additional stem cell markers, teratocarcinoma-derived growth factor 1 (TDGF1, or CRIPTO) and zinc finger protein 42 (ZFP42, or Rex1), were also expressed at levels equivalent to those in H9 cells (Figure S1A).
Sequencing confirmed that the PGRN S116X iPSC lines retained the GRN nonsense mutation (g.4627C > A, c.347C > A; Figure 1D).
Analysis of the OCT4 promoter region showed that undifferentiated iPSCs were hypomethylated relative to the respective fibroblasts from which they were derived (Figure 1E).
In addition, iPSCs expressed the stem-cell-specific surface proteins SSEA4, TRA-1-60, TRA-1-81, NANOG, and OCT4, as shown by immunostaining (Figures 1F and S1B).
All nine iPSC lines maintained a normal karyotype after reprogramming (Figures 1F and S1C) and could spontaneously differentiate into cell types of all three germ layers in vitro (Figures 1G and S1C).
Moreover, representative iPSC lines from the subjects (control line 20, sporadic line 9, and PGRN S116X line 26) transplanted into severe combined immunodeficiency (SCID) mice gave rise to teratomas in vivo (Figure 1G).
These findings confirm the successful reprogramming and generation of FTD-patient-specific iPSC lines, and demonstrate that these lines are similar to those in controls in terms of both their expression of stem cell markers and their pluripotency.
Differentiation of FTD-Patient-Specific iPSCs into Neurons
Next, we differentiated three fully reprogrammed iPSC lines at passages 20-26 from each subject into postmitotic neurons, using a protocol available in our lab (Delaloy et al., 2010).
The differentiation starts with neural induction, which is followed by expansion of the neural progenitor cells and neural maturation.
The first step, induction of multilineage differentiation and embryoid body (EB) formation, was inefficient when iPSCs were maintained on feeder cells.
Adaptation of iPSCs to feeder-free conditions allowed robust and reliable formation of EBs (Figures 2A and 2B).
After 5-6 days in suspension, neural induction was initiated with basic fibroblast growth factor and N2 supplement, and rosettes (elongated cells arranged in circular structures) appeared (Figure 2C).
Ten days later, the rosettes were isolated, expanded in suspension as neurospheres for 3-4 weeks (Figure 2D), and dissociated into single cells.
Terminal differentiation was induced with glial-cell-line-derived neurotrophic factor, brain-derived neurotrophic factor, ascorbic acid, and cyclic AMP.
Two weeks later, the cells displayed typical neuronal morphology (Figure 2F).
Both FTD and control iPSCs differentiated at similar rates.
We then sought to determine whether the disease and/or the mutation affected the percentage of neurons obtained with this protocol.
After 2 weeks, ∼80% of cells in culture were positive for the neuronal marker microtubule-associated protein 2 (MAP2) and had neuronal morphology (Figure 2G), and <4% of cells were positive for the glial marker glial fibrillary acidic protein, regardless of the genetic mutation of the iPSC line used (Figures 2G and 2K).
Thus, the PGRN S116X mutation did not affect the percentage of neurons generated with the differentiation protocol.
Approximately 40% of the MAP2+ cells were presumably glutamatergic and expressed VGLUT1 (Figure 2H), and <10% of cells were GABA+ inhibitory neurons or tyrosine hydroxylase (TH)+ dopaminergic neurons (Figures 2I and 2J).
Again, the percentages of neurons differentiated from control and FTD-patient-specific iPSC lines were indistinguishable.
Additional analysis at the messenger RNA (mRNA) level indicative of glutamatergic (VGLUT1), GABAergic (GAD67) and dopaminergic (TH) neuronal subtypes or postsynaptic density (PSD95) detected no significant differences across the different lines (Figures S2A-S2D).
Thus, the PGRN S116X mutation did not affect neural differentiation of iPSCs into specific type of neurons.
We next performed whole-cell voltage-clamp recordings and measured membrane properties and synaptic transmission on neurons differentiated from two iPSC lines (control line 20 and PGRN S116X line 26; Figures 2L-2N).
Most cells in culture were capable of inducing tetrodotoxin-sensitive action potentials (control: 79.2%; PGRN S116X: 75%), which is consistent with the finding that ∼80% of cells are MAP2-positive neurons (Figure 2G).
The resting membrane potential between two cell lines does not show a statistically significant difference (control neurons: -62.5 ± 1.5 mV; PGRN S116X: -60.0 ± 1.9 mV; n = 24, p = 0.17).
To address whether these cells can form functional synaptic connections, we found that PSD95 puncta were present on dendrites of these neurons (Figure S2E), and also measured AMPA-type glutamate receptor-mediated miniature excitatory postsynaptic currents (mEPSCs).
Neurons differentiated from PGRN S116X iPSCs showed synaptic connections indistinguishable from those of control neurons (mEPSC amplitude of control neurons: 12.1 ± 1.7 pA, PGRN S116X: 14.37 ± 1.7 pA, p = 0.36; frequency of control neurons: 3.2 ± 0.6 pA, PGRN S116X: 2.2 ± 0.2 pA, n = 10, p = 0.12).
These results indicate that postmitotic neurons differentiated from control and PGRN S116X iPSCs are functional.
A Human Neuronal Model of PGRN Haploinsufficiency
To establish a human neuronal model of PGRN haploinsufficiency, we first examined the expression levels of PGRN in fibroblasts from each subject by qRT-PCR.
GRN mRNA levels were similar in cells from the control subject and sporadic FTD patient (Figure 3A), but in cells from the FTD patient with the PGRN S116X mutation, the mRNA level was only ∼30% of that found in the control (Figure 3A).
This observation is consistent with the substantially lower average plasma PGRN levels in FTD patients harboring GRN mutations compared with those without such mutations (Coppola et al., 2008; Finch et al., 2009).
However, after reprogramming, the GRN mRNA was 50% lower in all three PGRN S116X iPSC lines (Figure 3B), as expected.
Moreover, the relative expression levels of GRN mRNA in all control or sporadic FTD iPSCs showed little variation (Figure 3B).
Correspondingly, PGRN S116X iPSCs secreted 50% less PGRN than iPSCs from the control subject and sporadic FTD patient (Figure 3C).
Upon differentiation, GRN mRNA levels were ∼41% lower in PGRN S116X neurons than in control and sporadic FTD neurons differentiated from multiple iPSC lines (Figure 3D).
The levels of both intracellular and secreted PGRN in these neurons were also correspondingly reduced, as measured by ELISA (Figures 3E and 3F).
Thus, we established a patient-specific human neuronal model of PGRN haploinsufficiency.
We were also able to differentiate these iPSCs into microglia as shown by expression of the microglia-specific marker Iba1 (Figures S3A and S3B).
PGRN secretion from these cells was also ∼50% lower than in control and sporadic FTD cells (Figures S3C and S3D).
PGRN S116X Neurons Are More Sensitive to Cellular Stress Induced by Inhibitors of the Phosphatidylinositol 3-Kinase/Akt and MEK/Mitogen-Activated Protein Kinase Signaling Pathways
Compared with many other neurodegenerative diseases, the cellular defects associated with FTD remain poorly defined.
Human neurons derived from patient-specific iPSCs are an excellent system in which to examine disease-gene-specific cellular phenotypes.
To conduct such an examination, we first used two iPSC lines from each patient and differentiated them into postmitotic neurons.
Under normal culture conditions, PGRN S116X and control neurons show similar viability.
As a late-onset disease, FTD likely results from damage that accumulates over time rather than from an acute effect of the reduced PGRN levels.
Very little is known about the cellular defects caused by PGRN haploinsufficiency in human neurons.
Thus, to identify pathways that might be compromised in PGRN S116X neurons, we performed a systematic screen with a variety of inducers of cellular stress that affect different key cellular functions/pathways, such as mitochondria, oxidative stress, endoplasmic reticulum (ER), proteasome, and cell survival.
Neurons derived from the healthy individual and the sporadic FTD patient were used as controls.
We tested two well-known inducers of mitochondrial dysfunction, oligomycin (an ATP synthase inhibitor) and rotenone (a complex I inhibitor), as well as a classical inducer of oxidative stress (hydrogen peroxide).
All three inducers of cell stress reduced cell viability in a dose-dependent manner, and all genotypes or disease states were equally affected (Figures S4A and S4B; due to space limitations, data on oligomycin not shown).
In contrast, PGRN S116X neurons were more susceptible than control neurons to ER stress induced by tunicamycin, an inhibitor of protein N-glycosylation (Figure 4A), and proteasome activity inhibition induced by lactacystin (Figure S4C).
Because sporadic FTD neurons also showed similar enhanced sensitivity to tunicamycin and lactacystin (Figures 4A and S4C), we concluded that these cellular phenotypes are not specific to PGRN deficiency.
To further explore PGRN-dependent cellular defects in FTD neurons, we also tested the effect of staurosporine, a broad-spectrum kinase inhibitor that induces apoptosis (Figure 4B).
Interestingly, PGRN S116X neurons were more sensitive to staurosporine than control or sporadic FTD neurons (Figure 4B).
This finding suggests that PGRN deficiency affects kinase pathways involved in cell survival, causing them to be more susceptible to inhibition of such pathways.
To validate the findings of the cell viability assay, we also measured the activation of caspase-3, a well-studied mediator of apoptotic cell death.
Consistent with the results of the cell viability assay, PGRN S116X neurons showed greater caspase-3 activation in response to staurosporine than control or sporadic FTD neurons, whereas tunicamycin increased caspase-3 activity in both PGRN S116X and sporadic FTD neurons (Figure 4D).
Because TDP-43 pathology is a hallmark in the brains of FTD patients with PGRN deficiency (Neumann et al., 2006), and increased caspase-3 activity leads to enhanced cleavage and mislocalization of TDP-43 (Zhang et al., 2007), we also analyzed the cellular distribution of TDP-43 under stress to confirm our initial findings.
After exposure to staurosporine, the percentage of neurons with redistribution of TDP-43 from the nucleus to the cytoplasm was significantly higher in PGRN S116X neurons than in control or sporadic FTD neurons (Figure S4D).
In contrast, tunicamycin resulted in similar increases in the percentages of PGRN S116X and sporadic neurons with cytoplasmic TDP-43.
Interestingly, in the absence of a stress inducer, the percentage of PGRN S116X neurons with cytoplasmic TDP-43 was higher than in control or sporadic FTD neurons (Figure S4D).
Thus, both the caspase-3 and TDP-43 assays confirm the intrinsic vulnerability of PGRN S116X neurons under stress.
Because staurosporine is a broad-spectrum kinase inhibitor that affects several signaling pathways, we next tested more specific kinase inhibitors to identify specific pathways affected by reduced PGRN levels.
PGRN S116X neurons were more susceptible than control or sporadic FTD neurons to wortmannin (Figure 4C) and LY294002 (data not shown), two phosphatidylinositol 3-kinase (PI3K) inhibitors, and PD98059, an MEK inhibitor (Figure S4E).
These findings suggest that PGRN deficiency impairs the PI3K/Akt and MEK/mitogen-activated protein kinase (MAPK) signaling pathways in human neurons.
The Cellular and Molecular Defects of PGRN S116X Neurons Can be Rescued by PGRN Expression
We next examined the causal relationship between PGRN haploinsufficiency and enhanced sensitivity to cellular stress induced by inhibitors of the PI3K/Akt and MEK/MAPK pathways in PGRN S116X neurons.
To that end, we used a CS-CW-GRN-IG lentiviral vector to express PGRN in most (if not all) of the human neurons in culture.
The decreased cell viability (Figure 4E) in staurosporine-treated PGRN S116X neurons was rescued by PGRN expression.
A similar result was obtained when increased caspase-3 activation was used as the assay (Figure 4F), confirming the validity of the cell viability assay.
In contrast, the increased sensitivity of PGRN S116X neurons to the ER stress induced by tunicamycin was not rescued by PGRN expression (Figure 4E).
More importantly, the increased sensitivity of PGRN S116X neurons to inhibitors of the PI3K/Akt and MEK/MAPK pathways was also rescued (Figure 4E).
Thus, the novel cellular defects of PGRN S116X neurons uncovered under stress are specific to PGRN deficiency.
Next, we sought to identify misregulated components in the PI3K/Akt and MEK/MAPK pathways by performing gene expression analyses on two to three replicate neuron cultures differentiated from each iPSC line and four iPSC lines per individual (30 samples total).
We compared PGRN S116X neurons and sporadic FTD neurons versus control neurons, and identified a number of differentially expressed genes, both shared between PGRN S116X and sporadic FTD neurons, and specific to PGRN S116X neurons (Figure 4G).
In addition, a clustering analysis showed that the gene expression patterns in neurons differentiated from three separate iPSC lines of the same individual were remarkably similar to each other (Figure 4G).
Among the top downregulated genes in PGRN S116X neurons (but not in control or sporadic FTD neurons) was the ribosomal protein S6 kinase beta-2 (RPS6KB2; Figure 4H).
This gene encodes S6K2, a member of the S6 kinase family of serine/threonine kinases that has been shown to play an important role in both the PI3K/Akt and MEK/MAPK signaling pathways (Fenton and Gout, 2011), and is part of a coordinated network of differentially expressed genes, including GRN (Figure S4H).
The downregulation of the RPS6KB2 gene was confirmed by qRT-PCR (Figure S4F) and its mRNA could be restored to wild-type level by PGRN expression (Figure S4G).
More importantly, S6K2 protein level is reduced by ∼50% in PGRN S116X neurons, which can be rescued to wild-type level by PGRN expression (Figures 4I and 4J).
Taken together, these studies reveal novel cellular and molecular defects of PGRN S116X neurons in the PI3K/Akt and MEK/MAPK signaling pathways, which can be rescued by PGRN expression (Figure S4I).
Discussion
PGRN haploinsufficiency is a major cause in FTD pathogenesis (Baker et al., 2006; Cruts et al., 2006).
The underlying mechanism remains poorly understood, in part due to the lack of suitable model systems.
Even in Grn knockout mice, neuronal cell loss is limited, and mechanistic studies are further complicated by the finding that PGRN levels may vary widely among patient brains in the later stages of disease (Chen-Plotkin et al., 2010).
The iPSC-based human neuronal model of PGRN haploinsufficiency, as established in this report, provides a platform for testing small molecules that can restore PGRN levels.
It also serves as a valuable and more physiologically relevant model for elucidating the mechanisms of FTD.
FTD is an age-dependent neurodegenerative disease, and some intrinsic vulnerabilities of human neurons are more likely to manifest under stress conditions in culture.
Indeed, this approach has been used recently to recapitulate some key features of major neurodegenerative diseases in human neurons derived from patient-specific iPSCs (e.g., Nguyen et al., 2011).
However, in contrast to the well-studied Alzheimer disease and Parkinson disease (PD), little is known about neuronal defects in FTD patients that are caused by PGRN deficiency.
Differential sensitivity of neurons to a particular stressor in culture within a short time window may reveal partially defective molecular pathways relevant to FTD pathogenesis.
A previous work showed that PGRN deficiency leads to molecular alterations in apoptotic pathways and Wnt signaling (Rosen et al., 2011).
Our data here show that PGRN S116X human neurons are more prone to reduced cell viability induced by specific protein kinase inhibitors, implicating the PI3K/Akt and MEK/MAPK signaling pathways in the molecular pathogenesis of FTD.
This cellular defect is rescued by ectopic expression of PGRN in human PGRN S116X neurons, consistent with previous findings that PGRN promotes the survival of rodent primary neurons (Van Damme et al., 2008; Ryan et al., 2009; Xu et al., 2011) and activates the PI3K/Akt/S6K pathway in cancer cells (Zanocco-Marani et al., 1999).
We also found that S6K2, a component in both PI3K/Akt and MEK/MAPK signaling pathways, is specifically downregulated in PGRN S116X neurons as part of a coordinated gene network, and its expression level can be restored to normal by ectopic PGRN expression.
Interestingly, our reanalysis of the gene expression data published by Chen-Plotkin et al. (2008) revealed that RPS6KB2 mRNA is downregulated by ∼40% in the frontal cortex, but not in the hippocampus or cerebellum, of FTD patients with PGRN mutations.
Taken together, these findings reinforce the notion that the PI3K/Akt and MEK/MAPK signaling pathways are compromised in PGRN S116X neurons, and highlight the primary role PGRN plays in promoting neuronal survival.
ER stress and mitochondrial impairment have both been closely linked to neurodegenerative diseases (Matus et al., 2011; Schon and Przedborski, 2011).
Our finding that neither PGRN S116X nor sporadic FTD neurons show enhanced sensitivity to mitochondrial or oxidative stressors argues that these pathways are unlikely to be affected by reduced PGRN levels in cultured neurons.
However, mitochondrial dysfunction and oxidative stress may develop at later stages of disease progression in FTD patients.
On the other hand, both PGRN S116X and sporadic FTD neurons are more susceptible to inducers of ER stress and inhibitors of proteasome function than control neurons.
This cellular defect appears to be PGRN-independent since PGRN expression levels are normal in sporadic FTD neurons.
In accordance with our findings, it was recently reported that ER stress and unfolded protein response activation contribute to both sporadic FTD and familial FTD caused by MAPT mutations (Nijholt et al., 2012).
Moreover, both Aβ and increased levels of phosphorylated tau induce ER stress in Alzheimer disease (e.g., Hoozemans et al., 2009), as does the accumulation of misfolded α-synuclein in PD (Colla et al., 2012).
Therefore, altered ER stress responses are likely to be a general feature in a variety of neurodegenerative diseases.
In summary, we have established neuronal models of human PGRN deficiency and demonstrated specific and reversible defects that affect the survival of these neurons.
Our findings suggest that chronic weakening of prosurvival signaling pathways may render neurons more sensitive to environmental insults in FTD patients with PGRN deficiency.
Thus, in addition to strategies to increase PGRN levels, therapeutic approaches that generally enhance neuronal survival through growth factor signaling may be beneficial in slowing disease progression in these patients.
Experimental Procedures
Isolation of Primary Human Skin Fibroblasts and Generation of iPSCs
This study was approved by the Institutional Review Board and Ethics Committees of the University of California, San Francisco, and written informed consent was obtained in all cases.
The patient with the PGRN S116X mutation followed the classic clinical progression for FTD and developed parkinsonism, as do all FTD patients with PGRN mutations, but he did not show typical features of PD dementia.
The patient with sporadic FTD also showed parkinsonism.
Skin biopsies were collected, cut into small pieces, and placed on culture dishes to allow the fibroblasts to expand.
The cells were maintained in Dulbecco's modified Eagle's medium supplemented with 10% fetal bovine serum, 1X nonessential amino acids, and penicillin/streptomycin (100 U/ml). iPSCs were generated as described previously (Takahashi et al., 2007).
Please see Supplemental Information for more details.
qRT-PCR, Immunocytochemistry, Differentiation and Characterization of iPSCs, and Electrophysiology
Most of the experiments involving qRT-PCR, immunocytochemistry, differentiation and characterization of iPSCs, and electrophysiology were performed as previously described (Delaloy et al., 2010) with minor adjustments.
Please see Supplemental Information for more details.
PGRN Measurements
Fresh culture medium was added to the cells 24 hr before collection.
After the medium was collected, the cells were washed once with phosphate-buffered saline (PBS), lysed with NP-40 buffer, and subjected to three freeze-thaw cycles.
Both the culture medium and the cell lysates were centrifuged at 12,000 rpm at 4°C for 10 min to clear cellular debris.
Cell lysate supernatants were assayed for protein concentration with the BioRad reagent assay.
Total cell lysates and culture medium were diluted, and the PGRN levels were determined with an ELISA kit (Alexis Biochemicals, San Diego, CA) according to the manufacturer's instructions.
Data were normalized to protein concentration.
Stress-Induced Toxicity Assay
Two-week-old neurons were exposed for 24 hr to the following stress inducers: tunicamycin, lactacystin, rotenone, oligomycin, hydrogen peroxide, staurosporine, wortmannin, LY294002, PD98059, or DMSO.
Cell viability was determined with the WST1 cell-proliferation assay (Roche Applied Science, Penzberg, Germany) according to the manufacturer's instructions.
Caspase-3 activity assay is described in the Supplemental Information.
PGRN Rescue Experiments
Human GRN (NM_002087.2) was inserted into a CS-CGW lentiviral vector with Nhe I and Xho I.
The vector also expressed green fluorescent protein through an internal ribosome entry site.
One-week-old neurons were transduced overnight with lentivirus expressing PGRN or empty vector.
The next morning, the medium was doubled and thereafter replaced every other day.
One week after transduction, the neurons were exposed to 10 nM staurosporine, 0.5 μM tunicamycin, 50 μM PD98059, 75 nM wortmannin, or DMSO for 24 hr.
Cells were assayed for cell viability, caspase-3 activation, and S6K2 levels.
A multiplicity of infection of 50 was used in all cases.
Extended Experimental Procedures
Generation of iPSC Lines
Fibroblasts were seeded at 8 × 105 cells per 100-mm dish and transduced with equal volumes of supernatants from retroviruses encoding human OCT3/4, SOX2, KLF4, and c-MYC.
The next day, the medium was removed and replaced with fresh viral supernatants.
Seven days after the first infection, cells were collected and seeded (5 × 104 cells per 100-mm dish) onto mitomycin C-treated SNL feeder cells.
One day later, the medium was replaced with iPSC medium containing basic fibroblast growth factor (4 ng/ml); thereafter, the medium was changed every other day.
Five weeks after viral transduction, colonies were picked and transferred to 12-well plates containing feeder cells.
For expansion, cells were dissociated in a 1:2 solution of accutase/PBS for 1 min at room temperature, washed twice with PBS, and scraped off the dish with a cell lifter in iPSC medium.
Larger colonies were further broken up by pipetting and transferred to 6-well plates containing feeder cells.
After the initial characterization, selected iPSC lines were adapted and maintained in feeder-free conditions.
In the absence of feeder cells, iPSC colonies were seeded onto plates coated with Matrigel (BD Biosciences) and cultured in mTeSR1 medium (StemCell Technologies).
Bisulfite Sequencing
Genomic DNA was isolated from fibroblasts and iPSCs with the DNeasy blood and tissue kit (QIAGEN), and the EpiTect bisulfite kit (QIAGEN) was used for bisulfite treatment.
The OCT4 promoter sequence of interest was amplified by PCR with previously described primers (Freberg et al., 2007, primer pair 7).
The amplified products were cloned with the TOPO cloning kit (Invitrogen, Carlsbad, CA), purified, and sequenced.
RNA Extraction and qRT-PCR
Total RNA was isolated with RNeasy kits (QIAGEN), and 500 ng of RNA was reverse transcribed to cDNA with TaqMan reverse transcription reagent kits (Applied Biosystems, Foster City, CA) according to the manufacturer's instructions.
Quantitative PCR was performed with SYBR Green Master Mix (Applied Biosystems) and forward and reverse primers (10 μM; Table S1) or with TaqMan Gene Expression Master Mix and TaqMan primers (Applied Biosystems).
Ct values for each sample and gene were normalized to GAPDH.
The 2-ΔΔCt method was used to determine the relative expression of each gene.
The putative iPSC lines were first analyzed for the four reprogramming factors at passage #8, almost two months after the picking of the colonies.
The primers are listed in Table S1.
At this stage, the transgene expression was between one to four times higher than the endogenous levels of ESC H9, except for two lines where the KLF4 transgene was twenty times higher than the endogenous levels.
These two lines were not used in this study.
Thus, our assay allows us to detect transgene expression levels of at least 20-fold above the endogenous ESC H9 levels.
Immunocytochemistry
Cells were fixed in 4% paraformaldehyde (pH 7.4) for 10 min and permeabilized with 0.2% Triton X-100.
After incubation with 3% bovine serum albumin for 30 min, cells were incubated with primary antibodies for 1 hr at room temperature.
The primary antibodies used were mouse anti-OCT4 (Santa Cruz Biotechnology, Santa Cruz, CA; 1:100), mouse anti-TRA-1-60 and TRA-1-80 (Millipore; 1:100), goat anti-NANOG (R&D Systems, Minneapolis, MN; 1:100), mouse anti-SSEA4 (Abcam, Cambridge, MA; 1:100), rabbit anti-desmin (Thermo Scientific, Waltham, MA; 1:100), mouse anti-βIII-tubulin (Promega, Madison, WI; 1:200), mouse anti-α-fetoprotein (R&D Systems; 1:200), mouse anti-MAP2 (Sigma, St.
Louis, MO; 1:500), rabbit anti-glial fibrillary acidic protein (Dako, Carpinteria, CA; 1:1000), rabbit anti-VGLUT1 (Synaptic Systems, Goettingen, Germany; 1:500), rabbit anti-GABA (Sigma; 1:100), rabbit anti-tyrosine hydroxylase (Millipore, Billerica, MA; 1:500), rabbit anti-TDP43 (Protein Tech Group, Chicago, IL; 1:100).
After three washes with PBS, the cells were incubated with Alexa Fluor secondary antibodies (Invitrogen; 1:300) for 1 hr at room temperature followed by counter staining with 1 μg/ml Hoechst or DAPI.
Immunostained cells were examined with an Axiovert 200 fluorescence microscope coupled to an Axiocam camera (Zeiss).
For neuron experiments, on average 200 cells were analyzed per experimental condition, n = 3 independent cultures.
Neuronal Differentiation of iPSC Lines
iPSC colonies were detached with accutase (Millipore) and grown as embryoid bodies (EBs) in suspension for 5-6 days in iPSC medium without basic fibroblast growth factor.
EBs were allowed to attach and form rosettes.
Ten-day-old rosettes were collected and grown in suspension as neurospheres.
Neurospheres were dissociated after 3-4 weeks, and the cells were plated on glass coverslips (BD Biosciences) or plates coated with poly-D-lysine (0.1 mg/ml) and laminin (10 μg/m).
Neurons were used after 2-4 weeks in culture.
For spontaneous differentiation, EBs were obtained as described for neural differentiation and grown for 8 days in suspension.
EBs were then allowed to adhere to Matrigel-coated glass coverslips and to further differentiate for 8 days in mTeSR1 medium.
Cells migrating out of the attached EBs were stained and analyzed by fluorescence microscopy for markers of the three germ layers.
Karyotyping and Teratoma Formation
Karyotype analysis was performed at Cell Line Genetics (Madison, WI) and Cytogenetic Laboratory, University of Massachusetts Memorial Hospital (Worcester, MA).
Teratoma formation was analyzed at Applied StemCell (Sunnyvale, CA).
Briefly, iPSCs (1-2 × 106 cells/site) were injected into the kidney capsule of Fox Chase SCID-beige mice.
Teratomas formed 7-10 weeks later.
Tissues were fixed in 10% formalin, embedded in paraffin, cut into 5-μm serial sections, and stained with hematoxylin/eosin.
Differentiation of iPSCs into Microglia
iPSCs were differentiated into microglial precursors in vivo with a modified version of a method designed for mouse ESCs (Napoli et al., 2009).
Briefly, EBs were formed as described for neuronal differentiation, cultured in suspension for 6 days, and allowed to adhere to dishes coated with 0.1% gelatin.
Twenty-four hours later, nestin-positive cells were selected by incubation with 5 μg/ml fibronectin (Sigma) and 1X insulin-transferrin-selenite (GIBCO) in Dulbecco's modified Eagle's medium (DMEM)/F12 for 6 days.
Cells were expanded in DMEM/F12 containing 1% N2, 1 μg/ml laminin, and 5 ng/ml basic fibroblast growth factor for 6 additional days.
Differentiation was induced by removing the growth factor from the medium.
Twenty-one days later, precursor cells were dissociated with 0.125% trypsin and seeded on gelatin-coated, 100-mm dishes in DMEM/F12 medium containing 1% N2 and 5% fetal bovine serum.
Microglial cells migrating out of the aggregates were collected 14 days later and seeded on gelatin-coated 24-well plates.
Experiments were performed 48 hr later.
Electrophysiology
Electrophysiological recording were performed on 3-4 week-old neurons.
The extracellular solution consisted of (in mM) 119 NaCl, 2.5 KCl, 4 CaCl2, 4 MgCl2, 26 NaHCO3, 1 NaH2PO4, and 11 glucose; it was gassed with 5% CO2/95% O2 and had a pH of 7.4, unless otherwise noted.
The coverslips were mounted on an upright microscope (BX61WI, Olympus), and cells were identified visually by infrared (IR) differential interference contrast video microscopy with an IR-CCD camera (Olympus).
Whole-cell recordings were made with a Multiclamp 700B amplifier, a Digidata 1320, and Clampex 9 (Molecular Devices).
The patch recording pipettes (3-5 MΩ) were filled with internal solution containing (in mM): 115 potassium methanesulfonate, 20 KCl, 10 HEPES, 2.5 MgCl2, 4 adenosine triphosphate disodium salt, 0.4 guanosine triphosphate trisodium salt, 10 sodium phosphocreatine, and 0.6 EGTA, pH 7.25, with KOH.
Membrane potentials were corrected for a liquid junction potential (10 mV) between the extracellular and pipette solutions.
All experiments were performed at room temperature (24-25°C).
Action potentials were recorded in whole-cell current-clamp mode.
Membrane potentials were kept around -55 to 65 mV, and step currents were injected to evoke action potentials.
AMPAR-mediated miniature excitatory postsynaptic currents (mEPSCs) were measured with whole-cell voltage-clamp recordings performed with tetrodotoxin (0.002 mM, Ascent Scientific) and picrotoxin (0.1 mM, Sigma) in the extracellular solution.
Miniature events were analyzed with Mini Analysis software (Synaptosoft).
Events smaller than 5 pA were excluded from the analysis.
Data were digitized at 10k Hz with a 2-kHz low-pass filter.
Results are reported as mean ± SEM.
The statistical significance of differences in mEPSCs was determined by t test; p < 0.05 was considered statistically significant.
Caspase-3 Activity Assay
Cells were washed once in PBS and lysed with M-PER Reagent (Thermo Scientific) supplemented with protease inhibitor cocktail (Roche Applied Science).
After three freeze-thaw cycles, cellular debris was removed by centrifugation at 13,000 rpm for 10 min at 4°C, and the supernatants were assayed for protein concentration with the BioRad reagent assay.
To measure caspase-3-like activity, 10 μg of protein was incubated with 15 μM Ac-DEVD-AFC (Caspase-3 Substrate VII, Calbiochem, San Diego, CA) in reaction buffer (25 mM HEPES, 10% sucrose, 0.1% CHAPS, pH 7.5, and 15 mM 1,4 dithiothreitol) for 2 hr at 37°C.
AFC fluorescence was detected with excitation at 400 nm and emission at 505 nm in a SpectraMax M3 Microplate Reader (Molecular Devices, Sunnyvale, CA).
Caspase-like activity was calculated as the increase above control (DMSO-treated cells).
Microarray Experiments
Total RNA was extracted from neuronal cultures with standard methods, and then amplified, biotin-labeled, and hybridized on Illumina HT12 version 4 Beadchip Expression microarrays, following the manufacturer's recommendations.
Slides were scanned using Illumina BeadStation, and the signal was extracted by using Illumina BeadStudio software.
Raw data were analyzed by using Bioconductor packages [Gentleman R, Carey V, Huber W et al.
Bioinformatics and Computational Biology Solutions Using R and Bioconductor: Springer, 2005].
Quality assessment was performed analyzing at the inter-array Pearson correlation, and clustering based on top variant genes was used to assess overall data coherence.
Contrast analysis of differential expression was performed by using the LIMMA package [Smyth GK, Gentleman R, Carey V et al.
Limma: linear models for microarray data.
Bioinformatics and Computational Biology Solutions using R and Bioconductor: Springer, 2005:397-420].
After linear model fitting, a Bayesian estimate of differential expression was calculated.
Data analysis was aimed at characterizing the gene expression differences between neurons from a sporadic FTD subject and a carrier the PGRN S116X mutation compared to neurons derived from one normal control.
Pathway analysis was performed by using the Functional Analysis Annotation tool in the Ingenuity Pathways Analysis software (Ingenuity Systems, http://www.ingenuity.com).
Statistical Analysis
Values are expressed as mean ± SEM.
The significance of differences among multiple groups was determined with a one-way analysis of variance (ANOVA) followed by a Tukey-Kramer post hoc test (GraphPad Prism version 5.04).
Differences were considered significant at p < 0.05.
Acknowledgments
We thank our patients and their families, whose generosity made this research possible.
We also thank S. Ordway for editorial assistance, J.A.
Lee for help at the early stage of this project, and J. Miller and members of the Gao laboratory for discussions.
This work was initiated with a grant from the California Institute for Regenerative Medicine (RL1-00650 to F.B.G. and R.V.F., Jr.) when F.B.G. was at the J. David Gladstone Institutes and supported by startup funds from the University of Massachusetts Medical School (F.B.G. and K.F.).
This work was also partially supported by the National Institutes of Health (NS057553 to F.B.G., and AG019724 and AG023501 to B.L.M.) and the Consortium for FTD Research (F.B.G. and D.H.G.).
Accession Numbers
Microarray data are available at the NCBI Gene Expression Omnibus database under the series accession number GSE40378.
Supplemental Information
Supplemental Information includes Extended Experimental Procedures, four figures, and one table and can be found with this article online at http://dx.doi.org/10.1016/j.celrep.2012.09.007.
Licensing Information
This is an open-access article distributed under the terms of the Creative Commons Attribution 3.0 Unported License (CC-BY; http://creativecommons.org/licenses/by/3.0/legalcode).
Supplemental Information
Table S1.
Primer for qPCR and PCR Reactions, Related to Experimental ProceduresDocument S1.
Article plus Supplemental Information

Thermal expansion behavior and high-temperature electrical conductivity of A2-xAx'Cu1-yCoyO4+-δ (A=La, Pr; A'=Pr, Sr) oxides with the K2NiF4-type structure
La2-xPrxCuO4+-δ (x = 0.1; 0.2; 0.4) and La2-xPrxCu1-yCoyO4+-δ (0.1 x 0.5; y = 0.05; 0.1) samples were prepared by conventional solid-state route at 1273 K, 90 h in air with one intermediate regrinding. La2O3, Pr6O11, CuO and Co3O4 were used as initial reagents. Sitrate method was used for the preparation of Pr2-xSrxCu1-xCoxO4+-δ (x = 0.25; 0.5; 0.75). The excess of citric acid monohydrate C6H8O7*H2O was melted in a porcelain cup whereupon stoichiometric amount of SrCO3 (99.999%) was dissolved. Before adding to melt, CuO (99.999%) and Pr2(SO3)3*2.27N2O (99.999%) were dissolved in strong nitric acid, Co(NO3)2*6H2O (99.999%) was dissolved in a minimal amount of water. The mixture was heated in air until the formation of dark solid. The obtained precursors were preliminary annealed at 773 K for 12 h in air. Further pelletized samples were annealed at 1273 K for 48 h in air.Intrinsic electrochemical performance and precise control of surface porosity of graphene-modified electrodes using the drop-casting technique

Graphite and other chemicals were obtained from Sigma-Aldrich and are of reagent grade or higher. In the production of graphene, the first step was the oxidation of graphite to graphite oxide using the modified-Hummers method as reported in the literature [24] and [25]. The graphite oxide was subsequently thermally exfoliated at 1000 degC to give reduced graphene based on our previous method [26]. Glassy carbon (GC) electrodes (CH Instruments, USA) of 3 mm diameter were used as the working electrode, and a platinum electrode as the auxiliary electrode. An Ag/AgCl electrode (1 M KCl) was used as the reference. Graphene dispersions were prepared in DMF at concentrations ranging from 1.0 mg mL- 1 to 7.0 mg mL- 1, by ultrasonication for an initial 10 mins followed by 5 mins before each subsequent drop-casting event. Deionised water with a resistivity of 18.2 MΩ cm was used throughout for preparation of solutions. Scanning electron microscopy was performed with a JSM-7600F field-emission scanning electron microscope (JEOL, Japan) in gentle-beam mode at 2 kV. Graphene was drop-casted onto silicon wafers (Graphene Supermarket, USA) for imaging under conditions detailed in Section 3.2.

1. A precursor of a positive electrode active material for a secondary battery comprising a single layer-structured secondary particle in which pillar-shaped primary particles radially oriented in a surface direction from the particle center are aggregated, wherein the secondary particle has a shell shape;
and the primary particle includes a composite metal hydroxide of Ni-Co-Mn of the following Chemical Formula 1: [Chemical Formula 1] Ni1-(x+y+z)CoxMyMnz(OH)2 wherein, in Chemical Formula 1, M includes any one, or two or more elements selected from the group consisting of Al, Zr, Mg, Zn, Y, Fe and Ti;
and x, y and z are each 0<x<1, 0<=y<1, 0<z<1 and 0<x+y+z<1.2. The precursor of a positive electrode active material for a secondary battery of Claim 1, wherein 0<x+y+z<0.5 in Chemical Formula 1.3. The precursor of a positive electrode active material for a secondary battery of Claim 1, wherein the primary particle has a length ratio of 0.3 to 1 with respect to a radius of the secondary particle when considering a length of a major axis passing through the particle center as a length of the primary particle.4. The precursor of a positive electrode active material for a secondary battery of Claim 1, wherein the primary particle has an average aspect ratio of 5 to 30 when considering a ratio of a length of a major axis, which is perpendicular to a minor axis passing through the particle center, with respect to a length of the minor axis as an aspect ratio.5. The precursor of a positive electrode active material for a secondary battery of Claim 1, which has an average particle diameter (D50) of 7 .micro.m to 20 .micro.m and a BET specific surface area of 5.0 m2/g to 30.0 m2/g.6. A method for preparing the precursor of a positive electrode active material for a secondary battery of Claim 1, the method comprising: preparing a metal-containing solution by mixing a nickel raw material, a cobalt raw material and a manganese raw material;
and and introducing an ammonium cation-containing complex forming agent and a basic compound to the metal-containing solution and co-precipitation reacting the result under a pH of 10.50 to 12.00 and a temperature of 50C to 70C, wherein the ammonium cation-containing complex forming agent is introduced at a rate of 0.5 times to 1.5 times with respect to an introduction rate of the metalcontaining solution.7. The method for preparing the precursor of a positive electrode active material for a secondary battery of Claim 6, wherein the ammonium cation-containing complex forming agent and the basic compound are used in a molar ratio of 1:10 to 1:2.8. A positive electrode active material for a secondary battery comprising a single layer-structured secondary particle in which pillar-shaped primary particles radially oriented in a surface direction from the particle center are aggregated, wherein the secondary particle has a shell shape;
and the primary particle includes a lithium composite metal oxide of Ni-Co-Mn of the following Chemical Formula 2 and exhibits mono-modal-type particle distribution: [Chemical Formula 2] Li.alpha.[Ni1-(x+y+z)CoxMyMnz]O2 wherein, in Chemical Formula 2, M includes any one, or two or more elements selected from the group consisting of Al, Zr, Mg, Zn, Y, Fe and Ti;
x, y and z are each 0<x<1, 0<=y<1, 0<z<1 and 0<x+y+z<1;
and a is 1.0<=a<=1.5.9. The positive electrode active material for a secondary battery of Claim 8, which has an average particle diameter of 7 .micro.m to 15 .micro.m and a BET specific surface area of 0.1 m2/g to 1.0 m2/g.10. The positive electrode active material for a secondary battery of Claim 8, which has tap density of 1.7 g/cc to 3.0 g/cc.11. A positive electrode for a secondary battery comprising the positive electrode active material of any one of Claims 8 to 10.12. A lithium secondary battery comprising the positive electrode of Claim 11.Minority carrier lifetime in silicon photovoltaics: The effect of oxygen precipitation

Abstract
Single-crystal Czochralski silicon used for photovoltaics is typically supersaturated with interstitial oxygen at temperatures just below the melting point.
Oxide precipitates therefore can form during ingot cooling and cell processing, and nucleation sites are typically vacancy-rich regions.
Oxygen precipitation gives rise to recombination centres, which can reduce cell efficiencies by as much as 4% (absolute).
We have studied the recombination behaviour in p-type and n-type monocrystalline silicon with a range of doping levels intentionally processed to contain oxide precipitates with a range of densities, sizes and morphologies.
We analyse injection-dependent minority carrier lifetime measurements to give a full parameterisation of the recombination activity in terms of Shockley-Read-Hall statistics.
We intentionally contaminate specimens with iron, and show recombination activity arises from iron segregated to oxide precipitates and surrounding defects.
We find that phosphorus diffusion gettering reduces the recombination activity of the precipitates to some extent.
We also find that bulk iron is preferentially gettered to the phosphorus diffused layer rather than to oxide precipitates.
Highlights
•
Oxygen precipitation reduces absolute efficiency of silicon solar cells by up to 4%.
•
Full parameterisation of minority carrier lifetime due to oxide precipitates.
•
Iron contamination of oxide precipitates increases recombination activity.
•
Recombination activity reduced by phosphorus diffusion gettering.
•
Bulk iron gettered to phosphorus diffused layer, not oxide precipitates.

Introduction
At present the majority of solar cells are made from bulk crystalline silicon.
Minority carrier lifetime is the main parameter used to assess the quality of wafers from which cells are produced.
For a given generation rate, the minority carrier lifetime is largely determined by recombination processes.
Some recombination is intrinsic (band-to-band and Auger), while other is determined by defects in the bulk or at surfaces.
It is necessary to understand which defects are typically present in solar wafers before processing, and what effect processing has on those defects.
Moreover, it is important to understand the mechanism by which the relevant defects give rise to recombination, as well as to quantify their recombination activity.
Monocrystalline Czochralski silicon (Cz-Si) typically contains ~1018cm-3 of interstitial oxygen, which is mainly incorporated from the silica crucible which contains the melt.
This level of oxygen is supersaturated below ∼1200°C, so the equilibrium state is reached by the formation of SiO2 particles (oxide precipitates) [1].
The morphology of such particles changes as they grow, from unstrained particles initially, to strained precipitates, which are eventually surrounded by dislocations and sometimes stacking faults [2,3].
The first stage in precipitation is nucleation, and the rate of this is strongly enhanced by the presence of crystal defects.
In modern Cz-Si used for integrated circuits the grown-in defect concentration is insufficient for oxygen precipitation to occur unintentionally.
However, even in the highest quality Cz-Si, oxide precipitates can nucleate upon prolonged annealing at 650°C to 850°C [1,4].
In silicon for microelectronics thermal processes are often used to force oxygen precipitation to provide gettering centres for harmful metallic contaminants [5,6].
For photovoltaics, the Cz-Si wafers used are often of lower crystal quality, and several studies have found concentric rings of oxide precipitates in wafers or cells after growth or processing [7-10].
It is also noted that oxygen precipitation occurs at dislocations in multicrystalline silicon (mc-Si) during ingot cooling [11-13].
The undesirable precipitation of oxygen in Cz-Si is not a new problem, having been widely studied (and essentially solved) in Cz-Si for integrated circuits.
Oxide precipitates are known to form in vacancy-rich regions [14].
The formation of such regions can be essentially eliminated by carefully controlling the so-called v/G criterion (where v is the crystal growth rate and G is the near-interface temperature gradient) [15].
Although it is possible to reduce intrinsic point defect concentrations to levels which are essentially negligible, doing so requires slow growth rates and these are not always compatible with the commercial constraints of the silicon photovoltaics industry.
Thus, although the problem can in principle be eliminated, the fact is many commercial Cz-Si solar wafers do contain vacancy-rich regions in which oxide precipitates form [7-9].
Oxide precipitates have been linked to a substantial detrimental impact on conversion efficiencies in silicon solar cells [7,16].
A study by Haunschild et al. associated oxide precipitates with a 4% (absolute) efficiency reduction [7].
Interestingly, they found that the recombination activity to be strongly affected by a 10s anneal at 800°C.
This, and earlier studies on iron-contaminated samples [17,18], suggests that impurities might play a role in the recombination mechanism.
The aim of this paper is to answer some open questions regarding the effects of oxygen precipitation in silicon photovoltaics.
These include:•
Can the impact of oxide precipitates on minority carrier lifetime be systematically quantified?
•
What is the mechanism of recombination at oxide precipitates? Do impurities play a role?
•
What happens to the recombination activity of oxide precipitates after phosphorus diffusion gettering used in solar cell processing? Which is the more effective gettering system for bulk iron: oxide precipitates or phosphorus diffusion?
This paper builds upon our previously published work in this area [19-23] by assimilating the key ideas in one article, adding new experimental data, and focussing specifically on photovoltaics.
After presenting a general methodology for parameterising injection-dependent minority carrier lifetime in semiconductors, we apply this methodology to determine the recombination parameters of oxide precipitates in silicon.
Parameterisation of carrier lifetimes
Linear formulation of Shockley-Read-Hall statistics
Shockley-Read-Hall (SRH) statistics [24,25] are frequently used to quantify the bulk minority carrier lifetime in semiconductor materials.
SRH statistics enable the recombination activity of states associated with point-like defects to be quantified by using just three parameters: the energy position of the defect in the bandgap (ET), its capture coefficient for electrons (αn) and its capture coefficient for holes (αp).
(Alternatively capture cross-sections for electrons (σn) and holes (σp) can be used instead of capture coefficients.
The capture coefficient is the product of the capture cross-section and the thermal velocity).
Capture coefficients (or cross-sections) are temperature-dependent empirically-determined parameters which quantify the propensity of the states to capture carriers.
In some circumstances the use of simple SRH statistics is an oversimplification (see for example [26]), but for the most part the SRH approach is invaluable.
The most commonly used formation of SRH statistics describes the minority carrier lifetime in terms of the excess concentration of minority carriers (Δn for electrons; Δp for holes) (see Ref.
[27] for example).
In our work we use a different form which we have derived in an earlier paper [21].
We express the minority carrier lifetime as a linear function of the ratio of the total carrier concentrations.
In p-type material, for example, the electron lifetime (τn) is expressed as a linear function of the ratio of the total electron concentration (n=n0+Δn) to the total hole concentration (p=p0+Δp) according to:(1)τn=1αnN[1+Qn1p0+p1p0+X(Q-Qn1p0-p1p0)]where X=(n/p)=(1/p0+n), where p0 is the equilibrium hole concentration, n0 is the equilibrium electron concentration, Q=(αn/αp)=(σn/σp), and N is the concentration of the defect.
The so-called SRH densities for electrons (n1) and holes (p1) are given by:(2)n1=NCexp(-EC-ETkT)(3)p1=NVexp(-ET-EVkT)where EC and EV are the energies of the conduction band and valence band edge, respectively, and NC and NV are the densities of states in the conduction band and valence band, respectively.
An equation analogous to Eq.
(1) for the hole lifetime (τp) in n-type material can be derived as [21]:(4)τp=1αpN[1+n1n0+p1n0Q+Y(1Q-n1n0-p1Qn0)]where Y=(p/n)=(p/n0+p).
Extracting defect parameters from injection-dependent lifetime data
The linear formulation of SRH statistics provides an elegant route to parameterise recombination due to specific defects.
The key feature of Eqs.
(1) and (4) is that all the injection-dependence of the minority carrier lifetime is consumed into X or Y.
All other terms in the Equations depend upon the properties of the defect, material or temperature.
In the p-type case, information on the key SRH parameters can be extracted by taking the derivative of Eq.
(1) with respect to X and dividing this by the high injection limit of lifetime (τn as X→1), which gives:(5)dτndX/τnX→1=Q1+Q-1p0(Qn1+p11+Q)By studying material with different values of p0, Eq.
(5) can be used to deduce values of Q and Qn1+p1, and an example to show this is given in Section 2.3 below.
Information can also be gained from inspecting the low injection limit of Eq.
(1) (τn as X→0), which gives:(6)τnX→0=1αnN[1+1p0(Qn1+p1)]If Qn1+p1 is known it is thus possible to use Eq.
(6) to extract αnN, which is useful as, for constant αn, it is proportional to the state density.
It is important to note that from single temperature injection-dependent lifetime measurements alone it is not possible to extract unique values for αn, αp, or ET.
Such measurements on samples with different doping levels enable the deduction of Q=(αn/αp) and Qn1+p1 (which depends upon ET via Eqs.
(2) and (3)).
To isolate separate values of αn and αp it is necessary to determine the state density, N, by another technique.
Temperature-dependent lifetime measurements can be used to determine whether Qn1 or p1 dominates, and hence can allow ET to be uniquely found.
In principle the linear SRH formulation is valid at all injection levels.
However, it is important to note that care must be taken when using the approach over a wide range of injection levels.
Under typical conditions, X=0.01 corresponds to Δn≈0.011p0.
Thus processes which manifest themselves at low injection, such as trapping [28] and surface effects [29], can occur very close to the X→0 limit.
Example of FeB in silicon
We demonstrate the merits of this linear SRH formulation by considering the well-established example of FeB pairs in silicon.
In Fig. 1(a) we have simulated the injection-dependent lifetime due to (only) this defect.
We note that different values of the SRH parameters for FeB exist in the literature [30-32].
For our example here we use the parameters of Rein and Glunz [30], which are ET=EC-0.26eV, σn=2.5×10-15cm2, σp=5.5×10-15cm2, and we have used 2×107cms-1 for the thermal velocity, 25°C for the temperature and a defect concentration of 1×1012cm-3.
Values of NC and NV were taken from Ref.
[33].
The first feature to note in Fig. 1(a) is that the lifetime plotted against X is linear (Eq.
(1)).
Second, the lifetime at high injection (as X→1) tends to the so called 'ambipolar' lifetime given by Eq.
(1) as (1/αn+1/αp)/N, which is independent of the doping level.
Third, at low injection (as X→0), the lifetime is dependent upon the doping level.
It would also be dependent upon the defect's energy level.
Fig. 1(b) is a plot of the gradient of the data plotted in Fig. 1(a) normalised by the high injection lifetime, plotted against the reciprocal of doping in accordance with Eq.
(5).
The y-intercept of depends only upon Q, whereas the gradient depends only upon Q and Qn1+p1.
Unsurprisingly the plot in Fig. 1(b) gives the characteristic Q value for the FeB pair and the Qn1+p1 value consistent with that of the FeB pair.
Other information (such as temperature-dependent measurements) is needed to isolate whether the Qn1 or the p1 term is dominant.
The same methodology is drawn upon in this paper to quantify recombination at oxide precipitates in silicon, for which the parameters are not well established.
Experimental methods
Sample production
A set of samples with different oxide precipitate densities, sizes and morphologies were produced from high purity integrated circuit wafers with different doping levels, types and interstitial oxygen concentrations.
A four-stage precipitation treatment, as described in detail in Ref.
[19], is applied to the samples under test.
P-type samples were doped with 3.9×1014cm-3 to 8.2×1015cm-3 of boron.
N-type samples were doped with 5×1013cm-3 to 1.0×1015cm-3 of phosphorus.
Strained oxide precipitate densities (Nstrained) were measured by Schimmel etching.
Precipitate densities ranged from 3×106cm-3 to 7×1010cm-3.
Some of the p-type samples have been characterised by transmission electron microscopy (TEM) and the results are discussed in detail elsewhere [3,19].
The TEM study performed on a subset of the samples measured enabled the identification of samples in which dislocations, and in some cases stacking faults, were found to surround the oxide precipitates.
Some of the data presented in this paper were obtained using samples in which substantial denuded zones free of oxide precipitates existed near the surfaces.
This typically had a depth of ∼15μm to ∼35μm on each side.
The denuded zones were removed prior to processing using a planar chemical etch (HF (40% aqueous by volume), HNO3 (69% aqueous by volume) and CH3COOH (100%) mixed in the volume ratio 8:75:17).
Each sample was etched for 40min four times, so that between 40μm and 70μm of material was removed from each side.
Surface passivation and measurement of lifetime
Samples were cleaved into 3.5cm by 3.5cm or 5cm by 5cm pieces, which were RCA cleaned.
Silicon nitride was then deposited on both surfaces by plasma enhanced chemical vapour deposition (PECVD).
Two different PECVD processes were used for the results presented in this paper.
The data presented in Figs.
3-5 were from samples passivated by remote plasma PECVD at ISFH, which has previously been shown to give a surface recombination velocity below 10cms-1 [36].
The data presented in Figs.
6-8 were from samples passivated using a direct plasma Oxford Instruments Plasmalab 80 Plus PECVD system at the University of Oxford.
The surface recombination velocity associated with this latter scheme has not been studied in detail, but is estimated to be of order 100cms-1.
The lower quality scheme was used for samples with lower bulk lifetimes.
The lifetime data for such samples could be fitted with the same recombination parameters extracted from the samples for which the better surface passivation scheme was used.
It is possible that both PECVD processes introduces hydrogen into the bulk, which may passivate certain defects [37].
Minority carrier lifetime was measured using transient or quasi-steady-state photoconductance [38] methods, with a Sinton WCT-120 lifetime tester.
The injection level range studied varied with the lifetime of the sample, but was usually in the range 1013cm-3 to 1016cm-3.
It is our aim to determine the absolute lifetime associated with oxygen precipitation, so care was taken to prevent or factor out well-understood recombination processes.
Boron-oxygen defects were eliminated by storing the samples in the dark after passivation, or by performing a 10 min pre-anneal at 200°C [39] prior to lifetime measurement.
The samples were subjected to ∼50 close-up flashes from the lifetime tester to dissociate FeB pairs [40], after which an initial lifetime measurement was made immediately.
It is noted that the aggregated illumination time of the flashes of light used to dissociate the iron-boron pairs is very short (<20 ms), so any effect on the formation of boron-oxygen defects is kept to a minimum [39].
A second lifetime measurement was made more than 24h later, which was sufficient time to reassociate the FeB pairs [19,34,41].
The two lifetime measurements are then analysed to give the concentration of iron that exists in FeB pairs using an established method [30,40,42].
This concentration is henceforth referred to as the bulk iron concentration, and excludes iron present in other forms such as iron silicide precipitates, or iron bound to, or precipitated at, oxide precipitates and any surrounding defects.
The specific analysis approach used is described in a previous publication [19].
The essential feature is that SRH statistics (Eq.
(1)) are used with the recombination parameters of Rein and Glunz [30] to determine the bulk iron concentration required to account for a lifetime change at a given injection level.
For the results presented in this paper, the injection level used was 0.2p0.
The bulk iron concentrations in the "uncontaminated" samples were always ≤1.5×1012cm-3.
We express our data in terms of a "residual" minority carrier lifetime, τresidual, defined according to:(7)1τresidual=1τmeasured-(1τband-to-band+1τCEAuger+1τFei)where τmeasured is the measured minority carrier lifetime with iron in the interstitial state, τband-to-band is the lifetime due to band-to-band recombination (from [43]), τCE Auger is the lifetime due to Coloumb-enhanced Auger recombination (from [44]) and τFei is the lifetime due to SRH recombination at bulk interstitial iron (parameters from [30]).
More details of these corrections are given in our previous paper [19].
It is not possible to measure the interstitial iron concentration in n-type samples, so no correction is made for bulk iron-related recombination in such samples, and, besides which, the recombination activity of iron in n-type silicon is generally much less significant than in p-type silicon [45].
The effects of any remaining surface recombination are not factored out of the injection-dependent lifetime data.
This is not believed to have a substantial effect on our findings.
The work of Aberle et al. shows the surface recombination velocity to be dependent on injection-level, with the surface recombination velocity increasing at lower levels of injection [29].
However, this alone is insufficient to explain the injection-level dependence we observe.
It is noted that the same fit parameters can be used to fit lifetime data from samples with a high bulk lifetime (low precipitate density) and low bulk lifetime (iron contaminated precipitates with a high density), which suggests that differences between surface recombination rates between samples are not substantial.
Phosphorus diffusion gettering
Some uncontaminated samples were subjected to a phosphorus diffusion gettering process.
The injection-dependent minority carrier lifetime and the bulk iron concentration were first measured as above.
The silicon nitride was then removed from both surfaces by HF (40%) for up to ∼15s at room temperature.
Samples were then RCA cleaned, followed by a phosphorus diffusion gettering process in a standard quartz-tube furnace with the temperature profile shown in Fig. 2.
The process used is the standard POCl3-based process for formation of high efficiency passivated emitter and rear cell (PERC) type solar cells at ISFH.
The phosphorus glass was then removed by immersion in HF (40%) for 3 min at room temperature, and the emitter was removed by a KOH etch for 3min at ∼90°C.
This removed a total of ∼10μm of material from the surfaces, reducing the wafer thickness from ∼700μm to ∼690μm.
The amount of material removed was calculated by weighing the wafer before and afterwards.
Samples were then subjected to another RCA clean followed by silicon nitride surface passivation and lifetime measurement, as described above.
Results
"Uncontaminated" samples
In Fig. 3, the residual minority carrier lifetime measured in uncontaminated samples containing oxide precipitates is plotted against X=n/p for two p-type specimens and Y=p/n for two n-type specimens.
The supporting TEM study [3] found dislocations and stacking faults to surround the oxide precipitates in one of the p-type sample types, but not the other.
The two n-type samples have different doping levels.
Similar curves to those in Fig. 3(a) and (b) were obtained in ~100 other p-type samples studied.
Eq.
(1) shows that a single SRH centre gives a lifetime response which is linear with X=n/p, but the variation obtained experimentally is clearly not linear.
Similarly Fig. 3(c) shows clear non-linearity with Y=p/n in the n-type case too.
Thus it is the case that oxygen precipitation introduces more than simple single-level SRH recombination centre.
We have previously ruled out the possibility that precipitation introduces a single defect with two energy levels [21].
We find in all cases that the injection-dependence of the lifetime can be fitted with two single-levels that act independently, i.e.
(1/τresidual)=((1/τ1)+(1/τ2)).
These are referred to simply as "Defect 1" and "Defect 2", and their individual and combined effects that best fit the experimental data are plotted on the graphs in Fig. 3.
To determine the properties of the two defects, the same approach as in the example shown in Fig. 1 was used.
The gradients of the fits to the experimental data for both defects for a large number of p-type samples were normalised by the high injection (X→1) limits and were plotted against 1/p0 according to Eq.
(5).
A similar approach was taken for the n-type samples (see Ref.
[21]).
The plots are shown in Fig. 4.
These plots were used to deduce the value of Q from the intercept and the values of Qn1+p1 (for p-type) and n1+p1/Q (for n-type) from the gradient.
For Defect 1, both the p-type and n-type data give the ratio of the capture coefficient for electrons (αn1) to that of holes (αp1) as Q1=αn1/αp1=157.
For Defect 2, both the p-type and the n-type data give the ratio of the capture coefficient for holes (αp2) to that of electrons (αn2) for Defect 2, 1/Q2=αp2/αn2=1,200.
For Defect 1, for which n1 and p1 are the SRH densities given by Eqs.
(2) and (3), respectively, the parameter Q1n1+p1 is 4.8×1015cm-3 from the p-type data and the parameter n1+(p1/Q1) is 3.1×1013cm-3 from the n-type data.
For Defect 2, for which n2 and p2 are the corresponding SRH densities, the parameter Q2n2+p2 is 1.0×1015cm-3 from the p-type and the parameter n2+(p2/Q2) is 1.3×1018cm-3 from the n-type data.
Temperature-dependent lifetime data reported previously [21] were used to determine the half of the band-gap in which the energy levels of the two defects lie.
We deduce that Defect 1 is at EV+0.22eV, and that Defect 2 is at EC-0.08eV.
The SRH parameters are summarised in Table 1.
The temperature dependent lifetime data show that the capture coefficient for holes at Defect 1 (αp1) decreases with temperature with a 0.20eV activation energy, and that the capture coefficient for electrons at Defect 2 (αn2) decreases with temperature with a 0.14eV activation energy [21].
As well as extracting SRH parameters from the lifetime data, it is possible to extract information on the density of each defect and to correlate this with the precipitate density measured by chemical etching.
It is not possible to determine the absolute densities of Defect 1 (N1) and Defect 2 (N2) from lifetime measurements alone.
However, using Eq.
(6) it is possible to deduce N1αn1 and N2αn2 for each lifetime measurement in p-type material.
A similar approach for n-type material allows for N1αp1 and N2αp2 to be extracted.
The p-type parameters are plotted against the measured concentration of strained precipitates in Fig. 5(a) for the case when oxide precipitates are not surrounded by dislocations and stacking faults and in Fig. 5(b) in the case when the oxide precipitates are surrounded by other extended defects.
In both plots, the correlation of N1αn1 and N2αn2 with Nstrained is approximately linear.
The gradients of the linear fit shown are given alongside the SRH parameters in Table 1.
Similar plots for n-type material have been published previously [21].
We note that the relationship is only approximately linear, and we suggest this is due to differences in the decoration of the precipitates by impurities, as discussed in the next section.
Iron contaminated samples
Injection-dependent lifetime measurements on a set of p-type iron contaminated samples are plotted in Fig. 6.
The samples used all came from the same wafer (so have very similar precipitate densities), and different levels of iron were diffused into the samples by varying the contamination temperature.
The residual lifetime reduces with contamination temperature.
It is noted that the lifetime component associated with bulk iron is factored out of τresidual (Eq.
(7)), so reductions in lifetime are assumed to be due to recombination associated with iron segregated to oxide precipitates and any surrounding defects (dislocations and stacking faults).
It is remarkable that the injection response of the residual lifetime has the same form in iron-contaminated and "uncontaminated" samples.
In fact, the same defect parameters can be used to fit the experimental data in both cases.
Nαn terms for both Defect 1 and Defect 2 have been extracted from fits to the data in Fig. 6.
The values of these fit parameters increase with contamination temperature, and the ratio of N1αn1 to N2αn2 is fairly consistent at between 2.8 and 4.0.
We discuss the relationship between recombination activity and iron decoration of the oxide precipitates and surrounding defects in Section 5.2.
Samples subjected to phosphorus diffusion gettering
Fig. 7 shows residual minority carrier lifetime plotted against X=n/p for a typical sample before and after phosphorus diffusion gettering using the temperature profile shown in Fig. 2.
The injection response before and after gettering can be fitted using the parameters for Defect 1 and Defect 2 shown in Table 1.
The gettering process increases the overall lifetime, and the Nαn term for both defects is reduced by ∼20%.
The gettering process reduces the bulk iron concentration by a factor of ∼2.7.
Discussion
Parameterisation in terms of SRH statistics
By studying the injection-dependence of minority carrier lifetime as a function of doping level in both p-type and n-type silicon it has been possible to quantify recombination at oxide precipitates in terms of SRH statistics.
The recombination processes can be parameterised in terms of two independent single-level SRH centres, the parameters for which are stated in Table 1.
It is not possible to determine the absolute density of these centres from our lifetime measurements alone.
However, we have provided an empirical correlation between the strained oxide precipitate density (determined by etching) and Nαn for each defect.
The existence of dislocations/stacking faults around the precipitates was found to increase the Nαn per precipitate values by a factor of ∼2 to 3.
These other defects did not however introduce any other detectable recombination centres.
It is interesting to note that the same parameterisation of the recombination activity can be used before and after phosphorus diffusion gettering (Fig. 7).
This rules out the possibility that iron silicide precipitates form in the bulk during gettering.
It is likely that these would have different recombination parameters, and our results show that the gettering process does not introduce other recombination centres in detectable concentrations for the conditions studied.
The role of iron in the recombination process
The data presented in Fig. 6 shows that the residual lifetime in iron-contaminated samples can be parameterised using the same two defects found in the "uncontaminated" samples.
Iron contamination does not change the energy levels and the ratio of capture coefficients required to fit the injection-dependent lifetime data.
However, the Nαn parameters required to fit the lifetime data increased with increasing contamination temperature.
Although iron solubility is clearly a factor in the resultant recombination activity, it is not the only one, as we have previously shown that the cooling rate after the contamination anneal also plays a role [23].
The bulk iron concentrations in the contaminated samples measured by photodissociation of FeB pairs are usually considerably lower than the solubility values [23].
We assume that the difference in concentrations (ΔFe) has segregated to the oxide precipitates according to:(8)ΔFe=[Fesolubility]-[Febulk]where [Fesolubility] is the solubility of iron at the contamination temperature used, and [Febulk] is the bulk iron concentration in the contaminated sample measured by photodissociation of FeB pairs.
The iron solubility in the relevant temperature range is given by[Fesolubility]=1.3×1021exp(-1.8eVkT)cm-3where T is the contamination temperature [34,35].
Fig. 8 shows the relationship between the αnN parameter per strained precipitate and ΔFe per strained precipitate for the five samples for which the lifetime data are plotted in Fig. 6.
This relationship is approximately linear for Defect 1 and Defect 2.
This, combined with previously published data on samples with a wider variety of precipitate concentrations [23], suggests that segregated iron determines the recombination activity of the strained oxide precipitates and surrounding defects.
On the assumption that αn is invariant, the density of the recombination centres appears to be proportional to the number of iron atoms segregated to the precipitates.
This implies that atomic decoration is more likely than iron precipitation at the oxide precipitates.
It is known that oxygen precipitation creates so-called Pb dangling bonds [20,46] which are known to give rise to recombination activity [20].
However, these can be passivated by hydrogen [47,48], which is likely to be introduced into our samples during the PECVD surface passivation step [49].
It is therefore possible that the only recombination activity we detect (and that would be expected in a completed solar cell) is due to impurities segregated to oxide precipitates and surrounding defects.
We can estimate the number of iron atoms required to explain the recombination activity we measured in so-called "uncontaminated" samples.
Fig. 5 shows the recombination rate via both defects in "uncontaminated" samples varies approximately linearly with precipitate density, with N1αn1/Nstrained=(1.7→2.9)×10-5cm3s-1 and N2αn2/Nstrained=(1.8→5.1)×10-6cm3s-1 (Table 1).
Assuming the state density to be proportional to ΔFe, the linear relationship shown in Fig. 8 is consistent with each strained precipitate in "uncontaminated" samples being decorated with ≤50 atoms of iron.
The idea that very low levels of impurity contamination can have substantial effects on the electrical properties of extended defects is not a new one.
It has been previously shown that the recombination activity of dislocation is strongly enhanced by contamination with metallic impurities [50,51].
It would not be surprising that oxide precipitates in monocrystalline Cz-Si solar wafers are decorated by such low levels of metal impurities.
The iron concentration at the precipitates required for the electrical activity seen in "uncontaminated" samples is much less than the iron solubility at temperatures at used in solar cell processing.
Haunschild et al. found a 10s thermal treatment at 800°C to result in a strong enhancement of the recombination activity of rings of oxide precipitates.
Their thermal process would have been sufficient to redistribute low concentrations of bulk iron in the material, resulting in increased iron decoration of the precipitates.
The efficiency reductions of up to 4% (absolute) that can arise due to oxygen precipitation in Cz-Si [7], is therefore likely to involve transition metal contamination of the precipitates and surrounding defects.
It is interesting to note that the recombination activity of oxide precipitates in both "uncontaminated" and contaminated samples appears to be approximately dependent upon precipitate density and not size.
An explanation for this is that iron segregates to regions of the precipitates whose number is invariant with size, such as in the vicinity of precipitate corners.
Extended versus point-like defects
It is interesting that although oxide precipitates and surrounding dislocations and stacking faults are extended defects, we find that their effect on bulk minority carrier lifetime is that expected from point-like defects.
Our measurements were obtained in a wide range of samples (both types, different doping levels, a wide range of precipitate densities of morphologies, with/without iron contamination) and previously at different temperatures [21].
Using our lifetime study alone, we have yet to find any feature of the process which cannot be explained using SRH statistics.
We note that others have applied a SRH-like approach to this problem in the past [52,53].
However, we also acknowledge that other previous studies have invoked forms of barrier-controlled capture at the precipitates [54].
Reconciliation of the two approaches is a topic that requires further study.
Lifetime in n-type and p-type silicon
There has been considerable recent interest in producing photovoltaics from n-type silicon substrates [55].
This is at partly because n-type material can have very high carrier lifetimes [56] due to the absence of light-induced degradation in boron-free material [57], and reduced recombination at interstitial iron [30,45].
Do oxide precipitates and associated defects affect recombination in one type more than another?
The parameterisation established here and previously [21,23] can be used to estimate this.
We have established that iron decoration of oxide precipitates strongly enhances the recombination activity, so for a fair comparison we need to account for the iron decoration.
Fitting the injection-dependent lifetime data alone does not allow for the separation of the state density (N) from the capture coefficient for electrons or holes (αn or αp, respectively).
However, from assuming all the recombination activity arises due to iron decoration, we have previously combined iron-loss measurements with the lifetime data to estimate αn1=2×10-6cm3s-1 and αn2=4×10-7cm3s-1 [23].
This assumes that each iron atom at an oxide precipitate gives rise to one of each of Defect 1 and Defect 2.
We can use the values of Q in Table 1 to estimate αp1=1.3×10-8cm3s-1 and αp2=4.8×10-4cm3s-1.
Under the stated assumptions, this gives us the full parameterisation necessary to estimate carrier lifetime due to iron-decorated oxide precipitates in p-type and n-type silicon.
A plot of minority carrier lifetime multiplied by state density (N=N1=N2) versus excess minority carrier concentration in n-type and p-type silicon with different doping levels is shown in Fig. 9.
Under the assumptions stated above, the state density is the density of iron atoms at the oxide precipitates.
The estimated residual minority carrier lifetime depends strongly upon doping level and injection level, with the lifetimes at high injection tending to the ambipolar lifetime.
At typical substrate doping levels for photovoltaics (∼1016cm-3), the lifetime in p-type material is higher than in n-type for injection levels at which the lifetime is not at the ambipolar limit.
For higher substrate doping levels the lifetime benefit of p-type over n-type becomes more substantial.
For lower substrate doping levels, the lifetime values are similar in the p-type and n-type cases, with any predicted differences likely to lie within the errors of the SRH parameters.
In summary, the recombination activity associated with just the (iron-decorated) oxide precipitates is slightly better in typical p-type PV substrates than in n-type.
That said, the increased recombination activity at bulk interstitial iron in p-type material [30,45] could easily eliminate any possible advantage.
Competition between gettering systems
In silicon wafers for integrated circuits oxide precipitates and associated defects are frequently used as a way of internally gettering transition metal impurities, thus preventing the impurities harming the devices [5,6].
We performed phosphorus diffusion gettering on samples containing oxide precipitates, which enables us to assess the relative strength of the two gettering systems.
For the results shown in Fig. 7, the bulk iron concentration was 1.4×1012cm-3 before gettering and 5.2×1011cm-3 after gettering, and Nαn terms associated with the oxide precipitates fell by ∼20%.
As we have shown that the Nαn parameters correlate with the iron lost to the precipitates (Fig. 8), we conclude that the bulk iron lost has not been gettered to the oxide precipitates, and has most likely been gettered to the phosphorus diffused layer.
Thus, for the conditions investigated, phosphorus diffusion gettering is more effective at gettering bulk iron than oxide precipitates.
This is very encouraging from the perspective of solar wafers containing oxide precipitates and suggests that the recombination activities will not increase (and may decrease slightly) during the phosphorus diffusion gettering process.
It is also noted that our findings are consistent with those of Rinio et al., who found evidence to suggest that impurities are gettered preferentially to the emitter rather than native crystal defects in mc-Si [58].
The correlation between iron loss and recombination activity shown in Fig. 8 implies that our analysis approach is capable of measuring very low relative concentrations of atomic iron at oxide precipitates.
The results presented in Fig. 7 show that the states associated with oxide precipitates are reduced by ∼20% by the phosphorus diffusion gettering process used.
An explanation for this reduction is that this proportion of iron has dissociated from the oxide precipitates and surrounding defects.
The future focus of our research is to use this approach with different thermal conditions to understand the fundamental thermodynamics of the interaction between iron and oxide precipitates.
Conclusions
As a consequence of using fast crystal pulling rates, some monocrystalline Cz-Si solar wafers contain vacancy-rich regions which act as preferential sites for precipitation of supersaturated bulk oxygen.
Oxide precipitates and surrounding defects are associated with recombination activity, and this can have a substantial detrimental impact on the conversion efficiency of solar cells.
We have used injection-dependent minority carrier lifetime measurements to study this recombination activity in ∼100 samples with different types, doping levels, precipitate morphologies and sizes.
In all cases we find the lifetime can be parameterised in terms of two independent defect states.
The first defect is at EV+0.22eV and has a capture coefficient for electrons ∼157 times greater than that for holes.
The second defect is at EC-0.08eV and has a capture coefficient for holes ~1200 times greater than that for electrons.
The concentration of centres appears to scale with the density (not size) of the precipitates, with surrounding dislocations and stacking faults increasing the density of centres per precipitate.
Remarkably, we find the same defect states in oxide precipitate-containing samples which have been intentionally contaminated with iron.
The density of the states per precipitate is dependent on the iron loss from the bulk per precipitate.
We therefore conclude that at least some of the recombination activity of oxide precipitates in "uncontaminated" samples is due to the segregation of iron to the precipitates and surrounding defects.
We find that a phosphorus diffusion gettering process reduces the density of states associated with the oxide precipitates by ∼20%, and that bulk iron is preferentially gettered to the phosphorus diffused layer rather than to the oxide precipitates.
Acknowledgements
The authors thank D.
Gambaro, M. Cornara, and M. Olmo (SunEdison) for performing precipitation treatments and characterization, R.
Chakalova (University of Oxford) for assistance with sample cleaning and surface passivation, and P.R.
Wilshaw (University of Oxford) for helpful discussions.
JDM is the holder of a Royal Academy of Engineering/EPSRC Research Fellowship and an EPSRC First Grant (EP/J01768X/1).


An experimental comparison of classification algorithms for imbalanced credit scoring data sets

Abstract
In this paper, we set out to compare several techniques that can be used in the analysis of imbalanced credit scoring data sets.
In a credit scoring context, imbalanced data sets frequently occur as the number of defaulting loans in a portfolio is usually much lower than the number of observations that do not default.
As well as using traditional classification techniques such as logistic regression, neural networks and decision trees, this paper will also explore the suitability of gradient boosting, least square support vector machines and random forests for loan default prediction.
Five real-world credit scoring data sets are used to build classifiers and test their performance.
In our experiments, we progressively increase class imbalance in each of these data sets by randomly under-sampling the minority class of defaulters, so as to identify to what extent the predictive power of the respective techniques is adversely affected.
The performance criterion chosen to measure this effect is the area under the receiver operating characteristic curve (AUC); Friedman's statistic and Nemenyi post hoc tests are used to test for significance of AUC differences between techniques.
The results from this empirical study indicate that the random forest and gradient boosting classifiers perform very well in a credit scoring context and are able to cope comparatively well with pronounced class imbalances in these data sets.
We also found that, when faced with a large class imbalance, the C4.5 decision tree algorithm, quadratic discriminant analysis and k-nearest neighbours perform significantly worse than the best performing classifiers.

Introduction
The aim of credit scoring is essentially to classify loan applicants into two classes, i.e., good payers (i.e., those who are likely to keep up with their repayments) and bad payers (i.e., those who are likely to default on their loans).
In the current financial climate, and with the recent introduction of the Basel II Accord, financial institutions have even more incentives to select and implement the most appropriate credit scoring techniques for their credit portfolios.
It is stated in Henley and Hand (1997) that companies could make significant future savings if an improvement of only a fraction of a percent could be made in the accuracy of the credit scoring techniques implemented.
However, in the research literature, portfolios that can be considered as very low risk, or low default portfolios (LDPs), have had relatively little attention paid to them in particular with regards to which techniques are most appropriate for scoring them.
The underlying problem with LDPs is that they contain a much smaller number of observations in the class of defaulters than in that of the good payers.
A large class imbalance is therefore present which some techniques may not be able to successfully handle.
Typical examples of low default portfolios include high-quality corporate borrowers, banks, sovereigns and some categories of specialised lending (Van Der Burgt, 2007) but in some countries even certain retail lending portfolios could turn out to have very low numbers of defaults compared to the majority class.
In a recent FSA publication regarding conservative estimation of low default portfolios, regulatory concerns were raised about whether firms can adequately asses the risk of LDPs (Benjamin, Cathcart, & Ryan, 2006).
A wide range of classification techniques have already been proposed in the credit scoring literature, including statistical techniques, such as linear discriminant analysis and logistic regression, and non-parametric models, such as k-nearest neighbour and decision trees.
But it is currently unclear from the literature which technique is the most appropriate for improving discrimination for LDPs.
Table 1 provides a selection of techniques currently applied in a credit scoring context, along with references showing some of their reported applications in the literature.
Hence, the aim of this paper is to conduct a study of various classification techniques based on five real-life credit scoring data sets.
These data sets will then have the size of their minority class of defaulters further reduced by decrements of 5% (from an original 70/30 good/bad split) to see how the performance of the various classification techniques is affected by increasing class imbalance.
The five real-life credit scoring data sets used in this empirical research study include two data sets from Benelux (Belgium, Netherlands and Luxembourg) institutions, the German Credit and Australian Credit data sets which are publicly available at the UCI repository (http://kdd.ics.uci.edu/), and the fifth data set is a behavioural scoring data set, which was also obtained from a Benelux institution.
The techniques that will be applied in this paper are logistic regression (LOG), linear and quadratic discriminant analysis (LDA, QDA), least square support vector machines (LS-SVM), decision trees (C4.5), neural networks (NN), nearest-neighbour classifiers (k-NN10, k-NN100), a gradient boosting algorithm and random forests.
We are especially interested in the power and usefulness of the gradient boosting and random forest classifiers which have yet to be thoroughly investigated in a credit scoring context.
All techniques will be evaluated in terms of their area under the receiver operating characteristic curve (AUC).
This is a measure of the discrimination power of a classifier without regard to class distribution or misclassification cost (Baesens et al., 2003).
To make statistical inferences from the observed difference in AUC, we followed the recommendations given in a recent article (Demšar, 2006) that looked at the problem of benchmarking classifiers on multiple data sets.
The recommendations given were for a set of simple robust non-parametric tests for the statistical comparison of the classifiers (Demšar, 2006).
The AUC measures will therefore be compared using Friedman's average rank test, and Nemenyi's post hoc test will be employed to test the significance of the differences in rank between individual classifiers.
Finally, a variant of Demšar's significance diagrams will be plotted to visualise their results.
The organisation of this paper is as follows.
Section 2 will begin by providing a literature review of the work that has been conducted on the topic of classification for imbalanced data sets.
A brief explanation will then be given for the ten classification techniques to be used in the analysis of the data sets.
Secondly, the empirical set up and criteria used for comparing the classification performance will be described.
Thirdly, the results of our experiments are presented and discussed.
Finally, conclusions will be drawn from the study and recommendations for further research work will be outlined.
Literature review
A wide range of different classification techniques for scoring credit data sets has been proposed in the literature, a non-exhaustive list of which was provided earlier in Table 1.
In addition, some benchmarking studies have been undertaken to empirically compare the performance of these various techniques (e.g., Baesens et al., 2003), but they did not focus specifically on how these techniques compare on heavily imbalanced samples, or to what extent any such comparison is affected by the issue of class imbalance.
For example, in Baesens et al. (2003) seventeen techniques including both well-known techniques such as logistic regression and discriminant analysis and more advanced techniques such as least square support vector machines were compared on eight real-life credit scoring data sets.
Although more complicated techniques such as radial basis function least square support vector machines (RBF LS-SVM) and neural networks (NN) yielded good performances in terms of AUC, simpler linear classifiers such as linear discriminant analysis (LDA) and logistic regression (LOG) also gave very good performances.
However, there are often conflicting opinions when comparing the conclusions of studies promoting differing techniques.
For example, in Yobas, Crook, and Ross (2000), the authors found that linear discriminant analysis (LDA) outperformed neural networks in the prediction of loan default, whereas in Desai, Crook, and Overstreet (1996), neural networks were reported to actually perform significantly better than LDA.
Furthermore, many empirical studies only evaluate a small number of classification techniques on a single credit scoring data set.
The data sets used in these empirical studies are also often far smaller and less imbalanced than those data sets used in practice.
Hence, the issue of which classification technique to use for credit scoring, particularly with a small number of bad observations, remains a challenging problem (Baesens et al., 2003).
The topic of which good/bad distribution is the most appropriate in classifying a data set has been discussed in some detail in the machine learning and data mining literature.
In Weiss and Provost (2003) it was found that the naturally occurring class distributions in the 25 data sets looked at, often did not produce the best-performing classifiers.
More specifically, based on the AUC measure (which was preferred over the use of the error rate), it was shown that the optimal class distribution should contain between 50% and 90% minority class examples within the training set.
Alternatively, a progressive adaptive sampling strategy for selecting the optimal class distribution is proposed in Provost, Jensen, and Oates (1999).
Whilst this method of class adjustment can be very effective for large data sets, with adequate observations in the minority class of defaulters, in some low default portfolios there are only a very small number of loan defaults to begin with.
Various kinds of techniques have been compared in the literature to try and ascertain the most effective way of overcoming a large class imbalance.
Chawla, Bowyer, Hall, and Kegelmeyer (2002) proposed a synthetic minority over-sampling technique (SMOTE) which was applied to example data sets in fraud, telecommunications management, and detection of oil spills in satellite images.
In Japkowicz (2000), over-sampling and downsizing were compared to the author's own method of "learning by recognition" in order to determine the most effective technique.
The findings, however, were inconclusive but demonstrated that both over-sampling the minority class and downsizing the majority class can be very effective.
Subsequently, Batista (2004) identified ten alternative techniques in dealing with class imbalances and trialed them on thirteen data sets.
The techniques chosen included a variety of under-sampling and over-sampling methods.
Findings suggested that generally over-sampling methods provide more accurate results than under-sampling methods.
Also, a combination of either SMOTE (Chawla et al., 2002) and Tomek links or SMOTE and ENN (a nearest-neighbour cleaning rule), were proposed.
Overview of classification techniques
This study aims to compare the performance of a wide range of classification techniques within a credit scoring context, thereby assessing to what extent they are affected by increasing class imbalance.
For the purpose of this study, ten classifiers have been selected which provide a balance between well-established credit scoring techniques such as logistic regression, decision trees and neural networks, and newly developed machine learning techniques such as least square support vector machines, gradient boosting and random forests.
A brief explanation of each of the techniques applied in this paper is presented below.
Logistic regression
For this paper, we will be focusing on the binary response of whether a creditor turns out to be a good or bad payer (i.e., non-defaulter vs. defaulter).
For this binary response model, the response variable, y, can take on one of two possible values; i.e., y=0 if the customer is a bad payer, y=1 if he/she is a good payer.
Let us assume x is a column vector of M explanatory variables and π=Pr(y=1|x) is the response probability to be modelled.
The number of observations is denoted by N.
The logistic regression model then takes the form:(1)logit(π)≡logπ1-π=α+βTx,where α is the intercept parameter and βT contains the variable coefficients (Hosmer & Stanley, 2000).
Linear and quadratic discriminant analysis
Discriminant analysis assigns an observation to the response, y(y∈{0,1}), with the largest posterior probability; i.e., classify into class 0 if p(0|x)>p(1|x), or class 1 if the reverse is true.
According to Bayes' theorem, these posterior probabilities are given by(2)p(y|x)=p(x|y)p(y)p(x).Assuming now that the class-conditional distributions p(x|y=0), p(x|y=1) are multivariate normal distributions with mean vector μ0, μ1, and covariance matrix Σ0, Σ1, respectively, the classification rule becomes: classify as y=0 if the following is satisfied:(3)x-μ0T∑0-1x-μ0-x-μ1T∑1-1x-μ1<2logP(y=0)-log(P(y=1))+log|Σ1|-log|Σ0|Linear discriminant analysis is then obtained if the simplifying assumption is made that both covariance matrices are equal, i.e., Σ0=Σ1=Σ, which has the effect of cancelling out the quadratic terms in the expression above.
Neural networks (Multi-layer perceptron)
Neural networks (NN) are mathematical representations modelled on the functionality of the human brain (Bishop, 1995).
The added benefit of a NN is its flexibility in modelling virtually any non-linear association between input variables and target variable.
Although various architectures have been proposed, our study focuses on probably the most widely used type of NN, i.e., the multilayer perceptron (MLP).
A MLP is typically composed of an input layer (consisting of neurons for all input variables), a hidden layer (consisting of any number of hidden neurons), and an output layer (in our case, one neuron).
Each neuron processes its inputs and transmits its output value to the neurons in the subsequent layer.
Each such connection between neurons is assigned a weight during training.
The output of hidden neuron i is computed by applying an activation function f(1) (for example the logistic function) to the weighted inputs and its bias term bi(1):(4)hi=f(1)bi(1)+∑j=1MWijxj,where W represents a weight matrix in which Wij denotes the weight connecting input j to hidden neuron i.
For the analysis conducted in this paper, a binary prediction will be made; hence, for the activation function in the output layer, we will be using the logistic (sigmoid) activation function, f(2)(x)=11+e-x to obtain a response probability:(5)π=f(2)b(2)+∑j=1nhvjhj,with nh the number of hidden neurons and v the weight vector where vj represents the weight connecting hidden neuron j to the output neuron.
During model estimation, the weights of the network are first randomly initialised and then iteratively adjusted so as to minimise an objective function, e.g., the sum of squared errors (possibly accompanied by a regularisation term to prevent over-fitting).
This iterative procedure can be based on simple gradient descent learning or more sophisticated optimisation methods such as Levenberg-Marquardt or Quasi-Newton.
The number of hidden neurons can be determined through a grid search based on validation set performance.
Least square support vector machines (LS-SVMs)
Support vector machines (SVMs) are a set of powerful supervised learning techniques used for classification and regression.
Their basic principle is to construct a maximum-margin separating hyperplane in some transformed feature space.
Rather than requiring one to specify the exact transformation though, they use the principle of kernel substitution to turn them into a general (non-linear) model.
The least square support vector machine (LS-SVM) proposed by Suykens, Van Gestel, De Brabanter, De Moor, and Vandewalle (2002) is a further adaptation of Vapnik's original SVM formulation which leads to solving linear KKT (Karush-Kuhn-Tucker) systems (rather than a more complex quadratic programing problem).
The optimisation problem for the LS-SVM is defined as:(6)minw,b,eJ(w,b,e)=12wTw+γ12∑i=1Nei2,subject to the following equality constraints:(7)yiwTφ(xi)+b=1-ei,i=1,…,N,Where w is the weight vector in primal space, γ is the regularisation parameter, and yi=+1 or -1 for good (bad) payers, respectively (Suykens et al., 2002).
A solution can then be obtained after constructing the Lagrangian, and choosing a particular kernel function K(x,xi) that computes inner products in the transformed space, based on which a classifier of the following form is obtained:y(x)=sign∑i=1NαiyiK(x,xi)+b,where by K(x,xi)=φ(x)Tφ(xi) is taken to be a positive definite kernel satisfying the Mercer theorem.The hyper parameter γ for the LS-SVM classification technique is tuned using 10-fold cross validation.
C4.5. decision trees
A decision tree consists of internal nodes that specify tests on individual input variables or attributes that split the data into smaller subsets, and a series of leaf nodes assigning a class to each of the observations in the resulting segments.
For our study, we chose the popular decision tree classifier C4.5, which builds decision trees using the concept of information entropy (Quinlan, 1993).
The entropy of a sample S of classified observations is given by(8)Entropy(S)=-p1log2(p1)-p0log2(p0),where p1(p0) are the proportions of the class values 1(0) in the sample S, respectively.
C4.5 examines the normalised information gain (entropy difference) that results from choosing an attribute for splitting the data.
The attribute with the highest normalised information gain is the one used to make the decision.
The algorithm then recurs on the smaller subsets.
k-NN (memory based reasoning)
The k-nearest neighbours algorithm (k-NN) classifies a data point by taking a majority vote of its k most similar data points (Hastie, Tibshirani, & Friedman, 2001).
The similarity measure used in this paper is the Euclidean distance between the two points:(9)dxi,xj=‖xi-xj‖=xi-xjTxi-xj1/2.
Random forests
Random forests are defined as a group of un-pruned classification or regression trees, trained on bootstrap samples of the training data using random feature selection in the process of tree generation.
After a large number of trees have been generated, each tree votes for the most popular class.
These tree voting procedures are collectively defined as random forests.
A more detailed explanation of how to train a random forest can be found in Breiman (2001).
For the Random Forests classification technique two parameters require tuning.
These are the number of trees and the number of attributes used to grow each tree.
Gradient boosting
Gradient boosting (Friedman, 2001, 2002) is an ensemble algorithm that improves the accuracy of a predictive function through incremental minimisation of the error term.
After the initial base learner (most commonly a tree) is grown, each tree in the series is fit to the so-called "pseudo residuals" of the prediction from the earlier trees with the purpose of reducing the error.
This leads to the following model:(10)F(x)=G0+β1T1(x)+β2T2(x)+⋯+βnTn(x),where G0 equals the first value for the series, T1,…,Tn are the trees fitted to the pseudo-residuals, and βi are coefficients for the respective tree nodes computed by the gradient boosting algorithm.
A more detailed explanation of gradient boosting can be found in Friedman (2001, 2002).
The gradient boosting classifier requires tuning of the number of iterations and the maximum branch size used in the splitting rule.
Experimental set-up and data sets
Data set characteristics
The characteristics of the data sets used in evaluating the performance of the aforementioned classification techniques are given below in Table 2.
The Bene1 and Bene2 data sets were obtained from two major financial institutions in the Benelux region.
For these two data sets, a bad customer was defined as someone who had missed three consecutive months of payments.
The German credit data set and the Australian Credit data set are publicly available at the UCI repository (http://www.kdd.ics.uci.edu/).
The Behav data set was also acquired from a Benelux institution.
As all the data sets used have a reasonable number of observations they will each be split into a training (two thirds) and a test set (one third).
This test set will remain unchanged throughout the analysis of the techniques.
Re-sampling setup and performance metrics
In order for the percentage reduction in the bad observations, in each data set, to be relatively compared, the Bene1 set, Australian credit and the Behavioural Scoring set have first been altered to give a 70/30 class distribution.
This was done by either under-sampling the bad observations (from a total of 1041 bad observations in the Bene1 data set, only 892 observations have been used; and from a total of 307 bad observations in the Australian credit data set, only 164 observations have been used) or under-sampling the good observations in the behavioural scoring data set, (from a total of 1436 good observations, only 838 observations have been used).
For this empirical study, the class of defaulters in each of the training data sets was artificially reduced, by a factor of 5% up to 95% then by 2.5% and 1%, so as to create a larger difference in class distribution.
As a result of this reduction, eight data sets were created for each of the five original data sets.
The percentage splits created were 75%, 80%, 85%, 90%, 95%, 97.5%, 99% good observations.
For this empirical study our focus is on the performance of classification techniques on data sets with a large class imbalance.
Therefore detailed results will only be presented for the data set with the original 70/30 split, as a benchmark, and data sets with 85%, 90% and 99% splits.
By doing so, it is possible to identify whether techniques are adversely affected in the prediction of the target variable when there is a substantially lower number of observations in one of the classes.
The performance criterion chosen to measure this effect is the area under the receiver operator characteristic curve (AUC) statistic as proposed by Baesens et al. (2003).
The receiver operating characteristic curve (ROC) is a two-dimensional graphical illustration of the trade-off between the true positive rate (sensitivity) and false positive rate (1-specificity).
The ROC curve illustrates the behaviour of a classifier without having to take into consideration the class distribution or misclassification cost.
In order to compare the ROC curves of different classifiers, the area under the receiver operating characteristic curve (AUC) must be computed.
The AUC statistic is similar to the Gini coefficient which is equal to 2×(AUC-0.5).
An example of an ROC curve is depicted in Fig. 1:
The diagonal line represents the trade-off between the sensitivity and (1-specificity) for a random model, and has an AUC of 0.5.
For a well performing classifier the ROC curve needs to be as far to the top left-hand corner as possible.
In the example shown in Fig. 1, the classifier that performs the best is the ROC1 curve.
Parameter tuning and input selection
The linear discriminant analysis (LDA), quadratic discriminant analysis (QDA) and logistic regression (LOG) classification techniques require no parameter tuning.
The LOG model was built in SAS using proc logistic and using a stepwise variable selection method.
Both the LDA and QDA techniques were run in SAS using proc discrim.
Before all the techniques were run, dummy variables were created for the categorical variables.
The AUC statistic was computed using the ROC macro by DeLong, DeLong, and Clarke-Pearson (1988), which is available from the SAS website (http://.support.sas.com/kb/25/017.html).
For the LS-SVM classifier, a linear kernel was chosen and a grid search mechanism was used to tune the hyper-parameters.
For the LS-SVM, the LS-SVMlab Matlab toolbox developed by Suykens et al. (2002) was used.
The NN classifiers were trained after selecting the best performing number of hidden neurons based on a validation set.
The neural networks were trained in SAS Enterprise Miner using a logistic hidden and target layer activation function.
The confidence level for the pruning strategy of C4.5 was varied from 0.01 to 0.5, and the most appropriate value was selected for each data set based on validation set performance.
The tree was built using the Weka (Witten & Frank, 2005) package.
Two parameters have to be set for the Random Forests technique: these are the number of trees and the number of attributes used to grow each tree.
A range of [10,50,100,250,500,1000] trees has been assessed, as well as three different settings for the number of randomly selected attributes per tree ([0.5,1,2].M), whereby M denotes the number of attributes within the respective data set (Breiman, 2001).
As with the C4.5 algorithm, Random Forests were also trained in Weka (Witten & Frank, 2005), using 10-fold cross-validation for tuning the parameters.
The k-Nearest Neighbours technique was applied for both k=10 and k=100, using the Weka (Witten & Frank, 2005) IBk classifier.
For the gradient boosting classifier a partitioning algorithm was used as proposed by Friedman (2001).
The number of iterations was varied in the range [10,50,100,250,500,1000], with a maximum branch size of two selected for the splitting rule (Friedman, 2001).
The gradient boosting node in SAS Enterprise Miner was used to run this technique.
Statistical comparison of classifiers
We used Friedman's test (Friedman, 1940) to compare the AUCs of the different classifiers.
The Friedman test statistic is based on the average ranked (AR) performances of the classification techniques on each data set, and is calculated as follows:(11)χF2=12DK(K+1)∑j=1KARj2-K(K+1)24,whereARj=1D∑i=1Drij.In (13), D denotes the number of data sets used in the study, K is the total number of classifiers and rij is the rank of classifier j on data set i.
χF2 is distributed according to the Chi-square distribution with K-1 degrees of freedom.
If the value of χF2 is large enough, then the null hypothesis that there is no difference between the techniques can be rejected.
The Friedman statistic is well suited for this type of data analysis as it is less susceptible to outliers (Friedman, 1940).
The post hoc Nemenyi test (Nemenyi, 1963) is applied to report any significant differences between individual classifiers.
The Nemenyi post hoc test states that the performances of two or more classifiers are significantly different if their average ranks differ by at least the critical difference (CD), given by(12)CD=qα,∞,KK(K+1)12D.In this formula, the value qα,∞,K is based on the studentised range statistic (Nemenyi, 1963).
Finally, the results from Friedman's statistic and the Nemenyi post hoc tests are displayed using a modified version of Demšar (2006) significance diagrams (Lessmann, Baesens, Mues, & Pietsch, 2008).
These diagrams display the ranked performances of the classification techniques along with the critical difference to clearly show any techniques which are significantly different to the best performing classifiers.
Results and discussion
The table on the following page (Table 3) reports the AUCs of all ten classifiers on the five credit scoring data sets at varying degrees of class imbalance.
For each level of imbalance, the Friedman test statistic and corresponding p-value is shown.
As these were all significant (p<0.005) a post hoc Nemenyi test was then applied to each class distribution.
The technique achieving the highest AUC on each data set is underlined as well as the overall highest ranked technique.
Table 3 shows that the gradient boosting algorithm has the highest Friedman score (average rank (AR)) on two of the five different percentage class splits.
However at the extreme class split (99% good, 1% bad) Random Forests provides the best average ranking across the five data sets (Random Forests also ranks first on the 10% data set).
In the majority of the class splits, the AR of the QDA and Lin LS-SVM classifiers are statistically worse than the AR of the Random Forests classifier at the 5% critical difference level (α=0.05), as shown in the significance diagrams included next.
Note that, even though the differences between the classifiers are small, it is important to note that in a credit scoring context, an increase in the discrimination ability of even a fraction of a percent may translate into significant future savings (Henley & Hand, 1997).
The following significance diagrams display the AUC performance ranks of the classifiers, along with Nemenyi's critical difference (CD) tail.
The CD value for all the following diagrams is equal to 6.06.
Each diagram shows the classification techniques listed in ascending order of ranked performance on the y-axis, and the classifier's mean rank across all five data sets displayed on the x-axis.
Two vertical dashed lines have been inserted to clearly identify the end of the best performing classifier's tail and the start of the next significantly different classifier.
The first significance diagram (see Fig. 2) displays the average rank of the classifiers at the original class distribution of a 70% good, 30% bad split:
At this original 70/30% split, the linear LS-SVM is the best performing classification technique with an AR value of 1.2.
This diagram clearly shows that the k-NN10, QDA and C4.5 techniques perform significantly worse than the best performing classifier with values of 7.7, 8.5 and 9.1 respectively.
The following significance diagram displays the average rank of the classifiers at an 85% good, 15% bad class split:
At the level where only 15% of the data sets are bad observations, it is shown in the significance diagram that gradient boosting becomes the best performing classifier (see Fig. 3).
The gradient boosting classifier performs significantly better than the quadratic discriminant analysis (QDA) classifier.
From these findings we can make a preliminary assumption that when a larger class imbalance is present, the QDA classifier remains significantly different to the gradient boosting classifier.
All the other techniques used are not significantly different.
At a 90% good, 10% bad class split the significance diagram shown in Fig. 4 indicates that the C4.5 and QDA algorithms are significantly worse than the random forests classifier.
It can be noted that the Linear LS-SVM classifier however is progressively becoming less powerful as a large class imbalance is present (see Fig. 5).
The final split, displaying a 99% good, 1% bad class split, indicates that, at the most extreme class distribution analysed, two classification techniques are significantly worse (Lin LS-SVM and QDA).
This displays an interesting finding that at the extreme split, LOG is now close to being significantly worse than the Random Forests algorithm.
The logistic regression technique therefore shows limited power in correctly classifying observations where only a small number of bad observations exist.
It can also be concluded that the random forests classifier performs surprisingly well given a large class imbalance.
In summary, when considering the AUC performance measures, it can be concluded that the gradient boosting and random forest classifiers yield a very good performance at extreme levels of class imbalance, whereas the Lin LS-SVM sees a reduction in performance as a larger class imbalance is introduced.
However, the simpler, linear classification techniques such as LDA and LOG also give a relatively good performance, which is not significantly different from that of the gradient boosting and random forest classifiers.
This finding seems to confirm the suggestion made in Baesens et al. (2003) that most credit scoring data sets are only weakly non-linear.
However, techniques such as QDA, C4.5 and k-NN10 perform significantly worse than the best performing classifiers at each percentage reduction.
The majority of classification techniques yielded classification performances that are quite competitive with each other.
Conclusions and recommendations for further work
In this comparative study we have looked at a number of credit scoring techniques, and studied their performance over various class distributions in five real-life credit data sets.
Two techniques that have yet to be fully researched in the context of credit scoring, i.e., gradient boosting and random forests, were also chosen to give a broader review of the techniques available.
The classification power of these techniques was assessed based on the area under the receiver operating characteristic curve (AUC).
Friedman's test and Nemenyi's post hoc tests were then applied to determine whether the differences between the average ranked performances of the AUCs were statistically significant.
Finally, these significance results were visualised using significance diagrams for each of the various class distributions analysed.
The results of these experiments show that the gradient boosting and random forest classifiers performed well in dealing with samples where a large class imbalance was present.
It does appear that in extreme cases the ability of random forests and gradient boosting to concentrate on 'local' features in the imbalanced data is useful.
The most commonly used credit scoring techniques, linear discriminant analysis (LDA) and logistic regression (LOG), gave results that were reasonably competitive with the more complex techniques and this competitive performance continued even when the samples became much more imbalanced.
This would suggest that the currently most popular approaches are fairly robust to imbalanced class sizes.
On the other hand, techniques such as QDA and C4.5 were significantly worse than the best performing classifiers.
It can also be concluded that the use of a linear kernel LS-SVM would not be beneficial in the scoring of data sets where a very large class imbalance exists.
Finally, as stated in the literature review section of this paper, there have been several approaches already researched in the area of over-sampling techniques to deal with large class imbalances.
Further research into this and their effect on credit scoring model performance would be beneficial.
Acknowledgements
The authors of this paper would like to thank the EPSRC and SAS UK for their financial support to Iain Brown.


4. The precursor according to claim 1, wherein the transition metal has a composition of NixCoyMn1-(x+y)wherein 0.3≤x≤0.9, 0.1≤y≤0.6, and x+y≤1.8. The precursor according to claim 1, wherein the salt ions includes nitrate ions (NO3).9. The precursor according to claim 1, wherein the salt ions have a content of 0.2 to 0.6% by weight, based on the total weight of the precursor.10. A precursor for the preparation of a lithium transition metal oxide, wherein 0.1 to 0.7% by weight of sulfate ions (SO4) is detected based on the total weight of the precursor, upon chromatographic measurement after dissolution of the precursor with addition of an acid.11. A lithium transition metal oxide which is prepared by sintering the precursor of claim 1 and a lithium-containing material.12. The oxide according to claim 11, wherein the lithium-containing material is lithium carbonate (Li2CO3) and/or lithium hydroxide (LiOH).13. A cathode active material for a lithium secondary battery, comprising the lithium transition metal oxide of claim 11.14. A lithium secondary battery comprising the cathode active material of claim 13.
Advancement in liquid exfoliation of graphite through simultaneously oxidizing and ultrasonicating
Graphite powder (Micro 0850) was provided by Asbury Carbons Pty Ltd. Potassium permanganate, sulphuric acid (95-98%) and phosphoric acid (85 wt%) were purchased from Sigma-Aldrich. Hydrogen peroxide (30 wt%) was bought from Chem-Supply.
KMnO4 (0.4 g) was dissolved in a mixture of concentrated H2SO4 (6.0 g) and H3PO4 (26.0 g). Graphite powder (0.1 g) was added into the mixture and stirred for 1 min. Then the mixture was immediately covered and placed into an ultrasonic bath (200 W and 42 kHz) for simultaneous oxidization and ultrasonication (oxidi-sonication). A cooler was connected with the bath to prevent heat build-up. During the oxidi-sonication, graphite flakes could exfoliate and split into graphene sheets; meanwhile, they were oxidized by Mn2O7 that was created by 2KMnO4 (s) + H2SO4 (aq) - Mn2O7 (l) + K2SO4 + H2O. The reaction can be terminated anytime by moving the mixture out of the bath and then adding 120 g water slowly, followed by vacuum filtrating and washing three times with water and hydrogen peroxide for removal of ions and acids. The prepared sheets should be stored in solvents such as water or acetone for the following film fabrication. For characterization in need of powder samples, the sheets were dried in an air-ventilated oven at 60 degC and then in a vacuum oven at 100 degC for 4 hours to remove crystalline water. Although we started with 0.1 g graphite, the product was weighed to be 0.105 g; the slight increment would be caused by the O element added by the oxidi-sonication and also likely due to the combined water.1. A precursor of a positive electrode active material for a secondary battery comprising a single layer-structured secondary particle in which pillar-shaped primary particles radially oriented in a surface direction from the particle center are aggregated,
wherein the secondary particle has a shell shape; and
the primary particle includes a composite metal hydroxide of Ni—Co—Mn of the following Chemical Formula 1:
Ni1-(x+y+z)CoxMyMnz(OH)2[Chemical Formula 1]
wherein, in Chemical Formula 1,
M includes any one, or two or more elements selected from the group consisting of Al, Zr, Mg, Zn, Y, Fe and Ti; and
x, y and z are each 0<x<1, 0≤y<1, 0<z<1 and 0<x+y+z<1.2. The precursor of a positive electrode active material for a secondary battery of claim 1, wherein 0<x+y+z<0.5 in Chemical Formula 1.3. The precursor of a positive electrode active material for a secondary battery of claim 1, wherein the primary particle has a length ratio of 0.3 to 1 with respect to a radius of the secondary particle when considering a length of a major axis passing through the particle center as a length of the primary particle.4. The precursor of a positive electrode active material for a secondary battery of claim 1, wherein the primary particle has an average aspect ratio of 5 to 30 when considering a ratio of a length of a major axis, which is perpendicular to a minor axis passing through the particle center, with respect to a length of the minor axis as an aspect ratio.5. The precursor of a positive electrode active material for a secondary battery of claim 1, which has an average particle diameter (D50) of 7 μm to 20 μm and a BET specific surface area of 5.0 m2/g to 30.0 m2/g.6. A method for preparing the precursor of a positive electrode active material for a secondary battery of claim 1, the method comprising:
preparing a metal-containing solution by mixing a nickel raw material, a cobalt raw material and a manganese raw material; and
and introducing an ammonium cation-containing complex forming agent and a basic compound to the metal-containing solution and co-precipitation reacting the result under a pH of 10.50 to 12.00 and a temperature of 50° C. to 70° C.,
wherein the ammonium cation-containing complex forming agent is introduced at a rate of 0.5 times to 1.5 times with respect to an introduction rate of the metal-containing solution.7. The method for preparing the precursor of a positive electrode active material for a secondary battery of claim 6, wherein the ammonium cation-containing complex forming agent and the basic compound are used in a molar ratio of 1:10 to 1:2.8. A positive electrode active material for a secondary battery comprising a single layer-structured secondary particle in which pillar-shaped primary particles radially oriented in a surface direction from the particle center are aggregated,
wherein the secondary particle has a shell shape; and
the primary particle includes a lithium composite metal oxide of Ni—Co—Mn of the following Chemical Formula 2 and exhibits mono-modal-type particle distribution:
Liα[Ni1(x+y+z)CoxMyMnz]O2[Chemical Formula 2]
wherein, in Chemical Formula 2,
M includes any one, or two or more elements selected from the group consisting of Al, Zr, Mg, Zn, Y, Fe and Ti;
x, y and z are each 0<x<1, 0≤y<1, 0<z<1 and 0<x+y+z<1; and
a is 1.0≤a≤1.5.9. The positive electrode active material for a secondary battery of claim 8, which has an average particle diameter of 7 μm to 15 μm and a BET specific surface area of 0.1 m2/g to 1.0 m2/g.10. The positive electrode active material for a secondary battery of claim 8, which has tap density of 1.7 g/cc to 3.0 g/cc.11. A positive electrode for a secondary battery comprising the positive electrode active material of claim 8.12. A lithium secondary battery comprising the positive electrode of claim 11.1. One Li-Ni composite oxide particles, characterized in:
The Li-Ni composite oxide has a composition of LixNi1-y-a-bCoyM1aM2bO2, wherein, 1.00 ≤ x ≤ 1.10, 0 y ≤ 0.25, 0 a ≤ 0.25, 0 ≤ b ≤ 0.10, selected from the group Al M1 is, at least one element Mn, selected from the group Zr M2 is, at least one element Mg,
Obtained by x-ray diffraction X of metal lithium sites occupancy Rietveld analysis (%) is obtained by parsing by the rietveld crystallite size (nm) is above 700 product of 1400 or less.2. Li-Ni composite oxide particles according to claim 1, characterized in:
Li-Ni composite oxide of lithium sites is obtained by parsing rietveld by 2% or more 7% metal occupancy is less.3. Li-Ni composite oxide particles according to claim or 2 1, characterized in:
Li-Ni of composite oxide of a crystallite size below 500 nm obtained by Rietveld analysis.4. 1 according to claim-any one of claims 3 Li-Ni composite oxide particles, characterized in:
An average particle diameter 1-20 µm, a specific surface area BET 0.1-1.6 m2/g.5. Claim 1-4 of, any one Li-Ni producing composite oxide particles, characterized in:
With powder of a lithium compound Ni-Co hydroxide particles mixed powder, firing the obtained mixture,
Ni-Co hydroxide particles obtained by powder, aqueous solution comprising a metal sulfate, aqueous ammonia and aqueous sodium hydroxide mixed, controls such that the reaction tank ammonia concentration is 1.4 mol/L or less, and the (ammonia concentration of the reaction vessel)/(concentration of remaining reaction tank hvdroxide) of 6 or more, obtaining Ni-Co hydroxide.6. Claim 1-4 of, any one Li-Ni producing composite oxide particles, characterized in:
A powder of a lithium compound, Ni-Co hydroxide particle powder, aluminum powder and/or zirconium compound mixing powder of the compound, firing the obtained mixture,
Ni-Co hydroxide particles obtained by powder, aqueous solution comprising a metal sulfate, aqueous ammonia and aqueous sodium hydroxide mixture, ammonia concentration is 1.4 mol/L so that the reaction tank is controlled below, and the (ammonia concentration of the reaction vessel)/(concentration of the reaction vessel remaining hvdroxide) of 6 or more, obtaining Ni-Co hydroxide.7. A non-aqueous electrolyte secondary battery, characterized in:
Claim 1-4 used by containing, in any one Li-Ni constituting the composite oxide particles of the positive electrode active material of the positive electrode.High-capacity graphene oxide/graphite/carbon nanotube composites for use in Li-ion battery anodes
All chemical reagents used to prepare graphene oxide were analytical grade (purchased from Chengdu Kelong Chemicals Co., Ltd.) and used as received. Graphene oxide was synthesized from high-purity natural graphite flakes (about 200 meshes, 99.999% purity; Changsha Shenghua Research Institute) using the Hummers method [11]. In a typical synthesis, 5 g of natural graphite flakes was added to 115 ml of H2SO4 (98%) with vigorous stirring in an ice bath. Then, 15 g KMnO4 was slowly added to the reaction vessel five times. Afterwards, 230 ml of deionized water and 30 ml of H2O2 were added to the suspension. The ice bath was removed and then subjected to centrifugal separation and repeated washing using dilute hydrochloric acid and deionized water. The colloidal dispersion of the as-synthesized graphene oxide in deionized water at a concentration of 1 mg ml-1 was prepared with the aid of ultrasonication (20 kHz ultrasound probe) for about 15 min until stabilization and browning of the dispersion.

Synthesis and characterization of nanocrystalline La2Mo2O9 fast oxide-ion conductor by an in-situ polymerization method

Nanocrystalline La2Mo2O9 powder was prepared by an in situ polymerization method using polyacrylates of La and Mo as the precursor compound. The polymeric precursor was made by solution polymerization of aqueous solution of acrylic acid in presence of La(NO3)3 and (NH4)6Mo7O24 with (NH4)S2O8 as the initiator. The typical experimental procedure is first to dissolve the stoichiometric amount of La(NO3)3*6H2O and (NH4)6Mo7O24*4H2O in triple distilled water and then the solution is pour into a acrylic acid solution (acrylic acid:H2O, 70:30 wt%) with constant stirring. To this a small amount of 5% (NH4)S2O8 solution was added as initiator to promote the polymerization. Under heating at ~80 degC, a well-distributed polyacrylates of La and Mo is formed. Afterward the resultant polyacrylates were dried at 120 degC for 1-2 h. The dried polymeric precursor was further heated to 520 degC for 5 h in air with a heating rate of 5 degC/min to eliminate the residual organic phase and get a nanocrystalline lanthanum molybdate powder.

1. A heterogeneous metal bearing material, comprising a host material composed of primary particles agglomerated into secondary particles, and a particulate dopant material,characterised in thatthe particulate dopant material is homogeneously distributed within the secondary particles of said host material.3. A heterogeneous metal bearing material according to claim 2, wherein said heterogeneous metal bearing material has the general formula (dopant material)<sub>a</sub>(host material)<sub>b</sub>, where a and b are weight fractions, with 0 < a < 0.4, preferably 0.001 < a < 0.4, and more preferably 0.001 < a < 0.02, and where b = 1 - a.5. A heterogeneous metal bearing material according to claim 3, wherein the dopant material is TiO2and the host material is either one or a mixture of NixMnycozhydroxide, oxyhydroxide, and oxide, where x, y, z are atomic fractions, with 0≤x≤1,0≤1,0≤z ≤ 1, and x+y+z = 1;said host material optionally comprising a homogeneous distribution of Mg atoms.8. A heterogeneous metal bearing material according to any one of claims 1 to 7, wherein said dopant material has a size range of ≥5 nm and ≤ 200 nm, and preferably between 10 and 50 nm.12. Process according to any one of claims 9 to 11, wherein said dopant material is either one or more of MgO, Cr2O3, ZrO2, Al2O3, and TiO2, and has a size range of ≥ 5 nm and ≤ 200 nm.
Perovskite membrane of La1-xSrxTi1-yFeyO3-δ for partial oxidation of methane to syngas

Powders of La2O3 (Wako Pure Chemical Industries, Ltd., grain size 1 μm), SrCO3 (Wako, grain size 1 μm), TiO2 (Wako, grain size 1 μm), and Fe2O3 (Wako, grain size 1 μm) were mixed to prepare various compositions of La1-xSrxTi1-yFeyO3-δ with x = 0.1-0.8 and y = 0.6-1.0. The powders were mixed using a ball mill with YSZ balls for 5 h, and then calcined at 1000 degC for 6 h. The calcined powder was mixed with a PVA aqueous solution binder and compacted into pellets (bar of 5 x 5 x 30 mm, disk of φ = 25 mmxt = 3 mm) using a compact press of 100 MPa. These compacts were fired at 1400degC in air for 3 h.


0.5:1, 1:1, and 2:1 2,2'-Bipyridine-SnCl2 complexes were prepared in MeOH (20 mL) by adding respectively 2.5, 5.0, and 10.0 mmol 2,2'-bipyridine to 5 mmol SnCl2 under a nitrogen atmosphere. The color of the solution turned to yellow after the addition of 2,2'-bipyridine and a precipitate appeared. The mixture was further stirred for 0.5 h before filtration. The solid obtained was washed two times with methanol and dried in vacuo to yield the 2,2'-bipyridine-SnCl2 complex.

The Sn nanoparticles/graphite (molar ratio of 1:12) nanocomposite was prepared by treatment of the 2,2'-bipyridine-SnCl2 complex with sodium borohydride in the presence of graphite.

In a typical synthesis, 5 mmol of SnCl2 coordinated with 2,2'-bipyridine was dispersed in methanol (20 mL) and UF4 graphite (Le Carbone Lorraine) was added. The mixture was stirred for 10 min at room temperature under a nitrogen atmosphere. NaBH4 (5 mmol) was then added. A mild exothermic reaction was observed. The solution was further stirred at room temperature for 2 h. The reaction mixture was then concentrated in vacuo and the product washed with water (2 x 5 mL) and acetone (2 x 5 mL) by centrifugation (4000 rpm for 5 min). Such a washing process leads to a significant loss of tin present in the crude material [18].
1. A lithium transition metal oxide prepared from a transition metal precursor,
wherein a ratio of the tap density of the precursor to the average particle diameter D50 satisfies a condition represented by the following formula 1,
wherein a ratio of the average particle diameter D50 of the lithium transition metal oxide to the average particle diameter D50 of the transition metal precursor satisfies a condition represented by the following formula 3:


wherein the transition metal precursor is selected from the group consisting of transition metal oxides, transition metal sulfides, transition metal nitrides, transition metal phosphides, and transition metal hydroxides.2. The lithium transition metal oxide according to claim 1, wherein the transition metal precursor comprises at least two transition metals.3. The lithium transition metal oxide according to claim 2, wherein the at least two transition metals are at least two selected from the group consisting of Ni, Co, Mn, Al, Cu, Fe, Mg, B, Cr and 2 nd cycle transition metals.4. The lithium transition metal oxide according to claim 3, wherein the at least two transition metals comprise two transition metals selected from the group consisting of Ni, Co and Mn or all of the transition metals.5. The lithium transition metal oxide according to claim 1, wherein the precursor particles constituting the transition metal precursor are transition metal hydroxide particles.6. The lithium transition metal oxide according to claim 5, wherein the transition metal hydroxide particles are a compound represented by the following formula 2:
M(OH1-x)2(2)
wherein M is at least two selected from the group consisting of Ni, Co, Mn, Al, Cu, Fe, Mg, B, Cr, and a period 2 transition metal; and x is more than or equal to 0 and less than or equal to 0.5.7. The lithium transition metal oxide according to claim 6, wherein M comprises two transition metals or all transition metals selected from the group consisting of Ni, Co and Mn.8. The lithium transition metal oxide according to claim 1, wherein the average particle diameter D50 of the transition metal precursor is 1 μm to 30 μm.9. The lithium transition metal oxide according to claim 1, wherein the lithium transition metal oxide is a compound represented by the following formula 4:
LiaNixMnyCozMwO2-tAt(4)
wherein a is more than 0 and less than or equal to 1.2, x is more than or equal to 0 and less than or equal to 0.9, y is more than or equal to 0 and less than or equal to 0.9, z is more than or equal to 0 and less than or equal to 0.9, w is more than or equal to 0 and less than or equal to 0.3, a + x + y + z + w is more than or equal to 2 and less than or equal to 2.3, and t is;
m is a cation of at least one metal selected from the group consisting of Al, Cu, Fe, Mg, B, Cr and a 2 nd period transition metal; and is
A is at least one monovalent or divalent anion.10. The lithium transition metal oxide according to claim 9, wherein, in formula 4, x > y and x > z.11. A lithium secondary battery, wherein a unit cell comprising a positive electrode, a negative electrode and a polymer film interposed between the positive electrode and the negative electrode is mounted in a battery case, the positive electrode comprising the lithium transition metal oxide according to any one of claims 1 to 10.12. The lithium secondary battery according to claim 11, wherein the lithium secondary battery is a lithium ion battery.13. The lithium secondary battery according to claim 11, wherein the lithium secondary battery is a lithium ion polymer battery.14. The lithium secondary battery according to claim 11, wherein the lithium secondary battery is a lithium polymer battery.2. The positive active material for a lithium secondary battery according to claim 1, wherein the lithium transition metal composite oxide is represented by the composition formula Li<sub>1+x</sub>(Ni<sub>a</sub>Co<sub>b</sub>Mn<sub>c</sub>)<sub>1-x</sub>O<sub>2</sub> (-0.1 < x < 0.1, 0.5≤a≤0.9, 0.1≤b≤0.3, 0.03 ≤ c ≤ 0.3, a + b + c = 1).3. The positive active material for a lithium secondary battery according to claim 1 or 2, wherein in the lithium transition metal composite oxide, a particle size distribution does not have two or more local maximal values.6. An electrode for a lithium secondary battery containing the positive active material for a lithium secondary battery according to any one of claims 1 to 3.1. A transition metal precursor for preparation of a lithium transition metal oxide, in which a ratio of tap density to average particle diameter D50 of the precursor satisfies a condition represented by Equation 1 below, and wherein transition metal precursor particles are transition metal hydroxide particles represented by Formula 2 below:
2000≤Tap densityAverage particle diameter D50 of transition metal precursor≤3500g/cc⋅cm
M(OH1-x)2(2)   wherein M consists of Ni, Co, and Mn; and 0≤x≤0,5, and wherein the tap density indicates a bulk density of a powder obtained by vibrating a container under a constant conditions when filled with the powder, wherein 50 g of the transition metal precursor was added to a 100cc cylinder for tapping and then tapped 3000 times.2. A lithium transition metal oxide in which a ratio of average particle diameter D50 of the lithium transition metal oxide to average particle diameter D50 of a transition metal precursor according to claim 1 for preparation of the lithium transition metal oxide satisfies the condition represented by Equation 3 below:
<maths id="math0004" num="(3)"><math display="block"><mn>0</mn><mo><</mo><mfrac><mi>Average particle diameter D50 of lithium transition metal oxide</mi><mi>Average particle diameter D50 of transion metal precursor</mi></mfrac><mo><</mo><mn>1.2</mn></math><image height="24mm" id="ib0004" img-content="math" key="EP2902364" src="imgb0004.tif" width="129mm"/></maths>3. The lithium transition metal oxide according to claim 2, wherein the lithium transition metal oxide is a compound represented by Formula 4 below:
LiaNixMnyCozO2-tAt(4)
wherein 0<a≤1.2, 0<x≤0.9, 0<y≤0.9, 0<z≤0.9, 2≤a+x+y+z≤2.3, and 0≤5t<0.2; and
A is at least one monovalent or divalent anion.4. The lithium transition metal oxide according to claim 3, wherein, in Formula 4, x>y and x>z.5. A lithium secondary battery in which a unit cell comprising a positive electrode comprising the lithium transition metal oxide according to any one of claims 2 to 4, a negative electrode, and a polymer membrane disposed between the positive electrode and the negative electrode is accommodated in a battery case.6. The lithium secondary battery according to claim 5, wherein the lithium secondary battery is a lithium ion battery.7. The lithium secondary battery according to claim 5, wherein the lithium secondary battery is a lithium ion polymer battery.8. The lithium secondary battery according to claim 5, wherein the lithium secondary battery is a lithium polymer battery.
Microwave assisted hydrothermal synthesis of single crystalline ZnO nanorods for gas sensor application

ZnO nanorods were synthesized similar to our previous work with a little modification [11]. In a typical procedure, Zn(NO3)2*6H2O (Reagent Grade, 98% Sigma-Aldrich) was dissolved in 100 ml of deionized water to make 0.1 M solution. Ammonia water was added to this solution to make pH 7, followed by vigorous stirring for 1 h. A white precipitate (Zn(OH)2) was produced which was collected by centrifugation and washed thoroughly with deionized water. Then the precipitate was transferred into 35 ml water containing 0.1 M of CTAB (Aldrich) and charged in a 100 ml capacity autoclave with Teflon liner. Microwave assisted hydrothermal reaction was carried out at 150 degC for 1 h in microwave oven (MARS; CEM). After completion of the reaction, it was cooled to room temperature and powder sample was collected by centrifugation. Powdered sample was thoroughly washed with deionized water and ethanol. Finally, sample was dried at 80 degC for 12 h.
Layered nickel metal-organic framework for high performance alkaline battery-supercapacitor hybrid devices

All reagents used were of analytical grade and used without any purification. In a typical process, NiCl2*6H2O (0.173 g) and p-benzenedicarboxylic acid (PTA) (0.332 g) were dissolved in 10 mL and 5 mL N,N-dimethylformamide (DMF), respectively under vigorous stirring for 20 min at room temperature. Subsequently, the DMF solution of NiCl2*6H2O was slowly added to the ligand solution (PTA) drop by drop. After forming a greenish solution, the as-prepared mixture was transferred into a Teflon-lined stainless steel autoclave (50 mL volume) and maintained at 120 degC for 16 h. After the autoclave cooled down to room temperature, the precipitate was washed via centrifugation at 10000 rpm for 5 min with DMF and alcohol several times followed by solvent exchange with methanol four times over two days. The samples were fully activated by removing solvent with vacuum at 100 degC over 12 h.
3D flower-like NaHTi 3 O 7 nanotubes as high-performance anodes for sodium-ion batteries
NaHTi3O7 nanotubes were synthesized at a low temperature through a hydrothermal method. Initially, 1.4 g TiO2 powder (anatase) was dispersed in a 70 mL 15 M NaOH aqueous solution in a sealed Teflon vessel. After being stirred for 5 h, the solution mixture was heated at 130 degC for 72 h in an oven, and was subsequently cooled to room temperature in air. The obtained preliminary precipitate was washed with deionized water until a pH value of 7 was reached. After alcohol-cleaning, the precipitate was collected by using a centrifuge and then the mixture was dried in an oven at 60 degC for 12 h; the precursor (NaHTi3O7*2H2O) was obtained. Finally, the precursor was heated in a muffle furnace at 600 degC and incubated for 2 hours to release the crystal water, and then the as-prepared NaHTi3O7 nanotubes were obtained.Ionic Liquid-Promoted Oxidant-Free Dehydrogenation of Alcohols with Water-Soluble Ruthenium Nanoparticles in Aqueous Phase
To the aqueous solution of RuCl3 (0.038 M, 5 mL) were added 2.0 g (0.05 mmol) of PVP. The mixture was further stirred for 30 min under a bath of 0 degC. Then, an aqueous solution of NaBH4 (0.1 M, 19.0 mL) was rapidly added into the mixture under vigorous stirring. The color of the reaction mixture immediately turned from red-brown to black, indicating the formation of Ru nanoparticles. The hydrosol of the Ru nanoparticles was dialyzed overnight to remove inorganic impurities such as Na+ and Cl- by using a cellulose ester dialysis membrane with a cut-off molecular-weight of 10 kDa. The hydrosol of the Ru nanoparticles thus dialyzed was diluted to 30 mL. The pH value of the solution was measured to be 7.
The reaction was carried out in a reaction vessel equipped with a reflux condenser and connected to a bubbler. The vessel was evacuated and filled with N2, then, 3 mL of Ru nanoparticles aqueous solution (0.2 g PVP, 0.019 mmol Ru in 3 mL H2O) were placed in the reaction vessel, followed by addition of alcohol (0.48 mmol) and additive (1 mmol), and the reaction mixture was vigorously stirred under reflux conditions under an N2 atmosphere for 24 h. After reaction, the mixture was extracted with ethyl ether three times. The conversion and selectivity were determined by GC analysis with biphenyl as an internal standard.The invention claimed is:
1. A lithium metal oxide powder for a positive electrode material in a rechargeable battery, having the general formula Li<sub>1+a</sub>M<sub>1−a</sub>O<sub>2 </sub>where M=Ni<sub>x</sub>Mn<sub>y</sub>Co<sub>z</sub>A<sub>v</sub>, A being a dopant, wherein −0.05≤a≤0.25, 0.20≤x≤0.90, 0.10≤y≤0.67, and 0.10≤z≤0.40, v≤0.05, and x+y+z+v=1, the powder having a particle size distribution with 10 μm≤D50≤20 μm, a specific surface with 0.9≤BET≤5, the BET being expressed in m<sup>2</sup>/g, the powder further comprising a sodium and sulfur impurity, wherein the sum (2*Na<sub>wt</sub>)+S<sub>wt </sub>of the sodium (Na<sub>wt</sub>) and sulfur (S<sub>wt</sub>) content expressed in wt % is more than 0.4 wt % and less than 1.6 wt %, and wherein the sodium to sulfur molar ratio (Na/S) is 0.4<Na/S<2.2. The lithium metal oxide powder of claim 1, comprising a secondary LiNaSO4phase.3. The lithium metal oxide powder of claim 2, wherein the relative weight of the secondary LiNaSO4phase is at least 0.5 wt %, as determined by Rietveld analysis of the XRD pattern of the powder.4. The lithium metal oxide powder of claim 1, wherein either:
0.4<Na/S<1, and the powder further comprises Na<sub>2</sub>SO<sub>4</sub>; or
1<Na/S<2, and the powder further comprises 112504.5. The lithium metal oxide powder of claim 1, wherein A is selected from the group consisting of Mg, Al, Ti, Zr, Ca, Ce, Cr, Nb, Sn, Zn and B.6. The lithium metal oxide powder of claim 1, wherein the lithium metal oxide powder has a rocksalt structure.High-density iron nanoparticles encapsulated within nitrogen-doped carbon nanoshell as efficient oxygen electrocatalyst for zinc-air battery

Dicyandiamide and ammonium ferric citrate are used as pyrolysis precursors.

In a typical experiment, 8 g of dicyandiamide (C2H4N4, Alfa Aesar, denoted as DCDA) and 1 g of ammonium ferric citrate (C6H11FeNO7, J&K Chemical Ltd., denoted as AFC) were dissolved in 100 mL of de-ionized water. The solution was continuously stirred and dried at 80 degC. The obtained mixture was placed in a quartz tube of a horizontal furnace. The pyrolysis of the mixture was performed in Ar atmosphere at a flow rate of 50 mL min-1. The furnace was heated to the target temperature at a rate of 10 degC min-1 and kept at the target temperature for 2 h, then cooled to room temperature. The target temperature was set as 600, 700, 800 and 900 degC, respectively. Then the samples were leached in 0.5 M HClO4 solution at 80 degC for 8 h to remove unstable iron species, and washed thoroughly with de-ionized water. Finally, the samples were dried at 60 degC in an oven. The samples are labeled as Fe@N-C-X, in which X represents pyrolysis temperature. For highlighting the role of iron precursor, ferric chloride was used instead of AFC, and the mixture of DCDA and ferric chloride was pyrolyzed at 700 degC, followed by acid-leaching and washing, which is denoted as D-FC-700. The weight ratio of DCDA to iron was kept identical in the DCDA-AFC and DCDA-FeCl3 mixtures.

1. A mixed metal oxidized hydroxide precursor material represented by the chemical formula: z(Ni d Coe Mn f(OH )2(d-e+f) - Co e'Mn r(OOH)e'+f - Mn f-O2f').2. The precursor material of claim 1, wherein z < 0.1; (1-z)(Ni a Co b Mn c(OH)2(a+b+c) - Co b - Mn c'(OOH)b - +c' - Mn c"O2c")' A < D, B > E, C > F. A=a, B=b+b', C=c+c'+c", A+B+C = 1 and 0 < A < 1, 0 < B < 1, 0 < C < 1;3. The precursor material of claim 1, wherein the precursor material comprises spherical and non-spherical particles having a surface and an interior. D=d, E=e+e', F=f+f+f" D+E+F=1 and 0 < D < 1, 0 < E < 1, 0 < F < 1; and4. The precursor material of claim 3, wherein the particles have a gradient structure wherein a molar ratio of Ni, in comparison to Co and Mn, is in the majority at the surface, and a composition with a metal molar ratio that varies from the surface towards the interior of the particles.5. The precursor material of claim 4, wherein the surface of the particles has a Ni:Co:Mn ratio of about 8:1:1.6. The precursor material of claim 3, wherein the particles are doped with at least one metal ion selected from the group consisting of Mg, Al, Zr, Ti, Ni, Co, and Mn.9. The precursor material of claim 1, wherein the precursor material has a tap density in the range from 0.8-2.8 g/cm3.16. A method of preparing a mixed metal oxidized hydroxide precursor material comprising the steps of: co-precipitating a solution comprising a plurality of metal salts, wherein the metals of the metal salts is selected from the group consisting of nickel, cobalt, manganese, and combinations thereof, with an alkaline hydroxide solution and ammonia to form a precipitate; filtering the precipitate; washing the precipitate; and drying the precipitate to form the mixed metal oxidized hydroxide precursor material, wherein the precursor material is represented by a chemical formula of: (1-z)(Ni a Co b Mn c(OH)2(a+b+c) - Co b'Mn c'(OOH)b'+c' - Mn c"O2c"). z(Ni d Co e Mn f(OH)2(d+e+f) - Co e Mn f(OOH)e'+f' Mn f-O2f").17. The method of claim 16, wherein 0. z < 0.1; A=a, B=b+b', C=c+c'+c", A+B+C = 1 and 0 < A < 1, 0 < B < 1, 0 < C < 1; D=d, E=e+e', F=f+f'+f" D+E+F=1 and 0 < D < 1, 0 < E < 1, 0 < F < 1; and A < D, B > E, C > F.18. The method of claim 16, wherein the method is conducted via precipitation in first and second sequential reactors.19. The method of claim 18, wherein at least 90%, but less than 100% of the metals precipitate in the first reactor.20. The method of claim 18, wherein at least one metal ion selected from the group consisting of Mg, Al, Zr, and Ti are added in the first sequential reactor and/or at least one metal ion selected from the group consisting of Mg, Al, Zr, Ti, Ni, Co, and Mn are added in the second sequential reactor to modify the composition of the precipitate.26. The method of claim 16, wherein the ammonia:metal molar ratio of the solution is in the range from about 0.1-3Ø27. The method of claim 26, wherein the ammonia:metal molar ratio of the solution is in the range from about 0 5-1.5.30. The method of claim 27, wherein the precursor material has an average particle size (D50) in the range from 7-13 microns.31. The method of claim 16, wherein the precursor material has a tap density in the range from 0.8-2.8 g/cm3.33. The method of claim 16, wherein the precursor material has a surface area in the range from 2-20 m2/g.Characterization of copper oxide supported on ceria-modified anatase
TiO2 support was prepared via hydrolysis of titanium alkoxides, the product was washed, dried and then calcined in flowing air at 500 degC for 5 h. The anatase crystalline form of the product was identified by XRD [27], and the BET surface area is 83 m2 g-1.
CeO2/TiO2 (ceria-modified TiO2) was prepared by impregnating TiO2 with an aqueous solution of cerious nitrate followed by drying at 100 degC overnight and then calcined in flowing air at 500 degC for 7 h.
CuO/CeO2/TiO2 samples were prepared by impregnating CeO2/TiO2 with an aqueous solution of cupric nitrate followed by drying at 100 degC overnight and then calcined in flowing air at 500 degC for 7 h. For the sake of simplicity, CuO/CeO2/TiO2 samples were noted as xCu-yCe-Ti, e.g. 4Cu-3Ce-Ti corresponds to the sample with copper oxide and ceria loading amount of 4 and 3 wt.% respectively. The results of BET surface area of ceria-modified TiO2 suggest that the change of support surface area could be neglected in this system, which is consistent to the results reported in the literature [7].Strength and microstructure of water treatment residue-based geopolymers containing heavy metals
Geopolymer was synthesized from calcined WTR with an initial SiO2:Al2O3 ratio of 1.78. Analytical-grade NaOH solution from Merck was reacted with calcined and non-calcined WTR at an Na2O:Si2O ratio of 0.25. The proportion of each mixture is shown in Table 2. The water used for each mixture was determined using ASTM Method C 187-68 [2]. NaOH was dissolved in the water mixture and then added to the solid mixture and mixed for 15 min to homogenize the sample. The mixtures were then transferred to cylindrical PVC molds (diameter 35 mm, height 70 mm), and vibrated for 2 min to remove entrapped air bubbles. The samples were demolded after 24 h, wrapped with cling film to prevent the loss of water and allowed to cure at an ambient temperature of 28 +- 2 degC. The development of strength and the characteristics of the microstructure were used to understand the geopolymerization reaction between the WTR and NaOH. The optimum calcining temperature of the WTR was determined according to the development of strength in the specimens and was used to prepare WTR for further study of metal immobilization. EPS was added to the binders (WTR and NaOH) at 0%, 30%, and 50% by weight.
1. A precursor of a positive electrode active material for a secondary battery comprising a single layer-structured secondary particle in which pillar-shaped primary particles radially oriented in a surface direction from the particle center are aggregated, wherein the secondary particle has a shell shape; and the primary particle includes a composite metal hydroxide of Ni-Co-Mn of the following Chemical Formula 1:          [Chemical Formula 1]     Ni1-(x+y+z)CoxMyMnz(OH)2
wherein, in Chemical Formula 1,
M includes any one, or two or more elements selected from the group consisting of Al, Zr, Mg, Zn, Y, Fe and Ti; and
x, y and z are each 0<x<1, 0≤y<1, 0<z<1 and 0<x+y+z<1.5. The precursor of a positive electrode active material for a secondary battery of Claim 1, which has an average particle diameter (D50) of 7 µm to 20 µm and a BET specific surface area of 5.0 m2/g to 30.0 m2/g.6. A method for preparing the precursor of a positive electrode active material for a secondary battery of Claim 1, the method comprising:
preparing a metal-containing solution by mixing a nickel raw material, a cobalt raw material and a manganese raw material; and
and introducing an ammonium cation-containing complex forming agent and a basic compound to the metal-containing solution and co-precipitation reacting the result under a pH of 10.50 to 12.00 and a temperature of 50°C to 70°C,
wherein the ammonium cation-containing complex forming agent is introduced at a rate of 0.5 times to 1.5 times with respect to an introduction rate of the metal-containing solution.8. A positive electrode active material for a secondary battery comprising a single layer-structured secondary particle in which pillar-shaped primary particles radially oriented in a surface direction from the particle center are aggregated, wherein the secondary particle has a shell shape; and the primary particle includes a lithium composite metal oxide of Ni-Co-Mn of the following Chemical Formula 2 and exhibits mono-modal-type particle distribution:          [Chemical Formula 2]     Liα[Ni1-(x+y+z)CoxMyMnz]O2
wherein, in Chemical Formula 2,
M includes any one, or two or more elements selected from the group consisting of Al, Zr, Mg, Zn, Y, Fe and Ti;
x, y and z are each 0<x<1, 0≤y<1, 0<z<1 and 0<x+y+z<1; and
a is 1.0≤a≤1.5.9. The positive electrode active material for a secondary battery of Claim 8, which has an average particle diameter of 7 µm to 15 µm and a BET specific surface area of 0.1 m2/g to 1.0 m2/g.Biomass derived hierarchical porous carbons as high-performance anodes for sodium-ion batteries
Hierarchical porous carbons were prepared by carbonization and activation of peanut skin with and without hydrothermal pretreatment. The employed peanut skins were peeled off from peanuts grown in the Shandong region of China. Route one: 2.0 g of peanut skin and 50 mL of diluted sulfuric acid were placed in a 100 mL stainless steel autoclave. The autoclave was heated at 180 degC for 24 h and then cooled down naturally. The resulting biochar was collected by filtration, washed with distilled water, and dried. The activation agent (KOH) and dried biochar, in a mass ratio of 1:1 or 2:1, were thoroughly ground in an agate mortar, and then the mixture was heated at 800 degC with a heating rate of 5 degC min-1 for 1 h under argon flow. After that, the activated samples were thoroughly washed with 2 M HCl and distilled water, and finally dried in an oven at 100 degC for 12 h. The obtained carbons were denoted as HPC-1-n, where n represents the KOH to biochar mass ratio. Route two: 1.0 g of peanut skin was impregnated in 50 mL KOH solution (2 M or 4 M) for 2 days. After that, the peanut skin-KOH mixture was collected by filtration and drying. Activation was also carried out in a tubular furnace at 800 degC (5 degC min-1) for 1 h under argon flow. The activated sample was also thoroughly washed with 2 M HCl and distilled water, and finally dried in an oven at 100 degC for 12 h. The resultant carbon was labeled as HPC-2-m, where m represents the concentration of KOH solution.Membranes of MnO Beading in Carbon Nanofibers as Flexible Anodes for High-Performance Lithium-Ion Batteries
Polyacrylonitrile (PAN, Mw = 80000) was made in laboratory52. Potassium permanganate (KMnO4, AR), sulfuric acid (H2SO4, AR) and N,N-dimethylformamide (DMF, AR) were purchased from Shanghai Lingfeng Chemical Reagent Co., Ltd. Manganese sulfate monohydrate (MnSO4*H2O, AR) was purchased from Sinopharm Chemical Reagent Co., Ltd. All these reagents were used without further purification.
MnO2 NWs were synthesized by a hydrothermal method. Briefly, aqueous solutions of MnSO4*H2O (1 mmol) and KMnO4, (1.5 mmol) (Mn(II)/Mn(VII) = 3:2) were mixed with vigorously stirring. The PH value of the mixture was adjusted ~2 with 5 M H2SO4 aqueous solution. Then the solution was transferred to an autoclave and reacted in oven at 140 degC for 12 h. After cooling down, the product was collected by filtration and washed repeated with distilled water and absolute ethanol. Then the MnO2 NWs powder was obtained.
The obtained MnO2 NWs powder was washed three times with DMF, centrifuged and then added into a certain-mass 8 wt% PAN/DMF solution as an electrospinning solution. Intensive stirring was conducted for 12 h in order to get a homogeneously distributed solution. Then the blended solution was electrospun into MnP membranes, with a controlled syringe pump of 25 μL min-1 and an applied voltage of 20 kV with a distance of 20 cm between the electrospinning jet and the collector. The adding amounts of MnO2 NWs based on PAN were 10 wt%, 20 wt% and 30 wt% for MnP-1, MnP-2 and MnP-3, respectively.Solution-processed indium oxide electron transporting layers for high-performance and photo-stable perovskite and organic solar cells
Indium nitrate hydrate (In(NO3)3*xH2O, 99.99%), indium acetylacetonate (In(AcAc)3, 99.99%), indium chloride (InCl3, 99.999%), P3HT (regioregular, 99%), PCBM (>99%), bis(trifluoromethane)sulfonamide lithium salt (Li-TFSI, 99.95%), anhydrous acetonitrile (ACN, 99.8%), anhydrous ethanol (>99.5%), anhydrous dimethylsulfoxide (DMSO, 99.9%), anhydrous dimethylformamide (DMF, 99.8%), anhydrous diethyl ether (99.9%), anhydrous toluene (99.8%), 4-tert-butylpyridine (tBP, 96%), anhydrous chlorobenzene (CB, 99.8%), and Triton X-100 were purchased from Sigma-Aldrich. PEDOT:PSS (AI 4083) was purchased from Clevios. PbI2 (99.9985%) was purchased from Alfa Aesar. Methylammonium iodide (MAI) (99.98%) was purchased from Dyesol. Poly(triaryl amine) (PTAA, Mn = 17500 g mol-1) was purchased from EM Index. The indium precursor solution was prepared by dissolving 0.1 M indium nitrate hydrate in anhydrous ethanol. The CH3NH3PbI3 precursor solution was prepared via the Lewis base adduct approach according to a previous report; 461 mg of PbI2, 159 mg of MAI, and 78 mg of DMSO were mixed with 600 mg of DMF.36,37 The P3HT/PCBM solution was prepared by dissolving 1 wt% P3HT and 1 wt% PCBM in 1 mL of CB at 60 degC for 30 min. A PTAA solution was prepared by dissolving 15 mg of PTAA in 1 mL of toluene. For Li-doping, 15 μL of Li-TFSi/ACN (170 mg of Li-TFSi in 1 mL of ACN) and 15 μL of tBP/ACN (1:1, v/v) were added to the PTAA solution. The PEDOT:PSS solution was modified by mixing 1 wt% Triton X-100. All the solutions were prepared in a N2-filled glove box.Synthesis and characterization of acrylic rubber/silica hybrid composites prepared by sol-gel technique
Acrylic rubber (ACM, Nipol AR51, density at 25degC = 1100 kg/cm3, Mooney viscosity, ML1+4 at 100degC = 51) was obtained from Nippon Zeon Co. Ltd. (Tokyo, Japan). It was reported to have epoxy cure site and was made from ethyl acrylate monomer. Tetraethoxysilane (TEOS, density = 930 kg/m3) was procured from Acros Organics (USA). Tetrahydrofuran (THF, 99% pure) was purchased from Merck (India). The precipitated silica (Ultrasil VN3, particle size range = 40-100 nm, oil absorption = 2.4 g/kg, pH = 6) was supplied by Bayer AG (Germany). Benzoyl peroxide (BPO, 97% purity) was purchased from Aldrich Chemicals (USA). Hexamethylenediamine carbamate (HMDC, DIAK#1) was supplied by Nicco Corp. Ltd. (India). Ammonium benzoate (AmBz) was prepared in the laboratory by reacting ammonium hydroxide and benzoic acid in 1:1 molar ratio in a water bath at around 60degC for 30 min. The salt formation was confirmed by Fourier transform infrared spectroscopy as well as by studying its solubility. Deionized water and concentrated hydrochloric acid of laboratory grade were obtained from indigenous sources.
The desired amount of ACM was dissolved in THF solvent. A proportion of the rubber to solvent was maintained at about 1 : 10 all throughout the experiment to retain the uniform viscosity of the reaction medium. TEOS, deionized water, and concentrated HCl as catalyst, in the molar ratio of 1 : 2 : 0.06,15 were throughly mixed by vigorous stirring for 15 min and then the mixture was added to the rubber solution under stirring conditions at ambient temperature. The proportion of TEOS was from 0 to 50 wt % of the ACM. Beyond 50 wt % TEOS, macrophase separation occurred. The formulations used in this study are given in Table I. The precursor solution for preparing the composites was stirred for 30 min and then poured over a uniform and thoroughly cleaned glass plate. The initial evaporation of the solvent was carried out under controlled conditions for 24 h, and then, in the next phase, further evaporation for 4 days was allowed to remove the residual solvent and byproducts (water and ethanol). The optimum gelling time was taken when practically no weight variation of the hybrid composites was noticed. All the films were transparent in appearance. For comparison, composites were also prepared with precipitated silica up to 30 wt % of its loading (beyond which the resultant film lost its homogeneity). The precipitated silica was initially dried at 120degC for 24 h and then dispersed in the ACM solution. All the composite films with precipitated silica were opaque and completely white in color. To crosslink the rubber phase, curatives were added to the solvated rubber. The two different curatives systems used in this study were BPO and a mixed crosslinked system comprising AmBz and HMDC, following an earlier work on acrylic rubber in our laboratory.16 Optimization of the doses of different curatives (Table I) was carried out by studying the maximum gel content in THF.
Addition of curatives was made only after complete mixing of TEOS, water, and HCl with solvated ACM for 30 min. The crosslinkers were dispersed under ambient conditions and then stirred for another 30 min for homogenous mixing. The films were cast over a plain glass plate as before and kept for controlled solvent evaporation for 24 h. In the next phase, peroxide-containing composite films were kept in the oven at 70degC for 2 h for curing, whereas the samples having mixed crosslinked system were cured at 170degC for 30 min. The above cure times were optimized from the maximum gel content values.Efficient activation of peroxymonosulfate by manganese oxide for the degradation of azo dye at ambient condition

All chemicals were of analytical purity, which were obtained from Sinopharm Chemical Reagent Co., Ltd., China, and used without further purification. The MnOx catalysts were prepared using a simple coprecipitation method. In a typical procedure, Mn(NO3)2 (50%, v/v) solution was diluted with deionized water to 2.5%, and then the pH of the solution was adjusted to 9, 9.5, 10, 11, and 13 by dropwise adding an aqueous solution NaOH (2 mol/L) under magnetic stirring (200 rpm), respectively. The solution was kept stirring for half an hour. Subsequent precipitates were collected by centrifugation and washed with deionized water for three times, and were dried at 70 degC for 6 h. The as-obtained samples were noted as M-9, M-9.5, M-10, M-11 and M-13, respectively. The Mn-10 sample was chosen to be further calcined at 400, 600, 800 and 1000 degC with a heat rate of 10 degC /min followed by slow cooling to room temperature because precipitate was rarely generated when the pH of the solution was below 10. The as-obtained samples were named hereinafter as M-10-400, M-10-600, M-10-800 and M-10-1000.A highly efficient, green, rapid, and chemoselective oxidation of sulfides using hydrogen peroxide and boric acid as the catalyst under solvent-free conditions
General procedure for the oxidation of sulfides to sulfoxides. The sulfide (1 mmol) was added to a solution of 30% H2O2 (1.2 equiv, 0.5 g) and boric acid (0.1 mmol, 0.006 g), and the mixture was stirred at room temperature for the time specified in Table 2. The progress was monitored by TLC or GC. After completion of the reaction, the product was extracted with CH2Cl2 (3 x 10 mL) and the combined organics was washed with brine (15 mL) and dried over anhydrous Na2SO4. The solvent was removed under reduced pressure to give the corresponding pure sulfoxide in most cases. Further purification was achieved by short-column chromatography on silica gel with EtOAc/n-hexane (1/10) as eluent. All the products are known and were characterized by IR, 1H NMR, and by melting point comparisons with those of authentic samples. 8, 23, 24, 25 and 26
General procedure for the oxidation of sulfides to sulfones. The sulfide (1 mmol) was added to a solution of 30% H2O2 (3.6-4.8 equiv) and boric acid (0.2-0.3 mmol), and the mixture was stirred at room temperature for the time specified in Table 2. The progress was monitored by TLC or GC. After completion of the reaction, the product was extracted with CH2Cl2 (3 x 10 mL) and the combined organics was washed with brine (15 mL) and dried over anhydrous Na2SO4. The solvent was removed under reduced pressure to give the corresponding pure sulfone in most cases. Further purification was achieved by recrystallization from EtOH. All the products are known and were characterized by IR, 1H NMR, and by melting point comparisons with those of authentic samples. 7, 8 and 251. A precursor of a transition metal oxide represented by the following chemical formula 1:
[ chemical formula 1]NiaMnbCo1-(a+b+c+d)ZrcMd[OH(1-x)2-y]A(y/n)
Wherein M is at least one of W and Nb, A is one or more anions other than OH, 0.3. ltoreq. a.ltoreq.0.9, 0.05. ltoreq. b.ltoreq.0.5, 0. ltoreq. c.ltoreq.0.05, 0. ltoreq. d.ltoreq.0.05, a + b + c + d. ltoreq.1, 0. ltoreq. x.ltoreq.0.5, 0. ltoreq. y.ltoreq.0.05, and n is the oxidation number of A.3. The precursor of a transition metal oxide according to claim 1, wherein said a is at least one selected from the group consisting of: PO (PO)4、CO3、BO3And F.4. The transition metal oxide precursor of claim 1, wherein said a comprises PO4And F.5. The transition metal oxide precursor of claim 1, wherein the transition metal oxide precursor has a tap density of from 1.0g/cc to 2.5 g/cc.6. A composite of lithium and a transition metal oxide, which comprises a product obtained by calcining a precursor of the transition metal oxide according to any one of claims 1 to 5 and a lithium compound.7. The complex of lithium and transition metal oxide of claim 6, wherein the lithium compound is at least one of: lithium hydroxide, lithium carbonate and lithium oxide.8. The complex of lithium and a transition metal oxide according to claim 6, wherein the lithium compound is 0.95 to 1.2mol with respect to 1mol of the precursor of the transition metal oxide.9. The composite of lithium and transition metal oxide according to claim 6, wherein the calcination is performed at 600 ℃ to 1000 ℃.10. A positive electrode comprising the complex of lithium and transition metal oxide according to claim 6.11. A secondary battery comprising the positive electrode according to claim 10.Extremely low thermal conductivity and high thermoelectric performance in liquid-like Cu2Se1-xSx polymorphic materials

Polycrystalline Cu2Se1-xSx (x = 0.2, 0.3, 0.5, and 0.7) samples were synthesized by a combination of melting and long-term high-temperature annealing. High purity raw elements, Cu (shot, 99.999%, Alfa Aesar), Se (shot, 99.999%, Alfa Aesar), and S (shots, 99.9999%, Alfa Aesar) were combined in their stoichiometric ratios and placed in boron nitride crucibles, which were then sealed in fused silica tubes under vacuum. The temperature of the tubes was slowly (100 K h-1) raised to 1423 K and maintained at that temperature for 12 h, and then cooled down to 1073 K in 24 h. After annealing at 1073 K for 8 days, the tubes were furnace cooled to room temperature. Small single crystals were extracted from the polycrystalline ingot sample after the annealing process. Finally, the annealed ingots were crushed into powders and consolidated by spark plasma sintering (Sumitomo SPS-2040) at 873 K under a pressure of 65 MPa for 5 minutes. Electrically insulating and thermally conducting BN layers were sprayed onto the carbon foils and the inner sides of the graphite die before the SPS process in order to prohibit DC pulsed currents going through the powders.
A simple, one-step hydrothermal approach to durable and robust superparamagnetic, superhydrophobic and electromagnetic wave-absorbing wood
All chemicals were supplied by Shanghai Boyle Chemical Co. Ltd (Shanghai. China). and used without further purification. The wood slices were cut to dimensions of 10 mm (length) x 10 mm (width) x 10 mm (height), and then the slices were ultrasonically rinsed with deionized water for 30 minutes and dried at 80 degC in a vacuum.
In a typical synthesis, FeCl3[?]6H2O (3.24 g) and MnCl2[?]4H2O (0.94 g) in a stoichiometric ratio of 2:1 were dissolved in 80 mL of a deionized water solution with the wood samples under magnetic stirring for 2 h at room temperature. During the stirring process, FAS-17 (1 mL) and the proper amount of ammonia (3 mL) were added dropwise into the solution. Then, the obtained homogeneous mixture was transferred into a 100 mL Teflon-lined stainless autoclave, and this vessel was sealed and heated to 120 degC for 8 hours. Subsequently, the autoclave was left to cool to room temperature. Finally, the prepared magnetic wood samples were removed from the solution, ultrasonically rinsed with deionized water for 30 minutes, and dried at 45 degC for over 24 hours in vacuum.Hollow zeolite encapsulated Ni-Pt bimetals for sintering and coking resistant dry reforming of methane
Silicalite-1 (S-1) was synthesized by the clear solution method. Typically, 15.4 ml of tetraethyl orthosilicate (TEOS) was mixed with a certain amount of tetrapropylammonium hydroxide (TPAOH) solution. The molar composition of the synthesis mixture was 1 TEOS:0.27 TPAOH:37H2O. After being stirred for 3 h at 35 degC, the solution was heated at 80 degC to remove the ethanol generated during the hydrolysis of TEOS and then water was added to maintain constant volume. After crystallization at 170 degC for 3 days, the product was recovered by centrifugation and dried overnight at 100 degC. Finally, the template was removed by calcination in static air at 540 degC for 6 h.
2.2 Preparation of Ni@Hol S-1, Pt@Hol S-1 and Ni-Pt@Hol S-1
NiO/S-1 was synthesized by an incipient-wetness impregnation method. In brief, the calcined silicalite-1 was impregnated with an aqueous solution of NiCl2; after drying overnight at 100 degC, the product was calcined in static air at 400 degC for 2 h. The ideal Ni loadings on NiO/S-1 were 1.5, 3 and 5 wt%, respectively. The resulting samples are denoted as 1.5NiO/S-1, 3NiO/S-1 and 5NiO/S-1, respectively.The transterminator ion flow at Venus at solar minimum

Abstract
The transterminator ion flow in the Venusian ionosphere is observed at solar minimum for the first time.
Such a flow, which transports ions from the day to the nightside, has been observed previously around solar maximum.
At solar minimum this transport process is severely inhibited by the lower altitude of the ionopause.
The observations presented were those made of the Venusian ionospheric plasma by the ASPERA-4 experiment onboard the Venus Express spacecraft, and which constitute the first extensive in-situ measurements of the plasma near solar minimum.
Observations near the terminator of the energies of ions of ionospheric origin showed asymmetry between the noon and midnight sectors, which indicated an antisunward ion flow with a velocity of (2.5±1.5)kms-1.
It is suggested that this ion flow contributes to maintaining the nightside ionosphere near the terminator region at solar minimum.
The interpretation of the result was reinforced by observed asymmetries in the ion number counts.
The observed dawn-dusk asymmetry was consistent with a nightward transport of ions while the noon-midnight observations indicated that the flow was highly variable but could contribute to the maintenance of the nightside ionosphere.
Highlights
► The transterminator ion flow in the Venusian ionosphere is observed at solar minimum.
► This flow has a velocity of (2.5±1.5)kms-1.
► The occurrence of this flow is highly variable, but can be a significant source of the nightside ionosphere.

Introduction
The nightside ionosphere of Venus has a dynamic and complex structure (Brace et al., 1979).
To date the most extensive set of in situ observations of the ionospheric plasma were obtained by Pioneer Venus Orbiter (PVO).
Although the PVO mission covered an entire solar cycle the ionospheric measurements were largely restricted to a limited period close to solar maximum between 1978 and 1980 when the PVO periapsis was at a sufficiently low altitude to allow sampling of the ionosphere.
The solar flux during this period was about 200 solar flux units (sfu).
These PVO observations covered all local time sectors.
In the nightside ionosphere they showed that precipitating electrons could contribute only ∼25% of the plasma densities observed and that changes in ionospheric densities were much more variable than, and not correlated with, changes in the flux of precipitating electrons (Spenner et al., 1981).
Observations of the flux of atomic oxygen ions across the terminator from the day to nightside showed that this ion flux was sufficient to explain the observed ion densities in the nightside ionosphere at solar maximum (Knudsen et al., 1980).
The ions were assumed to follow ballistic trajectories and theoretical calculations predicted that 80% of the ions that crossed the terminator had recombined with electrons before they reached a solar zenith angle (SZA) of 110°.
Only those ions that crossed the terminator at the highest altitudes reached the central region of the nightside ionosphere.
A modelling study by Cravens et al. (1983) predicted that ions which crossed the terminator at altitudes below 500km recombined before reaching a SZA of 120°, whilst ions that crossed the terminator at 876km influenced the entire night sector.
Taken collectively these results showed that the primary source of the nightside ionosphere was plasma transport from the dayside.
The plasma flow from the subsolar region toward the nightside is primarily driven by the day-to-night pressure gradient (Knudsen et al., 1981).
Knudsen et al. (1982) showed that the flow speed across the terminator was highly variable but was typically several kilometres per second.
The average value of the antisunward component of the velocity in the terminator region at solar maximum increased with altitude from a few hundred metres per second at an altitude of 150km to ∼4kms-1 at 800km (Knudsen and Miller, 1992).
The altitude of the ionopause in the terminator region played an important role in the total number of ions transported from the day to the nightside.
Its altitude in this region was variable (Elphic et al., 1980) but was typically around 1000km (Brace et al., 1983).
This variability was attributed to changes in the solar EUV flux and the solar wind dynamic pressure, the balance of which altered the ionopause altitude (Knudsen and Miller, 1992).
As the ionopause moved to lower altitudes the total number of ions transported antisunward was reduced (Knudsen et al., 1981).
Theoretical calculations by Brace et al. (1995) showed that the transterminator flow could transport more ions antisunward than were required to maintain the nightside ionosphere and it was suggested that some of these ions might be lost to the solar wind.
Limited in situ ionospheric observations aboard PVO were made in the pre-dawn sector at low latitudes in 1992 in the declining phase of solar cycle 22 under conditions of moderate solar flux (∼120sfu).
The observed ion densities in this sector were significantly larger than those that would be expected in the absence of an antisunward ion flow.
This suggested that ion transport was significant in this sector (Brannon et al., 1993).
The PVO observations showed that the total transterminator flux was 23% of that at solar maximum and that the largest reductions in the number of ions transported antisunward occurred at the highest altitudes (Spenner et al., 1995).
The PVO mission did not include in situ observations of the Venusian ionosphere around solar minimum, however the behaviour of the ionopause was inferred from PVO radio occultation profiles, for which the temporal data coverage was less extensive than for the in situ measurements.
The ionopause was at significantly lower altitudes at solar minimum than at solar maximum, typically between 200km and 300km for all SZA (Kliore and Luhmann, 1991).
The radio occultation profiles from PVO also showed that the transport process was severely inhibited (Knudsen et al., 1987).
Radio occultation profiles from Venera 9 and 10 observed the ionopause at higher altitudes in the terminator region at solar minimum with altitudes between 600km and 800km (Gavrik and Samoznaev, 1987).
Between August 2008 and October 2009 Venus Express (VEX) was in an orbit with periapsis near 86°N and an altitude between 185km and 215km with about 10min spent in the ionosphere during each orbit.
Taken collectively over many orbits the in situ ionospheric measurements cover all local time sectors, with each orbit sampling the terminator region at polar latitudes.
In the current study these observations are used to determine the plasma distribution near the terminator and to show that the transport process contributes to the maintenance of the nightside ionosphere close to solar minimum.
Instrumentation
Venus EXpress (VEX) is the first European mission to Venus (Titov et al., 2006).
The VEX spacecraft was inserted into a near polar orbit in April 2006 and so every orbit sampled the terminator region at polar latitudes.
The Analyser of Space Plasmas and Energetic Atoms (ASPERA-4) package on VEX contains an ELectron Spectrometer (ELS), an Ion Mass Analyzer (IMA), a Neutral Particle Detector (NPD) and a Neutral Particle Imager (NPI) (Barabash et al., 2007).
In August 2008 periapsis was lowered from an altitude of around 300km to 185km, allowing the spacecraft to sample deeper into the ionosphere.
Observations made using the IMA sensor once this manoeuvre had occurred are of particular interest to the present study.
This instrument observes the ion energy per charge, E/q, the mass per charge, m/q, and the arrival direction of each ion as well as the number of ions observed.
It has a 360° instantaneous field of view in azimuth and ±45° field of view in elevation in the spacecraft frame of reference and an energy range of 10eV-30keV.
The standard observing mode used during the period considered in this study was a scan in decreasing energy through 96 equal logarithmic steps, observing for 250ms at each.
These measurements were made at all azimuths simultaneously at a given elevation.
The elevation angle was varied through eight positions, which gave a total cycle time of 192s.
Observations
Data subsequent to the lowering of the periapsis of VEX were considered for the study.
One Venus year of data were selected between 4th August 2008 and 17th March 2009 allowing the spacecraft to sample all local time sectors twice as it transited these sectors at high latitudes in opposite directions half a Venusian year apart.
Periapsis was at 86°N during this interval.
The ion counts as a function of energy observed by the IMA during a spacecraft transit between 04:30 UT and 06:30 UT on 9th August 2008 are shown on a logarithmic scale in the upper panel of Fig. 1.
The ion counts as a function of mass channel number are shown in the lower panel of Fig. 1 with lower channel numbers corresponding to higher mass ions (Barabash et al., 2007).
These data from 9th August 2008 are considered as an example to show how data from the entire year were selected and processed.
The data in the lower panel show two clear ion populations; one with a higher ion mass per unit charge (lower mass channel number) observed between 05:28 UT and 05:47 UT and one with a lower ion mass per unit charge (higher mass channel number) observed before and after this time interval.
Prior to 04:46 UT and after 06:03 UT the IMA observed ions with energies of some 300-800eV (Fig. 1, upper panel) with a low mass to charge ratio (high channel number in Fig. 1, lower panel) indicating that the spacecraft was in the solar wind.
In the intervals from 04:46 UT to 05:28 UT and 05:47 UT to 06:03 UT the IMA sensor observed ions over a larger range of energies than observed in the solar wind, from some 200eV to 1keV with mass to charge ratios similar to that observed in the solar wind.
These data suggested that the spacecraft was in the shocked solar wind, downstream of the bow shock.
The observations closest to periapsis, between 05:28 UT and 05:47 UT, showed ions at energies below some 50eV with higher masses than those observed in the solar wind.
These low energy ions were interpreted as being of planetary origin.
Inspection of the datasets from a large number of orbits showed that it was convenient to locate the Ion Composition Boundary (ICB), which marks the transition between the shocked solar wind and the planetary plasma (e.g. Martinecz et al., 2008), by considering the mass channel number at which the largest number of ions was observed in each 192s cycle.
Data from times at which the mass channel number of the maximum ion count was 15 or less were taken to correspond to altitudes below the ICB.
These data were then considered for further analysis.
For example, in the data set for 9th August 2008 shown in Fig. 1, the data between 05:28 UT and 05:47 UT were interpreted as being from inside the ICB.
These data are shown within the pink box in Fig. 1, and it was these data that were considered for further analysis in this particular example.
The spacecraft velocity at periapsis (∼200km) was ∼10kms-1, which was larger than the ion velocities of ∼3kms-1 observed by PVO at these altitudes (Knudsen and Miller, 1992).
To ensure that the ions were detected, observations were only considered if the spacecraft ram direction was within the field-of-view of the IMA.
This selection criterion meant that observations were only considered when the spacecraft attitude was suitable for observing the ions.
The IMA observed in the ram direction for all, or part, of the time when VEX was within the ionosphere on 136 orbits, and data from this sub-set of orbits (136 orbits out of 226 orbits) was considered for further analysis.
In this subset of 136 orbits, ions were observed at eight elevation angles during each cycle of 192s duration.
For each cycle of each orbit the ion count at the elevation angle with the maximum ion count was found and considered further.
Using the counts from this elevation the next step was to obtain the "summed ion count" for the cycle, defined as the total ion count summed over all energy levels below 100eV.
Thus a value of the summed ion count was determined for each cycle.
The duration of each complete cycle was 192s, however, the summed ion count corresponded to observations from only one of the eight elevations and only a proportion of the 96 energy levels, with the actual observations at one elevation angle and at energies below 100eV being conducted in 6.5s.
During this time interval the spacecraft moved some 65km (6.5s times the satellite velocity of ∼10kms-1).
Thus the summed ion count was observed over a horizontal distance of some 65km which is approximately 0.01 Rv where Rv is the radius of Venus (6052km).
The summed ion counts for all cycles are plotted in Venus Solar Orbital (VSO) coordinates in Fig. 2.
The positive x-axis is directed towards the Sun.
The positive y-axis is orthogonally directed and opposite to the planetary orbital velocity i.e. towards dawn, which is opposite to the Earth due to the retrograde rotation of Venus.
The largest summed ion counts were in the polar region close to periapsis where the spacecraft sampled the lowest altitudes.
In this region the spacecraft was in the topside ionosphere, where the ion density decreases with increasing altitude.
Data in Fig. 2 exhibit asymmetries in both the dawn-dusk and noon-midnight directions.
To investigate the dawn-dusk asymmetry data were selected from a narrow region aligned with the dawn-dusk axis.
This region was centred on the terminator and had a width of 0.4 Rv (the x coordinate was restricted to |x|<0.2 Rv).
The observations in this region were then binned into intervals of 0.1 Rv in the dawn-dusk direction (y-direction) near the y=0 axis.
The small number of points further from this axis required larger bins and an interval of 0.2 Rv was considered between |y|=0.3 Rv and 0.5 Rv and an interval of 0.25 Rv between |y|=0.5 Rv and 0.75 Rv.
The median and quartile values of the ion counts in each bin are plotted in the upper panel of Fig. 3.
A strong dawn-dusk asymmetry was observed, with the median counts larger on the dusk side than on the dawn side by almost an order of magnitude with median values of ∼6×105 on the dusk side and ∼5×104 around dawn.
A similar plot for a noon-midnight narrow region is shown in the lower panel of Fig. 3 with a restriction that |y|<0.2 Rv.
The observations were binned into intervals of 0.1 Rv between |x|=0.0 Rv and 0.3 Rv, 0.2 Rv between |x|=0.3 Rv and 0.5 Rv, 0.5 Rv between |x|=0.5 Rv and 1.0 Rv, and 1.0 Rv for -0.2 Rv<x<-1.0 Rv.
This ensured sufficient numbers of points in each bin.
A noon-midnight asymmetry is apparent, with larger median summed ion counts ∼3×105 in the noon sector.
Variability is observed on the dayside where the counts are expected to decrease away from the terminator as the spacecraft moves to higher altitudes and to increase because of a decreasing solar zenith angle.
The ion counts decrease rapidly on the midnight side to values of ∼5×104.
However, the upper quartile showed that significant numbers of ions could be present nightward of the terminator (located at x=0) and that these values could be comparable to those on the dayside ionosphere with values as large as ∼8×105 recorded in both the day and night sectors.
The summed ion counts considered in the preceding paragraphs were for energies less than 100eV.
The energy level within this range at which the largest number of ions occurred during each cycle of 192s was determined.
For each cycle, the energy of this level was then corrected for the spacecraft potential using the method of Coates et al. (2008) based on the analysis of the ionospheric photoelectron peaks, and the corrected value considered as the energy representative of the ions at the location of the spacecraft.
To investigate ion flow in the terminator region an additional constraint was imposed to restrict observations to within ∼30° latitude of the pole.
Periapsis was close to 86°N throughout the study period of one Venus year, and the restriction was done by considering only observations at an altitude of 500km or lower.
The resulting data were then divided into four bins depending upon the direction of travel of the spacecraft;•
Spacecraft travelling essentially from noon-to-midnight (within 45° of this direction);
•
Spacecraft travelling essentially from midnight-to-noon (within 45° of this direction);
•
Spacecraft travelling essentially from dawn-to-dusk (within 45° of this direction);
•
Spacecraft travelling essentially from dusk-to-dawn (within 45° of this direction).
The spacecraft velocity at periapsis was essentially constant for all observations, with a mean value of (9.78±0.01)kms-1.
For each bin the median value of the observed energy was determined.
This was (11±3)eV for the noon-to-midnight bin and (20±4)eV for the midnight-to-noon bin, with the uncertainties set by the upper and lower quartiles.
The larger ion energies in the midnight-to-noon bin suggested that these ions had a velocity component that was antiparallel to the spacecraft direction of travel and the smaller values in the noon-to-midnight bin suggested that these ions had a velocity component that was parallel to the spacecraft direction of travel.
Taken together both of these observations suggest that the ions travelled in the noon-to-midnight direction.
For both the dawn-to-dusk and dusk-to-dawn bins the energies were (18±4)eV.
The difference in the ion energies of these bins was zero within the error margin, which suggested that there was no net ion flow in this direction.
Discussion
Results have been presented of ion counts and energies measured by the ASPERA-4 experiment onboard the VEX spacecraft as it traversed the Venusian ionosphere at polar latitudes.
Strict selection criteria were applied to the data to ensure that the measurements used in the study were of ionospheric ions.
Median ion energy values near the midnight-noon meridian were larger when the spacecraft traversed from midnight-to-noon than from noon-to-midnight.
The larger values of the former case suggested that the ions had a velocity component that was antiparallel to the spacecraft direction of travel, while the smaller values of the noon-to-midnight traversal suggested that the ions had a velocity component parallel to the spacecraft direction of travel.
This suggested the nightward transport of the ions at polar latitudes.
Median values near the dawn-dusk meridian were identical for traversal from dawn-to-dusk and from dusk-to-dawn within the error margins suggesting that there was no net ion flow in this direction.
Taken collectively the observed ion energies therefore indicated an ion flow predominantly in the noon-to-midnight direction.
The spacecraft velocity near periapsis was essentially the same for all orbits and all directions of travel and so the difference in the ion energy between the midnight-to-noon and noon-to-midnight traversals, (9±7)eV, may be attributed to the flow of ions.
By using the same assumption as Knudsen and Miller (1992) that the ions were primarily singly ionised oxygen, and that the measured energy difference was representative kinetic energy of the ions a nightward ion velocity of (2.5±1.5)kms-1 is estimated.
It is appreciated that there are substantial uncertainties in this velocity and that the IMA was operating close to the lowest energies it could observe, however it is encouraging that this velocity is in broad agreement with Knudsen and Miller (1992) who reported antisunward ion flows of some ∼3kms-1 at these altitudes.
A dawn-dusk asymmetry in the plasma distribution of the Venusian ionosphere has been reported by Miller and Knudsen (1987) with larger plasma densities observed in the dusk sector.
Their study was conducted at low- and mid-latitudes around solar maximum, and the observation associated with the asymmetry of plasma transport where higher density plasma was drawn antisunward (nightward) from the post-noon sector as a transterminator flow.
The dawn-dusk ion asymmetry in the current study (Fig. 3, upper panel) was consistent with their interpretation.
The observed ion counts in the noon-midnight plane (Fig. 3, lower panel) suggested that the transterminator flow was highly variable.
The median values of the three points immediately sunward of the terminator showed the largest values.
The median values fell rapidly nightward of the terminator, as expected in the absence of a plasma source.
The lower median value of ∼8×104 on the dayside at 0.4 Rv was a likely consequence of the spacecraft sampling at higher altitudes where the ion densities were expected to be lower.
Indeed, sunward of 0.5 Rv no data points were recorded.
This may be explained by the altitude of the ionopause falling to 200km-300km on the dayside (Kliore and Luhmann, 1991) and the spacecraft sampling above these altitudes when it was located ∼0.3 Rv sunward of the terminator.
The upper quartile values varied substantially between adjacent bins.
Upper quartile values in the nightside at a distance of less than 0.5 Rv from the terminator were similar to, or greater than, the median values on the dayside.
This suggested that in a substantial number of cases the ion counts nightward of the terminator were comparable to the values in the dayside ionosphere, although in general the ion counts nightward of the terminator were lower than those observed on the dayside as expected in the absence of a plasma source.
This indicated that, at times, a process was operating to maintain the nightside ionosphere although the occurrence of this process was highly variable.
In summary the observations of ion energies indicated that a nightward ion flow across the terminator at solar minimum can occur.
The ion counts show that such a flow is highly variable but the results indicate that it can contribute to the maintenance of the nightside ionosphere.
Conclusions
In situ ion observations made by the ASPREA-4 experiment onboard the Venus Express spacecraft at solar minimum have shown dawn-dusk and noon-midnight asymmetries.
Ion energies observed when the spacecraft trajectory was directed midnight-to-noon were significantly higher than those observed when the trajectory was directed noon-to-midnight.
This difference in ion energies suggested an antisunward transterminator flow with a velocity of (2.5±1.5)kms-1.
It is suggested that this flow contributes to maintaining the nightside ionosphere near the terminator region at solar minimum.
The interpretation of the antisunward flow was reinforced by observed asymmetries in the ion number counts.
The dawn-dusk ion asymmetry showed larger numbers of ions on the dusk side than on the dawn side consistent with the previously reported observations of antisunward transterminator flow at solar maximum from PVO.
For the noon-midnight asymmetry larger numbers of ions occurred on the dayside and there was substantial variability in the observations of counts on the nightside.
In a substantial number of cases the number of ions nightward of the terminator was comparable to the number observed on the dayside.
In other cases the number of ions nightward of the terminator was much lower, as expected in the absence of a plasma source.
These observations suggested that the transterminator flow was highly variable and, in some cases, did not operate at all.
Acknowledgements
The authors would like to thank the ASPERA-4 team for their extensive work planning, constructing and operating these instruments on the Venus Express spacecraft, and the subsequent dissemination of these data.
The assistance of Neville Shane from Mullard Space Science Laboratory, University College London in implementing software tools at Aberystwyth University is gratefully acknowledged.
Financial support for this paper was provided by the UK Science and Technology Facilities Council under grant PP/E001157/1.

Characteristics and electrochemical performance of LiFe0.5Mn0.5PO4/C used as cathode for aqueous rechargeable lithium battery

LiFe0.5Mn0.5PO4/C was synthesized by a sol-gel and calcinations process. CH3COOLi*2H2O, FeCl2*4H2O, MnCl2*4H2O, P2O5 and critic acid with a molar ratio of 1:0.5:0.5:1:1 were dissolved in 40 mL ethanol solution. The mixture solution was rigorous stirred for 12 h in nitrogen gas, and then heated at 353 K to get xerogel. Finally, the xerogel was fired at 773 K for 5 h in the purified argon gas flowing to prevent the oxidation of Fe2+ to obtain LiFe0.5Mn0.5PO4/C. LiV3O8 was synthesized as describe as previous publication [5].


An assessment of a three-beam Doppler lidar wind profiling method for use in urban areas

Abstract
Currently there are few observations of the urban wind field at heights other than rooftop level.
Remote sensing instruments such as Doppler lidars provide wind speed data at many heights, which would be useful in determining wind loadings of tall buildings, and predicting local air quality.
Studies comparing remote sensing with traditional anemometers carried out in flat, homogeneous terrain often use scan patterns which take several minutes.
In an urban context the flow changes quickly in space and time, so faster scans are required to ensure little change in the flow over the scan period.
We compare 3993h of wind speed data collected using a three-beam Doppler lidar wind profiling method with data from a sonic anemometer (190m).
Both instruments are located in central London, UK; a highly built-up area.
Based on wind profile measurements every 2min, the uncertainty in the hourly mean wind speed due to the sampling frequency is 0.05-0.11ms-1.
The lidar tended to overestimate the wind speed by ≈0.5ms-1 for wind speeds below 20ms-1.
Accuracy may be improved by increasing the scanning frequency of the lidar.
This method is considered suitable for use in urban areas.
Graphical abstract
Highlights
•
There is a need for more wind speed data collected above rooftop level in cities.
•
Remote sensing instruments can measure throughout the boundary layer.
•
Rapid evolution of the flow above cities means a short scan time is needed.
•
Data using a 3-beam Doppler lidar profiling method was compared with an anemometer.
•
The 3-beam method was found to be suitable for use in urban areas.

Introduction
Comprehensive knowledge of the urban wind field is important to a wide variety of applications, including air quality, micro-generation of electricity and building design.
Dispersal of pollutants in urban areas is a well-studied area and data for this, and for studies of rooftop wind turbines, may be obtained using conventional instrumented masts, or roof-mounted equipment.
It is often challenging to obtain wind profile data in urban areas as it is generally not possible to use radiosondes or tethered balloons within a city, and erecting masts at the height of many tall buildings is not feasible.
The majority of studies of the urban boundary layer (UBL) have, therefore, been carried out using instrumented masts or roof-mounted instruments.
This has led to a lack of observations at greater heights (Roth, 2000), a problem which may be solved if remote sensing instruments such as sodars and lidars can be successfully deployed in urban environments.
Wind profile data, as opposed to the point measurements collected by traditional anemometers, are essential to the compilation of a complete urban wind climatology.
Given the sometimes complex way in which wind profiles adjust to the urban surface, this information could be extremely useful for calculating potential wind loadings on tall buildings, as well as for producing accurate weather forecasts for urban areas.
Another potential advantage of using remote sensing is that it is relatively simple to acquire data from above the roughness sublayer (RSL).
Within the RSL the flow is directly influenced by roughness elements at the surface, such as trees and buildings, and may vary widely in the horizontal as well as the vertical.
If we wish to obtain data that is representative of the wider surface, our measurements must be made above the RSL, which can be considered to extend up to 2-5 times the mean building height (Rotach, 1999; Cheng and Castro, 2002).
This can be difficult to achieve using, for example, an anemometer, as this type of instrument is generally mounted on a mast or building, which are both likely to be within the RSL.
There are several scan types that may be employed to obtain wind profiles using a Doppler lidar including Velocity Azimuth Display (Browning and Wexler, 1968), Range Height Indicator (Davies et al., 2003) and Doppler Beam Swinging (Pearson et al. 2009).
A VAD scan involves making observations at a single elevation angle and many azimuth angles, so that the lidar beam describes a cone.
An RHI scan takes samples at a single azimuth angle, and many elevations, so that the lidar samples a vertical 'slice' of the atmosphere.
During a DBS scan the lidar measures vertically, and then tilted in at least two other perpendicular directions (e.g. north and east).
Because a DBS scan involves scanning in fewer directions than an RHI or VAD scan, it can be completed more quickly (i.e. in seconds rather than minutes).
These three scans can all be carried out using a single Doppler lidar, but two or more lidars may also be used to measure different components of the wind in a common volume (Collier et al., 2005), or to create a 'virtual tower' (Calhoun et al., 2006) using intersecting RHI scans for measuring wind profiles.
Before deciding which scan type to use, it is necessary to consider the likely characteristics of the local wind field.
Pearson et al. (2009) suggest that Doppler Beam Swinging (DBS) is suitable for areas where the flow cannot be considered to be uniform over the area sampled during a velocity azimuth display (VAD) scan, or steady over the time it takes for such a scan to be completed.
The much shorter time required to complete a DBS scan should allow unsteady flow to be captured more completely because many scans may be completed in the same time required to carry out a VAD scan.
Due to the extremely rough nature of the urban surface, high turbulence intensities may be found up to a substantial distance above the mean building height, suggesting that a DBS scan may be suitable for use in observing the urban wind field.
This paper builds on the work of Pearson et al. (2009) by testing the DBS scanning method in an urban setting.
As part of the ACTUAL project (Advanced Climate Technology Urban Atmospheric Laboratory) a pulsed Doppler lidar was located in central London, at a site on the Marylebone Road, with the aim of making observations throughout the UBL.
Wind speed observations from the lidar using the DBS method were compared with data from a sonic anemometer located at 190m during the period 06/07/2010-11/01/2012.
An estimate of the uncertainty of the lidar wind speeds was calculated and the suitability of the DBS method for urban wind profiling was assessed.
Method
Instrument locations
The two instrument sites are located within central London, UK (Fig. 1).
A HALO Photonics Streamline pulsed Doppler lidar is located on the roof of the Westminster City Council building (WCC) on the Marylebone Road (51.5213°N, 0.1606°W), and a sonic anemometer is positioned on an open lattice tower on top of the BT Tower (51.5215°N, 0.1389°W).
See Barlow et al. (2011b) for wind-tunnel simulations of flow around the tower, and Wood et al. (2010) for previous work carried out at this site.
The heights of the lidar and anemometer above local ground level are 18m and 190m, respectively, and the distance between the two sites is 1.6km.
The area around the two sites is primarily commercial and residential, although there are two large parks in the vicinity.
The nearest point of Regent's Park (1.66km2) is 0.4km to the north-east of WCC, and 0.7km north-west of the BT Tower.
The nearest point in Hyde Park is located 0.9km to the south of WCC and 1.7km to the south-west of the BT Tower.
Hyde Park extends westwards and, together with the adjacent Kensington Gardens, comprises 2.53km2 of grass, lake and woodland.
Within 1-10km of the BT Tower (the tower's approximate source area in neutral conditions-Wood et al., 2010) the mean building height is 8.8±3.0m, so the instruments at the top of the BT Tower are ≈22 times the mean building height (Wood et al., 2010).
In order to compare data from an anemometer with the lidar, the height above ground of the anemometer must exceed the minimum range of the lidar.
In this case, this means the anemometer must be at least 105m above ground (see Section 2.2.1).
The BT Tower site provides a rare opportunity to collect data continuously so far above an urban surface, allowing a long-term comparison with the lidar.
To avoid interference with the lidar data from turbulence generated by the tower, the lidar must be sited further from the tower than the length of the wake, which is estimated to be ten times the width of the tower, or 10×20m=200m.
The WCC site is far enough away that the BT Tower's wake will not affect the lidar measurements, and the choice to use this site was made to build on previous work (Barlow et al., 2009; Harrison et al., 2012).
Instruments
Doppler lidar
The lidar used here is a pulsed, heterodyne Doppler lidar.
It is eye-safe, and the scan pattern is configurable by the user (for an example, see Wood et al., 2013).
The lidar has several built-in scanning modes, including a DBS wind profiling setting which is used here.
Some technical specifications for this lidar configuration are given in Table 1.
Observations of light scattered from aerosol particles in the atmosphere are received as a function of time from transmission to detection.
Data are combined by the lidar software into 30 m-long gates along the lidar beam.
Although there are 80 gates, the data from the first three are not useable as the geometry of the lidar causes only part of the return signal at short distances to be detected (Wandinger, 2005).
The seemingly low maximum Doppler velocity (±11ms-1) refers to the maximum measurable unambiguous velocity along the beam; it is still possible to measure higher horizontal wind speeds when measurements from several beams are trigonometrically combined.
Doppler beam swinging (DBS)
The DBS method of lidar wind profiling uses a beam pointed consecutively in three directions: vertically, tilted east and tilted north (Fig. 2).
The pre-programmed DBS mode of the lidar takes two consecutive samples in each direction, with each sample taking 1s to obtain and 2.5s to process.
The entire scan cycle is completed in approximately 21s.
The interval between scans is set to the allowed minimum of 120s.
This translates to an effective sampling frequency of 0.008Hz.
The radial velocities VRN (north-tilted), VRE (east-tilted), and VRZ (vertical) are:(1)VRN=vsinγ+wcosγ(2)VRE=usinγ+wcosγ(3)VRZ=wwhere u, v, and w are the east-west, north-south and vertical components of the wind, and γ is the angle between the tilted and vertical beams.
In this case γ=15°.
The radial velocities are then used to calculate the horizontal wind components:(4)u=(VRE-VRZcosγ)/sinγ(5)v=(VRN-VRZcosγ)/sinγ
The magnitude of the horizontal wind is then U=u2+v2.
Compared to other lidar wind profiling methods (e.g. VAD), which take many more samples, DBS is relatively fast.
Given that each DBS scan samples only 6s of data, averaging is required to give more accurate mean wind speeds, which is discussed in Section 2.2.4.
Lidar measurement error
The theoretical standard deviation of a single Doppler lidar velocity estimate can be approximated using an equation derived from Rye and Hardesty (1993) (Eq.
(6)) In this context, a single velocity estimate consists of many pulses averaged over several seconds to produce an estimate of the error in the Doppler velocity (Barlow et al., 2011a, after Pearson et al., 2009):(6)σe=(Δv22αNp(1+1.6α+0.4α2))0.5where Np is the accumulated photon count:(7)Np=SNR⋅M⋅nand α is the ratio of the lidar detector photon count to the speckle count:(8)α=SNR(2π)0.5(Δv/B)Δv is the signal spectral width, SNR is the signal to noise ratio, M is the number of points per range gate, n is the number of pulses averaged, and B is the bandwidth.
The relevant values for the lidar used in this study are given in Table 2.
σe=0.15ms-1 was selected as the maximum acceptable uncertainty for a single lidar wind speed measurement.
This is equivalent to a minimum SNR threshold of ~-20dB and is close to the threshold used by Barlow et al. (2011a).
Pearson et al. (2009) found that the threshold SNR for a reliable Doppler estimate was ~-23dB.
An upper SNR threshold of 2dB is applied to filter out returns from cloud and rain droplets (Pearson et al. 2009).
The final quality control applied to the lidar data is the removal of data collected in the first 3 gates, as they are unreliable.
This results in a blind area in the first 90m of the lidar beam.
Sampling error
Because of the rapid variation in wind speed in the UBL, each wind speed profile obtained by the lidar must be regarded as a snapshot, rather than as representative of the mean wind speed.
In order to produce a more reliable estimate of the mean wind speed, the data was averaged into one-hour blocks.
This relatively long time period is used because the lidar has a low sampling frequency (0.008Hz).
Because the time between scans is larger than the integral timescale of the flow, the error on the mean wind speed measured by the lidar can be estimated as the uncertainty attributed to the sampling frequency, which is estimated in this section.
The error variance σx¯2 in the mean wind speed due to the sampling frequency when the time between scans is greater than the integral timescale may be estimated as follows (Kaimal and Finnigan, 1994):(9)σx¯2=σx2Nwhere N=T/Δt, and σx¯2 is the variance of the dataset.
T is the averaging period, and Δt is the time between scans (120s).
Table 3 shows the mean, median and interquartile range of the lidar error due to the sampling rate for T=3600s over the 3993h of the data set.
It is possible to mitigate the effects of a low sampling frequency by increasing the averaging period, although over longer time periods, the data is likely to be statistically non-stationary.
For these reasons, an averaging period of T=3600s is used in the comparison between the lidar and the sonic anemometer.
Sonic anemometer
The sonic anemometer is a Gill Instruments R3-50.
It measures both horizontal (u, v) and vertical (w) components of the wind and samples at a rate of 20Hz.
It is located on a 12.2m tall scaffolding tower at the top of the main BT Tower, with a number of other meteorological instruments (Fig. 3).
The head of the anemometer is raised ≈750mm above the other instruments, and has good exposure to all wind directions.
This instrument will be used as a reference with which to compare wind data gathered by the lidar.
The position of the sonic anemometer, and the lidar gate to which it was compared (≈190m above ground level), mean that they are likely to be above the roughness sublayer.
The depth of the roughness sublayer may be expected to be between ≈18m and ≈45m, which correspond to 2 and 5 times the mean building height, respectively (Britter and Hanna, 2003; Cheng and Castro, 2002).
This means that the flow being measured will be more homogeneous than the flow close to the ground.
Because the interval between samples taken by this instrument is smaller than the integral timescale, the sampling error must be estimated using a slightly different method than that applied to the lidar.
The error variance in each hour can be estimated as:(10)σx¯2=σx2N/τwhere τ is the integral timescale of the flow.
The spectra of the horizontal flow are calculated using the method used by Wood et al. (2010), which is also used to determine the integral lengthscale Λ.
The integral timescale is then τ=Λ/U¯, where U¯ is the mean wind speed.
In order to evaluate the performance of the DBS scanning technique, data from the sonic anemometer at the BT Tower was compared with data from the lidar gate corresponding to the height of the BT Tower instruments.
The distance between the instruments (1.6km) means that it is not expected that the comparison will reveal an exact match.
Possible reasons for this could be influences of the different sites, differing surface types upwind of the two instruments, or evolution of the flow between the two sites.
The purpose of this study, however, is to test the long-term performance of the DBS method.
If the method produces good estimates of the mean wind speed, a strong linear correlation would be expected between the data from the two instruments.
In this case a slope not significantly different from one, and an intercept close to zero would be expected, with some amount of scatter due to the distance between the instruments.
Results and discussion
Whether or not the instrument is being affected by flow distortion from the tower, or turbulent wakes shed by other objects may be determined by calculating the turbulence intensity associated with each wind direction.
This can be calculated as:(11)TI=σU/U¯where U¯ is the 60min averaged wind speed, and σU is the standard deviation of the wind speed over the same hour-long period.
A peak in the turbulence intensity suggests that the instrument is being affected by the wake from another object.
Fig. 4 shows the median turbulence intensity measured by the sonic anemometer within each 15° sector.
In most sectors, the median turbulence intensity is close to 0.2 which is in line with the findings of Barlow et al. (2009).
There is also a broad peak of ≈0.4 apparent in the 90° sector centred on north.
If this was the result of interference by the turbulent wake of a nearby object, a sharper peak would be expected.
The small magnitude and broad width of this peak suggest that the source of the turbulence is further away, and the direction is consistent with the flow being distorted by the scaffolding tower.
Data collected when the sonic anemometer records a wind direction between 315° and 45° are removed as wind speed data from the sonic anemometer in these conditions is considered to be unreliable.
Effect of tilt
Another potential source of error in the lidar wind speed measurements is a tilting of the instrument so that the beam is slightly offset from its intended position.
This would mean that, for example, when the beam is in the vertical position, it will not only be measuring vertical motion; it will capture some horizontal motion as well.
This would give inaccurate estimates of the horizontal wind speed.
The same effect would occur if the flow was distorted by the local terrain so that it consistently intersected the lidar beam at an angle from the horizontal.
It is possible to determine whether a tilt or deflection of the flow is present by fitting a function of the form Asin(θ+B)+C to a plot of θ=tan-1(w/U) against wind direction, where A is the tilt angle, B-90 is the azimuth direction in which the tilt occurs, C is a vertical offset from zero, w is the mean vertical wind speed and U is the mean horizontal wind speed.
The method used here is the same as that used by Barlow et al. (2011b) to determine whether a sonic anemometer mounted on the BT Tower was tilted.
Fig. 5 shows θ=tan-1(w/U) against wind speed in gate closest to the height of the BT Tower (midpoint=180m).
Below the median wind speed of 7.83ms-1 the variability in θ increases dramatically.
Low wind speeds are often associated with unstable conditions where larger fluctuations in w would be more common, increasing the variability in θ.
In fitting the sine function only data where the horizontal wind speed was greater than the median wind speed were used, in order to reduce the variability in θ.
A sine function was fitted to the data using a least squares method (Fig. 6).
Table 4 lists all of the coefficients (A=tilt angle, B=azimuth+90° and C=vertical offset) which describe the sine functions fitted to the data recorded at the same height as in Fig. 5.
The upper and lower bounds of the coefficients (95% confidence intervals) are also listed.
The tilt of the instrument is estimated to be of the order of 0.5°.
The direction of the tilt is not consistent with the location of any slopes in the surrounding area, so it is thought that the instrument itself is tilted, rather than the flow being displaced from horizontal by the local terrain.
The tilt is corrected during processing using a rotation matrix.
Instrument comparison
Fig. 7 shows the 60min averaged horizontal wind speeds obtained using the lidar DBS method and the sonic anemometer.
The lidar-derived wind speeds are taken from the range gate closest to the height of the sonic anemometer.
The midpoint of this gate is 180m.
A linear fit has been applied to the data using a weighted total least squares, with the weighting determined by the sampling error of each hourly mean wind speed from both the sonic anemometer and the lidar (Section 2.2.4).
Despite the large horizontal distance between the two instruments, and the low sampling frequency of the lidar, there is a close agreement between the datasets (0.99x+0.81).
There is a large spread in the data around the fitted curve (root mean squared error=1.12ms-1), especially when compared with similar studies carried out in flat, homogeneous terrain, with less favourable conditions filtered out (for example, Gottschall et al., 2012).
Some of the spread may be explained by the sampling error (as discussed in Section 2.2.4.), and some by the distance between the two instruments (Bradley et al., 2012).
The fact that the slope of the linear fit is not significantly different from one and that the intercept is close to zero give confidence that this method is suitable for long-term wind speed observations.
Separating the data into stable, neutral and unstable conditions shows small differences based on stability (Table 5).
The stability was determined by the stability parameter ζ=z′/L, where L=-u⁎3/(κg(w′T′¯)/T) is the Obukhov length, κ=0.41 is the von Karman constant, g=9.81ms-2 is acceleration due to gravity, w′Τ′¯ is the mean heat flux (w' and T′ being fluctuations about the means), T is the air temperature, the friction velocity u⁎2=u′w′2¯+v′w′2¯ and z′=190-4.3; the height of the sensor minus a displacement height (Wood et al., 2010).
Neutral conditions were defined as |ζ|<0.1, unstable conditions as ζ<-0.1 and stable conditions as ζ>0.1.
The linear fit is best during stable conditions, with the y-intercept at 0.36, compared to 0.81 when all of the data is used, and 1.05 in unstable conditions.
Fig. 8 shows the mean difference between wind speeds measured by the lidar and sonic anemometer against anemometer wind speed.
The data was divided into bins of 1ms-1.
The error bars show the standard error of the data in each bin.
The mean difference between the two instruments remains fairly constant, with the lidar overestimating the wind speed by between 0 and 0.5ms-1.
At wind speeds greater than 20ms-1 the lidar appears to be overestimating the wind speed.
Further observations would be required to determine whether this is an accurate reflection of the performance of this method at very high wind speeds.
Conclusions
A three-beam Doppler lidar wind profiling method, consisting of one vertical beam and two tilted beams (elevation 75°), was evaluated to determine whether it was suitable for use in urban areas.
The reference instrument used was a sonic anemometer located at 190m above ground, at a site 1.6km from the lidar.
After removing data from the sonic anemometer affected by flow distortion, and correcting for a ≈0.5° tilt of the lidar, the data from the two instruments were compared.
The horizontal wind speeds were averaged over 60min to reduce variability.
A strong correlation was found between the two datasets, although the lidar has a tendency to overestimate the wind speed by ≈0-0.5ms-1 at speeds of less than 20ms-1.
At higher wind speeds there are few data, so it is not possible to draw a robust conclusion for these conditions.
The error in the lidar-derived wind speeds varies with the stability of the atmosphere; from 0.36ms-1 in stable conditions, to 1.05ms-1 in unstable conditions, and 0.86ms-1 in neutral conditions.
There is a considerable amount of spread around the fitted curve, which can be attributed to the low sampling frequency of the lidar (30 scans per hour), and the large distance between the instruments (1.6km).
This result could be improved by increasing the sampling frequency of the lidar, as well as using instruments located closer together.
The 3-beam DBS wind profiling method is considered to be appropriate for use in urban locations.
This method is capable of providing accurate wind speed data throughout the depth of the urban boundary layer suitable for both wind engineering and meteorological applications.
Acknowledgements
Thanks to BT and Westminster City Council (Steve Neville) for use of their buildings; to Stuart Bradley, Sue Grimmond and Humphrey Lean for useful discussion and comments; to Ewan O'Connor for assistance with lidar data archiving and quality assurance; to Rosy Wilson, Andrew Lomas, Dawn Turner, Marc Stringer, Gary Robinson, John Lally and HALO Photonics for technical support; to EPSRC EP/G029938/1 and UK Met Office (CASE award) for funding.

Polymer-Assisted Direct Deposition of Uniform Carbon Nanotube Bundle Networks for High Performance Transparent Electrodes
Arc-discharge carbon nanotubes are purchased from ILJIN Nanotech, grade ASP-100F; 80 mg of these tubes is mixed with 2 g of J.T. Baker sodium dodecyl sulfate (SDS) and 200 mL of Invitrogen 0.1 μm filtered ultrapure water. This mixture is placed in an ice water bath and sonicated in a Cole-Parmer ultrasonic cup-horn sonicator for 30 min at 750 W. After sonication, the dispersion is centrifuged at 15 000 rpm for 4 h at 4 degC in a Sorvall RC5C Plus centrifuge, and the supernatant decanted. The supernatant is diluted with anhydrous acetone, which dissociates the SDS from the nanotubes, and centrifuged to collect the precipitated tubes. This process of acetone rinsing followed by precipitate collection is repeated four times. Finally, the mixture is filtered through a Millipore 0.45 μm pore size PTFE membrane to collect the nanotubes. The tubes form a sheet on top of the filter, which is then peeled off and dried at 50 degC under vacuum overnight.

One to 1.5 mg of the SWNTs prepared during nanotube purification is dispersed in chloroform at a concentration of 100 μg/mL, by sonicating at 180 W for 30 min in an ice bath, using the same Cole-Parmer ultrasonic cup-horn sonicator as during the purification step. Separately, a few mg of rr-P3HT from Sigma Aldrich is dissolved in 4 mL of chloroform under gentle heating and shaking. The relevant quantity of rr-P3HT is then added to 2 mL of the nanotube dispersion using a glass microliter syringe, and the mixture is again sonicated at 180 W for 30-60 min, until the suspension is uniform to the eye. The total volume of solution is then refreshed to 2 mL by adding additional chloroform.

A 1.5 cm x 1.5 cm cut display-grade TFT glass (Eagle glass by Corning) is cleaned by placing in chloroform and then in a Branson 3510 100 W/42 kHz ultrasonic cleaner for 10 min, then switching the solvent to ethanol and repeating. This is followed by drying the substrates under a nitrogen stream and placing them in a Jelight Model 42 UV-Ozone cleaner for 20 min. A substrate is set spinning at 7000 rpm, and the predetermined amount of solution is dropped, one drop at a time, onto the substrate using a glass pipet. After spin-coating, the back of the substrate is cleaned with methanol, and the sample is annealed at about 120 degC on a hot plate, soaked in chloroform for an additional 10 min, and dried to remove any accessible excess polymer.Precipitation behavior of perovskite and anosovite crystals from high Ti-bearing blast furnace slag with small amount of B2O3
CaO was obtained by calcining CaCO3 at 900 degC for 8 hours. The compositions of the synthetic slag (seeing Table 2) in the present study were selected based on the blast furnace slag compositions of pan-steel. These chemicals were weighed according to Table 2 and mixed using an agate mortar and pestle for 20 min. The mixtures were then pelletized. Each pellet (approximate 0.2 g) was placed in a high-purity graphite crucible (purity 99.9%, 10 mm inner diameter and 10 mm inner height, Mathews Industrial Products Pty Ltd, Australia).Pd on carbon nanotubes-supported Ag for formate oxidation: The effect of Ag on anti-poisoning performance
Multi-walled carbon nanotubes (CNTs) were obtained from Shenzhen Nanotech Port Co., Ltd. and the average diameter of CNTs are 40 ~ 60 nm. Sodium oleate, potassium formate (HCOOK) and other reagents were of analytical purity and used without further purification.
The as-received CNTs (300 mg) were first ultrasonicated in HCl (1 M, 50 mL) for 1 h to dissolve possible metal residues, then filtered, washed, and dried to obtain purified CNTs. For deposition of Ag particles on CNTs, 210 mg of purified CNTs were dispersed in a beaker containing 253 mg sodium oleate and ultrasonicated for 30 min, and then aqueous solution containing 141 mg AgNO3 was added dropwise under magnetic stirring to form silver oleate on CNTs. After filteration and aqueous washing, the as-obtained sample of ~510 mg was collected and dried at 100 degC. Then 300 mg as-prepared samples were heated in a tube at 550 degC for 5 min under N2 protection to decompose the silver oleate. Finally 160 mg of Ag/CNTs was obtained with a theoretical Ag content of 30 wt. %.Adaptive resource configuration for Cloud infrastructure management

Abstract
To guarantee the vision of Cloud Computing QoS goals between the Cloud provider and the customer have to be dynamically met.
This so-called Service Level Agreement (SLA) enactment should involve little human-based interaction in order to guarantee the scalability and efficient resource utilization of the system.
To achieve this we start from Autonomic Computing, examine the autonomic control loop and adapt it to govern Cloud Computing infrastructures.
We first hierarchically structure all possible adaptation actions into so-called escalation levels.
We then focus on one of these levels by analyzing monitored data from virtual machines and making decisions on their resource configuration with the help of knowledge management (KM).
The monitored data stems both from synthetically generated workload categorized in different workload volatility classes and from a real-world scenario: scientific workflow applications in bioinformatics.
As KM techniques, we investigate two methods, Case-Based Reasoning and a rule-based approach.
We design and implement both of them and evaluate them with the help of a simulation engine.
Simulation reveals the feasibility of the CBR approach and major improvements by the rule-based approach considering SLA violations, resource utilization, the number of necessary reconfigurations and time performance for both, synthetically generated and real-world data.
Highlights
► We apply knowledge management to guarantee SLAs and low resource wastage in Clouds.
► Escalation levels provide a hierarchical model to structure possible reconfiguration actions.
► Case-Based Reasoning and rule-based approach prove feasibility as KM techniques.
► In-depth evaluation of rule-based approach shows major improvements towards CBR.
► KM is applied to real-world data gathered from scientific bioinformatic workflows.

Introduction
The vision of Cloud Computing is to provide computing power as a utility, like gas, electricity or water [1].
For the underlying infrastructure this means that it has to deal with dynamic load changes, ranging from peak performance to utilization gaps.
This brings up two issues: on the one hand, the management of a Cloud Computing infrastructure has to guarantee pre-established contracts despite all the dynamism of workload changes.
On the other hand it has to efficiently utilize resources and reduce resource wastage.
As to the former, the pre-established contracts, so called Service Level Agreements (SLAs), contain Service Level Objectives (SLOs) that represent Quality of Service (QoS) goals, e.g., "storage should be at least 1000 GB", "bandwidth should be at least 10 Mbit/s" or "response time should be less than 2 s", and penalties that have to be paid to the customer if these goals are violated.
This work can be integrated into the Foundations of Self-governing ICT Infrastructure (FoSII) project [2], but is on its own completely self-sufficient.
The FoSII project aims at developing an infrastructure for autonomic SLA management and enforcement.
Besides the already implemented LoM2HiS framework [3] that takes care of monitoring the state of the Cloud infrastructure and its applications, the knowledge management (KM) system presented in this article can be viewed as another building block of the FoSII infrastructure.
[4] proposes an approach to manage Cloud infrastructures by means of Autonomic Computing, which in a control loop monitors (M) Cloud parameters, analyzes (A) them, plans (P) actions and executes (E) them; the full cycle is known as MAPE [5].
According to [6] a MAPE-K loop stores knowledge (K) required for decision-making in a knowledge base (KB) that is accessed by the individual phases.
This paper addresses the research question of finding a suitable KM system (i.e., a technique of how stored information should be used) and determining how it interacts with the other phases for dynamically and efficiently allocating resources.
One of the imminent problems that come up when dealing with the MAPE-K loop is to define possible actions that can be executed at the end of the loop.
Due to the plethora of possible reconfiguration actions in Clouds, e.g., increasing/decreasing available memory or storage for virtual machines (VMs), choosing VMs to migrate to selected physical machines (PMs), determining PMs to power on/off, etc., it is not trivial to identify the most beneficial action in a certain situation.
On the one hand it is not trivial to retrieve and store all necessary information in a Cloud infrastructure.
On the other hand, and more important in our work, dealing with the complexity of recommending an action based on this information is, as we will see, in most cases NP-hard.
To tackle this, we structure all possible actions and organize them in a hierarchical model of so called escalation levels.
In [7,8] we have shown that approaches using Case Based Reasoning (CBR) and rules as knowledge management techniques succeed in autonomically enacting SLAs and governing important parts of Cloud computing infrastructures.
Case Based Reasoning was chosen, because it offers a natural translation of Cloud status information into formal knowledge representation and an easy integration with the MAPE phases.
Moreover, it promises to be scalable (as opposed to e.g., Situation Calculus) and easily configurable (as opposed to rule-based systems).
Related work has not observed the usage of CBR nor has it evaluated different KM techniques in Cloud environments.
However, we determined some drawbacks of CBR as far as its learning performance and its scalability were concerned.
Therefore, we also designed and implemented a rule-based knowledge management approach.
Using rules [8] we managed to improve not only SLA adherence and resource allocation efficiency as discussed in [7], but also attained an efficient use of reallocation actions and high scalability.
Yet, evaluating the KM system on a real environment is not a trivial task because of two reasons: First, Cloud infrastructures usually are huge data centers consisting of hundreds of PMs and even more VMs.
Thus, a first step is to simulate the impact of autonomic management decisions on the Cloud infrastructure to determine the performance of the KM decisions.
Consequently, we designed and implemented a simulation engine that mimics the MAPE-K cycle on large Clouds.
Second, workload data for a large number of VMs has to be provided as input for the simulation.
We decided to go two ways: On the one hand, we generated synthetic workload data categorized into different workload volatility classes.
These workload volatility classes are determined by the speed and intensity of workload change.
On the other hand, we gathered real world data from monitoring scientific workflow applications in the field of bioinformatics [9].
These workflows need a huge, yet unpredictable and varying amount of resources, and are thus-due to the needed flexibility and scalability-a perfect match for a Cloud computing application [10].
The main challenge in this work is to evaluate KM techniques for autonomic SLA enactment in Cloud computing infrastructures that fulfill the three following conflicting goals: (i) achieving low SLA violation rates; (ii) achieving high resource utilization such that the level of allocated but unused resources is as low as possible; and (iii) achieving (i) and (ii) by as few time- and energy-consuming reallocation actions as possible.
We will call this problem the resource allocation problem throughout the rest of the paper.
The main contributions of this paper are: 1.
Design and implementation of a generic (KM-technique agnostic) simulation engine to assess the quality of the KM and decision-making techniques.
2.
Partitioning the resource allocation problem for Cloud infrastructures into several subproblems by proposing escalation levels that structure all possible reaction possibilities into different subproblems using a hierarchical model.
3.
Design, Implementation and Evaluation of two KM techniques for one escalation level, i.e., VM resource configuration: CBR, and the rule-based approach.
4.
Application of the rule-based approach to real-world monitoring data from scientific workflow applications in the field of bioinformatics.
The remainder of this work is divided as follows: In Section 2 we present related work.
Section 3 gives some background information by explaining the MAPE-K loop and the FoSII project.
In Section 4 we structure the problem into the mentioned escalation levels, and in Section 5 we describe how to use the two KM techniques (CBR and rules) to tackle the resource allocation problem for a certain escalation level.
Section 6 shows the evaluation of both approaches, especially focusing on the rule-based approach.
Section 7 concludes this contribution and points out future work.
Related work
Concerning related work, we have determined four different ways to compare our work with other achievements in this area.
Whereas the first level compares other works dealing with SLA enactment and resource efficiency, the second one considers the area of knowledge management, and the third one compares commercial products to our approach.
Fourthly, the FoSII project is briefly related to other projects in this field.
Firstly, there has been some considerable work on optimizing resource usage while keeping QoS goals.
These papers, however, concentrate on specific subsystems of Large Scale Distributed Systems, such as [11] on the performance of memory systems, or only deal with one or two specific SLA parameters.
Petrucci et al. [12] or Bichler et al. [13] investigate one general resource constraint and Khanna et al. [14] only focuses on response time and throughput.
A quite similar approach to our concept is provided by the Sandpiper framework [15], which offers black-box and gray-box resource management for VMs.
Contrary to our approach, though, it plans reactions just after violations have occurred.
Also the VCONF model by Rao et al. [16] has similar goals as presented in Section 1, but depends on specific parameters, can only execute one action per iteration and it neglects the energy consumption of executed actions.
Other papers focus on different escalation levels (as described in Section 4).
[17,18] focus on VM migration and [19] on turning on and off physical machines, whereas our paper focuses on VM re-configuration.
Additionally, none of the presented papers uses a KB for recording past action and learning.
Hoyer et al. [20] also undertake a speculative approach as in our work by overbooking PM resources.
They assign VMs to PMs that would exceed their maximum resource capacities, because VMs hardly ever use all their assigned resources.
Computing this allocation they also take into consideration workload correlation of different VMs.
Borgetto et al. [21] tackle the trade-off between consolidating VMs on PMs and turning off PMs on the one hand, and attaining SLOs for CPU and memory on the other.
However, the authors assume a static setting and do not consider dynamically changing workloads.
So, e.g., they do not take the number of migrations into account.
Stillwell et al. [22] in a similar setting define the resource allocation problem for static workloads, present the optimal solution for small instances and evaluate heuristics by simulations.
Nathani et al. [23], e.g., also deal with VM placement on PMs using scheduling techniques.
[24] react to changing workload demands by starting new VM instances; taking into account VM startup time, they use prediction models to have VMs available already before the peak occurs.
Other works such as [25] have already considered the last escalation level (see Section 4), i.e., outsourcing of applications to other Clouds.
Summarizing we can say that there has been a great deal of work on the different escalation levels, whereas VM configuration has not been observed yet.
Secondly, there has been work on KM of SLAs, especially rule-based systems.
Paschke and Bichler [26] look into a rule based approach in combination with the logical formalism ContractLog.
It specifies rules to trigger after a violation has occurred, but it does not deal with avoidance of SLA violations.
Others inspected the use of ontologies as KBs only at a conceptual level.
[27] viewed the system in four layers (i.e., business, system, network and device) and broke down the SLA into relevant information for each layer, which had the responsibility of allocating required resources.
Again, no details on how to achieve this have been given.
Bahati and Bauer [28] also use policies, i.e., rules, to achieve autonomic management.
They provide a system architecture including a KB and a learning component, and divide all possible states of the system into so called regions, which they assign a certain benefit for being in this region.
A bad region would be, e.g., response time>500 (too slow), fair region response time<100 (too fast, consuming unnecessary resources) and a good region 100≤response time≤500.
The actions are not structured, but are mixed together into a single rule, which makes the rules very hard to manage and to determine a salience concept behind them.
However, we share the idea of defining "over-utilized", "neutral" and "under-utilized" regions.
Our KM system allows us to choose any arbitrary number of resource parameters that can be adjusted on a VM.
Moreover, our paper provides a more wholesome approach than related work and integrates the different action levels that work has been carried out on.
Thirdly, commercial Cloud IaaS platforms such as Amazon EC2 [29], Rackspace [30] or RightScale [31] have a very limited choice of preconfigured and static VM resource provisioning types.
Amazon EC2 only offers VM instance types such as small, medium or large with predefined storage, computing units, and memory without the possibility of reconfiguring or fine-tuning them beforehand, not to mention during runtime.
Rackspace only offers storage on the IaaS level, and RightScale focuses more on integrating different IaaS platforms such as Amazon EC2 or Rackspace into a holistic view.
Fourthly, compared to other SLA management projects like SLA@SOI [32], the FoSII project in general is more specific on Cloud Computing aspects like deployment, monitoring of resources and their translation into high level SLAs instead of just working on high-level SLAs in general service-oriented architectures.
Background
In this section we describe how the KM approach can be integrated within a more holistic Cloud management project that, e.g., also consists of a monitoring component.
Yet, the KM approach does not depend on the specific used monitoring framework, as long as it correctly measures the current values of the parameters specified in the SLA.
In this case, the FoSII project will serve as a running example.
We will describe how the KM approach relates to other components of the FoSII project.
Generally, the project distinguishes between system set-up and run time.
During system set-up, applications, their corresponding SLAs and used infrastructure are tailored and adapted.
Once the application is deployed, we consider monitoring, knowledge management and execution phases during run time.
In this section, in particular, we focus on the adaptation, monitoring, and knowledge management phases, as shown in Fig. 1.
Thus, the MAPE-K loop is extended to the A-MAPE-K loop, where the additional A stands for the adaptation phase during system set-up.
This adaptation phase, however, should not be confused with later adaptation and re-configuration of resources during system run time.
Quite evidently, we especially focus on the knowledge management phase in this paper.
The three mentioned phases are described as follows: 
Adaptation
As shown in Fig. 1, part 1, the adaptation phase comprises all steps necessary to be done before successful deployment and start of the application.
This includes SLA contract establishment and tailoring of the monitoring systems for the particular application.
We assume that Cloud providers register their resources to particular databases containing public SLA templates.
Thereafter, Cloud users can look up resources that they want to use for the deployment of their applications.
Similar to the providers, Cloud users also have an SLA template utilized for their private business processes.
We assume that the private SLA template cannot be changed, since it could also be part of some other local business processes and has usually to comply with different legal and security guidelines.
If matching SLA templates are found, an SLA contract can be negotiated and established and the application can be deployed.
Thus, during this phase it has to be ensured that private templates of the provider and consumers match publicly available templates.
However, public and private templates may differ.
A typical mismatch between templates would be between different measurement units of attributes, as for example for the SLO clock speed or missing attributes.
Therefore, a mechanism is required for the automatic adaptation between different templates without changing the templates themselves.
A possible solution for this is the so called SLA mapping approach presented in [33].
This approach can include handling of missing SLA parameters, inconsistencies between attributes and translation between different attributes.
More complex adaptations would include automatic service aggregation, including third party services, if, for example, the clock speed attribute is completely missing in the public template, but required in the private template.
A third party provider (e.g., a computer hardware reseller) could be integrated to deliver information about the clock speed attribute.
Detailed information on the adaptation phase including the SLA mapping approach are found in [33,34].
Monitoring
Current monitoring systems (e.g., ganglia [35]) facilitate monitoring only of low-level systems resources, such as free_disk or packets_sent, but SLA parameters typically are, e.g., storage and outgoing bandwidth.
Thus, SLA parameters required by an application usually differ from the parameters measured by the monitoring tools.
To achieve a mapping from the low-level metrics to the high-level SLA parameters, the monitoring phase should comprise two core components, namely the host monitor and the run-time monitor (see Fig. 1, part 2).
The former is responsible for monitoring low-level resource metrics, whereas the latter is responsible for metric mapping, and consequently for the monitoring of SLAs and informing the KM phase about SLA violations.
This monitoring framework has proven to be highly scalable and is presented in more detail in [3].
Knowledge Management
Since the analysis, plan and KB parts are highly interweaved with each other, we call the ensemble of these phases the Knowledge Management Phase (see Fig. 1, part 3).
The knowledge management component receives current information about SLA parameters of each running application from the run-time monitor of the monitoring component.
Depending on the KM technique in use, the KM phase analyzes this data to determine critical situations, where either SLA parameters are about to be violated or too many resources are wasted.
The analysis component receives the monitoring data, stores it in the KB and queries it to recommend an action to be executed.
The plan phase maps these actions onto PMs or plans outsourcing them to other Cloud providers.
Finally, the actions are executed (Execution phase) with the help of actuators.
Additionally, the KB does not only enable decision making out of current data, i.e., suggesting actions to be executed, but also improving the quality of decisions by keeping track of the success or failure of previous decisions, i.e., learning.
Structuring the problem: escalation levels
This section presents a methodology of dividing the resource allocation problem into smaller subproblems using a hierarchical approach.
It demonstrates which actions can be executed in what level to achieve SLA adherence and efficient resource allocation for Cloud infrastructures.
In general, we can think of the following reallocation actions: 1.
for individual applications: (a)
Increase incoming bandwidth share by x%.
(b)
Decrease incoming bandwidth share by x%.
(c)
Increase outgoing bandwidth share by x%.
(d)
Decrease outgoing bandwidth share by x%.
(e)
Increase memory by x%.
(f)
Decrease memory by x%.
(g)
Add allocated storage by x%.
(h)
Remove allocated storage by x%.
(i)
Increase CPU share by x%.
(j)
Decrease CPU share by x%.
(k)
Outsource (move application) to other Cloud.
(l)
Insource (accept application) from other Cloud.
(m)
Migrate application to different VM.
2.
for VMs: (a)
Increase incoming bandwidth share by x%.
(b)
Decrease incoming bandwidth share by x%.
(c)
Increase outgoing bandwidth share by x%.
(d)
Decrease outgoing bandwidth share by x%.
(e)
Increase memory by x%.
(f)
Decrease memory by x%.
(g)
Add allocated storage by x%.
(h)
Remove allocated storage by x%.
(i)
Increase CPU share by x%.
(j)
Decrease CPU share by x%.
(k)
Outsource (move VM) to other Cloud.
(l)
Insource (accept VM) from other Cloud.
(m)
Migrate VM to different PM.
3.
for physical machines (computing nodes): (a)
Add x computing nodes.
(b)
Remove x computing nodes.
4.
Do nothing.
For an application, under "increase incoming bandwidth share" we understand to increase the application's share of all the available incoming bandwidth of a VM, and for a VM the share relates to all the available incoming bandwidth of a PM.
The idea of bandwidth sharing is a common idea in network systems as described in [36].
Similar arguments account for outgoing bandwidth or CPU share.
We then group these actions into so called escalation levels that we define in Table 1.
The idea is that every problem that occurs should be solved on the lowest escalation level.
Only if this is not possible, the problem is tried to be solved on the next level, and again, if this fails, on the next one, and so on.
The levels are ordered in a way such that lower levels offer faster and more local solutions than higher ones.
At every level it has to be decided, whether the proposed action should be executed or not, because it is important to know when to do nothing, since every reallocation action is time and energy consuming.
In fact, for every level there is the possibility not to execute the proposed action.
If the proposed action is not executed, then the decision-making process will stop and not evaluate whether the next escalation level should be considered or not.
The first escalation level ("change VM configuration") works locally on a PM and tries to change the amount of storage or memory, e.g., that is allocated to the VM from the PM resources.
Then, migrating applications (escalation level 2) is more lightweight than migrating VMs and turning PMs on/off (escalation levels 3 and 4).
For all three escalation levels already the whole system state has to be taken into account to find an optimal solution.
The problem stemming from escalation level 3 alone can be formulated into a binary integer problem (BIP), which is known to be NP-complete [37].
The proof is out of scope for this paper, but a similar approach can be seen in [12].
The last escalation level has least locality and greatest complexity, since the capacity of other Cloud infrastructures have to be taken into account, too, and negotiations have to be started with them as well.
Also the rule-based approach benefits from this hierarchical action level model, because it provides a salience concept for contradicting rules.
Without this concept it would be troublesome to determine which of the actions, e.g., "Power on additional PM with extra storage and migrate VM to this PM", "Increase storage for VM by 10%" or "Migrate application to another VM with more storage" should be executed, if a certain threshold for allocated storage has been exceeded.
The proposed KM approaches will present a solution for escalation level 1.
Fig. 2 visualizes the escalation levels from Table 1 before and after actions are executed.
Fig. 2(a) shows applications App1 and App2 deployed on VM1 that is itself deployed on PM1, whereas App3 runs on VM2 running on PM2.
Fig. 2(b) shows example actions for all five escalation levels.
The legend numbers correspond to the respective numbering of the escalation levels. •
Escalation level 1: At first, the autonomic manager considers whether it should change VM configuration or not.
Actions (1) show that the autonomic manager decided to change the VM configuration; VM1 is being up-sized and VM2 being down-sized.
•
Escalation level 2: If VM reconfiguration has taken place, or if it has been recommended, but cannot be fulfilled yet, because some resource cannot be increased anymore due to the constraints of the PM hosting the VM, in level 2 the autonomic manager considers migrating the application to another larger VM that fulfills the required specifications from level 1.
So if, e.g., provided storage needs to be increased from 500 to 800 GB, but only 200 GB are available on the respective VM, then the application has to be migrated to a VM that has at least the same resources as the current one plus the remaining 100 GB of storage.
Action (2) shows the re-deployment of App2 to VM2.
Due to possible confinements of some applications to certain VMs, e.g., a user deployed several applications that need to work together on one VM, this escalation might be skipped in some scenarios.
Also for Infrastructure as a Service (IaaS) providers, who directly provide the VMs without caring about the applications running on them, this escalation level is omitted.
•
Escalation level 3: If there is no appropriate VM available in level 2, or if level 2 is skipped and VM configurations have been recommended in level 1, in level 3 the autonomic manager considers creating a new VM on an appropriate PM or migrating the VM to a PM that has enough available resources.
Action (3) shows the re-deployment of VM2 to PM1.
•
Escalation level 4: Again, if there is no appropriate PM available in level 3, the autonomic manager suggests turning on a new PM (or turning it off if the last VM was emigrated from this PM) in level 4.
Action (4) shows powering on a new PM (PM3).
•
Escalation level 5: Finally, the last escalation level 5 tries to outsource the application to another Cloud provider as explained, e.g., in the Reservoir project [38].
Action (5) outsources App3 to another Cloud provider.
For an IaaS provider omitting escalation level 2, the sequence of these escalation levels is quite obvious: If VM sizes are not changed in escalation level 1, there is no need to trigger escalation level 3 as VMs have not changed, and no better allocation of VMs to PMs can be found, if the previous one was already optimal.
However, if VM sizes were changed, escalation level 3 can still come to the conclusion that VM migrations are unnecessary.
On the other hand, if VM migrations were recommended, some PMs could be then turned off in escalation level 4.
Similarly, if no migrations were triggered, thinking about turning off PMs is unnecessary, as no PMs run idle now that have not been running idle before.
Finally, if all the previous actions were successfully executed without the help of another Cloud provider, there is no need to consider outsourcing applications.
Only if the last possibility failed, outsourcing applications should be considered.
(Other business incentives for outsourcing applications such as cheaper execution costs in other Clouds, etc., are not considered here.) For providers of other Cloud delivery models such as SaaS or PaaS, the sequence of placing application migration after VM reconfiguration is arguable; another model could also propose an inverse sequence for these two levels.
Implementing the knowledge management phase
In this section we present the implementation of the Knowledge Management phase using CBR and a rule-based approach.
Prerequisites
This subsection subsumes all the common assumptions for both approaches.
We assume that customers deploy applications on an IaaS Cloud infrastructure.
SLOs are defined within an SLA between the customer and the Cloud provider for every application.
Furthermore, there is a 1:1 relationship between applications and VMs.
One VM runs on exactly one PM, but one PM can host an arbitrary number of VMs with respect to supplied vs. demanded resource capacities.
After allocating VMs with an initial capacity (by estimating initial resource demand) for every application, we continuously monitor actually used resources and re-allocate resources according to these measurements.
For tackling the resource allocation for VMs, we need to define how measured, provided and agreed values interrelate, and what actually constitutes an SLA violation.
An example is provided in Table 2.
First, we deal with the measured value (1), which represents the amount of a specific resource that is currently used by the customer.
Second, there is the amount of allocated resource (2) that can be used by the customer, i.e., that is allocated to the VM which hosts the application.
Third, there is the SLO agreed in the SLA (3).
A violation therefore occurs, if less is provided (2) than the customer utilizes (or wants to utilize) (1) with respect to the limits set in the SLA (3).
Considering Table 2 we can see that rows 1 and 3 do not represent violations, whereas row 2 does represent an SLA violation.
In order to save resources we envision a speculative approach: Can we allocate less than agreed, but still more than used in order not to violate an SLA? The most demanding questions are how much can we lower the provisioning of resource without risking an SLA violation.
This heavily depends on the characteristics of the workload of an application, especially its volatility.
Case Based Reasoning
Case Based Reasoning is the process of solving problems based on past experience [39].
In more detail, it tries to solve a case (a formatted instance of a problem) by looking for similar cases from the past and reusing the solutions of these cases to solve the current one.
In general, a typical CBR cycle consists of the following phases assuming that a new case was just received: 1.
Retrieve the most similar case or cases to the new one.
2.
Reuse the information and knowledge in the similar case(s) to solve the problem.
3.
Revise the proposed solution.
4.
Retain the parts of this experience likely to be useful for future problem solving.
(Store new case and found solution in KB.)
To adapt CBR to our problem, three issues have to be solved.
First, it has to be decided how to format an instance of the problem.
Second, it has to be decided when two cases are similar.
Third, good reactions have to be distinguished from bad reactions.
As to the first problem we assume that each SLA has a unique identifier id and a collection of SLOs.
SLOs are predicates of the form (1)SLOid(xi,comp,πi)with comp∈{<,≤,>,≥,=}, where xi∈P represents the parameter name for i=1,…,nid,πi the parameter goal, and comp the appropriate comparison operator.
Then, a CBR case c is defined as (2)c=(id,m1,p1,m2,p2,…,mnid,pnid), where id represents the SLA id, and mi and pi the measured (m) and provided (p) value of the SLA parameter xi, respectively.
To use the SLA parameters storage and incoming bandwidth for example, a typical use case looks like this: SLA id=1 with SLO1 ("Storage", ≥, 1000) and SLO1 ("Bandwidth", ≥, 50.0).
A corresponding case received by the measurement component is therefore written as c=(1,500,700,20.0,30.0).
A result case rc=(c-,ac,c+,utility) includes the initial case c-, the executed action ac, the resulting case c+ measured some time interval later, which corresponds to one iteration in the simulation engine, and the calculated utility described later.
In order to give the KB some knowledge about what to do in specific situations, several initial cases are stored in the KB as described in [7] in more detail.
Secondly, to define similarity between two cases is not straightforward, because due to their symmetric nature Euclidean distances, e.g., do not recognize the difference between over- and under-provisioning.
Following the principle of semantic similarity from [40] for the summation part this leads to the following equation (3)d(c-,c+)=min(wid,|id--id+|)+∑x∈Pwx|(px--mx-)-(px+-mx+)maxx-minx|, where w=(wid,wx1,…,wxn) is the weight vector; wid is the weight for non-identical SLAs; wx is the weight, and maxx and minx the maximum and minimum values of differences px-mx for parameter x.
As far as the third issue is concerned, every action is evaluated by its impact on violations and utilization.
This way CBR is able to learn whether an action was appropriate for a specific measurement or not.
The utility of an action is calculated by comparing the initial case c- with the resulting final case c+.
The utility function is composed by a violation and a utilization term weighed by the factor 0≤α≤1: (4)utility=∑x∈Pviolation(x)+α⋅utilization(x).
Higher values for α strengthen the utilization of resources, whereas lower values the non-violation of SLA parameters.
We further note that c(x) describes a case only with respect to parameter x.
E.g., we say that a violation has occurred in c(x), when in case c the parameter x was violated.
We define the violation function for every parameter x as follows: (5)violation(x)={1,No violation occurred in c+(x),but in c-(x)1/2,No violation occurred in c+(x)and c-(x)-1/2Violation occurred in c+(x) and c-(x)-1Violation occurred in c+(x),but not in c-(x).
The utilization function is calculated by comparing the used resources to the provided ones.
We define the distance δ(x,y)=|x-y|, and utilization for every parameter as (6)utilization(x)={1,δ(px-,mx-)>δ(px+,ux+)-1,δ(px-,mx-)<δ(px+,ux+)0,otherwise . A utilization utility of 1 is retrieved if less over-provisioning of resources takes place in the final case than in the initial one, and a utilization utility of -1 if more over-provisioning of resources takes place in the final case than in the initial one.
The whole CBR process works as follows: Before the first iteration, we store the mentioned initial cases consisting of an initial measurement, an action and a resulting measurement.
Then, when CBR receives a new measurement, this measurement is compared to all cases in the KB.
From the set of closest cases grouped by a clustering algorithm we choose the one with the highest utility and execute exactly the same action as in the chosen case.
Afterwards, this action, the resulting measurement and the utility of the action is added to the initial measurement, and stored as a complete case.
Rule-based approach
For the rule-based approach we first introduce several resource policy modes to reflect the overall utilization of the system in the VM configuration rules.
Dealing with SLA-bound resource management, where resource usage is paid for on a "pay-as-you-go" basis with SLOs that guarantee a minimum capacity of these resources as described above, raises the question, whether the Cloud provider should allow the consumer to use more resources than agreed.
We will refer to this behavior as over-consumption.
Since the consumer will pay for every additional resource, it should be in the Cloud provider's interest to allow over-consumption as long as this behavior does not endanger the SLAs of other consumers.
Thus, Cloud providers should not allow over-consumption when the resulting penalties they have to pay are higher than the expected revenue from over-consumption.
To tackle this problem, we introduce five policy modes for every resource that describe the interaction of the five escalation levels.
As can be seen in Table 3 the policy modes are green, green-orange, orange, orange-red and red.
They range from low utilization of the system with lots of free resources left (policy mode green) over a scarce resource situation (policy mode orange) to an extremely tight resource situation (policy mode red), where it is impossible to fulfill all SLAs to their full extent and decisions have to be made which SLAs to deliberately break and which applications to outsource.
In order to know whether a resource r is in danger of under-provisioning or already is under-provisioned, or whether it is over-provisioned, we calculate the current utilization utr=userprr×100, where user and prr signify how much of a resource r was used and provided, respectively, and divide the percentage range into three regions using the two "threat thresholds" TTlowr and TThighr: •
Region -1: Danger of under-provisioning, or under-provisioning (>TThighr).
•
Region 0: Well provisioned (≤TThighrand≥TTlowr).
•
Region +1: Over-Provisioning (<TTlowr).
The idea of this rule-based design is that the ideal value that we call target value tv(r) for utilization of a resource r is exactly in the center of region 0.
So, if the utilization value after some measurement leaves this region by using more (Region -1) or less resources (Region +1), then we reset the utilization to the target value, i.e., we increase or decrease allocated resources so that the utilization is again at tv(r)=TTlowr+TThighr2%.
As long as the utilization value stays in region 0, no action will be executed.
E.g., for r=storage,TTlowr=60%, and TThighr=80%, the target value would be tv(r)=70%.
Fig. 3 shows the regions and measurements (expressed as utilization of a certain resource) at time steps t1,t2,…,t6.
At t1 the utilization of the resource is in Region -1, because it is in danger of a violation.
Thus, the KB recommends to increase the resource such that at the next iteration t2 the utilization is at the center of Region 0, which equals the target value.
At time steps t3 and t4 utilization stays in the center region and consequently, no action is required.
At t5, the resource is under-utilized and so the KB recommends the decrease of the resource to tv(r), which is attained at t6.
Additionally, if over-provisioning is allowed in the current policy mode, then the adjustment will always be executed as described regardless of what limit was agreed in the SLA.
On the other hand, if over-provisioning is not allowed in the current policy mode, then the rule will allocate at most as much as agreed in the SLA (SLOr).
The concept of a rule increasing resource r is depicted in Fig. 4.
The rule executes if the current utilization utr and the predicted utilization utpredictedr of the next iteration (cf.
next paragraph) both exceed TThighr (line 2).
Depending on what policy level is active the rule either sets the provided resource prr to the target value tv(r) for policy levels green and green-orange (line 3) or to at most what was agreed in the SLA (SLOr) plus a certain percentage ϵ to account for rounding errors when calculating the target value in policy levels orange, orange-red and red (line 5).
A similar rule scheme for decreasing a resource can be seen in Fig. 5.
The main difference is that it does not distinguish between policy modes and that it sets the provisioned resource to at least a minimum value minPrr, which may be 0, that is needed to keep the application alive (line 4).
The rule is executed if the current utilization utr and the predicted utilization utpredictedr of the next iteration both lie below TTlowr (line 2).
A large enough span between the thresholds TTlowr and TThighr helps to prevent oscillations of repeatedly increasing and decreasing the same resource.
However, to further reduce the risk of oscillations, we suggest to calculate a prediction for the next value based on the latest measurements.
Thus, an action is only invoked when the current AND the predicted measurement exceed the respective TT.
So, especially when only one value exceeds the TT, no action is executed.
The rules have been implemented using the Java rule engine Drools [41].
The Drools engine sets up a knowledge session consisting of different rules and a working memory.
Rules get activated when specific elements are inserted into the working memory such that the conditional "when" part evaluates to true.
Activated rules are then triggered by the simulation engine.
In our case, the simulation engine inserts measurements and SLAs of applications into the working memory.
Different policy modes will load slightly modified rules into the Drools engine and thus achieve a high adaptability of the KM system reacting to the general performance of the Cloud infrastructure.
As opposed to the CBR approach in [7], the rule-based approach is able to fire more than one action at the same iteration, which inherently increases the flexibility of the system.
Without loss of generality we can assume that one application runs on one VM (several applications' SLAs can be aggregated to form one VM SLA) and we assume the more interesting case of policy modes orange, orange-red or red, where over-provisioning is not allowed.
Listing 1 shows the rule to increase parameter storage formulated in the Drools language following the pattern presented in Fig. 4.
Line 1 defines the name of the rule that is split into a condition part (when, lines 2-12) and an execution part (then, lines 13-17).
Line 4 tries to find the SLA of an application, and stores its id in $slaID and the SLA into $slaApp.
Line 6 looks for a set of actions for this $slaID where no storage action has been added yet (storage == false) in order to avoid contradicting actions for storage for one measurement.
Line 8 searches for a measurement for the appropriate VM (vmID == $slaID) that has been inserted into working memory that is no prediction ($prediction == false) and where the percentage of utilized storage exceeds TThighr (storage_utilized>storage_HighTT), and stores used and provided values into $s_used and $s_provided, respectively.
The predicted measurement for the next iteration is handled similarly in line 10.
Finally, line 12 checks whether provided storage is still below the agreed value in the SLA.
This is done, because in policy modes orange to red over-consumption is prohibited.
The rules for policy modes green and green-orange would omit this line.
Now, if all these conditions are met, the rule gets activated.
When fired, line 15 calculates the new value for prr as explained in Fig. 4.
This line (as line 12) would also be altered for policy modes green and green-orange.
Line 17 then modifies the action container $as and inserts the appropriate storage action with the value for provided storage to be set.
Other rules follow the same pattern as described here and in Fig. 4 for rules increasing resource allocations and in Fig. 5 for rules decreasing resource allocations.
Evaluation and comparison
In this section we evaluate the two presented approaches with several different synthetic and real-world workload data.
For this purpose, we present a KM-agnostic simulation engine that implements the autonomic control loop and simulates executed actions and evaluates their quality responding to the workload data at stake.
Simulation engine implementing the MAPE-K loop
The goal of the simulation engine is to evaluate the quality of a KM system with respect to the number of SLA violations, the utilization of the resources and the number of required reallocation actions.
Furthermore, the simulation engine serves as an evaluation tool for any KM technique in the field of Cloud Computing, as long as it can implement the two methods of the KB management interface: 1.
public void receiveMeasurement(int slaID, String[] provided,
String[] measurements, List<String> violations); and
2.
public Actions recommendAction(int slaID);.
The parameter slaID describes the ID of the SLA that is tied to the specific VM, whose provided and measured values are stored in the arrays provided and measurements, respectively (cf.
Section 5.1).
The list violations contains all SLA parameters being violated for the current measurements.
The method receiveMeasurement inputs new data into the KB, whereas the method recommendActions outputs an action specific to the current measurement of the specified SLA.
The simulation engine traverses all parts of the MAPE-K loop as can be seen in Fig. 6 and described in Section 3.
The simulation engine is iteration based, meaning that in one iteration the MAPE-K loop is traversed exactly once.
(In reality, one iteration could last from some minutes to about an hour depending on the speed of the measurements, the length of time the decision making takes, and the duration of the execution of the actions, for example migrating a resource intensive VM to another PM.) The Monitoring component receives monitoring information from either synthetic or real-world workload from the current iteration.
It forwards the data into the Knowledge base (1).
The Knowledge base contains representations of all important objects in the Cloud and their characteristic information.
These objects are the running applications, the virtual machines, and the physical machines with the current state of their CPU power, memory, storage, etc., the corresponding SLAs with their SLOs, and information about other Clouds in the same federation.
Furthermore, the KB also has representations of the inserted measurements, and the available actions to execute (these have to be pre-defined).
Finally, the KB also contains a decision mechanism that interprets the state of available objects in order to recommend a reconfiguration action.
This mechanism can be substituted by any KM technique; as already mentioned, we used CBR and a rule-based mechanism.
The next step in the MAPE loop is the Analysis component, which queries the KB for actions to recommend (for a specific SLA id) (2); these actions are then returned to the analysis component (3).
The Planning component schedules the suggested actions, and the Execution component executes them.
The changed state configuration of the Cloud objects are automatically reflected in the KB (4).
The Monitoring and the Execution components are simulated.
This means that the monitoring data is not measured on a real system during the simulation, even though it handles input measured at a real system or synthetic workloads generated beforehand (see Sections 6.3 and 6.4).
The Execution component updates the object representation of the manipulated objects in the KB, but obviously does not actually manipulate real-world objects.
The quality of the decision making can ultimately be judged by the number of occurred SLA violations, resource wastage and the number of needed reallocation actions.
Performance indicators
The subsequent evaluations will be based on the following performance indicators: violations, utilization, actions, resource allocation efficiency (RAE), costs, and time efficiency.
Whereas the first three and the last one are rather self-explanatory, costs and RAE need a little more explanation.
So violations and actions measure (as a percentage) the amount of occurring violations/actions in relation to all possible violations/actions, and utilization the average utilization over all iterations (and over all SLA parameters, if they are not shown explicitly).
Time efficiency measures the average time that is needed to handle one VM in one iteration.
For resource allocation efficiency we want to relate violations and utilization.
The basic is idea is that RAE should equal utilization (100%-w, where w stands for wastage, see below) if no violations occur (p=0%, where p stands for penalty, see below), equal 0 if the violation rate is at 100%, and follow a linear decrease in between.
Thus, we define (7)RAE=(100-w)(100-p)100.
A more general approach also taking into account the cost of actions represents the definition of a generic cost function that maps SLA violations, resource wastage and the costs of executed actions into a monetary unit, which we want to call Cloud EUR.
First, we define a penalty function pr(p):[0,100]→R+ that defines the relationship between the percentage of violations p (as opposed to all possible violations) and the penalty for a violation of resource r.
Second, we define a function wastage wr(w):[0,100]→R+ that relates the percentage of unused resources w to the energy in terms of money that these resources unnecessarily consume.
Third, we define a cost function ar(a):[0,100]→R+ from the percentage of executed actions a (as opposed to all possible actions that could have been executed) to the energy and time costs in terms of money.
The total cost c is then defined as (8)c(p,w,c)=∑rpr(p)+wr(w)+ar(a).
We assume functions pr,wr and ar for this evaluation with pr(p)=100p, wr(w)=5w, and ar(a)=a for all r.
The intention behind choosing these functions is (i) to impose very strict fines in order to proclaim SLA adherence as top priority, (ii) to weigh resource wastage a little more than the cost of actions.
The cost function is currently not evaluated within the simulation engine, it is a value calculated after the simulation for comparison reasons.
Thus, the recommended actions do not depend on the specific functions we assumed.
However, it could be incorporated into the KB in order to adjust and learn the TTs for every resource r.
Evaluation and comparison of CBR and rules using synthetic data
To evaluate a great variety of workload data, one approach is to create them synthetically.
For this, we extended the workload generator as described in [7] to allow a categorization of data volatility.
The workload generator is intended to generate very general workloads for IaaS platforms dealing with slower developments as well as rapid changes.
For one parameter, the workload is generated as follows: The initial value of the workloads is randomly drawn from a Gaussian distribution with μ=SLO2 and σ=SLO8, where SLO represents the Service Level Objective value agreed in the SLA.
Then, an up- or down-trend is randomly drawn, as well as a duration of this trend between a pre-defined number of iterations (for our evaluation this interval of iterations equals [2,6]), both with equal probability.
For every iteration, as long as the trend lasts, the current measured value is increased or decreased (depending on the trend) by a percentage evenly drawn from the interval [iBegin,iEnd].
After the trend is over, a new trend is drawn and the iterations continue as described before.
Clearly, the values for iBegin and iEnd determine the difficulty for handling the workload.
A workload that operates with low iBegin and iEnd values exhibits only very slight changes and does not, consequently, need a lot of dynamic adaptations.
Large iEnd values, on the contrary, need the enforcement mechanisms to be very elastically tuned.
For the evaluation and comparison of CBR and the rule-based approach we defined a LOW_MEDIUM workload volatility class with iEnd=18%.
For the further evaluation of the rule-based approach we defined and tested LOW, MEDIUM, MEDIUM_HIGH and HIGH workload volatility classes (not shown here) with iEnd=10%, 50%, 75%, and 100%, respectively.
As a minimum change we set iBegin=2% for all classes.
As the crucial parameters for CBR and the rule-based approach differ, we define scenarios for both approaches separately, but still compare them to the aforementioned six performance indicators.
As resources for IaaS one can use all parameters that can be adapted on a VM.
For the evaluation we chose to take the following parameters and SLOs for CBR: storage≥1000GB,incomingbandwidth≥20Mbit/s, and the following parameters and SLOs for the rule-based approach: storage≥1000GB,incomingbandwidth≥20Mbit/s,outgoing bandwidth≥50Mbit/s,memory≥512MB, and CPU power≥100MIPS (Million Instructions Per Second).
As far as CBR is concerned, its behavior differs by the α value in Eq.
(4) (setting importance to avoiding violations or achieving high utilization), by the number of executed iterations, because of its inherent learning feature, and the initial cases.
At the beginning, we configure all 50 VMs exactly equally with 80% of the storage SLO value and two-thirds of the bandwidth SLO value provided.
Then, we execute 2, 5, 10 and 20 iterations with values for α being 0.1, 0.2, 0.3, 0.4, 0.5, 0.6 and 0.8.
We omit values 0.2 and 0.4 in the evaluation because their outcomes do not differ enough from the values shown, and all values > 0.5, because they reveal unacceptable high SLA violation rates.
Setting up the initial cases was done by choosing one representative case for each action that could be triggered.
For our evaluation the SLA parameters bandwidth and storage (even though not being tied to them in any way-we could have also named them, e.g., memory and CPU time) were taken into consideration resulting in nine possible actions "Increase/Decrease bandwidth by 10%/20%", "Increase/Decrease storage by 10%/20%", and "Do nothing".
Taking storage for example, we divide the range of distances for storage St between measured and provided resources into five parts as depicted in Fig. 7.
We choose some reasonable threshold for every action as follows: If pSt-mSt=-10 then action "Increase Storage by 20%" as this already is a violation; if pSt--mSt=+50 then action "Increase Storage by 10%" as resources are already scarce but not so problematic as in the previous case; if pSt-mSt=+100 then action "Do nothing" as resources are neither very over- nor under-provisioned; if pSt-mSt=+200 then action "Decrease Storage by 10%" as now resources are over-provisioned; and we set action "Decrease Storage by 20%" when we are over the latest threshold as then resources are extremely over-provisioned.
We choose the values for our initial cases from the center of the respective intervals.
Ultimately, for the initial case for the action, e.g., "Increase Storage by 20%" we take the just mentioned value for storage and the "Do nothing" value for bandwidth.
This leads to c=(id,0,-10,0,7.5), and because only the differences between the values matter, it is equivalent to, e.g., c=(id,200,190,7.5,15.0).
As far as the rule-based approach is concerned, its behavior differs by the set threat thresholds.
Thus, we investigate low, middle and high values for TTlowr and TThighr (as defined in Section 5.3), where TTlowr∈{30%,50%,70%} and TThighr∈{60%,75%,90%} for all resources stated above.
We combine the TTs to form eight different scenarios as depicted in Table 4.
We execute 100 iterations with 500 applications, and set the "safety slack" ϵ=5% (cf.
Listing 1).
Fig. 8 presents the aforementioned performance indicators of CBR.
The "No CBR" line means that the autonomic manager is turned off, which implies that the configuration of the VMs is left as set at the beginning, i.e., no adaptation actions due to changing demands are executed.
In Fig. 8(a) we see that up to more than half of the violations can be avoided when using α∈{0.1,0.3} instead of no autonomic management.
However, fewer SLA violations result in lower resource utilization (cf.
Fig. 8(b)), as more resources have to be provided than can actually be utilized.
Reconfiguration actions as depicted in Fig. 8(c) lie slightly below or at 50%, except for "No CBR", of course.
Another point that can be observed is that after a certain amount of iterations the quality of the recommended actions decreases.
This is probably due to the fact that the initial cases get more and more blurred when more cases are stored into CBR, as all new cases are being learned and there is no distinction made between "interesting" and "uninteresting" cases.
Nevertheless, when we relate SLA violations and resource utilization in terms of RAE, all CBR methods are generally better than the default method, especially for α∈{0.3,0.5} after five iterations.
Yet, RAE decreases strictly monotonically for all α.
Furthermore, costs-relating violations, utilization and reconfiguration actions-can also be reduced to half for α∈{0.1,0.3}.
However, there is a seemingly exponential increase in the average execution time per VM (cf.
Fig. 8(f)) due to higher number of cases stored in the KB.
Summing up, the simulation shows that learning did take place (and cost some time) and that CBR is able to recommend right actions for many cases, i.e., to correctly handle and interpret the measurement information that is based on a random distribution not known to CBR.
Fig. 9 shows the same evaluation for the rule-based approach evaluating the aforementioned eight scenarios.
From Fig. 9(a) we learn that in terms of SLA violations Scenario 1 achieves the best result, where only 0.0908% of all possible violations occur, and Scenario 8 yields the worst result, with a still very low violation rate of 1.2040%.
In general, the higher the values are for TThigh, the worse is the outcome.
The best result achieved with CBR was at 7.5%.
Thus, the rule-based approach achieves an up to 82 times better performance with the right TTs set, and still a six times better performance in the worst case.
Fig. 9(b) shows resource utilization.
We see that the combination of high TTlow and high TThigh (Scenario 8) gives the best utilization (84.0%), whereas low values for TTlow and TThigh lead to the worst utilization (62.0% in Scenario 1).
Still, compared to CBR which scored a maximum of 80.4% and a minimum of 51.8%, the rule-based approach generally achieves better results.
The percentage of all executed actions as compared to all possible actions that could have been executed is shown in Fig. 9(c).
One observes that the greater the span between TTlow and TThigh is, the fewer actions have to be executed.
Most actions (60.8%) are executed for Scenario 7 (span of only 5% between TT values), whereas least actions (5.5%) are executed for Scenario 3 (span of 60% between TT values).
CBR almost always recommended exactly one (out of two possible) actions and hardly ever (in about 1% of the cases) recommended no action.
As violations are very low in general, the resource allocation efficiency is very similar to the utilization.
The best value can be achieved with Scenario 8 (84.0%), the worst with Scenario 1 (62.0%).
CBR achieves a RAE of at most 69.7% (α=0.5 at iteration 2), and at least 45.5% (α=0.1 at iteration 20).
Fig. 8(e) shows the costs for each scenario using Eq.
(8).
The best trade-off between the three terms is achieved by Scenario 5 that has medium values for TTlowr and TThighr.
It has a very low violation rate of 0.0916%, a quite elaborate utilization of 72.9%, but achieves this with only 19.8% of actions.
Scenario 7 achieves a better violation and utilization rate but at the cost of an action rate of 60.8%, and consequently has higher costs.
The lowest cost value for CBR is 923.0 Cloud EUR, the highest 2985.3 Cloud EUR.
If the utility of the decision decreases for a certain time frame (as cost increases), the KB could determine the cost summand in Eq.
(8) that contributes most to this decrease.
For any resource r, if the term is p, then decrease TThighr.
If the term is w, then increase TTlowr.
Otherwise, if the term is c, then widen the span of TThighr and TTlowr, i.e., increase TThighr and decrease TTlowr.
We plan to investigate this in our future research.
As far as time performance and scalability are concerned, the performance tests are very encouraging.
We executed 100 iterations from 100 to 3000 VMs.
We performed every test twice and calculated the average execution time as well as the average time it took for the simulation engine to handle one VM.
As shown in Fig. 9(f) the execution time per VM stays quite constant for up to 1500 VMs, and thus average execution time is about linear.
For 3000 VMs, it took 647s/100=6.47s for one iteration to treat all VMs.
The high time consumption per VM for 100 VMs in Fig. 9(f) is due to the initialization of the rule knowledge base which takes over-proportionally long for just a small number of VMs and does not weigh so much for more VMs.
CBR took 240 s for 50 VMs and 20 iterations.
Thus, CBR took 240s/20=12s for one iteration to treat all VMs, which is twice as long as the rule-based approach takes, which even has 60 times more VMs.
However, CBR implements learning features, which the rule-based approach currently does not, and could be sped up by choosing only specific cases to be stored in the KB.
Summarizing, the rule-based approach highly outperforms CBR with respect to violations (up to 82 times better results), actions, cost, and time performance.
The rule-based approach also achieves better "best case" and better "worst case" results for the remaining performance indicators utilization and resource allocations efficiency.
In more detail, seven out of eight scenarios were better than the worst CBR value for utilization, whereas only one scenario was better than the best CBR utilization value.
Again, accumulating these results into cost, all rule-based scenarios outperform CBR by a factor of at least 4 (worst rule-based scenario (236) compared to the best CBR result (923)), which to a large extent is due to the huge number of violations that the rule-based approach is able to prevent and the high number of actions it can save.
Consequently, we consider the rule-based approach as the better technique to deal with VM reconfiguration in Cloud Computing infrastructures, and we will focus the remaining part of this article on a deeper investigation and understanding of the rule-based approach by evaluating it with real world workload.
A deeper investigation of synthetic workload also suggests the self-adaptation of the TTs from the rule-based approach.
A successful self-adaptation has been presented in [42].
Applying and evaluating a bioinformatics workflow to the rule-based approach
As detailed in [43,44], bioinformatics workflows have gained a great need for large-scale data analysis.
Due to the fact that these scientific workflows are very resource intensive and can take hours if not days to complete, provisioning them in an environment with fixed resources leads to poor performance.
On the one hand, the workflow might run out of resources and thus may have to be restarted on a larger system.
On the other hand, too many resources might be provisioned in order not to take risks of a premature abort, which may cause a lot of resources to be wasted.
Thus, Cloud computing infrastructures offer a promising way to host these sorts of applications [10].
The monitoring data presented in this Section was gathered with the help of the Cloud monitoring framework Lom2His [3].
Using Lom2His we measured utilized resources of TopHat [45], a typical bioinformatics workflow application analyzing RNA-Seq data [46], for a duration of about three hours [9].
In the following we briefly describe the bioinformatics workflow in more detail.
We here consider Next Generation Sequencing (NGS), a recently introduced high-throughput technology for the identification of nucleotide molecules like RNA or DNA in biomedical samples.
The output of the sequencing process is a list of billions of character sequences called 'reads', each typically holds up to 35-200 letters that represent the individual DNA bases determined.
Lately, this technology has also been used to identify and count the abundances of RNA molecules that reflect new gene activity.
We use the approach, called RNA-Seq, as a typical example of a scientific workflow application in the field of bioinformatics.
At first, in the analysis of RNA-Seq data, the obtained sequences are aligned to the reference genome.
The aligner presented here, TopHat [45], consists of many sub-tasks, some of them have to be executed sequentially, whereas others can run in parallel (Fig. 10).
These sub-tasks can have different resource-demand characteristics: needing extensive computational power, demanding high I/O access, or requiring extensive memory size.
In Fig. 10, the green boxes represent simplified sub-tasks of the workflow application, whereas the blue boxes represent the data transferred between the sub-tasks.
The first sub-task aligns input reads to the given genome using the Bowtie program [47].
Unaligned reads are then divided into shorter sub-sequences which are further aligned to the reference genome in the next sub-task.
If sub-sequences coming from the same read were aligned successfully to the genome, that may indicate that this read was straddling a 'gap' in the gene, falling on a so-called splice-junction.
After verification of candidate reads falling on splice junctions, these and the reads that were aligned in the first sub-task are combined to create an output with a comprehensive list of localized alignments.
We demonstrate by simulation that the rule-based approach can guarantee the resource requirements in terms of CPU, memory and storage for the execution of the workflow in a resource-efficient way.
Therefore, we define the SLA shown in Table 5 for TopHat with the maximum amount of available resources on the physical machine on which we are executing it.
The physical machine has a Linux/Ubuntu OS with a Intel Xeon(R) 3 GHz CPU, two cores, 9 GB of memory, and 19 GB of storage.
For CPU power, we convert CPU utilization into MIPS based on the assumption that an Intel Xeon(R) 3 GHz processor delivers 10000 MIPS for 100% resource utilization of one core, and linearly degrades with CPU utilization.
In order to validate our approach, we make three simulation categories, where we set up and manage our VMs differently: In the first category (Scenario 1) we assume a static configuration with a fixed initial resource configuration of the VMs.
Normally, when setting up such a testbed as described in [9], an initial guess of possible resource consumption is done based on early monitoring data.
From this data on, we assume quite generous resource limits.
The first ten measurements of CPU, memory, and storage lie in the range of [140, 12500] MIPS, [172, 1154] MB, [15.6, 15.7] GB, respectively.
So we initially configured our VM with 15000 MIPS, 4096 MB, and 17.1 GB, respectively.
The second category subsumes several scenarios, where we apply our autonomic management approach to the initial configuration in the first category.
The eight scenarios in this category depend on the chosen TTs.
According to Table 4 we define these scenarios as Scenario 2.1, 2.2,…,2.8, respectively.
As the third category (Scenario 3), we consider a best case scenario, where we assume we have an oracle that predicts the maximal resource consumption that we statically set our VM configuration to.
Moreover, according to the first measurements we decide to enforce a minimum of 1 MIPS CPU, 768 MB memory, and 1 GB storage.
As depicted in Fig. 11(a)-(c) one sees violations, utilization, as well as the number of reconfiguration actions, respectively, for every parameter (together with an average value) in the different scenarios.
Generally, the bars are naturally ordered beginning from Scenario 1, over Scenarios 2.1,…,2.8, ending with Scenario 3.
The number of violations in Scenario 1 reach 41.7% for CPU and memory, and 49.4% for storage, which leads to an average of 44.3%.
(For better visibility, these results have been excluded from Fig. 11(a).) Thus, we experience violations in almost half of the cases.
This is especially crucial for parameters memory and storage, where program execution could fail, if it runs out of memory or storage, whereas for a violation of the parameter CPU, we would "only" delay the successful termination of the workflow.
With Scenarios 2.* we can reduce the SLA violations to a minimum.
We completely avoid violations for storage in all sub-scenarios, as well as for memory in all but one sub-scenarios.
Also CPU violations can be reduced to 0.6% for sub-scenarios 2.1 and 2.4, and still achieve a maximum SLA violation rate of 2.8% with Scenario 2.8.
The average SLA violation rate can be lowered to 0.2% in the best case.
Scenario 3, of course, shows no violations.
However, it is unlikely to know the maximum resource consumption before workflow execution.
As to the utilization of the resources, it is clearly higher when a lot of violations occur, so Scenario 1 naturally achieves high utilization.
This is the case, because when a parameter is violated, then the resource is already fully used up, but even more of the resource would be needed to fulfill the needs.
On the opposite, Scenario 3 naturally achieves low utilization, as a lot of resources are over-provisioned.
Scenarios 2.* achieve a good utilization that is on average in between the two extremes and ranges from 70.6% (Scenario 2.1) to 86.2% (Scenario 2.8).
Furthermore, we observe some exceptions to this "rule" when considering individual parameters.
So, e.g., for memory we achieve a utilization of 85.0% with Scenario 2.8 or 80.0% with Scenario 2.6, which is higher than the utilization in Scenario 1 (77.4%).
The same is true for CPU utilization rates of 85.5% as compared to 84.3% for the Scenario 1 and 2.8, respectively.
Only for storage the utilization of all but one of the Scenarios 2.*, which is at 85.9%, is smaller than for Scenario 3 (90.1%).
When it comes to the overall costs of the scenarios (cf.
Fig. 12(a)), all 2.* scenarios approach the result achieved by the best case scenario 3.
Scenario 1 sums up costs of 4493.6, and has therefore been omitted in the figure.
Furthermore, the lowest cost is achieved using Scenario 2.6, which is even lower than the cost for Scenario 3.
This is possible, because Scenario 2.6 achieves a very good utilization and SLA violation rate with a very low number of reallocation actions.
Also resource allocation efficiency for Scenarios 2.* as shown in Fig. 12(b) achieves unambiguously better results than for Scenario 1 (RAE of 48.2%).
Furthermore, all scenarios of the second category achieve a better RAE than the RAE of Scenario 3 (69.3%).
Thus, we conclude that by using the suggested autonomic management technique, we can avoid most costly SLA violations, and thus ensure workflow execution, together with a focus on resource-efficient usage.
All this can be achieved by a very low number of time- and energy-consuming VM reallocation actions for many of the autonomic management scenarios.
Conclusion
The goal of this research field is to enact SLAs in a resource-efficient way with little human-based interaction in order to guarantee the scalability and strengthen the dynamic behavior and adaptation of the system.
Autonomically governing Cloud Computing infrastructures is the investigated method leading to this goal.
In this paper we have hierarchically structured all possible reallocation actions, and designed, implemented, and evaluated two knowledge management techniques, Case Based Reasoning and a rule-based approach to achieve the aforementioned goal for one reallocation level, i.e., VM reconfiguration.
After a comparison, we determined the rule-based approach to outperform CBR with respect to violations and utilization, but also to time performance.
Furthermore, we applied the rule-based approach to a real-world use case evaluating a scientific workflow from the area of bioinformatics.
We showed by simulation that the rule-based approach can effectively guarantee the execution of a workload with unpredictably large resource consumptions.
The next step will be to move from simulation to a real Cloud testbed.
Furthermore, the presented methods still involve some user-interaction for parameter tuning.
Thus, it will be of great interest to autonomically determine crucial parameters of the presented methods and to adapt them based on current performance.
Another related field is the autonomic generation of IaaS SLA out of SaaS or PaaS SLAs.
Theoretically, SaaS or PaaS applications can be perfectly set up on top of IaaS platforms.
The crucial point is to extract an SLA for the IaaS parameters like bandwidth, storage, CPU power and memory that fit to SaaS/PaaS parameters like response time.
It is obvious that response time directly relates to the mentioned IaaS parameters and user interaction.
It is not that obvious, however, how this translation should take place.
E.g., does the SLO " response time<2 s" translate into " memory>512 MB" and " CPU power>8000 MIPS" or rather " memory>4096 MB" and " CPU power>1000 MIPS"? Once the autonomic governance of IaaS infrastructures is up and running, the autonomic translation of these SLAs will probably leverage the usage and usability of IaaS even more.
Acknowledgments
The work described in this paper is supported by the Vienna Science and Technology Fund (WWTF) under grant agreement ICT08-018 Foundations of Self-Governing ICT Infrastructures (FoSII) and by COST-Action IC0804 on Energy Efficiency in Large Scale Distributed Systems.
We also want to thank Paweł P. Łabaj and David P. Kreil (Boku University Vienna) for providing us with the bioinformatics workflow application and Vincent C.
Emeakaroha (TU Vienna) for providing monitoring data on it.

What is claimed is:
1. A production method for producing transition metal composite hydroxide particles by a crystallization reaction to be a precursor for a cathode active material for a non-aqueous electrolyte rechargeable battery, comprising:
a nucleation process for performing nucleation by controlling an aqueous solution for nucleation that includes a metal compound that includes at least a transition metal and an ammonium ion donor so that the pH value at a standard liquid temperature of 25° C. becomes 12.0 to 14.0; and
a particle growth process for causing nuclei to grow by controlling an aqueous solution for particle growth that includes the nuclei that were obtained in the nucleation process so that the pH value is less than in the nucleation process and is 10.5 to 12.0;
the reaction atmosphere in the nucleation process and at the beginning of the particle growth process being a non-oxidizing atmosphere in which an oxygen concentration is 5% by volume or less; and
in the particle growth process, atmosphere control by which the reaction atmosphere is switched from the non-oxidizing atmosphere to an oxidizing atmosphere in which the oxygen concentration is greater than 5% by volume at timing from the start of the particle growth process within a range of 5% to 35% of the overall particle growth process time, and is then switched from the oxidizing atmosphere to a non-oxidizing atmosphere in which the oxygen concentration is 5% by volume or less so that the crystallization time in the oxidizing atmosphere in the particle growth process is 3% to 20% of the overall particle growth process time being performed at least one time.2. A production method for producing transition metal composite hydroxide particles by a crystallization reaction to be a precursor for a cathode active material for a non-aqueous electrolyte rechargeable battery, comprising:
a nucleation process for performing nucleation by controlling an aqueous solution for nucleation that includes a metal compound that includes at least a transition metal and an ammonium ion donor so that the pH value at a standard liquid temperature of 25° C. becomes 12.0 to 14.0; and
a particle growth process for causing nuclei to grow by controlling an aqueous solution for particle growth that includes the nuclei that were obtained in the nucleation process so that the pH value is less than in the nucleation process and is 10.5 to 12.0;
the reaction atmosphere in the nucleation process and at the beginning of the particle growth process being a non-oxidizing atmosphere in which the oxygen concentration is 5% by volume or less;
in the particle growth process, atmosphere control by which the reaction atmosphere is switched from the non-oxidizing atmosphere to an oxidizing atmosphere in which the oxygen concentration is greater than 5% by volume, and is then switched from that oxidizing atmosphere to a non-oxidizing atmosphere in which the oxygen concentration is 5% by volume or less being performed two times or more; and
the total crystallization reaction time in the oxidizing atmosphere in the particle growth process being 3% to 30% of the total particle growth process time, and the crystallization reaction time during each oxidizing atmosphere being 1% or more of the total particle growth process time.3. The production method for producing transition metal composite hydroxide particles according to claim 2, wherein in the particle growth process, the reaction atmosphere is switched from the non-oxidizing atmosphere to the oxidizing atmosphere at timing from the start of the particle growth process within a range of 5% to 35% of the overall particle growth process time.4. The production method for producing transition metal composite hydroxide particles according to claim 1, wherein the transition metal composite hydroxide particles are transition metal composite hydroxide particles that are expressed by the general expression (A): NixMnyCozMt(OH)2+a, where x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, 0≤a≤0.5, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W.5. The production method for producing transition metal composite hydroxide particles according to claim 4, further comprising a coating process for coating the transition metal composite hydroxide particles with a compound that includes the additional element M after the particle growth process.6. Transition metal composite hydroxide particles that are the precursor for cathode active material for a non-aqueous electrolyte rechargeable battery, comprising secondary particles that are formed by an aggregation of plate-shaped primary particles and fine primary particles that are smaller than the plate-shaped primary particles;
the secondary particles having a center section that is formed by an aggregation of the plate-shaped primary particles, and one layered structure of a low-density section that is formed by an aggregation of the fine primary particles and a high-density section that is formed by an aggregation of the plate-shaped primary particles on the outside of the center section;
the average value of the ratio of the center section outer diameter with respect to the particle size of the secondary particles being 30% to 80%, and the average value of the high-density section radial direction thickness with respect to the particle size of the secondary particles being 5% to 25%; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90−d10)/average particle size] that indicates the extent of the particle size distribution of 0.65 or less.7. Transition metal composite hydroxide particles that are the precursor for cathode active material for a non-aqueous electrolyte rechargeable battery, comprising secondary particles that are formed by an aggregation of plate-shaped primary particles and fine primary particles that are smaller than the plate-shaped primary particles;
the secondary particles having a center section that is formed by an aggregation of plate-shaped primary particles, and two or more layered structure of a low-density section that is formed by an aggregation of the fine primary particles and a high-density section that is formed by an aggregation of the plate-shaped primary particles on the outside of the center section; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90−d10)/average particle size] that indicates the extent of the particle size distribution of 0.65 or less.8. The transition metal composite hydroxide particles according to claim 7, wherein the average value of the ratio of the center section outer diameter with respect to the particle size of the secondary particles is 20% to 70%, and the average value of the high-density section radial direction thickness per layer with respect to the particle size of the secondary particles is 5% to 25%.9. The transition metal composite hydroxide particles according to claim 6, wherein the transition metal composite hydroxide particles are transition metal composite hydroxide particles that are expressed by the general expression (A): NixMnyCozMt(OH)2+a, where, x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, 0≤a≤0.5, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W).10. The transition metal composite hydroxide particles according to claim 9, wherein the additional element is uniformly distributed inside the secondary particles and/or coated on the surface of the secondary particles.11. A production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery comprising:
a mixing process for forming a lithium mixture by mixing the transition metal composite hydroxide particles according to claim 6 with a lithium compound; and
a calcination process for performing calcination of the lithium mixture formed in the mixing process at a temperature of 650° C. to 980° C. in an oxidizing atmosphere.12. The production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 11, wherein in the mixing process the lithium mixture is adjusted so that the ratio of the sum of the number of atoms of metals other than lithium included in the lithium mixture, and the number of atoms of lithium is 1:0.95 to 1.5.13. The production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 12, further comprising a heat treatment process for heat treating the transition metal composite hydroxide particles at 105° C. to 750° C. before the mixing process.14. The production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 11, wherein the cathode active material comprises layered hexagonal crystal lithium nickel manganese composite oxide particles that are expressed by the general expression (B): Li1+uNixMnyCozMtO2, where −0.05≤u≤0.50, x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W.Synthesis of zeolite NaX at 25degC and 95degC: Characterization, cobalt exchange and catalytic performance in epoxidation of styrene
The synthesis of zeolite NaX was performed by the following: sodium silicate (Na2SiO3[?]9H2O > 98%) was completely dissolved in deionized water to get solution A; sodium hydroxide (99%) was dissolved in deionized water, followed by addition of sodium aluminate (anhydrous) to get solution B. Solution A was poured into solution B under vigorous stirring, the molar composition of the resultant hydrogel was 4.0Na2O:0.2 Al2O3:1.0SiO2:200H2O. The hydrogel was stirred for 30 min and then transferred to the polypropylene bottles, sealed and placed in water bath (T = 25 degC) and oil bath (T = 95 degC). After a given period of synthesis, the solid products were filtered off, washed with deionized water until the pH reached around 8, then dried in air at 100 degC for 24 h.

Co2+ ions were introduced into zeolite NaX by a simple ion exchange method. In a typical run, 5.0 g of zeolite was treated with 0.3 M aqueous solution of the cobalt nitrate (150 ml) at 300 K for 24 h. The solid powder was recovered by filtration, washed several times with deionized water until no acidic residue was detected, and followed by drying at 40 degC in vacuum for 48 h. Zeolite NaX samples with varying amounts of cobalt exchange were prepared by repeated ion exchanges of the zeolite.

The epoxidation of styrene with O2 was performed in a 50 ml three-necked flat-bottomed glass flask equipped with a liquid condenser under atmospheric pressure. Typically, 10 mmol of styrene (>99%), a measured amount of solvent N,N'-dimethylformamide (99.5%, DMF) and a certain amount of catalyst were added to the reactor. The reaction mixture was magnetically stirred at 600 rpm and heated to the desired temperature. The reaction was started by bubbling O2 into the liquid.

1. Pulverulent compound of the formula LiaNibM1cM2d(O)2(SO4)x, in which M1 denotes at least one element selected from the group consisting of Fe, Co, Cr, Mg, Zn, Cu and/or mixtures thereof, M2 denotes at least one element selected from the group consisting of Mn, Al, B, Ca, Sr, Ba, Si and/or mixtures thereof, and 0.95 ≤ a ≤ 1.1, 0.3 ≤ b ≤ 0.83, 0.1 ≤ c ≤ 0.5, 0.03 ≤ d ≤ 0.5 and 0.001 ≤ x ≤ 0.03,characterized in thatsecondary particles of the pulverulent compound have a compressive strength of at least 200 MPa, said secondary particles being composed of a multiplicity of primary particles.2. Pulverulent compound according to Claim 1,characterized in thatit has a compressive strength of at least 300 MPa.3. Pulverulent compound according to either claim 1 or claim 2,characterized by:
a) a porosity of up to 0.01 cm3/g measured according to ASTM D 4222; or
b) a porosity of up to 0.008 cm3/g measured according to ASTM D 4222; or
c) a porosity of up to 0.006 cm3/g measured according to ASTM D 4222.4. Pulverulent compound according to at least one of Claims 1 to 3,characterized in thatthe secondary particles have a spheroidal shape, and, optionally,in thateither:
a) the secondary particles thereof have a shape factor greater than 0.8; or
b) the secondary particles thereof have a shape factor greater than 0.9.5. Pulverulent compound according to at least one of Claims 1 to 4,characterized in that:
a) the D10 value, measured according to ASTM B 822, after compression of the material at a pressure of 200 MPa changes by not more than 0.5 µm compared with the starting material; or
b) the D90 value, measured according to ASTM B 822, after compression of the material at a pressure of 200 MPa changes by not more than 1 µm compared with the starting material.6. Pulverulent compound according to at least one of Claims 1 to 5,characterized in that:
a) the normalized width of the particle size distribution, measured according to the Formula (1)
D90−D10D50
in which D denotes the diameter of the secondary particles, is less than 1.4; or
b)the normalized width of the particle size distribution, measured according to the Formula (1)
D90−D10D50
in which D denotes the diameter of the secondary particles, is less than 1.2.7. Pulverulent compound according to at least one of Claims 1 to 6,characterized in thatit has a compressed density of at least 3.2 g/cm3at a compression pressure of 200 MPa.8. Pulverulent compound according to at least one of Claims 1 to 7,characterized in that:
a) it has a tapped density measured according to ASTM B 527, of at least 2.2 g/cm3; or
b) it has a tapped density measured according to ASTM B 527, of at least 2.4 g/cm3.9. Process for the preparation of the pulverulent compound according to at least one of Claims 1-8, comprising the following steps:
a. provision of a co-precipitated nickel-containing precursor having a porosity of less than 0.05 cm3/g, measured according to ASTM D 4222,
b. mixing the precursor according to a) with a lithium-containing component,
c. calcination of the mixture with multistage heating to temperatures of 1000°C with the use of a CO2-free (≤ 0.5 ppm of CO2), oxygen-containing carrier gas and production of a pulverulent product,
d. deagglomeration of the powder by means of ultrasound and sieving of the deagglomerated powder.10. Process according to Claim 9,characterized in that:
a) the nickel-containing component is a mixed oxide, mixed hydroxide, mixed oxyhydroxide, partially oxidized mixed hydroxide, partially oxidized mixed hydroxysulphate of the metals Ni, Co, Mn, Al, Fe, Cr, Mg, Zr, B, Zn, Cu, Ca, Sr, Ba and/or mixtures thereof; or
b) the lithium-containing component is lithium carbonate, lithium hydroxide, lithium hydroxide monohydrate, lithium oxide, lithium nitrate and/or mixtures thereof.11. Process according to either claim 9 or claim 10,characterized in that:
a) the calcination of the precursor mixture is effected at a temperature of 200-400°C for 2-10 hours in the first stage, at 500-700°C for 2-10 hours in the second stage and at 700-1000°C for 2-20 hours in the third stage; or
b) the calcination of the precursor mixture is effected at a temperature of 250-350°C for 2-10 hours in the first stage, at 550-650°C for 2-10 hours in the second stage and at 725-975°C for 2-20 hours in the third stage; or
c) the calcination of the precursor mixture is effected at a temperature of 250-435°C for 4-8 hours in the first stage, at 550-650°C for 4-8 hours in the second stage and at 725-975°C for 5-15 hours in the third stage.12. Process according to at least one of Claims 9 to 11,characterized in that:
a) the carrier gas contains 20 to 100% by volume of oxygen; or
b) the carrier gas contains 40 to 100% by volume of oxygen.13. Process according to at least one of Claims 9 to 12,characterized in thatthe reaction of the nickel-containing precursor takes place with retention of the shape of the secondary particles and/or particle size distribution.14. Pulverulent compounds obtainable according to at least one of Claims 9 to 13.15. Use of the pulverulent compound according to at least one of Claims 1 to 8 or 14 as electrode material in lithium secondary batteries.Layered sodium titanate nanostructures as a new electrode for high energy density supercapacitors

Analytical reagent grade NaOH (R&M Chemicals, India), titanium tetra isopropoxide (TTIP) (Acros Organic, Germany) and methanol were the starting materials. Sodium titanate nanoflowers (STNFs) were synthesized via hydrothermal reaction between NaOH and a mixture of TTIP and methanol. In a typical synthesis, methanol (50 ml) was added to TTIP (3.0 g) and then heated up to ~67 degC with stirring. The above solution was hydrolyzed using distilled water (0.72 g) and subsequently heated for 30 min. Upon completion of the hydrolysis, NaOH (20 ml, 5 wt%) solution was added to the mixture which was then transferred to a Teflon-lined autoclave. The hydrothermal reaction was carried out at 80 degC (STNF80C) and 120 degC (STNF120C) for 24 h. Upon completion of the hydrothermal reaction, the precipitate was filtered and washed with distilled water until pH ~9-10 was attained. For Na free TiO2, the STNF80C and the STNF120C samples were washed with 0.1 M HCl followed by distilled water until pH ~4 was obtained and subsequently dried at 70 degC for 1 h. The dried sample powder was then calcined at 500 degC for 3 h.
Synthesis and characterization of NiV3O8 powder as cathode material for lithium-ion batteries
In this work, all the chemical reagents mentioned are of analytical grade and used without further purification.VO2 (B) precursor was firstly prepared by a hydrothermal method. In a typical synthesis, commercial V2O5 powder (0.015 mol), oxalic acid (0.015 mol) and distilled water (65 mL) were mixed under vigorous magnetic stirring at room temperature and kept for 2 h, and then the resultant mixture was transferred into an 80 mL stainless autoclave. The autoclave was sealed and heated at 180 degC for 30 h, and then cooled to room temperature. The product was collected and washed by distilled water and absolute ethanol for several times. The produced VO2 precursor was dried at 80 degC in vacuum for 12 h. After that, NiO, V2O5 and VO2 in stoichiometric amounts (1:1:1 in molar ratio) were mixed and wet milled in a planetary-type ball mill for 3 h under the protection of nitrogen. The milling speed was set at 200 rpm. Then the mixture was collected and dried in vacuum at 80 degC for 10 h. The mixture, after grinding, was heated up to 600 degC for 30 h under Ar flux, and then cooled down with the furnace to the room temperature.Synthesis, characterization of mesoporous silica powders and application to antibiotic remotion from aqueous solution. Effect of supported Fe-oxide on the SiO2 adsorption properties
Cetyltrimethylammonium p-toluene sulfonate or tosylate (CTAT, MW = 455.7 gmol- 1), Pluronic F68 and tetraethyl orthosilicate (TEOS, 99%) were purchased from Aldrich. Iron(III) nitrate nonahydrate, potassium hydroxide, potassium chloride, potassium nitrate, nitric acid, hydrochloric acid, sodium acetate, acetic acid, sodium carbonate, sodium hydrogen carbonate, disodium phosphate anhydrous, and monosodium phosphate anhydrous were obtained from Anedra.
All chemicals were of analytical grade and used as received. Doubly distilled water was used for the preparation of solutions.
Mesoporous silica was prepared using a procedure similar to that described in an earlier work [12]. Briefly, 11.6 mL of TEOS were mixed with 2 mL of water and stirred in an autoclave flask for 10 min at 500 rpm. At the same time, 38 mL of CTAT-Pluronic F68 mixed solution were prepared with a 0.75:0.25 M ratio by adding the desired amount of surfactants to water. This mixture was stirred in a conical flask at 35 degC to form a transparent template solution and then it was left at room temperature. To obtain the mesoporous material, 20 mL of a 1.43 M HCl solution were added drop by drop to the TEOS solution under stirring and 2 min later the surfactant solution was incorporated. The resulting gel, whose composition was 1 TEOS:0.53 HCl:0.011 CTAT:0.0037 F68, was stirred for 5 min and then left for 48 h in an autoclave at 100 degC. After this, the gel was filtered and washed with distilled water and dried at room temperature. Finally, it was calcined in an air flux by increasing the temperature from room temperature to 540 degC with a heating rate of 2degCmin- 1, and holding for 7 h at 540 degC.
Fe-SiO2 was synthesized by a simple batch equilibrium adsorption method. In a beaker, 2 g of calcined SiO2 were mixed with 40 mL of a 0.13 M Fe(NO3)3 solution and stirred for 1 h at 600 rpm. Then, the solid was filtered and washed with water and dried at room temperature. Finally, it was calcined in an air flux for 2 h at 540 degC. The iron content on the silica support was 10.3 mgg- 1 (1.03 wt.%), which was measured by UV-VIS spectroscopy using the thiocyanate colorimetric method [23] after extracting the Fe(III) ions from the solid with concentrated HNO3[24].Fabrication of conductive polymer-coated sulfur composite cathode materials based on layer-by-layer assembly for rechargeable lithium-sulfur batteries
Polyethyleneimine (PEI, Mw 50-100 kDa), poly(allylamine hydrochloride) (PAH) (Mw 70 kDa), poly(styrenesulfonate sodium salt) (PSS) (Mw 70 kDa), gultaraldehyde (GA) and the water-soluble polypyrrole (PPY) (5 wt% in water) doped with organic acid were purchased from Sigma-Aldrich. The aniline monomer was used with further purification by decompressed distillation and then stored at 0-5 degC. The water used in all experiments was prepared in a three-stage Millipore Milli-Q Plus 185 purification system and had a resistivity higher than 18.2 MΩ cm.
Sulfur particles were separately dispersed in PEI solution (1 mg/mL in 0.5 M NaCl) and allowed to adsorb for 30 min. Then excess polyelectrolytes were removed by centrifugation and three times washing with 0.5 M NaCl. Subsequently the above suspensions were alternately dispersed in PAH and PSS solution (2 mg/mL in 0.5 M NaCl) for another 30 min, followed by three times washing in 0.5 M NaCl. After the assembly of a desired number of PAH/PSS layer, the coated particles were then dispersed in 5% glutaraldehyde (GA) aqueous solution for 30 min. Finally these sample suspensions were incubated at the high temperature for 20 min.
In a typical procedure, the prepared polymer-coated sulfur particles aqueous dispersion was added into the aniline monomer solution mixed with 1 M hydrochloric acid (HCl) under stirring. After 30 min, the equivalent number of moles of 22.8 wt% aqueous ammonium persulphate (APS) solution (with respect to aniline) was slowly added, followed by an oxidative polymerization at 0-3 degC. The reaction was carried out for 24 h. The resultant green solid, PANI/polymer-coated sulfur particles, were obtained by centrifugation and washed with water and ethanol thoroughly to remove excess ions and monomers. The procedure was repeated to increase the PANI content. The final product was dried under vacuum at ambient temperature for 24 h. The whole experiments were performed under the nitrogen atmosphere. The conductive polypyrrole (PPY) shells were prepared through a electrostatic interaction of the negatively charged PPY on the positively charged outer shell of (PAH/PSS)n/PAH-coated sulfur particles. The resultant gray solid was dried for further characterization and testing.Temperature effect on spinel Li4Ti5O12 as anode materials for lithium ion batteries

All chemicals were used as received. The stoichiometric amounts of lithium hydroxide monohydrate (LiOH*H2O) and tetrabutyl titanate (Ti(OC4H9)4) were used as starting materials. 20 mL of 2 mol/L LiOH aqueous solution was obtained at room temperature with stirring and marked solution A. 17 mL of tetrabutyl titanate was mixed in ethyl alcohol (1:1 in volume) with strong stirring and marked solution B. After that, the solution A was dropwise added into the solution B to form a white suspension with strong stirring for 1 h and then transferred to stainless-steel autoclave at 180 degC for 24 h. The obtained precursor was calcinated at different temperatures from 450 degC to 850 degC to get the products. The X-ray diffraction (XRD, Rigaku D/max-2000) and scanning electron microscopy (SEM, S-4800) were used to characterize the crystal structure and observe the morphology of the powders.


Synthesis and characterization of n=5, 6 members of the La4Srn-4TinO3n+2 series with layered structure based upon perovskite
The samples were prepared by traditional solid-state reaction. Stoichiometric amounts of pre-dried high-purity La2O3, SrCO3 and TiO2 (Aldrich) were mixed and ground and then calcined at 1200degC for 6 h. The resulting powder was subsequently ground, mixed and uniaxially pressed into pellets, which were deposited on alumina crucibles containing sacrificial powder and then they were fired at 1400-1600degC for 3-9 days with intermediate processes of grinding, mixing and "pelletizing". XRD data were collected with a Stoe StadiP X-ray diffractometer, using CuKα1 radiation in the range 2θ=5-70deg in steps of 0.1deg at 300 s/step. HRTEM imaging and SAED were carried out using a JEOL-JEM 2011 electron microscope operating at 200 kV and equipped with a side entry +-20deg double tilt specimen holder. EDS microanalyses were performed using an Oxford Link ISIS system in at least 25 different crystals to confirm compositional homogeneity.SnO2 nanospheres supported Pd catalyst with enhanced performance for formic acid oxidation

K2SnO3*3H2O (99.9%), glucose (99%) and Na2PdCl4 (Pd 36.4%) were purchased from Aladdin Chemistry Co., Ltd. Vulcan XC-72 carbon black was purchased from Cabot Corporation. NaBH4 (98%) were purchased from Sinopharm Chemical Reagent Co., Ltd. Pd/C (20% Pd on carbon, Johnson Matthey) was purchased from Alfa Aesar. Nafion (5 wt.%) was purchased from Sigam-Aldrich. Water used throughout all experiments was purified with the Millipore system.

The SnO2 nanospheres were synthesized by hydrothermal reaction of K2SnO3 in an aqueous glucose solution followed by calcination at 400 degC in air [18]. The SnO2 supported Pd catalyst was prepared through reduction of Na2PdCl4 with NaBH4. In a typical procedure, 50 mg of SnO2 was dispersed in 50 mL of water by sonication. Then, 2 mL of Na2PdCl4 solution (0.06 mM) was added, and the mixture was stirred for 1 h. A freshly prepared 10 mL of NaBH4 solution (0.1 M) was added to the solution at 0 degC, followed by stirring at room temperature overnight. Finally, it was filtered, washed with water and ethanol, and vacuum-dried at 70 degC for 6 h. The as-prepared product was denoted as Pd/SnO2-S. For comparison, the commercial SnO2 nanopowder and Vulcan XC-72 carbon black supported Pd catalyst with same Pd loading was prepared by the above procedure, which were denoted as Pd/SnO2-P and Pd/XC-72, respectively.
Influence of organic nitrogenous compounds phenothiazine and diphenyl amine in poly(vinylidene fluoride) blended with poly(ethylene oxide) polymer electrolyte in dye-sensitized solar cells
0.3 g of polymer blend, PVdF (0.23 g)-PEO (0.07 g), was dissolved in 25 ml of dimethyl formamide (DMF) under continuous stirring at 353 K. After about 2 h, 0.03 g of KI, 0.007 g of I2 and 0.007 g organic nitrogenous compounds (DPA in one portion and PT in another portion) were added. The polymer solution was stirred continuously for another 2 h until a homogenous viscous liquid was formed. The polymer blend solutions were then cast on to a petridish and vacuum dried at 333 K for 15 h to enable solvent evaporation.1. A rechargeable lithium battery comprising a metal oxide represented by the following Chemical Formula 1:
[NiaCobMcMnd]3O4[Chemical Formula 1]
wherein, in the above Chemical Formula 1,
M is a transition element,
0.2≦a≦0.9, 0≦b≦0.5, 0≦c≦0.05, 0.1≦d≦0.9, and a+b+c+d=1.2. The positive active material precursor for a rechargeable lithium battery of claim 1, wherein the M comprises Ti, V, Cr, Fe, Cu, Zn, Y, Zr, Nb, Mo, W, or a combination thereof.3. The positive active material precursor for a rechargeable lithium battery of claim 1, wherein the metal oxide represented by the above Chemical Formula 1 is a spherically-shaped powder.4. The positive active material precursor for a rechargeable lithium battery of claim 1, wherein the metal oxide represented by the above Chemical Formula 1 has an average particle diameter of 5 μm to 15 μm.5. The positive active material precursor for a rechargeable lithium battery of claim 1, wherein the metal oxide represented by the above Chemical Formula 1 has a tap density of 1.0 g/cc to 2.0 g/cc.6. A positive active material for a rechargeable lithium battery, comprising a compound represented by the following Chemical Formula 2 and obtained by using the positive active material precursor for a rechargeable lithium battery according to claim 1:
Li1+x[NiaCobMcMnd]1−xO2−yFy[Chemical Formula 2]
wherein, in the above Chemical Formula 2,
M is a transition element,
−0.05≦x≦0.25,
0≦y≦0.05,
0.2≦a≦0.9, 0≦b≦0.5, 0≦c≦0.05, 0.1≦d≦0.9, and a+b+c+d=1.7. The positive active material for a rechargeable lithium battery of claim 6, wherein the M comprises Ti, V, Cr, Fe, Cu, Zn, Y, Zr, Nb, Mo, W, or a combination thereof.8. The positive active material for a rechargeable lithium battery of claim 6, wherein the positive active material for a rechargeable lithium battery may include a secondary particle formed by agglomerating a plurality of primary particles.9. The positive active material for a rechargeable lithium battery of claim 8, wherein the primary particles have an average particle diameter of 1 nm to 500 nm.10. The positive active material for a rechargeable lithium battery of claim 6, wherein the positive active material for a rechargeable lithium battery has a tap density of 1.5 g/cc to 2.5 g/cc.11. The positive active material for a rechargeable lithium battery of claim 6, wherein the positive active material for a rechargeable lithium battery has a specific surface area of 1.0 m2/g to 10.0 m2/g.12. A method of preparing a positive active material for a rechargeable lithium battery, comprising:
mixing at least one of a nickel source, a cobalt source, and a manganese source, and a solvent, under an oxidizing atmosphere to form a positive active material precursor for a rechargeable lithium battery represented by the following Chemical Formula 1; and
mixing the positive active material precursor and a lithium source followed by heat treatment to prepare a compound represented by the following Chemical Formula 2:
[NiaCobMcMnd]3O4[Chemical Formula 1]
wherein, in the above Chemical Formula 1,
M is a transition element,
0.2≦a≦0.9, 0≦b≦0.5, 0≦c≦0.05, 0.1≦d≦0.9, and a+b+c+d=1,
Li1+x[NiaCobMcMnd]1−xO2−yFy[Chemical Formula 2]
wherein in the above Chemical Formula 2,
M is a transition element,
−0.05≦x≦0.25,
0≦y≦0.05
0.02≦a≦0.9, 0≦b≦0.5, 0≦c≦0.05, 0.1≦d≦0.9, and a+b+c+d=1.13. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the oxidizing atmosphere comprises an air atmosphere, an oxygen (O2) atmosphere, or a combination thereof.14. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the M comprises Ti, V, Cr, Fe, Cu, Zn, Y, Zr, Nb, Mo, W, or a combination thereof.15. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the nickel source comprises nickel sulfate, nickel nitrate, nickel acetate, nickel chloride, nickel phosphate, or a combination thereof.16. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the cobalt source comprises cobalt sulfate, cobalt nitrate, cobalt acetate, cobalt chloride, cobalt phosphate, or a combination thereof.17. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the manganese source comprises manganese sulfate, manganese nitrate, manganese acetate, manganese chloride, manganese phosphate, or a combination thereof.18. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the solvent comprises water, ethanol, methanol, or a combination thereof.19. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein in the step of forming the positive active material precursor for a rechargeable lithium battery, the nickel source is mixed in an amount of about 10 wt % to about 90 wt %, the cobalt source is mixed in an amount of about 0 wt % to about 50 wt %, the manganese source is mixed in an amount of about 10 wt % to about 90 wt %, and the solvent is mixed in an balance amount.20. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein in the step of forming the positive active material precursor, a transition element source is further mixed.21. The method of preparing a positive active material for a rechargeable lithium battery of claim 20, wherein the transition element source comprises a sulfate, nitrate, acetate, chloride, or phosphate of a transition element, or combination thereof.22. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the lithium source comprises lithium nitrate (LiNO3), lithium acetate (CH3COOLi), lithium carbonate (Li2CO3), lithium hydroxide (LiOH), or a combination thereof.23. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the active material precursor for a rechargeable lithium battery and the lithium source are mixed in a mole ratio of 1.0:0.95 to 1.0:1.25.24. The method of preparing a positive active material for a rechargeable lithium battery of claim 12, wherein the heat treatment is performed through primary firing at a temperature of 250° C. to 650° C. and secondary firing at a temperature of 700° C. to 1100° C.25. A rechargeable lithium battery, comprising:
a positive electrode including a positive active material;
a negative electrode comprising a negative active material; and
an electrolyte,
wherein the positive active material is the positive active material for a rechargeable lithium battery of claim 6.Facile synthesis of the sandwich-structured germanium/reduced graphene oxide hybrid: an advanced anode material for high-performance lithium ion batteries

In the synthesis of rGO/Ge/rGO, the NH4H(HGeO3)2 precursor solution was first obtained by the reaction of GeO2 and NH3*H2O aqueous solution.56 GeO2 (1 g) powder was dispersed in distilled water (50 mL) by ultrasonication for 10 min. Then concentrated NH3*H2O (68%, 10 mL) was added to the above mixture with magnetic stirring and a transparent solution A was formed. Graphite oxide (GO) was synthesized through a modified Hummer's method starting from graphite powder.57 The as-synthesized GO was dispersed in distilled water and exfoliated by ultrasonication to generate sheets. Subsequently, 10 mL of exfoliated GO solution (20 mg mL-1) was dissolved in solution A under constant stirring for 30 min to obtain a brown solution and then dried at 70 degC for 12 h. Thus the NH4H(HGeO3)2/GO composite was prepared. To obtain the rGO/Ge/rGO hybrid, the NH4H(HGeO3)2/GO was annealed under a 15% H2/85% Ar flow at 800 degC for 4 h. As a reference sample, pure Ge was also prepared following the same procedures except that GO was not added.

Solvothermal synthesis and electrochemical performance of Li2MnSiO4/C cathode materials for lithium ion batteries

Li2MnSiO4/C with Ni2+ doping was prepared via the solvothermal method by using starch as the carbon source. Firstly, C19H42BrN (0.1 mmol, Aladdin, 99%) was dissolved in methanol (45 ml). Secondly, C2H3O2Li*2H2O (Aladdin, 99%), MnC4H6O4*4H2O (Aladdin, 99%), Si (OC2H5)4 (Aladdin, 99%) and NiC4H6O4*4H2O (Aladdin, 99%) were added into the above solution with the molar ratio of 2.2:0.95:1:0.05. Finally, glacial acetic acid (1.5 mL) was added as a catalyst. After stirring for 6 h, the homogeneous solution was loaded into a 100 ml Teflon-lined autoclave and maintained at 120 degC for 20 h. The jelly-like product was dried at 60 degC 12 h in a vacuum. The obtained precursor was thoroughly ground and mixed with starch suspension with a weight ratio of Li2MnSiO4:C of 2:1. After stirring at 40 degC for 5 h, the mixture was dried at 60 degC for 12 h. The dried product was heat treated at 450 degC for 2 h and then 700 degC for 10 h in Ar (95%)/H2 (5%) atmosphere to obtain Li2MnSiO4 with carbon coating and Ni2+ doping.

1. A lithium transition metal composite particle comprising:
a lithium transition metal oxide particle;
a metal-doped layer formed by doping the lithium transition metal oxide particle; and
LiF formed on the lithium transition metal oxide particle including the metal-doped layer.2. The lithium transition metal composite particle of claim 1, wherein a metal included in the metal-doped layer comprises any one selected from the group consisting of aluminum (Al), zinc (Zn), zirconium (Zr), titanium (Ti), tungsten (W), strontium (Sr), boron (B), magnesium (Mg), yttrium (Y), molybdenum (Mo), niobium (Nb), silicon (Si), and tin (Sn), or a mixed metal of two or more thereof.3. The lithium transition metal composite particle of claim 1, wherein the metal-doped layer is included in an amount of 0.01 wt % to 3 wt % based on 100 wt % of the lithium transition metal composite particle.4. The lithium transition metal composite particle of claim 2, wherein the metal has a concentration gradient in which a concentration gradually decreases from a surface of the lithium transition metal oxide particle to inside thereof.5. The lithium transition metal composite particle of claim 4, wherein the metal-doped layer comprises composite particles of Chemical Formula 1:
<in-line-formulae>Li<sub>a</sub>M<sub>1-b</sub>Me<sub>b</sub>O<sub>2</sub>  <Chemical Formula 1></in-line-formulae>
where M=NixMnyCoz, (0.3≦x≦0.9, 0≦y≦0.6, and 0≦z≦0.6),
Me is any one selected from the group consisting of Al, Zn, Zr, Ti, W, Sr, B, Mg, Y, Mo, Nb, Si, and Sn, or a mixed element of two or more thereof,
0.9≦a≦1.3, and
0<b≦0.02.6. The lithium transition metal composite particle of claim 2, wherein the metal-doped layer further comprises an oxide including the metal.7. The lithium transition metal composite particle of claim 2, wherein a doping amount of the metal is greater than 0 mol % and equal to or less than 2 mol %.8. The lithium transition metal composite particle of claim 1, wherein the LiF is modified by reacting a fluoride-based polymer with at least a portion of lithium impurities included in the lithium transition metal composite particle.9. The lithium transition metal composite particle of claim 1, wherein the LiF is included in an amount of 0.1 wt % to 0.5 wt % based on a total weight of the lithium transition metal composite particles.10. The lithium transition metal composite particle of claim 8, wherein the lithium impurities comprise LiOH, Li2CO3, or a mixture thereof.11. The lithium transition metal composite particle of claim 8, wherein the lithium impurities are included in an amount of less than 0.3 wt % based on the total weight of the lithium transition metal composite particles.12. The lithium transition metal composite particle of claim 8, wherein the fluoride-based polymer comprises polyvinylidene fluoride (PVdF), a polyvinylidene fluoride-hexafluoropropylene copolymer (PVdF-co-HFP), or a mixture thereof.13. The lithium transition metal composite particle of claim 1, wherein the lithium transition metal oxide is any one selected from the group consisting of lithium-cobalt-based oxide, lithium-manganese-based oxide, lithium-nickel-manganese-based oxide, lithium-manganese-cobalt-based oxide, and lithium-nickel-manganese-cobalt-based oxide, or a mixture of two or more thereof.14. The lithium transition metal composite particle of claim 13, wherein the lithium transition metal oxide is any one selected from the group consisting of LiCoO<sub>2</sub>, LiNiO<sub>2</sub>, LiMnO<sub>2</sub>, LiMn<sub>2</sub>O<sub>4</sub>, Li(Ni<sub>a</sub>Co<sub>b</sub>Mn<sub>c</sub>)O<sub>2 </sub>(where 0<a<1, 0<b<1, 0<c<1, and a+b+c=1), LiNi<sub>1-Y</sub>Co<sub>Y</sub>O<sub>2</sub>, LiCo<sub>1-Y</sub>Mn<sub>Y</sub>O<sub>2</sub>, LiNi<sub>1-Y</sub>Mn<sub>Y</sub>O<sub>2 </sub>(where 0≦Y<1), Li(Ni<sub>a</sub>CoMn<sub>c</sub>)O<sub>4 </sub>(where 0<a<2, 0<b<2, 0<c<2, and a+b+c=2), LiMn<sub>2-z</sub>Ni<sub>z</sub>O<sub>4</sub>, and LiMn<sub>2-z</sub>Co<sub>z</sub>O<sub>4 </sub>(where 0<z<2), or a mixture of two or more thereof.15. A method of preparing lithium transition metal composite particles, the method comprising:
mixing a mixed transition metal precursor, a lithium compound, and a metal oxide and sintering to obtain lithium transition metal oxide particles including a metal-doped layer; and
mixing the lithium transition metal oxide particles including a metal-doped layer and a surface modifier, and performing a heat treatment.16. A method of preparing lithium transition metal composite particles, the method comprising:
mixing a mixed transition metal precursor doped with a metal and a lithium compound and sintering to obtain lithium transition metal oxide particles including a metal-doped layer; and
mixing the lithium transition metal oxide particles including a metal-doped layer and a surface modifier, and performing a heat treatment.17. The method of claim 15, wherein the sintering is performed in a temperature range of 800° C. to 1,000° C.18. The method of claim 15, wherein the heat treatment is performed in a temperature range of 300° C. to 500° C.19. The method of claim 15, wherein the mixed transition metal precursor is a compound having a composition of MOOH or M(OH)2(where M=NixMnyCoz, 0.3≦x≦0.9, 0≦Y≦0.6, 0≦z≦0.6, and x+y+z=1).20. The method of claim 15, wherein the metal oxide comprises any one selected from the group consisting of Al2O3, ZnO, ZrO2, TiO2, WO3, SrO2, B2O3, MgO, Y2O3, MoO3, Nb2O3, Nb2O6, SiO2, and SnO, or a mixture of two or more thereof.21. The method of claim 15, wherein the metal oxide is used in an amount of 0.1 wt % to 1 wt % based on a total weight of the lithium transition metal composite particles.22. The method of claim 15, wherein the surface modifier comprises polyvinylidene fluoride (PVdF), a polyvinylidene fluoride-hexafluoropropylene copolymer (PVdF-co-HFP), or a mixture thereof.23. The method of claim 15, wherein the surface modifier is used in an amount of 0.2 wt % to 0.5 wt % based on the total weight of the lithium transition metal oxide particles.24. A cathode active material comprising the lithium transition metal composite particles of claim 1.25. A cathode comprising the cathode active material of claim 24.26. A lithium secondary battery comprising:
a cathode;
an anode; and
a separator disposed between the cathode and the anode,
wherein the cathode is the cathode of claim 25.A zero-strain layered metal oxide as the negative electrode for long-life sodium-ion batteries
The resulting material was prepared by a solid-state reaction using precursors of Li2CO3 (99%), Na2CO3 (99%) and TiO2 (99.5%, anatase form). A phase-pure compound was obtained when an excess of 2 mol% Li2CO3 and Na2CO3 was used. The starting materials were ground in an agate mortar and pressed into pellets under pressure of 20 MPa. Then the pellets were heated at 1000 degC for 24 h in an alumina crucible.In situ synthesis and assembly of copper oxide nanocrystals on copper foil via a mild hydrothermal process
Cationic surfactant cetyltrimethylammonium bromide (CTAB), anionic surfactant sodium dodecyl sulfate (SDS), nonionic surfactant poly(oxyethylene) (1,1,3,3-tetramethylbutyl) phenyl ether (Tx-100), ammonium peroxydisulfate ((NH4)2S2O8, APS) and sodium hydroxide (NaOH) were purchased commercially and used without further purification. Copper foils (1.5 x 1.5 cm, 99.9%) with a thickness of 0.2 mm were washed with 1 M NaOH, 1 M HCl and deionized water before use.
In a typical procedure, the starting solution was prepared by mixing 6.4 g NaOH (5 M) and 1.0954 g (NH4)2S2O8 (0.15 M) in 32 ml water. After 5 min, 0.2333 g CTAB (0.02 M) was introduced into the aqueous solution under stirring, resulting in a white aqueous solution. After CTAB had completely dissolved in the water, the solution was transferred into Teflon-lined stainless steel autoclaves, and the previously-cleaned copper foil was then immersed in the solution. After that, the autoclaves was sealed and maintained at 160 degC for 24 h. After cooling to room temperature, the copper foil was then taken out from the solution and rinsed with distilled water, and a sample of black film on the copper foil was obtained.Response of the Jovian thermosphere to a transient 'pulse' in solar wind pressure

Abstract
The importance of the Jovian thermosphere with regard to magnetosphere-ionosphere coupling is often neglected in magnetospheric physics.
We present the first study to investigate the response of the Jovian thermosphere to transient variations in solar wind dynamic pressure, using an azimuthally symmetric global circulation model coupled to a simple magnetosphere and fixed auroral conductivity model.
In our simulations, the Jovian magnetosphere encounters a solar wind shock or rarefaction region and is subsequently compressed or expanded.
We present the ensuing response of the coupling currents, thermospheric flows, heating and cooling terms, and the aurora to these transient events.
Transient compressions cause the reversal, with respect to steady state, of magnetosphere-ionosphere coupling currents and momentum transfer between the thermosphere and magnetosphere.
They also cause at least a factor of two increase in the Joule heating rate.
Ion drag significantly changes the kinetic energy of the thermospheric neutrals depending on whether the magnetosphere is compressed or expanded.
Local temperature variations appear between ~-45and175K for the compression scenario and ~-20and50K for the expansion case.
Extended regions of equatorward flow develop in the wake of compression events - we discuss the implications of this behaviour for global energy transport.
Both compressions and expansions lead to a ~2000TW increase in the total power dissipated or deposited in the thermosphere.
In terms of auroral processes, transient compressions increase main oval UV emission by a factor of ~4.5 whilst transient expansions increase this main emission by a more modest 37%.
Both types of transient event cause shifts in the position of the main oval, of up to 1° latitude.
Highlights
•
Theoretical study of how rapid solar wind variations affects Jupiter's magnetosphere-ionosphere-thermosphere system.
•
A solar wind shock causes the reversal of currents and angular momentum transfer from the thermosphere to magnetosphere.
•
Solar wind shocks lead to large regions of equatorward flow in the thermosphere due to the reversal in ion drag acceleration.
•
These rapid magnetospheric reconfigurations result in local thermospheric temperature increases >25K.

Introduction
Jovian magnetosphere-ionosphere coupling
The interaction between the Jovian magnetosphere and ionosphere is complex.
The current systems which connect the planet's ionosphere and magnetosphere are controlled by a feedback mechanism involving the rotation of magnetospheric plasma, the conductance of the ionosphere and the wind system prevailing in the thermosphere (upper atmosphere).
Several studies, however, have made substantial progress in modelling this interaction (Hill, 1979; Pontius, 1997; Hill, 2001; Cowley and Bunce, 2001, 2003a,b; Nichols and Cowley, 2004; Cowley et al., 2005; Bougher et al., 2005; Cowley et al., 2007; Majeed et al., 2009; Tao et al., 2009; Ray et al., 2010; Nichols, 2011; Ray et al., 2012).
The models of Cowley and Bunce (2003a,b) and Nichols and Cowley (2004) were primarily used to study the interaction of the inner and middle magnetosphere and how these regions couple with the Jovian ionosphere; Cowley et al. (2005) and Cowley et al. (2007) expanded on the former studies by incorporating simplified models for the outer magnetosphere and polar cap region, and thus coupling the 'entire' magnetosphere to the ionosphere.
Nichols (2011) considered how a whole magnetosphere self-consistently interacted with the magnetosphere-ionosphere system.
The force balance formalism of Caudal (1986) was used in the Nichols (2011).
In addition, Cowley and Bunce (2003a,b) and Cowley et al. (2007) investigated how the coupled magnetosphere-ionosphere (M-I) system interacts with the solar wind - specifically, transient variations in the solar wind dynamic pressure which cause compressions and expansions of the magnetosphere.
These models have made realistic predictions regarding the corresponding response of magnetospheric and ionospheric currents, plasma angular velocity profiles and auroral emission (both in terms of the intensity of emission and its location in the ionosphere).
Many of these model predictions are supported by observations and complementary theoretical studies such as Nichols et al. (2009), Clarke et al. (2009) and Southwood and Kivelson (2001).
None of these aforementioned studies, however, have self-consistently accounted for the dynamics of the Jovian thermosphere.
In these studies the thermosphere is assumed to have an angular velocity ΩT, independent of altitude, which is derived from a constant 'slippage factor', K, given by(1)K=(ΩJ-ΩT)(ΩJ-ΩM).
In this expression ΩJ (1.76×10-4rads-1) is the angular velocity of the planet and ΩM is the angular velocity of the magnetospheric region conjugate to the thermosphere.
This ensures the ordering ΩJ>ΩT>ΩM, for a steady state, where angular momentum is transferred from ionosphere to magnetosphere.
Smith and Aylward (2009) expanded further on the current body of M-I models by coupling a simplified magnetosphere model with an azimuthally symmetric Global Circulation Model (GCM) of Jupiter.
Their approach allowed for the self-consistent calculation of the Jovian thermospheric angular velocity, in a coupled M-I system which had reached a steady state.
The study by Smith and Aylward (2009) produced some notable results such as(i)
Angular momentum transfer: meridional advection of momentum, rather than vertical viscous transport, is the main mechanism for transferring angular momentum in the high latitude thermosphere.
(ii)
Thermospheric super-corotation: largely due to (i), the thermosphere super-corotates (ΩT=1.05ΩJ) throughout those latitudes (~65-73°) where it magnetically maps to the middle magnetosphere (~6-25RJ).
(iii)
Distribution of heat: the simulated thermospheric winds develop two main cells of meridional flow, which cool lower latitudes (≲75°) whilst heating the polar regions (≳80°).
Yates et al. (2012) used the model of Smith and Aylward (2009) to study the influence of the solar wind on steady-state thermospheric flows of Jupiter.
They found that ionospheric and magnetospheric currents, thermospheric powers, temperature and auroral emission (by proxy of field-aligned current (FAC)) all exhibit increases with decreasing solar wind dynamic pressure (from 0.213nPa to 0.021nPa Joy et al., 2002).
Southwood and Kivelson (2001) suggested that a magnetospheric compression would cause an increase in the degree of magnetospheric plasma corotation (i.e. the quantity (ΩJ-ΩM) would decrease), and this would consequently lead to a sizeable decrease in M-I coupling currents and auroral emission.
They also argued that the reverse would be true for a magnetospheric expansion.
Simulations by Cowley et al. (2007) and Yates et al. (2012) confirmed these predictions, provided that the system is given enough time to achieve steady-state (≥50 rotations).
On the other hand, the studies of Cowley and Bunce (2003a,b) and, more recently, Cowley et al. (2007) simulated the 'transient' (short-term) response of the system to rapid (~2-3h) magnetospheric compressions and expansions.
This short-term behaviour was found to differ from the steady state case.
For rapid compressions (≲3h), the conservation of plasma angular momentum causes the magnetosphere to super-corotate compared to the planet and thermosphere.
The flow shear between the thermosphere and magnetosphere, represented by (ΩT-ΩM), is now negative and leads to current reversals at magnetic co-latitudes that are conjugate to the middle and outer magnetospheres (~10-17°).
Negative flow shear also causes energy to be transferred from the magnetosphere to thermosphere; in contrast to the steady-state, where energy is transferred from the thermosphere to the magnetosphere, in order to accelerate outflowing, magnetospheric plasma towards corotation.
For transient expansions, Cowley et al. (2007) showed that ΩM decreases but the flow shear increases, leading to a ~500% increase in the intensity of M-I currents (for an expansion from a dayside magnetopause radius of 45RJ-85RJ, RJ=71,492km).
For these transient events, where the magnetopause is displaced by ~40RJ, Cowley et al. (2007) predict differing auroral responses dependent on the nature of the event (compression or expansion).
For compressions, electron energy flux (~10% of which is used to produce ultraviolet (UV) aurora) at the open-closed field line boundary (polar emission) increases by two orders of magnitude, whilst the main emission is halved.
In the expansion case, there is a 30-fold increase in main emission mapping to the middle magnetosphere, whilst polar emission decreases to ~2% of its steady-state value.
Recent observations of auroral emission by Clarke et al. (2009) show a factor of two increase in total ultraviolet (UV) auroral power, near the arrival of a solar wind compression region, typically corresponding to an increase in solar wind dynamic pressure of ~0.01-0.3nPa.
Furthermore, Nichols et al. (2009) showed, using the same Hubble Space Telescope (HST) images as Clarke et al. (2009), that this increase in auroral emission consists of approximately even contributions from the so-called 'main oval' and the high-latitude polar emission.
Nichols et al. (2009) also showed that the location of the 'main oval' shifted polewards by ~1° in response to solar wind pressure increase of an order of magnitude.
For a rarefaction region in the solar wind, an order of magnitude decrease in solar wind pressure, Clarke et al. (2009) observed little, if any, change in auroral emission.
Jovian atmospheric heating
The Jovian upper atmospheric temperature is up to 700K higher than that predicted by solar heating alone (Strobel and Smith, 1973; Yelle and Miller, 2004).
This 'energy crisis' at Jupiter and the other giant planets has puzzled scientists for over 40 years.
Different theories have been put forward to explain Jovian upper atmospheric heating: gravity waves (Young et al., 1997), auroral particle precipitation (Waite et al., 1983; Grodent et al., 2001), Joule heating (Waite et al., 1983; Eviatar and Barbosa, 1984) and ion drag (Miller et al., 2000; Smith et al., 2005; Millward et al., 2005).
None of the aforementioned studies have been able to fully account for the observations.
M-I coupling models by Achilleos et al. (1998), Bougher et al. (2005), Smith and Aylward (2009), Tao et al. (2009) and Yates et al. (2012) have all discussed steady-state heating and cooling terms in the Jovian thermosphere.
Yates et al. (2012), whilst investigating the influence of solar wind on the steady-state thermospheric flows of Jupiter, found that ion drag energy and Joule heating increased by ≲200% (from a compressed to expanded magnetospheric configuration) result in a thermospheric temperature increase of ~135K.
Cowley et al. (2007) discussed 'transient' heating and dynamics in terms of power dissipated in the thermosphere via Joule heating and ion drag energy, as well as power used to accelerate magnetospheric plasma.
Cowley et al. (2007) considered displacements of the Jovian magnetopause by ~40RJ.
They found that, for compressions, there was a net transfer of power from magnetosphere to planet of ~325TW, due to the expected super-corotation of magnetospheric plasma.
For expansions, Cowley et al. (2007) found that the power dissipated in the thermosphere (and used to accelerate magnetospheric plasma) increased by a factor of ~2.5 resulting from a large increase in azimuthal flow shear between the expanded magnetosphere and the thermosphere.
In this study, we use the Yates et al. (2012) model, 'JASMIN' (Jovian Axisymmetric Simulator, with Magnetosphere, Ionosphere and Neutrals), to estimate the response of Jovian thermospheric dynamics, heating and aurora to transient changes in the solar wind dynamic pressure and, consequently, magnetospheric size.
By transient, we mean changes on time scales ≲3h, where the angular momentum of the magnetospheric plasma is approximately conserved (Cowley et al., 2007) as the time scales required for changes in the M-I currents to affect ΩM are much longer, ~10-20h.
Our coupled model responds to time-dependent profiles of plasma angular velocity in the magnetosphere.
We employ different ΩM(ρe,t) profiles (ρe represents equatorial radial distance and t denotes time) to represent compressions and expansions of the middle magnetosphere.
This is the first study to investigate how time-dependent variations in solar wind pressure influence both magnetospheric and thermospheric properties of the Jovian system, and to use a realistic GCM to represent the thermosphere.
In Section 2 we summarise the time scales involved in the Jovian system and Section 3 describes the model used in this study.
In Sections 4 and 5 we present our findings for the transient compression and expansion scenarios, respectively.
We discuss our findings and and their limitations in Section 6 and conclude in Section 7.
Time-dependence of the Jovian system
Variations in magnetic field, plasma angular velocity and thermospheric flow patterns due to solar wind pressure changes present challenges for modelling the Jovian system.
Various time-scales, such as those associated with M-I coupling, compression or expansion of the magnetosphere and thermospheric response, need to be considered.
The studies by Cowley and Bunce (2003a,b) and Cowley et al. (2007) are among the few to have addressed these issues, using the simplifying approximations discussed hereafter.(i)
M-I coupling time scale: The neutral atmosphere transfers angular momentum to the magnetosphere along magnetic field lines in order to accelerate the radially outflowing magnetospheric plasma towards corotation.
The time-scale on which this angular momentum is transferred has been estimated by Cowley and Bunce (2003a) to be ~5-20h, similarly to that found by Vasyliunas (1994).
(ii)
Compression (and expansion) of the magnetosphere: Large changes in magnetospheric size (~40RJ) can occur when the Jovian magnetosphere encounters a sudden change in solar wind dynamic pressure, such as would be caused by a Coronal Mass Ejection (CME) or Corotating Interaction Regions (CIR).
Cowley and Bunce (2003a) and Cowley et al. (2007) considered compressions (and expansions) occurring over ~2-3h, and were thus able to assume conservation of plasma angular momentum when calculating the response of the M-I system, since the coupling time scale discussed in (i) is large, by comparison.
(iii)
Thermospheric response time: The thermosphere and magnetosphere are coupled together via ion-neutral collisions in the ionosphere; therefore a change in plasma angular velocity would cause a corresponding change in the thermosphere's effective angular velocity.
Recent models for the thermospheric response are generally divided in two scenarios: (i) a system where the thermosphere responds promptly, on the order of a few tens of minutes as found by Millward et al. (2005), and (ii) a system where the thermosphere responds on the order of 2 days and, as such, is essentially unresponsive to transient events (Gong, 2005).
However, in this study, we do not make a distinction between thermospheric response models.
We simply allow the GCM to respond self-consistently to the imposed changes in plasma angular velocity assumed for the transient compressions and expansions, thus allowing a realistic, thermospheric response to these changes.
Model description
Thermosphere model
The thermosphere model used in this study is a GCM which solves the non-linear Navier-Stokes of energy, momentum and continuity, using explicit time integration (Müller-Wodarg et al., 2006).
The Müller-Wodarg et al. (2006) three-dimensional (3-D) GCM was created for Saturn's thermosphere, and later modified for Saturn and Jupiter respectively in Smith and Aylward (2008) and Smith and Aylward (2009).
It is the Smith and Aylward (2009) modified GCM that we use in this study.
The model assumes azimuthal symmetry, and is thus two-dimensional (pressure/altitude and latitude) whilst still solving the 3-D equations.
The Navier-Stokes equations are solved in the pressure coordinate system, providing time dependent distributions of thermospheric wind, temperature and energy.
The zonal and meridional momentum equations, along with the energy equation forming the basis of this particular GCM can be found in Achilleos et al. (1998), Müller-Wodarg et al. (2006) or Tao et al. (2009), should the reader be interested.
Our model is resolved on a 0.2° latitude and 0.4 pressure scale height grid, with a lower boundary at 2μbar (300km above the 1bar(B) level) and upper boundary at 0.02nbar.
Ionosphere model
We employ a simplified model of the ionosphere used in Smith and Aylward (2009) and Yates et al. (2012), who separate the model into two components: (i) a vertical part describing the relative change of conductivity with altitude, as defined by the 1D model of Grodent et al. (2001), and (ii) a horizontal part, which linearly scales the Grodent et al. (2001) model at all altitudes so that the height-integrated Pedersen conductance ΣP matches a global pattern prescribed by the user.
The sole difference between the model used in Yates et al. (2012) and that presented here lies in the horizontal part.
Here we employ a fixed value of ΣP between latitudes 60° and 74° whilst in the above studies, conductances in this region may be enhanced, above background levels, by FACs (Nichols and Cowley, 2004).
Table 1 shows the three different height-integrated Pedersen conductance regions employed in our model, their corresponding ionospheric latitudes and their assigned values of ΣP.
Section 6.4 discusses the limitations in using such assumptions.
Magnetosphere model
The axisymmetric Jovian magnetosphere model employed in this study is the same as that described in Yates et al. (2012).
It combines a detailed model of the inner and middle magnetosphere (Nichols and Cowley, 2004) with a simplified model of the outer magnetosphere and region of open field lines (Cowley et al., 2005).
The ability to reconfigure the magnetosphere depending on its size is also included by assuming that magnetic flux is conserved (Cowley et al., 2007).
Surfaces of constant flux function define shells of magnetic field lines with common equatorial radii ρe and ionospheric co-latitude θi.
These surfaces also allow for the magnetic mapping from the equatorial plane of the magnetodisc (middle magnetosphere) to the ionosphere.
This mapping requires an ionospheric flux function Fi(θi), a magnetospheric counterpart Fe(ρe) (representing the magnetic flux integrated between a given equatorial radial distance ρe and infinity) and the equality Fi(θi)=Fe(ρe), which represents the mapping between θi and ρe to which the corresponding magnetic field line extends (Nichols and Cowley, 2004).
The ionospheric flux function is given by(2)Fi=BJρi2=BJRi2sin2θi,where BJ is the equatorial magnetic field strength at the planet's surface and ρi is the perpendicular distance to the planet's magnetic/rotation axis (ρi=Risinθi, where Ri is the ionospheric radius.
We adopt BJ=426,400nT (Connerney et al., 1998), and Ri=67,350km (Cowley et al., 2007).
Note Ri<RJ due to polar flattening at Jupiter.
For further details on the magnetosphere model employed here the reader is referred to Yates et al. (2012) and the references therein.
A discussion on the currents which couple our model can be found in Appendix A.
Obtaining the transient plasma angular velocity
In steady state, plasma angular velocity profiles are obtained in a similar manner to that discussed in Smith and Aylward (2009) and Yates et al. (2012); by solving the Hill-Pontius equation in the inner and middle magnetosphere, but assuming a constant Pedersen conductance.
We now discuss the calculation of plasma angular velocity once the model has entered the transient regime i.e. once our initial, steady-state system begins to undergo a transient compression/expansion of the magnetosphere.
Our method of calculating transient plasma angular velocities follows that of Cowley et al. (2007).
Prior to the rapid compression or expansion, the system exists in a steady state, with plasma angular velocity ΩM(θi,t=0) as a function of co-latitude θi and time t.
Using the magnetic mapping method discussed in Section 3.3, the equatorial radial distance ρe(θi,t=0) of the local field line can be found.
The arrival of the solar wind pulse or rarefaction causes the magnetosphere to compress or expand by several tens of RJ (typical choice for the simulations) and the model enters the transient (time-dependent) regime.
Thus, a given co-latitude θi now maps to a new radial distance ρe(θi,t).
If, as discussed in Section 2, the solar wind pulse causes perturbations that occur on sufficiently small time scales (~2-3h), we can assume that plasma angular momentum is approximately conserved.
The plasma angular velocity profile throughout the 'pulse' in solar wind pressure is then given by(3)ΩM(θi,t)=ΩM(θi,t=0)(ρe(θi,t=0)ρe(θi,t))2,where the notation t=0 and t denote the initial (steady-state) and transient state (at each time-step throughout the event) respectively.
For this study, the time evolution of solar wind dynamic pressure, and thus magnetodisc size, is represented by a Gaussian function.
RMM(t) represents the magnetodisc radius as a function of time and is given by(4)RMM(t)=Ae-((t-to)2/2Δt2)+RMMO,where A=RMM(to)-RMMO and is the amplitude of the corresponding curve, RMMO is the initial magnetodisc radius, RMM(to) is the maximum or minimum radius, to is the time at which RMM(t)=RMM(to) (90min after pulse start time ts), and Δt controls the width of the 'bell' (obtained using (2/3)(to-ts)=22ln2Δt ).
After achieving steady-state, we run the model for a single Jovian day, transient mode is then initialised 3h prior to the end of the Jovian day (and model runtime).
Profiles of RMM(t) for compressions and expansions are shown in Fig. 1.
As indicated in Fig. 1, the simulated pulse lasts for a total of 3h, after which the magnetodisc returns to its initial size.
This is represented by the red (compression) and green (expansion) lines.
The black dashed line indicates the point of maximum compression/expansion (at t=to) where we take a 'snapshot' of model outputs in order to investigate the thermospheric response midway through the transient pulse (henceforth, this phase of the event is referred to as 'half-pulse').
As in Yates et al. (2012), we divided the magnetosphere into four regions: region I, representing open field lines of the polar cap; region II containing the closed field lines of the outer magnetosphere; region III (shaded in figures) is the middle magnetosphere (magnetodisc) where we assume that the Hill-Pontius equation is valid for steady-state conditions.
Region IV is the inner magnetosphere (which is assumed to be fully corotating in steady state).
Region III is our main region of interest throughout this study since it plays a central role in determining the morphology of auroral currents.
Plasma velocities are shown in Fig. 2(a) and (b) (dashed lines) along with their corresponding thermospheric angular velocities (solid lines).
Fig. 2(a) shows angular velocity profiles pertaining to the transient compression scenario.
The starting configuration (steady-state) is indicated by blue lines and is henceforth, referred to as 'case CS' (pre-Compression Steady-state).
Halfway through the pulse, when the magnetodisc radius is a minimum, angular velocity profiles are represented by red lines and will be referred to as case CH (Compression Half-pulse).
Case CF (Compression Full-pulse) profiles are indicated by green lines and represent the state of the system after the pulse subsides (see Table 2 for description of different cases).
In Fig. 2(a), there is significant super-corotation of the magnetodisc plasma throughout most of regions IV and III.
Plasma rotating faster than both the thermosphere and deep planet creates a reversal of currents and angular momentum transfer between the ionosphere and magnetosphere (Cowley et al., 2007).
Thus angular momentum is transported from the magnetosphere to the thermosphere, where rotation rate increases from its initial state.
We see an average of ~3% increase in peak ΩT in response to the transient compression event.
This is small compared to the factor of two increase in peak ΩM (for case CH).
The significant difference in response between the thermosphere and magnetosphere is due to the larger mass of the neutral thermosphere and thus, its greater resistance to change (inertia).
After the subsidence of the pulse, the magnetosphere returns to its initial size and, thus, the ΩM profile for case CF is equal to that of CS at all latitudes.
The same cannot be said for the thermospheric angular velocities; the CF thermosphere rotates slightly faster (~2% at maximum ΩT) for parts of regions III and I and all of region II.
This comparison highlights the difference in response between the thermosphere and magnetosphere to the prescribed changes in solar wind pressure.
Fig. 2(b) shows angular velocity profiles corresponding to the transient expansion scenario.
Like the compression scenario, we have cases ES (pre-Expansion Steady-state (initial value of RMM=45RJ)), EH (Expansion Half-pulse RMM=85RJ) and EF (Expansion Full-pulse) indicated by blue, red and green lines, respectively.
The behaviour is very different from the compression: midway through the event (case EH), the magnetodisc plasma sub-corotates to an even greater degree in regions IV and III compared to the initial steady-state case, ES.
The thermosphere also sub-corotates to a greater degree, but maintains a higher angular velocity than the disc plasma, meaning that current reversal does not occur.
Thermospheric angular velocities for cases ES and EF differ slightly, as in the compression scenario i.e. due to the greater lag in the thermospheric response time.
Fig. 2 theoretically demonstrates the effect that transient shocks and rarefactions in the solar wind have on both plasma and thermospheric angular velocities.
Sections 4 and 5 will discuss the effects on the M-I coupling currents and the global thermospheric dynamics.
Magnetospheric compressions
In this section we present findings for our transient magnetospheric compression scenario which lasts for a total of 3h.
Auroral currents
Fig. 3(a) shows FAC density as a function of latitude (computed from the horizontal divergence of IP (ionospheric Pedersen current density); see Eq.
(A.3)) for cases CS, CH and CF.
The blue line represents case CS, whilst the red and green lines respectively show cases CH and CF.
Both cases CS and CF possess upward (positive) FAC density (indicating downward moving electrons) peaking at ~74°, corresponding to the 'main auroral oval'.
Strong downward (negative) FAC densities are located at the region III/II boundary, indicating that electrons in this region are moving upwards along the magnetic field lines.
In regions II and I, FAC density profiles remain slightly negative.
Peaks in upward FAC arise from strong spatial gradients in ΩM (ΩM decreases by ~78% across ~2° caused by the breakdown in corotation of magnetodisc plasma), and consequently, flow shears located at or near magnetospheric region boundaries.
Downward FACs at the region III boundary are also caused by large spatial gradients in ΩM and to a lesser extent the change in ΣP encountered as we traverse this boundary (Yates et al., 2012).
The minor differences between these two cases are attributed to the response of the thermosphere to the transient pulse.
At full-pulse, ΩM(CF)=ΩM(CS) but ΩT(CF)≠ΩT(CS) as the thermosphere has not had sufficient time to settle back to a steady-state (due to its large inertia, as discussed in Section 3.4).
Although this is a subtle example of the atmospheric modulation of auroral currents, future simulations will aim at further exploring how this effect changes within the parameter space of the pulse duration and its change in solar wind pressure.
Case CH shows the largest deviation from steady state.
Its FAC density profile is directed downwards at latitudes up to ~73°.
This current reversal (compared to case CS) is due to a negative flow shear (ΩT-ΩM) caused by the significant super-corotation of the magnetosphere compared to the thermosphere (see Section 3.4 and Cowley et al., 2007).
Poleward of the main downward current region, the FAC density remains negative except for two locations:(i)
the 'main auroral oval': where a peak upward FAC density of ~1.2μAm-2 (a factor-of-two increase compared to case CS) is due to magnetodisc plasma transitioning from a super-corotational state to a significantly sub-corotational state.
(ii)
the region II/I (open-closed field line) boundary: with an upward FAC density peak of ~0.2μAm-2 caused by the differing ΩM in these two regions.
In region II ΩM is fixed at a value depending on magnetodisc size (Cowley et al., 2005).
In region I, we set ΩM=0.10ΩJ, for all cases, in accordance with the formula of Isbell et al. (1984).
We briefly compare FAC densities from case CH with transient results from Cowley et al. (2007) (compression from 85 to 45RJ).
Despite a resemblance in FAC profiles, upward FACs in the magnetodisc (region III) are ~2.5 times larger in case CH than the equivalent case (with a responsive thermosphere) in Cowley et al. (2007).
FACs in case CH are actually closer to those in Cowley et al. (2007)'s non-responsive thermosphere compression case.
This suggests that the thermosphere (represented by a GCM) in our study lies somewhere in between a responsive and non-responsive thermosphere (although closer to the latter, for the pulse parameters assumed).
Corresponding precipitating electron energy fluxes are shown in Fig. 3(b).
These fluxes are plotted as a function of latitude and obtained using Eq.
(B.1) and Table B1, which uses only the upward (positive) FAC densities presented in Fig. 3(a).
The line style code and labels are the same as in Fig. 3(a).
The latitudinal size of a Hubble Space Telescope (HST) ACS-SBC pixel (0.03×0.03arcsec) is represented by the dark grey rectangle (assuming that the magnetic axis of the Jovian dipole is perpendicular to the observer's line of sight) and the grey solid line indicates the limit of present detectability with HST instrumentation (~1kR; Cowley et al., 2007).
We initially compare electron energy fluxes for cases CS and CF.
These profiles are non-existent poleward of ~74°; equatorward of this location, case CF shows little deviation from CS, except that caused by the thermospheric lag discussed above.
In region III, we find that the peak energy flux for case CF is ~35% larger than that in case CS and the location of these peaks coincide with the location of the main auroral oval (~74°).
The slight increase in peak energy flux is due to a relative increase in flow shear as seen in Fig. 2(a).
Case CF would therefore produce main oval emission approximately ~200kR brighter than that of CS as indicated by the right axis in the figure (assuming that 1mWm-2 of precipitation creates ~10kR of UV output (Cowley et al., 2007)).
The Ef profile for case CH is different from those of both cases CS and CF.
There are three main changes in CH compared to CS: (i) peak energy flux in region III is ~280mWm-2, almost a factor of five larger, (ii) location of peak energy flux has shifted polewards by ~0.2° and (iii) presence of a second peak with an energy flux of 1.7mWm-2 at the region II/I boundary.
The large increase in electron energy flux is caused by a substantial increase in flow shear between the thermosphere and magnetosphere, resulting from the super-corotation of the magnetodisc plasma (see Fig. 2).
The presence of a second upward FAC region at the region II/I boundary is also due to flow shear increase across the boundary, as the magnetosphere in region II corotates at a larger fraction of ΩJ compared to case CS.
The result for this higher-latitude boundary should be regarded as preliminary, since it is sensitive to the values of ΩM we assume in the outer magnetospheric region and polar cap.
Flow velocities in these regions are poorly constrained, with few observations (Stallard et al., 2003).
The increase in Ef for case CH would lead to corresponding increases in auroral emission.
As such, we would expect 'main oval' emission for case CH to shift polewards by ~0.2° and be ~4.7× larger than emission in case CS i.e.
~2800kR compared to ~600kR.
Comparing the energy flux profile of case CH with the equivalent case in Cowley et al. (2007), we see that in the closed field regions (III and II), peak energy fluxes are two orders of magnitude larger in case CH.
This demonstrates the differences between using a GCM to represent the thermosphere and using a simple 'slippage' relation between thermospheric and magnetospheric angular velocities.
At the open-closed field line boundary (II/I boundary) our peak flux is an order of magnitude smaller than that in Cowley et al. (2007); this difference arises from the different models used to represent the outer magnetosphere.
The outer magnetosphere (region II) and open field line region (region I) in this study is modelled using plasma angular velocities from Cowley et al. (2005).
Thermospheric dynamics
In this section, we discuss the thermospheric response to the simulated transient magnetospheric compressions.
Fig. 4(a)-(c) shows the variation of thermospheric azimuthal velocity (in the corotating reference frame) in the high latitude thermosphere for cases CS-CF respectively.
Positive (resp.
negative) values of azimuthal velocity indicate super (resp.
sub)-corotating regions.
The direction of meridional flow is indicated by the black arrows, the locus of rigid corotation is indicated by the solid white line, strong super-corotation (<25ms-1) is indicated by the black contour, strong sub-corotation (>-2500ms-1) is indicated by the dashed white contour.
Magnetospheric regions are labelled and separated by black dotted lines.
Zonally, there are two prominent features in our transient compression cases:(i)
a low altitude small super-corotating jet, centred at ~72°.
In case CS, this jet is created by a small excess in the zonal Coriolis and advection momentum terms compared to the ion drag term.
At low altitude, the Coriolis force is primarily directed eastwards and unopposed can promote super-corotation in the neutrals (Smith and Aylward, 2009; Yates et al., 2012).
(ii)
a large sub-corotating jet, from region III to I (blue region in Fig. 4a-c).
This sub-corotational jet is caused by the drag of the sub-corotating magnetosphere on the thermosphere.
Zonal flows in this region are generally sub-corotational and acceleration terms are balanced in case CS, as the thermosphere is in steady-state.
Fig. 4(d)-(f) shows the variation of meridional flows in the high latitude thermosphere for our transient compression cases.
Magnetospheric labels, locus of corotation and arrows are the same as in Fig. 4(a)-(c).
These figures show the meridional flow patterns in the thermosphere, as well as localized accelerated regions (red/brown hues).
In steady state, flow patterns are as described by Smith et al. (2007), Smith and Aylward (2009) and Yates et al. (2012) - where at(i)
Low-altitude (<600km), ion drag acceleration becomes strong due to the Pedersen conductivity layer (maximum value of 0.1163mhom-1 at ~370km).
An imbalance is created between ion drag, Coriolis and pressure gradient terms; thus, giving rise to advection of momentum, which restores equilibrium in this low altitude region.
This results in mostly sub-corotational, poleward accelerated flow as shown by the black arrows and brown hues in Fig. 4(d).
(ii)
High-altitude (>600km), conditions are quite different, meridional Coriolis and pressure gradient accelerations are essentially balanced, whilst terms such as ion drag, advection and zonal Coriolis are small and insignificant.
This creates a 'jovistrophic' condition, whereby flow is directed very slightly equatorwards and is sub-corotational (see black arrows in Fig. 4(d)).
The above descriptions of zonal and meridional flow patterns pertain to steady state conditions (Fig. 4(a) and (d), respectively).
Zonal flows for case CH (Fig. 4b) show little change from the steady state zonal flow patterns described above.
The main differences lie in the magnitude of the velocities; velocity in the super-corotational jet doubles and the magnitude of azimuthal velocity has decreased by ~4% in the sub-corotational jet.
Meridional flows in Fig. 4(e) show two additional local acceleration regions either side of ~73° latitude and from altitudes >500km.
In addition, low altitude flow in region III is now directed purely equatorward.
This is in marked contrast to the steady state flow patterns.
All the changes in flows discussed for case CH result from the super-corotation of the magnetosphere which causes a reversal in the coupling current, subsequently leading to a change in the sign of ion drag momentum terms in region III (see Fig. 9 and corresponding discussion).
Zonal and meridional flows of case CF are respectively shown in Fig. 4(c) and (f).
The overall flow patterns are as described above for case CH: (i) a large sub-corotational jet combined with low altitude poleward flows and high altitude equatorward flows in regions II and I, and (ii) a low small low altitude super-corotational jet combined with equatorward flows in region III.
However, the degree and spatial extent of super-corotation has decreased and a number of local accelerated regions exist where the direction of meridional flow changes on relatively small spatial scales.
These complex flow patterns result from the highly perturbed nature of the case CF thermosphere and the imbalance between ion drag, Coriolis, pressure gradients and advection of momentum terms.
Thermospheric heating
Fig. 4(g) shows thermospheric temperature as a function of altitude and latitude for case CS.
Fig. 4(h) and (i) shows the difference in thermospheric temperature between cases CH and CS, and cases CF and CS.
We will use Fig. 5(a)-(f), showing contour plots for various thermospheric heating (Fig. 5(a)-(c)) and cooling (Fig. 5(d)-(f)) terms (see plot legends for details) to interpret the temperature response.
In Fig. 4(g) we see a clear temperature difference between upper (>75°) and lower (<75°) latitudes; lower latitudes are cooled whilst upper latitudes are significantly heated (Smith et al., 2007; Smith and Aylward, 2009).
We see a 'hotspot' (in region I) with a peak temperature of ~705K.
This arises from the poleward transport of Joule heating (from regions III and II) by the accelerated meridional flows as shown in Fig. 4(d) (Smith and Aylward, 2009; Yates et al., 2012).
Fig. 4(h) shows the temperature difference between cases CH and CS.
There are three prominent features in Fig. 4(h):(i)
Temperature increase up to ~26K across the region III/II boundary (z≥400km) resulting from a large (×2) increase in Joule heating and the addition of other heat sources, such as adiabatic heating (see Fig. 5(b)).
The large increase in Joule heating is caused by the increase in the rest-frame electric field, and the corresponding Pedersen current density.
(ii)
Temperature decrease down to ~-22K, at low altitudes of region II.
Fig. 5(b) shows that at low altitudes (≤500km) of region II there is, on average, a 20% decrease in energy dissipated by Joule heating and ion drag energy.
This coupled with the presence of energy lost by ion drag energy (Fig. 5(e)) in this region causes the significant decrease in temperature shown above.
All the factors discussed above result from the reversal and decrease (in magnitude) of the flow shear between the magnetosphere and thermosphere in case CH.
(iii)
A maximum of ~17K increase at low altitudes in region I.
The meridional velocity of case CH increases slightly (~2%) in this region and, as such, can transport heat from Joule heating and ion drag energy polewards somewhat more efficiently than in case CS.
Fig. 4(i) shows the temperature difference between cases CF and CS.
Immediately, we can see that there are changes in the distribution of temperature in the upper thermosphere of case CF.
There are four 'finger-like' regions with local temperature increases ≥50K (maximum of 175K; white contour encircles regions where temperature difference is ≥100K) and three regions with temperature decreases ≤40K.
These alternating temperature deviations increase with altitude and are collocated with accelerated meridional flow regions.
Considering Fig. 5(c) and (f), we see that the heating and cooling terms are now quite complex, with advective and adiabatic terms dominating (≥10× Joule heating and ion drag energy terms).
The CF thermosphere appears to be transporting heat, both equatorward and poleward from the region III/II boundary (see Fig. 4(f)).
Achilleos et al. (1998) also shows a similar phenomenon (see top left of Fig. 9 in Achilleos et al., 1998), whereby perturbations of high temperature are transported away from the auroral region by meridional winds.
The energy deposited in the auroral regions heats the local thermosphere which increases local pressure gradients.
Advection then attempts to redistribute this heat which momentarily cools the local area until enough heat is deposited again and the process restarts.
Fig. 5(g)-(i) shows powers per unit area as functions of ionospheric latitude for cases CS, CH and CF, respectively (calculated using Eqs.
(A.7)-(A.11)).
Blue lines represent total power transferred to the ionosphere from planetary rotation which is divided into the power used to accelerate the magnetospheric plasma (magnetospheric power; red lines) and power dissipated in the thermosphere (for atmospheric heating and changing kinetic energy; green lines).
Atmospheric power is subdivided into Joule heating (black lines) and ion drag energy (cyan lines).
In case CS, magnetospheric power is dominant up to ~73°, where atmospheric power quickly dominates for all poleward latitudes (see Fig. 5g).
This indicates that a relatively expanded M-I system (in steady-state) generally dissipates more heat in the atmosphere than in acceleration of outward-moving plasma (Yates et al., 2012).
In case CF, powers per unit area closely resemble those for case CS.
There are increases in peak magnetospheric power (~10%) and Joule heating (~25%) leading to an overall maximum increase in available power of ~10%.
This increase in total power is ultimately due to the lag in response of the thermosphere.
For case CH, Fig. 5(h), we see the effects of plasma super-corotation in region III, where magnetospheric power reverses (now negative) and energy is now transferred from magnetosphere to thermosphere.
As a consequence, heat dissipated as Joule heating doubles, positive ion drag energy decreases by ~70% and negative ion drag energy increases by two orders of magnitude.
These effects lead to the local temperature variations seen above.
Powers decrease in region II due to the decrease in azimuthal flow shear between the magnetosphere and thermosphere (see Fig. 2(a)).
Magnetospheric expansions
This section presents our findings for a transient magnetospheric expansion event with a 3h duration.
Auroral currents
FAC densities in the high latitude region are plotted for cases ES (blue line), EH (red line) and EF (green line) in Fig. 6.
Comparing cases ES with EH we see three main differences: (i) EH has two upward FACs peaks in region III (of similar magnitude to the peak in case ES) creating a large area of upward-directed FAC, (ii) the magnitude of downward FAC near the region III boundary has increased by a factor of four (from ES to EH) and (iii) FAC densities at the region II/I boundary are entirely downward-directed, unlike case ES.
As the magnetosphere expands, its magnetic field strength and plasma angular velocity decrease.
This change in ΩM (see Fig. 2(b)) increases the flow shear between the magnetosphere and thermosphere and thus increases the FAC density in region III by ~15%.
The strong downward FAC results from the large gradients in ΩM through the poleward boundary of region III, where magnetodisc plasma moves from a region with angular velocity of 0.9ΩJ to a region moving at 0.2ΩJ.
The lack of a peak at the region II/I boundary is due to the small change in ΩM as the model traverses these two regions.
Case EF shows only small differences with case ES due to the lag in response time of the thermosphere to transient magnetospheric changes on this time scale.
Looking now at case EH, and comparing FAC densities with the corresponding result from Cowley et al. (2007) (expansion from 45-85RJ), we notice a few differences: (i) the magnitude of peak upward FAC in case EH is ~25% larger than that in Cowley et al. (2007) and (ii) case EF has no upward FAC at the region II/I boundary, contrary to results in Cowley et al. (2007).
These differences emphasize the effect of using a time-dependent GCM for the thermospheric response.
For example, the 'double peak' structure in the upward Region III FACs is due to additional modulation of current density by thermospheric flow.
We interpret our FAC density profiles by considering the corresponding precipitating electron energy fluxes, as shown in Fig. 6(b).
Fluxes are plotted as functions of latitude.
The line style code and labels are the same as in Fig. 6(a), the latitudinal size of a HST ACS-SBC pixel is indicated by the dark grey rectangle and the grey solid line indicates the limit of present HST detectability (~1kR; Cowley et al., 2007).
We begin by comparing profiles for case ES with EF, which are almost identical and both have maxima at ~74° latitude, equivalent to the location of the 'main auroral oval', and at ~80°, the boundary between open (region I) and closed field lines (region II).
Therefore, we would expect a fairly bright auroral oval of ~88kR for case ES and ~79kR for case EF.
The electron energy flux for case EF (~7.85mWm-2) is ~10% smaller than case ES (~8.8mWm-2) due to ΩT(ES)>ΩT(EF) leading to a smaller flow shear.
Our model also predicts the possibility of observable polar emission (region II/I boundary) of ~15kR for both cases ES and EF.
However, this region is strongly dependent on the plasma flow model used and poorly constrained by observations.
Energy flux Ef for case EH is non-existent, poleward of ~74° latitude, due to the downward (negative) FAC density in this region.
In region III, there are two upward FAC peaks, separated by ~1°.
The first one, located at ~73° is ~37% larger than the second, at ~74°.
These peaks result from the large degree of magnetospheric sub-corotation and the modulation of the thermospheric angular velocity in region III (evident in Fig. 2(b)).
Comparing case EH with the equivalent expansion case in Cowley et al. (2007); case EH, in region III, has a maximum value of Ef (~12.6mWm-2) that is twice that in Cowley et al. (2007).
This study represents the thermosphere with a GCM which responds self-consistently to time-dependent changes in ΩM profiles.
Our results indicate that this response is not as strong as that in Cowley et al. (2007), who use a simple 'slippage' relation to model the thermospheric angular velocity.
At the open-closed field line (region II/I) boundary, Cowley et al. (2007) obtain larger energy fluxes due to their large change in ΩM across these regions; in our study, we obtain negligible changes in Ef due to our smaller change in imposed ΩM across this boundary.
Thermospheric dynamics
The altitude-latitude variation of azimuthal and meridional thermospheric velocities and temperature are shown in Fig. 7.
The first column in Fig. 7 shows thermospheric outputs for case ES; cases EH and EF are represented in columns two and three, respectively.
For case ES, the zonal (Fig. 7(a)) and meridional (Fig. 7(d)) flows are very similar to those discussed in Yates et al. (2012) as the only difference between both steady-state compressed cases is that here we assume a constant height-integrated Pedersen conductivity whilst in Yates et al. (2012) the conductivity is enhanced by FAC.
Zonal flows show a low-altitude super-corotational jet in region III and two sub-corotational jets across regions II and I.
The meridional flows show the previously discussed flow patterns, i.e. low-altitude poleward flow and high-altitude equatorward flow.
Thermospheric flows for case EH are slightly different from those of case ES.
A magnetospheric expansion decreases the degree of plasma corotation which subsequently decreases the thermospheric zonal velocities, i.e. they become more sub-corotational (see Fig. 7(b)).
In the meridional sense (Fig. 7(e)), low altitude flows remain poleward but with an increased magnitude (up to ~30%) and all flow in region II is now directed poleward.
Extra heating (see Section 5.3) near the region III/II boundary causes the forces in the local thermosphere to become unbalanced leading to accelerated flows in the poleward and equatorward (high-altitude of region III) directions.
The thermospheric velocities at the end of the transient expansion event (case EF) are shown in the third column of Fig. 7.
We see that the only change in zonal flow patterns is a slight increase in the zonal velocity (algebraic increase).
The meridional winds show a large poleward accelerated flow originating at low altitudes in region III and reaching the high altitudes of region I.
Two smaller regions of accelerated equatorward flow arise in the upper altitudes of regions III and II.
As the magnetosphere returns to its initial configuration, it weakly super-corotates over most of region III; this transfers angular momentum to the sub-corotating thermosphere which acts to 'spin up' the thermospheric gas.
Thermospheric heating
Fig. 7(g) shows temperature as a function of altitude and latitude for case ES.
Fig. 7(h) and (i) shows the difference in thermospheric temperature between cases EH and ES, and cases EF and ES, as functions of altitude and latitude.
Magnetospheric regions are labelled and separated by black dotted lines and temperatures are indicated by the colour bar.
Interpretation of the response of thermospheric temperature is aided by Fig. 8(a)-(f), showing contour plots for various thermospheric heating (Fig. 8(a)-(c)) and cooling (Fig. 8(d)-(f)) terms (see plot legends for details).
Fig. 7(g) shows similar results to those described in Section 4.3.
The main difference is related to the polar 'hotspot' which is considerably cooler (peak temperature of ~590K) than that for case CS (peak temperature ~705K).
As previously discussed, the 'hotspot' results from the meridional transport (via poleward accelerated flows) of Joule heating from lower latitudes (~73-84°; see Figs.
7(d) and 8(a)) (Smith and Aylward, 2009; Yates et al., 2012).
Fig. 7(h) exhibits the temperature difference between cases EH and ES.
The figure shows a maximum of ~50K temperature increase at low altitudes (<700km) in regions III and II.
Also evident are two minor temperature variations: (i) ~10K decrease at high altitude, centred on the region III/II boundary and (ii) ~10K increase in the polar 'hotspot' region.
Fig. 8(b) shows a large (≥4×) increase in ion drag energy and Joule heating rates which accounts for the temperature increase across regions III and II.
This low-altitude increase in temperature causes a local increase in pressure gradients leading to accelerated meridional flows being able to efficiently transport heat away from the region III/II boundary and towards the pole.
The high altitude cool region ensues from local equatorward and poleward meridional flows combined with factor-of-three increase in adiabatic cooling (Fig. 8(e)).
Fig. 7(i) shows the temperature difference between cases EF and ES.
The temperature profile has changed significantly from that in Fig. 7(h).
There are two regions where temperatures increase by up to ~50K: (i) extending from ~73-85° latitude and low altitudes in regions III and II, and all altitudes in region I (these map to the large poleward-accelerated region in Fig. 7(f)); and (ii) high-altitude (>600km) region, centred at ~66° latitude.
These regions are primarily heated by horizontal advection (high-altitude only) and adiabatic terms (all altitudes) as shown in Fig. 8(c); these heating rates have increased (from case ES) by, at most, 800% and 500%, respectively.
The final feature of note in Fig. 7(i) is the region cooled by up to ~-22K, lying between the two heated regions at altitudes >550km.
This cooling is caused by a combination of local increases in horizontal advection and adiabatic cooling, by factors of three and greater.
Similar to case CF, case EF's meridional flows seem to be transporting heat equatorward and poleward, although the majority of these flows act to transport thermal energy poleward.
Fig. 8(g)-(i) shows powers per unit area as functions of ionospheric latitude for cases ES-EF respectively.
Colour codes and labels are as in Fig. 5(g)-(i).
Fig. 8(g) shows the energy balance in the thermosphere for case ES.
As discussed in Yates et al. (2012), most of the energy in region III is expended in accelerating magnetospheric plasma; in region II we have a situation where magnetospheric power and atmospheric power (the sum of Joule heating and ion drag energy) are equal, due to ΩM=0.5ΩJ.
Atmospheric power is dominant in region I.
For case EH (Fig. 8(h)), the magnetodisc plasma sub-corotates to a large degree which causes the majority of available power to be used in accelerating the sub-corotating plasma.
Poleward of ~73° latitude, the large flow shear (ΩT-ΩM) leads to an increase in energy dissipated within the thermosphere, primarily through Joule heating.
The magnetosphere of case EF super-corotates, compared to the thermosphere, at latitudes ≤73°.
This causes a reversal in energy transfer, which now flows from magnetosphere to atmosphere and acts to spin up the sub-corotating neutral thermosphere (see Fig. 8(i)).
Polewards of 73°, the energy balance is similar to that of case ES.
Discussion
Effect of a non-responsive thermosphere on M-I coupling currents
Our work makes no a priori assumptions regarding the response of the thermosphere to magnetospheric forcing.
The GCM responds self-consistently by solving the Navier-Stokes equations for momentum, energy and continuity.
For completeness, we calculated M-I coupling currents for the case of a non-responsive thermosphere (Gong, 2005).
To do this, we assume that ΩT=ΩT(CS) throughout the entire transient event.
In this non-responsive thermosphere scenario, there is an average increase in M-I currents of ~20% midway through the pulse compared to case CH (obtained using GCM).
At full-pulse, however, the non-responsive case has M-I currents that are on average ~12% smaller than currents in case CF.
These differences between a non-responsive thermosphere and a responsive one (GCM), are related to the flow shear between thermosphere and magnetosphere; which, is maximal (resp.
minimal) at half-pulse (resp.
full-pulse) when using a non-responsive thermosphere.
A similar analysis for the expansion scenario results in an average of ~20% increase in the maximum magnitude of M-I currents in a non-responsive thermosphere, compared to the GCM thermosphere.
Here, ΩT(ES) is uniformly larger than ΩT for cases EH and EF (see Fig. 2) so the flow shear in the non-responsive scenario will always be greater than the flow shear obtained with the GCM thermosphere.
The auroral response: predictions and comparisons with observations
Figs. 3(b) and 6(b) respectively show the change in precipitating electron energy flux in response to transient magnetospheric compression and expansion events.
We also indicate (on the right axis of these figures) the corresponding UV emission associated with such energy fluxes (assuming that 1mWm-2 of precipitation creates ~10kR of UV output).
Considering the compression scenario, our results suggest that the arrival of a solar wind shock would increase the UV emission of the main oval from ~600kR to ~2800kR (factor of 4.7) and constrict the width of the oval by ~0.2°.
The HST detectability limit and the size of an HST pixel (dark grey box in Fig. 3(b)) suggest that such an increase in auroral emission would be detectable but the constriction of the main oval may be too small to be observed.
Clarke et al. (2009) and Nichols et al. (2009) observed that the brightness of UV auroral emission increased by a factor of two, in response to transient (almost instantaneous) increases in solar wind dynamic pressure (~0.01-0.3nPa or equivalently ~109-72RJ).
The increase in UV emission was also found to persist for a few days following the solar wind shock.
Nichols et al. (2009) also observed poleward shifts (constrictions) in main oval emission on the order of ~1° corresponding to the arrival of solar wind shocks.
Total emitted UV power may also be used to describe auroral activity, assuming that this quantity is ~10% of the integrated electron energy flux per hemisphere (Cowley et al., 2007).
Case CH has a total UV power of ~1.58TW (compared to ~420GW for case CS), which is a factor of two to three times larger than UV powers observed by both Clarke et al. (2009) and Nichols et al. (2009).
The profile of case CH also indicates the possibility of observable polar emission at region II/I (open-closed) boundary.
This conclusion is, however, sensitive to our model assumptions (see Section 6.4).
Our model results predict very different behaviour for the expansion scenario (Fig. 6(b)).
At maximum expansion we would expect a small increase (~40kR) in peak main oval brightness along with a ~1° equatorward shift (expansion) of the oval.
We also note the possible observation of a somewhat bifurcated main oval (see HST pixel in figure); with emission peaking at ~73° and ~74° latitude.
The main oval would, either way, appear considerably broader (~2-3°) as a result of the large increase in the spatial region of magnetospheric sub-corotation.
Clarke et al. (2009) observed little change in auroral brightness near the arrival of a solar wind rarefaction region, however Nichols et al. (2009) have seen changes in main oval location.
The total UV power in case EH is ~270GW (compared to ~78GW in case ES).
While this power is considerably smaller than that in case CH, it is comparable to UV powers calculated in Clarke et al. (2009) and Nichols et al. (2009), following solar wind rarefactions (~200-400GW).
Global thermospheric response
The arrival of solar wind shocks or rarefactions has, for the most part, a similar effect on thermospheric flows.
Our modelling shows a general increase (resp.
decrease) in the degree of corotation with solar wind dynamic pressure increases (resp.
decreases).
Zonal flow patterns remain essentially unchanged with a large sub-corotational jet and a small super-corotational jet.
Meridional flow cells however, respond to transient magnetospheric reconfigurations somewhat chaotically, with numerous poleward and equatorward accelerated flow regions developing (at altitudes >600km) with time throughout the event.
The overall low-altitude poleward flow remains fixed with solar wind rarefactions but reverses in response to a solar wind shock (see cases CH and CF in Fig. 4(e) and (f)).
This flow becomes equatorward due to a reversal in the direction of ion drag acceleration in the region III, as shown in Fig. 9.
This reversal, in turn, arises from the super-corotation of magnetospheric plasma.
Compared to the transient compression case CF, the EF thermosphere seems fairly stable, i.e. there are no sharp peaks and troughs in the upper boundary.
Our interpretation is that for the compression scenario the magnetosphere transfers a large amount of angular momentum to the thermosphere due to its large degree of super-corotation.
This surge in momentum and energy input to the thermosphere over a short time scale causes significant strain on the thermosphere and thus requires a drastic reconfiguration in order to attempt to re-establish dynamic equilibrium.
On the other hand, in our expansion scenario the magnetosphere significantly sub-corotates for most of the event and only super-corotates compared to the planet and thermosphere (slightly) nearing the end of the event.
Thus, for the majority of the expansion event the thermosphere is losing angular momentum to the magnetosphere.
This implies that its dynamics and energy input are generally smaller than the transient compression scenario, which leads to a less 'drastic' response.
The magnetospheric reconfigurations discussed above have been shown to have a significant impact on the dynamics and energy balance of the thermosphere.
We now attempt to globally quantify such changes in energy by calculating the integrated power per hemisphere obtained from the power densities in Figs.
5(g)-(i) and 8(g)-(i).
These integrated powers are presented in Fig. 10(a) and (b) for the compression and expansion scenarios respectively.
Blue bars represent the kinetic energy dissipated by ion drag, green bars indicate Joule heating, red bars represent the power used to accelerate magnetospheric plasma and orange bars simply represent the sum of all the above terms.
Positive powers indicate energy dissipated in/by the thermosphere whilst negative powers indicate energy deposited into the thermosphere.
Midway through the compression event (case CH), magnetospheric plasma super-corotates compared to the thermosphere and deep atmosphere.
This reverses the direction of momentum and energy transfer so that energy is now being transferred from the magnetosphere to the thermosphere.
Our results indicate that ~2000TW of total power (magnetospheric, Joule heating and ion drag energy) is gained by the coupled system as a result of plasma super-corotation.
Note that this is considerably larger than the ~325TW (closed and open field regions) calculated in Cowley et al. (2007) for a responsive thermosphere scenario.
This energy transfer from the magnetosphere would act to, essentially 'spin up' the planet (Cowley et al., 2007) and increase the thermospheric temperature.
In case CF, plasma is not super-corotating; thus the picture is fairly similar to case CS.
The main difference is that there is a ~20% increase in total power dissipated in the atmosphere and in acceleration of the magnetosphere.
This arises from increases in flow shear due to the 'lagging' thermosphere (see Fig. 2) and inevitably leads to the local temperature increases seen in Fig. 4(i) and discussed above.
The finite time required for thermospheric response results in the described 'residual' perturbations to the initial system (CS) even after the pulse has subsided (CF).
A transient magnetospheric expansion event creates a significant increase in both power dissipated in the atmosphere due to Joule heating (~6× that of ES) and ion drag energy (~3× that of ES).
Moreover, the power used to accelerate the magnetosphere towards corotation is ~7× that of ES, and is shown in Fig. 10(b).
These increases lead to a total power per hemisphere of ~2600TW which is three times larger than the responsive thermosphere case in Cowley et al. (2007).
These changes in heating and cooling create the local temperature increases discussed above.
For case EF, where we now have the magnetosphere rotating faster than the thermosphere, there is a ~75% decrease in the magnitude of 'magnetospheric' power.
The magnetosphere is thus transferring power to the thermosphere in this case, albeit a relatively small amount.
This effectively 'pulls' the thermosphere along, increasing its angular velocity in order to return to the steady-state situation where ΩT>ΩM.
We note that energy dissipated via Joule heating also decreases slightly due to the small decrease in flow shear.
Overall, then, the total power per hemisphere in case EF is only 30% that of case ES.
Results for cases CH and EH show large (approximately three orders of magnitude larger than solar heating) increases in energy either being deposited or dissipated in the thermosphere.
Observations by Stallard et al. (2001, 2002) of an auroral heating event at Jupiter were analysed by Melin et al. (2006).
These authors found that during this auroral heating event, which they attribute to being caused by a decrease in solar wind dynamic pressure, the combined ion drag energy and Joule heating rates increase from 67mWm-2 to 277mWm-2 over 3 days.
They proposed that this extra heat must then be transported equatorward from the auroral regions by an increase in equatorward meridional winds (Waite et al., 1983).
If we assume that their auroral region ranges from 65° to 85° latitude and that these heating rates are constant across such a region, the total energy dissipated by Joule heating and ion drag energy increases from ~193TW to ~800TW.
This increase is comparable to the increase of Joule heating and ion drag energy in our expansion scenario, going from case ES (~201TW) to EH (~942TW).
Increase in Joule heating and ion drag energy from case CS to CH is more modest (~499TW-~555TW) due to the reversal of kinetic energy exchange between atmospheric neutrals and ions and despite an increase in Joule heating.
Our modelling supports the work of Melin et al. (2006) in terms of (i) the magnitude of energy dissipated in the thermosphere and (ii) the type of magnetospheric reconfiguration required.
We do not however, see a significant increase in equatorward flows in our expansion scenario.
Our compression scenario, however, shows a large change in meridional flow patterns with a large portion of the thermosphere flowing equatorwards.
Model limitations
The main limitation to our transient model is the use of a fixed model for relative changes in conductivity with altitude, and a uniform Pedersen conductance ΣP for the ionosphere (see Section 3.2).
Whilst not ideal, we feel it is a suitable first step to developing a fully self-consistent, time-dependent model of the Jovian magnetosphere-ionosphere-thermosphere system.
Use of an enhanced conductivity model would concentrate all but background levels of conductance just equatorward of the main auroral oval location (~74°) (Yates et al., 2012); effectively increasing the coupling between the atmosphere and magnetosphere in this region.
We would thus expect the magnitude of current densities to increase in the region near the main oval (region III/II boundary in our model), along with an increase in the Joule heating rate.
The high conductivity at latitudes between 60° and 70° in the present model, combined with the super-corotation of the thermosphere, allows for the plasma magnetically mapped to these ionospheric latitudes to super-corotate slightly in steady state.
With an enhanced conductivity model, this region would have a super-corotating thermosphere but low, background-level conductances (e.g. Smith and Aylward, 2009; Yates et al., 2012).
Therefore, even though the super-corotating thermosphere acts to accelerate the magnetodisc plasma, the low conductances inhibit how efficiently the plasma is accelerated.
It is worth noting that despite the fact that, in this study, both the neutral thermosphere and magnetodisc plasma super-corotate compared to the deep atmosphere, as long as the plasma sub-corotates compared to the thermosphere, angular momentum and energy will be transferred from the upper atmosphere to the magnetosphere as is expected in steady state.
We plan to incorporate enhancements in Pedersen conductance due to auroral precipitation of electrons in a future study.
Other limitations to this model include(i)
Assumption of axial symmetry: Discussions in Smith and Aylward (2009) conclude that the assumption of axial symmetry with respect to the planet's rotation axis does not significantly alter the thermospheric outputs of our model.
They find that axial symmetry leads to modelling errors on the order of ~20% which are less than, or at least comparable to, errors derived from the various other assumptions and simplifications made in this coupled model.
(ii)
No development of field-aligned potentials: Our model does not currently include the development of field-aligned potentials, which accelerate electrons from the high latitude magnetosphere into the ionosphere.
We simply apply the linear approximation to the Knight relation (see Section Appendix B) to obtain precipitating electron energy fluxes.
Ray et al. (2009) show that significant field-aligned potentials develop at high-latitudes to supply the necessary FACs, and hence angular momentum, demanded by the magnetospheric plasma.
By applying the linear approximation to the Knight relation, we assume that the top of the acceleration region is far enough from the planet such that the ratio of the energy gained by a particle traveling through the potential drop to its thermal energy is significantly less than the mirror ratio between top and bottom of the acceleration region.
Consequently, possible current saturation effects are ignored, with the field-aligned current density increasing to values beyond those that would result from the entire electron distribution accelerated into the loss cone.
The M-I coupling modelling by Ray et al. (2010) also showed that including field-aligned potentials in a self-consistent treatment of the auroral current system alters the electric field mapping between the ionosphere and the magnetosphere, decoupling the ionospheric and magnetospheric flows.
Their model did not explicitly include thermospheric flows; however, the presence of field-aligned potentials may also plausibly alter the thermospheric angular velocity.
(iii)
Fixed plasma angular velocity in the polar cap region (latitudes >80°): The plasma angular velocity in the polar cap region ΩMpc is fixed at a constant value of ~0.1ΩJ, in accordance with the formulations in Isbell et al. (1984) which depend in part on the solar wind velocity vsw.
A change in solar wind dynamic pressure psw would generally be accompanied by a corresponding change in vsw, so when we change the magnetospheric configuration of our model, ΩMpc should also change depending on the new value of vsw.
If we assume that the solar wind density ρsw remains constant and that psw≈ρswvsw2, ΩMpc(CS)≈0.06ΩJ and ΩMpc(CH)≈0.17ΩJ.
We find the difference between the plasma angular velocities across the open-closed field line boundary with a constant or variable ΩMpc to be negligible for both compressed and expanded magnetospheres and thus do not expect this to significantly influence the results discussed above.
Conclusion
We investigated the effect of transient variations in solar wind dynamic pressure on the M-I coupling currents, thermospheric flows, heating and cooling rates and aurora of the Jovian system.
We considered two scenarios: (i) a transient compression event and (ii) a transient expansion event.
Both of these were imposed over a time scale of 3h.
A transient compression event consists of an initially expanded, steady-state magnetospheric configuration.
The model Jovian magnetosphere then encounters a shock in the solar wind, which compresses the system.
As the conceptual shock propagates past the magnetosphere, a rarefaction region follows and the magnetosphere subsequently expands back to its initial state.
The opposite occurs for our expansion event.
We have made an important initial step into investigating how time-dependent phenomena affect the Jovian system.
In steady state, the more expanded the magnetosphere is, the hotter Jupiter's thermosphere is likely to be Yates et al. (2012).
The caveat to this is that only the polar (high-altitude) region of the thermosphere (due to the poleward meridional winds) approaches the observable temperatures of ~900K (Seiff et al., 1998; Yelle and Miller, 2004; Lystrup et al., 2008).
The lower latitudes are still relatively cool with temperatures of ~ 200-300K, compared to polar temperatures of up to ~700K.
On the other hand, when we consider rapid magnetospheric reconfigurations, the situation is quite different.
We see a change in the direction of meridional winds as well as a large (at least a factor of two) increase in Joule heating and energy being dissipated in or deposited to the thermosphere.
These winds redistribute the extra heat, essentially sending 'wave-like perturbations' of high-temperature gas (higher than ambient surroundings) towards both the polar and equatorial regions (Waite et al., 1983; Achilleos et al., 1998; Melin et al., 2006).
The present results are not enough to increase the temperature of the equatorial thermosphere to its observed values but we stress that all the results presented herein occur within a period of 3h (approximately 1/3 of a Jovian day).
This leads to the potential of future, more realistic, time-dependent studies whereby one could vary the duration of such transient events, experiment with 'chains' of such events and/or more realistic solar wind dynamic pressure profiles, in order to model the dynamic response of the Jovian thermosphere over more extended periods of external perturbation.
Acknowledgements
JNY was supported by an STFC studentship award.
NA was supported by STFC's UCL Astrophysics Consolidated Grant ST/J001511/1.
The authors acknowledge support of the STFC funded Miracle Consortium (part of the DiRAC facility) in providing access to computational resources.
The authors express their gratitude to Chris Smith who developed the GCM used herein and to Licia Ray and Fran Bagenal for our useful discussions.
The authors would also like to thank two anonymous referees for their useful comments and suggestions.
Magnetosphere-ionosphere coupling
In this section we discuss the effect of coupling the magnetosphere and ionosphere together.
The meridional electric field in the rest frame of the thermosphere may be written as(A.1)Eθ=Biρi(ΩT-ΩM),where Bi is the magnitude of the (assumed) radial ionospheric magnetic field (Bi=2BJ).
The combination of electric field, magnetic field and ion-neutral collisions causes Pedersen currents to flow in the ionosphere, mainly perpendicular to the direction of the planetary magnetic field.
These ionospheric currents form part of a larger current circuit which includes the radial current flowing in the magnetodisc and the FAC flowing along the magnetic field lines.
The height integrated Pedersen current density iP and its azimuthally integrated form IP(θi) are (Cowley et al., 2007; Smith and Aylward, 2009)(A.2)iP=ρiΣP(ΩT-ΩM)Bi,and(A.3)IP(θi)=2πρi2ΣP(ΩT-ΩM)Bi,where ΩT is a weighted average, computed over altitude, of the angular velocity of the thermosphere.
For a more detailed description of this the reader is referred to Yates et al. (2012) and Smith and Aylward (2009).
The height-integrated radial current density in the magnetodisc is denoted by iρ and can be obtained using Eq.
(A.2) under the assumption of current continuity (zero divergence of current density).
We have (Nichols and Cowley, 2004; Smith and Aylward, 2009)(A.4)ρeiρ=2ρiiP,(A.5)Iρ=8πΣPFe(ΩT-ΩM),where Iρ is the azimuthally integrated disc current.
The third and final component of our M-I current circuit is the FAC density.
j||i(θi) represents the FAC density at the ionospheric footpoint (at co-latitude θi) of the respective field lines.
This current density is obtained from the horizontal divergence of the Pedersen current:(A.6)j||i(θi)=-12πRi2sinθidIPdθi,where the sign of j||i(θi) indicates FAC direction (positive upward from planet).
Eq.
(A.6) corresponds to the northern hemisphere, where the magnetic field points radially outward (approximately, in auroral region) (Cowley et al., 2007).
The final aspect of M-I coupling we examine in this study is the energy transfer from planetary rotation to the thermosphere and magnetosphere.
The angular momentum transfer to the magnetosphere is used to accelerate magnetospheric plasma towards corotation whilst the energy dissipated within the thermosphere is used for heating and increasing kinetic energy.
The total power per unit area of the ionosphere transferred from planetary rotation P is the sum of atmospheric power PA and magnetospheric power PM dissipated per unit area (Hill, 2001).
As shown by Smith et al. (2005) atmospheric power consists of two components: (i) Joule heating PJ and (ii) ion drag power PD, some of may be viscously dissipated as heat.
These power relations are (Cowley et al., 2005)(A.7)P=ΩJτ,(A.8)PM=ΩMτ,(A.9)PA=(ΩJ-ΩM)τ,(A.10)PJ=(ΩT-ΩM)τ,(A.11)PD=(ΩJ-ΩT)τ,where(A.12)τ=ρiiPBirepresents the torque exerted by the J×B force per unit area of the ionosphere.
Auroral energies
Once FAC densities have been calculated, we can use the methods of Knight (1973) and Lundin and Sandahl (1978), as presented in Cowley et al. (2007), to calculate the enhanced precipitating electron energy flux Ef:(B.1)Ef=Ef02((j||ij||i0)2+1),where Ef0 is the unaccelerated electron energy flux, j||i0 is the unaccelerated FAC density (or the maximum current that can be carried by the electrons in the absence of field-aligned potential drops) and j||i is the upward (positive) FAC density calculated using Eq.
(A.6).
To enable a comparison with similar, earlier studies, we use the same electron population values described in Cowley et al. (2007), which are based on observations by Scudder et al. (1981) and Phillips et al. (1993a,b).
These parameters are presented in Table B1.

1. A precursor of a positive electrode active material for a secondary battery, the precursor comprising a monolayer-structured secondary particle in which columnar primary particles radially oriented from the center of the particle toward the surface are aggregated,
wherein the secondary particles have a shell shape; and
the primary particles include a Ni-Co-Mn composite metal hydroxide of the following chemical formula 1:
[ chemical formula 1]
Ni1-(x+y+z)CoxMyMnz(OH)2
Wherein, in chemical formula 1,
m comprises any one or more than two elements selected from the following elements: al, Zr, Mg, Zn, Y, Fe and Ti; and
x, y and z are each 0< x <1, 0 ≦ y <1, 0< z <1, and 0< x + y + z < 1.2. The precursor of a positive electrode active material for a secondary battery according to claim 1, wherein 0< x + y + z <0.5 in chemical formula 1.3. The precursor of a positive electrode active material for a secondary battery according to claim 1, wherein the primary particles have a length ratio of 0.3 to 1 with respect to a radius of the secondary particles when a length of a long axis passing through a particle center of the primary particles is a length of the primary particles.4. The precursor of a positive electrode active material for a secondary battery according to claim 1, wherein the primary particles have an average aspect ratio of 5 to 30 when the aspect ratio is a ratio of a length of a long axis perpendicular to a short axis passing through a particle center of the primary particles to a length of the short axis.5. The precursor of the positive electrode active material for a secondary battery according to claim 1, having an average particle diameter (D) of 7 to 20 μm50) And 5.0m2G to 30.0m2BET specific surface area in g.6. A method of preparing a precursor of the positive electrode active material for a secondary battery according to claim 1, the method comprising:
preparing a metal-containing solution by mixing a nickel raw material, a cobalt raw material, and a manganese raw material; and
introducing an ammonium cation-containing complex former and a basic compound into the metal-containing solution and subjecting the resultant to a coprecipitation reaction at a pH of 10.50 to 12.00 and a temperature of 50 ℃ to 70 ℃,
wherein the ammonium cation-containing complex former is introduced at a rate of 0.5 to 1.5 times relative to the rate of introduction of the metal-containing solution.7. The method for preparing a precursor of a positive electrode active material for a secondary battery according to claim 6, wherein the complex-forming agent containing an ammonium cation and the basic compound are mixed in a ratio of 1: 10 to 1: 2 was used.8. A positive electrode active material for a secondary battery, comprising a monolayer-structured secondary particle in which columnar primary particles radially oriented from the center of the particle toward the surface are aggregated,
wherein the secondary particles have a shell shape; and
the primary particles include a lithium composite metal oxide of Ni-Co-Mn of the following chemical formula 2 and exhibit a monomodal particle distribution:
[ chemical formula 2]
Lia[Ni1-(x+y+z)CoxMyMnz]O2
Wherein, in chemical formula 2,
m comprises any one or more than two elements selected from the following elements: al, Zr, Mg, Zn, Y, Fe and Ti;
x, y and z are each 0< x <1, 0 ≦ y <1, 0< z <1, and 0< x + y + z < 1; and
a is more than or equal to 1.0 and less than or equal to 1.5.9. The positive electrode active material for a secondary battery according to claim 8, having an average particle diameter of 7 to 15 μm and 0.1m2G to 1.0m2BET specific surface area in g.10. The positive electrode active material for a secondary battery according to claim 8, having a tap density of 1.7 to 3.0 g/cc.11. A positive electrode for a secondary battery, comprising the positive electrode active material according to any one of claims 8 to 10.12. A lithium secondary battery comprising the positive electrode according to claim 11.5. The lithium transition metal composite particle of claim 4, wherein the metal-doped layer comprises composite particles of Chemical Formula 1:          <Chemical Formula 1>     Li<sub>a</sub>M<sub>1-b</sub>Me<sub>b</sub>O<sub>2</sub>
where M = NixMnyCoz, (0.3≤x≤0.9, 0≤y≤0.6, and 0≤z≤0.6),
Me is any one selected from the group consisting of Al, Zn, Zr, Ti, W, Sr, B, Mg, Y, Mo, Nb, Si, and Sn, or a mixed element of two or more thereof,
0.9≤a≤1.3, and
0<b≤0.02.14. The lithium transition metal composite particle of claim 13, wherein the lithium transition metal oxide is any one selected from the group consisting of LiCoO<sub>2</sub>, LiNiO<sub>2</sub>, LiMnO<sub>2</sub>, LiMn<sub>2</sub>O<sub>4</sub>, Li(Ni<sub>a</sub>Co<sub>b</sub>Mn<sub>c</sub>)O<sub>2</sub> (where 0<a<1, 0<b<1, 0<c<1, and a+b+c=1), LiNi<sub>1-Y</sub>Co<sub>Y</sub>O<sub>2</sub>, LiCo<sub>1-Y</sub>Mn<sub>Y</sub>O<sub>2</sub>, LiNi<sub>1-Y</sub>Mn<sub>Y</sub>O<sub>2</sub> (where 0≤Y<1), Li (Ni<sub>a</sub>Co<sub>b</sub>Mn<sub>c</sub>)O<sub>4</sub> (where 0<a<2, 0<b<2, 0<c<2, and a+b+c=2), LiMn<sub>2-z</sub>Ni<sub>z</sub>O<sub>4</sub>, and LiMn<sub>2-z</sub>Co<sub>z</sub>O<sub>4</sub> (where 0<z<2), or a mixture of two or more thereof.17. The method of claim 15 or 16, wherein the sintering is performed in a temperature range of 800°C to 1,000°C.18. The method of claim 15 or 16, wherein the heat treatment is performed in a temperature range of 300°C to 500°C.19. The method of claim 15 or 16, wherein the mixed transition metal precursor is a compound having a composition of MOOH or M(OH)2(where M = NixMnyCoz, 0.3≤x≤0.9, 0≤y≤0.6, 0≤z≤0.6, and x+y+z=1).22. The method of claim 15 or 16, wherein the surface modifier comprises polyvinylidene fluoride (PVdF), a polyvinylidene fluoride-hexafluoropropylene copolymer (PVdF-co-HFP), or a mixture thereof.23. The method of claim 15 or 16, wherein the surface modifier is used in an amount of 0.2 wt% to 0.5 wt% based on the total weight of the lithium transition metal oxide particles.1. A method for preparing power-type nickel cobalt lithium manganese oxide material, comprising:
adding organic acid into a mixed aqueous solution of a lithium source, a nickel source, cobalt source, and a manganese source;
aging, to obtain a sol precursor;
electrospinning, to obtain a gel fiber; and
calcinating to obtain the power-type nickel cobalt lithium manganese oxide material.2. The method according to claim 1, wherein in the mixed aqueous solution, a concentration of the nickel source is 1˜3 mol/L, wherein a concentration of the cobalt source is 1˜3 mol/L, wherein a concentration of the manganese source is 1˜3 mol/L, wherein a concentration of the lithium source is 1˜2 times of a total concentration of the nickel source, the cobalt source, and the manganese source.3. The method according to claim 1, wherein an amount of the organic acid is that a concentration of the organic acid in a system is 3˜5 mol/L after adding the organic acid.4. The method according to claim 1, wherein the organic acid is at least one of citric acid, tartaric acid, and oxalic acid.5. The method according to claim 1, wherein the lithium source is at least one of lithium acetate, lithium hydrate, and lithium carbonate; wherein the nickel source is at least one of nickel acetate, nickel hydroxide, and nickel carbonate; wherein the cobalt source is at least one of cobalt acetate, cobalt hydroxide, and cobalt carbonate; and wherein the manganese source is at least one of manganese acetate, manganese hydroxide, and manganese carbonate.6. The method according to claim 1, wherein the aging includes:
heating to 60˜70° C. first;
aging for 8˜10 hours till transparent; and
continuing aging at a room temperature till a viscosity is 2˜3 Pa·s.7. The method according to claim 1, wherein process conditions of the electrospinning include: a nozzle aperture being 500 μm, a feeding rate being 5˜10 mL/h, a voltage being 20˜40 kV, a fixed distance between the nozzle and a collector being 10˜30 cm, and a pressure being 0.3˜0.5 MPa.8. The method according to claim 1, wherein process of the calcination includes:
raising a temperature from the room temperature to 300˜400° C. at a rate of 0.5˜1° C./min and holding for 1˜3 hours;
raising the temperature to 600˜800° C. at a rate of 2˜4° C./min and holding for 8˜10 hours.9. A power-type nickel cobalt lithium manganese oxide material, wherein the power-type nickel cobalt lithium manganese oxide material is obtained through the preparing method according to claim 1.10. A use of the power-type nickel cobalt lithium manganese oxide material according to claim 9 in a battery.High-Performance Oxygen-Permeable Membranes with an Asymmetric Structure Using Ba0.95La0.05FeO3-δ Perovskite-Type Oxide
A porous BLF support was fabricated by an oxalate method 28. Ba (9.5 mmol) and La (0.5 mmol) acetates and Fe nitrate (10 mmol) were first dissolved in water (100 mL). The mixed metal salt solution was then added to ethanol (100 mL) containing oxalic acid (90 mmol). The mixing produced a yellow-colored suspension, which was allowed to stand for 1 h. The use of ethanol was critical to the precipitation of the metal oxalates. The suspension was filtrated to collect the precursor metal oxalate particles. The obtained precursor powder was dried at 120 degC for 2 h and then calcined at 700 degC for 2 h. The calcined powder was press-formed into a disk to form a green porous support disk. A BLF powder as the precursor of a dense layer was prepared by an AMP method 31. An aqueous malic acid solution (50 mL) was added to a solution (50 mL) containing the corresponding metal nitrates or acetates in a stoichiometric ratio under vigorous stirring. The molar ratio of malic acid to total metal ions was set to 1.5. The pH of the mixed aqueous solution was adjusted to 6 with aqueous ammonia (28%). The mixed solution was evaporated to dryness and the obtained powder was calcined in air at 800 degC. The calcined powder was ground and dispersed in ethanol to prepare a precursor slurry (10 wt %) for fabricating a dense layer on a porous support. A dense BLF layer was formed by dropping the slurry (0.6 mL) on the green porous support. The support disk was dried at room temperature and sintered at 1175 degC for 5 h to fabricate an asymmetric BLF membrane. For comparison, sintered-disk-type BLF membranes were also fabricated using the BLF powder prepared by the AMP method. The thickness of the sintered-disk membranes was controlled to about 1.0 and 0.5 mm by polishing the surfaces with emery paper (# 80). The morphology of the surface and cross-sections of the asymmetric membrane were observed with a SEM (JSM-6340F, JEOL Co., LTD.). The oxygen permeation flux through the fabricated asymmetric membrane was measured using the apparatus used in the previous study 18. The dense layer side of the asymmetric membrane was fixed on a quartz tube using a silver ring as an adhesive agent at 960-970 degC. Air (O2/N2 mixture gases) and He were supplied to the porous support side and dense layer side, respectively. The flow rates of air (O2/N2 mixture gases) and He were 200 and 150 mL min-1, respectively. The concentration of permeated oxygen from the air side to the He side was detected with a thermal conductivity detector (TCD) of a gas chromatograph directly connected to the effluent line of the dense layer side.
Effect of Ru substitution on the first charge-discharge cycle of lithium-rich layered oxides

The material synthesis began with a basic co-precipitation method. Required amounts of Mn and Ni acetate were dissolved in water and added dropwise to a stirring aqueous KOH solution at room temperature with no pH control. After the acetate solution was fully added, the resulting mixed-metal hydroxide particles were filtered, washed with de-ionized water, and dried overnight at 100 degC. The mixed-metal hydroxide powders were then ground together with required amounts of RuO2*xH2O and LiOH*H2O and fired to 900 degC for 15 h with heating and cooling rates of 3 degC min-1 and 5 degC min-1, respectively. The synthesized materials have the compositions of Li1.2Mn0.6-xRuxNi0.2O2 (x = 0.00, 0.025, 0.05, 0.10, 0.20, 0.40, 0.50, and 0.60).

Nanoclusters of Cu(ii) supported on nanocrystalline W(vi) oxide: a potential catalyst for single-step conversion of cyclohexane to adipic acid
Hydrogen peroxide (50 wt% in water) was purchased from Merck KGaA, Darmstadt, Germany. Ammonium metatungstate, hydrate, anhydrous cupric chloride, cetyltrimethylammonium bromide (CTAB), hydrazine (80% aq. solution), ammonium hydroxide, cyclohexane (purity > 99.9%), acetonitrile (HPLC grade) were purchased from Sigma-Aldrich Co. All the chemicals were used without further purification.
Cu(II) nanoclusters supported on nanocrystalline WO3 were prepared by modifying our own process, taking CuCl2 and (NH4)6H2W12O40*xH2O as precursors of Cu and W respectively.20 In a typical procedure 5.3 g (NH4)6H2W12O40*xH2O and 0.40 g of CuCl2 were dissolved in 16 g deionized water. Then the solution was made basic to a pH range of 8 using NH4OH solution dropwise. After that, a transparent and homogeneous precursor solution formed when 0.8 g CTAB was added into the solution. After the solution had been stirred for half an hour, aqueous hydrazine solution (0.4 g) was added to it. All the reagents were added maintaining the molar ratio: Cu:CTAB:H2O:N2H4 = 1:0.75:300:1. The resulting precursor solution was transferred into a Teflon-lined autoclave with a stainless steel shell. The autoclave was sealed and maintained at 180 degC for 24 h. After cooling naturally to room temperature, the white products were collected after centrifugation, washed sequentially with distilled water and absolute ethanol to remove ions possibly remnant in the products, and dried in an oven at 100 degC overnight. Finally, the white powder underwent calcinations under an oxygen atmosphere at 550 degC for 6 h to yield canary-yellow products (catalyst).Aluminium hydroxide waste based geopolymer composed of fly ash for sustainable cement materials
Geopolymerization was conducted for Al-waste and FA, when the wastes were mixed with aqueous NaOH solutions of 5, 10, and 15 M concentrations. Sodium silicate solutions containing 10.15% Na2O, 31.38% SiO2, and 58.47% H2O were used as alkaline activators. For all mixtures, the weight of the sodium silicate to sodium hydroxide solution was fixed at 2.5 for fly ash based geopolymer [11]. In this case, the Al-waste was heated at 110 degC for 24 h and then sieved through a 100 mesh screen before use. As presented in Table 2, different contents of the mixtures of FA and the Al-waste were used for 0, 10, 20, 40, and 60 wt% for the Al-waste at different NaOH concentrations. First, the sodium silicate (Na2SiO3) and NaOH solutions were premixed in each plastic container and were then left until the solutions reached room temperature. In the four mixed proportions of the Al-waste described above, geopolymerization was conducted using 5, 10 or 15 M NaOH solutions.TiN Nanoparticles on CNT-Graphene Hybrid Support as Noble-Metal-Free Counter Electrode for Quantum-Dot-Sensitized Solar Cells
TiCl4 (1 g) was dispersed in ethanol (2.53 mL), and urea (1583 mg), a nitrogen source, was added to the solution. The metal precursor/urea molar ratio was 1:5. After stirring for 1 h, a viscous metal-urea complex was transferred to a tubular furnace and calcined at 750 degC for 3 h under a N2 atmosphere with a flow rate of 100 mL s-1.
Graphene oxide (GO) was synthesized by applying the Hummer's method,42 and CNTs were purchased from Hanwha Nanotech (CMP-310F). GO, CNT, and a mixture of GO and CNTs (1:1 w/w) were used as starting materials for the syntheses of supports. The amount of Ti was fixed to 60 wt % in supported TiN catalysts. Thus, a solution of TiCl4 (1 g) in ethanol (2.53 mL) was dispersed ultrasonically in a solution of support material (GO, CNT, or CNT-GO, 120 mg) in ethanol (15 mL). After vigorous stirring for 1 h in the presence of urea (1583 mg), the resulting solution was dried in an oven at 100 degC to evaporate excess ethanol. After heat treatment under the same conditions as those used for the synthesis of TiN, supported TiN NPs were obtained.Multilayer graphene films as transparent electrodes for organic photovoltaic devices
To prepare the MLG electrode on glass for OSCs, a thin layer of graphene was grown on Cu foils using CVD [23]. Cu foils were placed inside of a quartz tube furnace and heated to 1000 degC, with 4 s.c.c.m. of flowing H2 at 400 mTorr. After annealing the Cu foils for 30 min at 1000 degC, graphene was synthesized with a flowing gas mixture of 20 s.c.c.m. CH4 and 4 s.c.c.m. H2 at 900 mTorr for 30 min. Polymethylmethacrylate (PMMA) was used as a supporting layer in the transfer of a thin layer of graphene from the Cu foils to glass substrates. PMMA was spin coated at 1000 rpm on graphene/Cu, and the underlying Cu was etched with an appropriate Cu etchant. The PMMA/graphene layer was rinsed with deionized (DI) water to remove the residual etchant and amorphous carbon on the surface. The PMMA/graphene layer was then gently placed onto the glass substrate. The PMMA/graphene/glass substrate was dipped into acetone for a while to remove the PMMA layer and was subsequently rinsed with isopropyl alcohol (IPA) and DI water. The transfer process was repeated four times to prepare the MLG films on the transparent glass substrate [24].Catalytic performance of sheet-like Fe/ZSM-5 zeolites for the selective oxidation of benzene with nitrous oxide
Fe/ZSM-5 zeolite nanosheets were prepared according to a literature procedure modified with the purpose of including Fe3+[25] and [26]. In a typical synthesis, 10.42 g of TEOS (tetraethylorthosilicate) was added to a solution containing 0.47 g of aluminum nitrate nonahydrate, 0.06 g of iron nitrate nonahydrate, and 20.0 g of distilled water. The mixture was stirred for 2 h. A solution containing 2.41 g of C16-6-6(Br)2 or C16-6-6(OH)2, 0.42 g sodium hydroxide (NaOH) and 15.7 g of demi-water was added to this mixture and shaken by hand for 5 min. After further stirring with a magnetic stirrer for 6 h at room temperature, the gel mixture was transferred to a Teflon-lined stainless-steel autoclave and heated at 140 degC for 9 days under tumbling (60 rpm). The zeolite product was filtered, washed with distilled water, and dried at 100 degC overnight. Using the OH form of the template, three different Si/Fe ratios were employed (Si/Fe = 84, 180 and 300). The as-synthesized zeolites were calcined at 580 degC for 4 h under a flow of air. The calcined samples were ion-exchanged into their ammonium form by triple ion-exchange with a 1 M aqueous solution of NH4NO3 at room temperature. The proton form of the zeolites was obtained by calcination at 550 degC under a flow of air for 4 h. The samples are denoted as Fe/ZSM-5-sheet(x, y) with x the anion of the template used (OH or Br) and y the Si/Fe ratio. Steam activation was carried out by heating an amount of sample in a flow of 10% water vapor (100 ml min-1) in artificial air at 700 degC for 3 h. The steamed samples are denoted by using the suffix "-st." A reference Fe/ZSM-5 catalyst was prepared by controlled hydrolysis of TEOS in the presence of tetrapropylammonium hydroxide (TPAOH) according to a literature procedure [10].
Minor and trace element emissions from post-combustion CO2 capture from coal: Experimental and equilibrium calculations

Highlights
•
Tested pilot scale, 25kWth CO2 capture reactor, using Ca-based CO2 sorbent.
•
Flue gas trace element emission sampling based on EPA Method 29.
•
EDS analysis undertaken of Ca-based sorbent for elemental analysis.
•
Sensitivity analysis carried out on MTDATA software, for 9 and 21 trace elements.
•
Thermodynamic equilibrium modelling undertaken to support experimental work.
Abstract
Elemental partitioning, including gaseous elemental emissions from pilot scale (25kWth), post combustion CO2 capture using a Ca-based sorbent, have been investigated for naturally occurring elemental impurities found in limestone, that have the potential to be released to the environment under carbonation and calcination conditions.
Inductively Coupled Plasma-Mass Spectrometry (ICP-MS) analysis of Longcliffe SP52 limestone was undertaken to identify other impurities present, and the effect of sorbent mass and SO2 concentration on elemental partitioning in the carbonator between solid sorbent and gaseous phase was investigated, using a bubbler sampling system.
Samples were analysed using ICP-MS, which showed that sorbent mass and SO2 concentration in the carbonator effected the concentrations of gaseous trace elements sampled.
Thermodynamic modelling of the carbonation and calcination process was also undertaken, based on molar quantities of trace elements identified from ICP-MS analysis of limestone, which provided useful information with regards to element stability and partitioning under realistic CO2 capture conditions.

Introduction
Calcium looping is a method of CO2 capture which utilises fluidised bed technology, whereby a particulate bed is fluidised using a gas within a reactor.
In this case the particulate bed comprises a limestone-derived CO2 sorbent, and the fluidising gas comprises CO2-containing flue gas.
The calcium looping cycle makes use of the reversible carbonation reaction, and the subsequent CO2 capture and release process is therefore cyclic.
The equilibrium carbonation-calcination reaction is provided by Eq.
(1), whereby carbonation is exothermic and calcination is endothermic.
The reactors for carbonation (carbonator) and calcination (calciner) are separate, meaning that there is no gaseous exchange between the two, ensuring the efficiency of the process is optimised.(1)CaO+CO2↔CaCO3ΔH°r=±178kJmol-1
The use of limestone as a CO2 sorbent in fluidised bed carbon capture systems from coal, is gaining attention due to the high availability, low cost, and potential for high CO2 sorption efficiency of Ca based sorbents.
However, limestone is a naturally-occurring material, and always contains other species in the form of trace elements.
The high temperature conditions under which carbonation and calcination occur, present opportunities for trace elements to be released to the environment, thereby representing a potential source of environmental contamination, in addition to that produced from coal combustion.
A large amount of research has been carried out on the use of limestone in the calcium looping cycle as a method of CO2 capture, but to date only minimal work has been carried out on the potential pollutants that may be produced by the calcium looping cycle as a result of limestone impurities.
Environmental and human health problems can arise from emissions of toxic heavy metals, with Pb, As, Hg, Cd, Cr and Zn of particular cause for concern.
In addition, the presence of such trace elements may cause technical uncertainties within energy production systems, including corrosion of construction materials [1,2].
Several studies have shown that trace elements released from coal during energy processes tend to partition either between gaseous emissions, or ash residues, dependant on their volatility.
Based on this partitioning tendency, it has been suggested that trace elements can be classed into three main groups [3-5]:•
Group I elements - least volatile, partition into ash residues e.g.
Mn, Be, Co, Cr.
•
Group II elements - moderate volatility, partition between ash residues and gaseous phase.
As gases cool, vapour species condense onto particulate matter e.g.
As, Cd, Pb, Sb.
•
Group III elements - High volatility and unlikely to condense from vapour phase e.g., Hg, and Se.
Group III and Group II elements are considered to represent the greatest risk to the environment and human health due to their increased volatility compared to Group I elements.
These groups describe element behaviour, and therefore are considered valid regardless of the process in question.
Córdoba et al. [6] investigated the partitioning of elements in a pulverised coal combustion power plant fitted with wet limestone flue gas desulphurisation (FGD).
The most volatile elements including S and F were retained by the FGD-derived gypsum, whilst moderately volatile elements including As and B were for the most part retained in the fly ash.
Gaseous emissions were below European directive 2001/80/EC limits for large combustion plants and the pollutant release and transfer register (Pollutant Release and Transfer Registry (PRTR)) threshold values, with the exception of Hg emissions and particulate Se, As, Zn, Cu, Ni, and Cr.
The fate of trace elements was investigated in a 90kW oxy-combustion pilot plant fed with coal and limestone [7].
It was shown that 82% of elemental Hg was emitted in the exhaust gas, as was 81% of Cl.
It was further suggested that the relatively low temperatures, and high Ca content in the system from limestone use promoted condensation and sorption of sulphate, fluoride and chloride species.
Meij [4] analysed the concentrations and distributions of trace elements within coal-fired power plants fitted with FGD technology, employing limestone as an SO2 sorbent.
It was found that the predominant source of trace elements within the FGD plants were from limestone, suggesting that the use of limestone-derived CO2 sorbents may represent a further potential source of trace elements within power stations, and in turn may impact on downstream processes.
Further, Sager [8] also suggests that lime used for gas scrubbing within a power plant may introduce trace elements to the system, including As, Pb, Cd and Zn.
By contrast, Furimsky [9] concluded that limestone sorption of trace elements may have a diluting effect on trace element content in coal ash, whilst Cheng et al. [10] suggested lime (CaO) may be beneficial in reducing trace element emissions from coal combustion.
Metal oxides other than CaCO3 have been identified in limestone, including MgO, Al2O3, SiO2 and Fe2O3, in addition to trace elements [11,12] whilst Barber [13] concluded that Ca, Mg, Mn and Sr are mainly restricted to the carbonate fraction of limestone, and that 'mineralogical associations' of trace elements in limestone are complex.
With regards to elemental emissions from limestone use during the calcium looping cycle for CO2 capture, Dean [14] carried out experiments at the bench scale to investigate the effect of coal use on limestone-derived sorbent trace element inventory.
Experiments carried out without coal use showed no change in sorbent trace element inventory.
Batch experiments carried out using La Jagua coal showed an increase in the concentration of Ba, Cr, K, Mn, Sr, and Ti, whilst concentrations of B, Na and K remain the same, whilst for Cu there was a small decrease.
For continuous experiments comparing 2 different coals and refuse-derived fuel (RDF), an increase in Al content was observed suggesting some ash mixed in with the sorbent.
The concentrations of Ba, K, Sr and Zn remained the same for all fuels.
For Lea Hall coal, there was an increase in B over eight cycles from ∼20 to 40ppm, though not for La Jagua.
For Cu, sorbent concentrations in the first cycle (∼15ppm) remained the same over eight cycles for the two coals, but increased over five cycles in the presence of RDF from ∼20ppm to ∼180ppm.
Na remained the same for the sorbent cycled in the presence of La Jagua, but there was an increase in results for Lea Hall coal and RDF from ∼250 to ∼550ppm and from ∼200 to 400ppm respectively, expected given that the Na content of the La Jagua coal is an order of magnitude lower than of the other two fuels.
Ti remained the same for La Jagua over eight cycles, though saw an increase in the sorbent cycled in the presence of RDF over five cycles from ∼50ppm to ∼150ppm.
There appear to be no further studies available in the literature investigating elemental partitioning as a result of limestone use in the calcium looping cycle.
Several studies within the literature make use of thermodynamic equilibrium modelling software in predicting trace element release during energy production.
Thompson and Argent [15,16] have used thermodynamic equilibrium modelling to investigate trace element mobilisation under both combustion and air-blown gasification conditions.
One of the most widely used software packages, Metallurgical and Thermodynamic Databank (MTDATA), designed and produced by the National Physical Laboratory (NPL), UK, has been used to investigate trace element release including during combustion of wood bark [17], combustion of sewage sludge with Polish coal [18], in investigating trace element release from biomass-fuelled gasification systems [19], and in investigating trace element, including mercury, emissions from gasification and combustion conditions [20].
Goni et al. [21] utilised MTDATA to model fusibility during combustion of different coal blends, and confirmed that the equilibrium calculations were a valid tool in doing so.
Khodier et al. [1] used MTDATA to model the combustion and deposition process when co-firing miscanthus with coal, and concluded it a useful predictive tool to support experimental analytical techniques.
With regards to CCS in particular, MTDATA has been used to predict the impact of impurities on CO2 transport in relation to the pipeline transport of dense phase CO2 from a capture plant to a subsurface storage site [22], but otherwise, it can be concluded that there is limited data available in the literature with regards to using thermodynamic modelling to support experimentally-derived trace element emissions results from CO2 capture.
This objective of this study was to experimentally investigate the elemental partitioning between solid sorbent and gaseous release from the use of limestone-derived CO2 sorbent, within a 25kWth pilot scale CO2 capture reactor.
In order to support the experimental study, MTDATA has been used here to investigate the likely compounds to be formed under realistic reaction conditions, and also to examine the stability of those compounds under typical Ca looping conditions.
Experimental procedure
Pilot scale CO2 capture facility
Elemental partitioning, including emissions tests were carried out in a 25kWth pilot scale CO2 capture reactor, comprising a 4.3m high, 0.1m diameter, entrained flow bed carbonator, and a 1.2m high, 0.165m diameter, bubbling fluidised bed calciner, as shown in Fig. 1.
Two cyclones present at the exit of the carbonator, ensure solids recycling to the calciner, and minimise particulate emissions to the atmosphere.
Two loop seals, one located at the base of the first cyclone leading to the calciner, and the other located between the lower third of the two reactors, allow the controlled transfer of solids between the two reactors, whilst at the same time preventing gas transfer in order to maintain process efficiency.
A total of thirteen pressure tappings along the length of the carbonator, and seven pressure tappings along the length of the calciner, allow pressure analysis of the system.
Temperatures within the reactors were measured using K-type thermocouples with metal sheaths.
Gases were introduced into the reactor using rotameters to ensure accurate flow rates.
Continuous looping experiments were undertaken to investigate the effect of 4.5kg, 6kg and 13kg bed inventory, and SO2 concentrations of 0ppm, 1000ppm and 2000ppm for a bed inventory of 13kg, on elemental partitioning between solid sorbent and gaseous emissions.
Sorbent analysis
Inductively Coupled Plasma-Mass Spectrometry (ICP-MS) analysis was undertaken of Longcliffe SP52 limestone from the UK, to determine the quantities of elemental impurities present in the naturally occurring rock, for which the major, minor and trace elements are given in Table 1, and ICP-MS results are provided in Table 2.
Throughout the literature, there are varying definitions for major, minor and trace elements.
In this case, major elements are defined as those of concentration >10ppm, minor elements are of concentration between 1 and 10ppm, and trace elements are of concentration <1ppm.
The ICP-MS process uses high temperature argon plasma to generate positively-charged ions from the sample, allowing concentrations of such ions to then be measured and quantified.
Mg appears to be the element found in the highest concentration (5.75ppm), as expected given the presence of a dolomitic fraction (CaMg(CO3)2) in most naturally occurring limestone.
Other elements found at higher concentrations appear to be Na, Al, P, K, Mn, Fe, Sr, Ba and Pb, all found at concentrations of over 26ppb.
ICP-MS analysis confirmed the presence of Al and Fe in similar quantities (414.5 and 354.8ppm respectively).
Samples taken after testing were also analysed using ICP-MS and compared to a blank acid digestion sample, to identify changes in elemental concentration as a result of use in the calcium looping cycle process.
Ca-derived sorbents sampled before, during and after tests were analysed for changes in morphology using environmental scanning electron microscopy (SEM) and electron dispersive spectroscopy (EDS).
The SEM-EDS analysis provides useful data, but due to it having a smaller range than ICP-MS, cannot show data for elements below a certain concentration, and therefore shows data for fewer elements than ICP-MS.
Nonetheless, EDS data is useful in outlining trends for the elements which are analysed, and for supporting results achieved using ICP-MS.
Flue gas analysis
An ADC 7000 gas analyser was used to measure real time online levels of CO2 and O2 in combustion gases from the carbonator and calciner, during pilot scale CO2 capture tests.
CO2 and O2 concentrations from the carbonator for a typical test are provided in Fig. 2.
Elemental sampling from the reactor took place from the carbonator.
Although the carbonator operates at lower temperatures (600-700°C) than the calciner (800-950°C) and therefore elemental release may be slightly lower due to lower volatility, it was considered that compared to the calciner, the carbonator will be exposed to the flue gas in its entirety, and also in terms of location of the sampling equipment the carbonator was the most suitable reactor from which to sample.
The elemental sampling method was undertaken in accordance with US Environmental Protection Agency (EPA) Method 29: Determination of Metals Emissions from Stationary Sources [23].
The experimental set up, as outlined in Fig. 3, comprises a 'sampling train' consisting of several bubblers through which a stack sample of the flue gas is passed.
Several bubblers contain aqueous acidic dilution to allow collection of condensed trace elements in the flue gas which passes through.
A pump allows the gas to be sampled through the bubblers, and a dry gas meter allows the recording of the volume of gas which is sampled.
A glass filter prevents particulate matter from passing through the bubblers.
Prior to, and between each experiment, the glassware is acid washed in 10% HNO3 acid in order to prevent contamination.
Further details of the procedure are provided in EPA Method 29 [23].
The sampling system was started as the reactor was heating up, at approximately 30min prior to the beginning of the experiment.
This allowed the sampling of gases for a longer time period, and to take into account any emissions prior to the start of the actual experiment, which may occur as both the reactor material and Ca-based CO2 sorbent were heated.
Although the majority of element release is likely to occur at the start of each test, with emissions likely to decrease to a steady level as looping cycles continue, the sampling of the initial release is important to identify total emissions.
Sampling was stopped when the burner was turned off, and therefore at the end of experiment.
Results are therefore shown for average test durations of approximately 2h.
Elemental analysis of the acidic solution was carried out using ICP-MS.
Thermodynamic modelling
Chemical equilibrium models have proven useful in estimating flue gas compositions and elemental partitioning from energy processes [1].
The MTDATA software computes chemical equilibrium based on Gibbs free energy minimisation, allowing the identification of the most prevalent species in a reaction, under specific conditions, including temperature and pressure.
Results and discussion
Elemental partitioning - experimental effect of sorbent mass
Major elements
Fig. 4 shows the effect of bed inventory on major element concentration for solid samples taken from the carbonator.
Samples were acid digested and analysed using ICP-MS, and compared with acid digestion blanks in order to obtain the final values.
The effect of bed inventory is apparent, with concentrations of Fe in particular increasing from 319ppm to 4598ppm for bed inventories of 4.5kg and 13kg respectively.
Similarly, concentrations of Cr, Mn, Zn and Pb also increased with increasing inventory.
For Cr, no positive recording was made for the lowest bed inventory of 4.5kg, but recordings were made at 6kg and 13kg.
For Mg, Al, Si, Ti, Sr and Y concentrations appeared to slightly decrease with increasing bed inventory.
Results of ICP-MS flue gas analysis provided in Fig. 5 show that most of the major elements present in the carbonator flue gas increase with the mass of the limestone in the reactor.
However, some elements are only present for the largest bed inventory of 13kg e.g., Ti, Cr, and Mn.
However, all elements included within Fig. 5 can be considered to be at very low concentrations of <2ppm.
Those elements at the higher concentrations in the flue gas include Na, Si, K, Zn, and Br.
For Na in particular which was present in reduced amounts in the solid, this suggests that some of it may be partitioning to the flue gas for bed inventories 6 and 13kg.
For Na, Al, K and Fe, the lowest bed inventory of 4.5kg resulted in a concentration in the flue gas which was less than that of the blank sample, values which then increased for the higher bed inventories.
This suggests that a certain amount of these elements is being absorbed, perhaps by the sorbent in the case of Fe, or by the reactor itself.
In the case of P, decreased values compared to the blank were found for bed inventories of 4.5kg and 13kg, but an increased value for 6kg inventory.
However, the values are very low at <0.01ppm and this anomalous result may be due to analytical errors.
Overall, although concentrations in the flue gas are small at <2ppm, increasing bed inventory does appear to increase the concentration of the majority of major elements present in the flue gas.
Minor elements
Fig. 6 shows the concentrations of minor elements in the solid sorbent, where increasing bed inventory resulted in increasing values observed for Co, Ni, Cu, Mo, Cd, and Sn.
In the case of Cu and Sn, values were obtained that were lower than that of the blank for a bed inventory of 4.5kg, which then increased to values of 22 and 0.22ppm respectively for 13kg.
In the case of Gd, Dy and U, small decreases in concentration were found for an increasing inventory, however values are low at <1ppm and the changes observed are small.
The remaining elements (Zr, Le, Ce, Pt, and Nd) recorded an increase in concentration for 6kg over 4.5kg, and then a decrease from 6kg to 13kg.
All minor element flue gas concentrations were <0.1ppm and therefore considered negligible.
Trace elements
Fig. 7 shows solid concentrations of trace elements, of which only Rb, Nb and W were found at concentrations greater than 0.1ppm.
W showed the greatest change, with concentration increasing from 0.06 to 0.42 to 0.74ppm for inventories 4.5, 6 and 13kg respectively.
All trace element flue gas concentrations were found at a concentration of <0.1ppm and therefore considered negligible.
As expected for major, minor and trace elements, the concentration is generally higher by several orders of magnitude for solids samples compared to flue gas samples.
It is clear that in the cases where elemental concentration decreased with increasing bed inventory, this is mostly seen for period 3 elements (Mg, Al, and Si), and some lanthanide rare earth metals (Sm, Gd, Dy, and Rb) and U.
The geochemistry of limestone is complex, with various hydrous and anhydrous forms of the mineral available.
Naturally formed CaCO3 (termed 'CaCO3 I' for the purposes of this study) generally has a rhombohedral-hexagonal lattice system, and is stable at atmospheric pressures and Longcliffe SP52 can be assumed to be in this form.
Other forms of CaCO3 (II-V) are formed over ranges of both increasing temperature and pressure [24].
Each Ca2+ in the CaCO3 structure is bonded with six oxygen atoms, resulting in a charge of +1/3 per bond.
Similarly, the O in each CO3 group is bonded with two Ca2+ ions, meaning each CO3 group is coordinated to six Ca ion.
This provides alternating layers of Ca and CO3 groups [25].
Elemental impurities have three possible locations for bonding in CaCO3 [26]:•
Substitution via cation exchange for Ca2+ occurs for 90-95% of elements e.g., Fe, Mn, and Sr.
•
Adsorption onto crystal faces to balance charge inbalances e.g., Na, and K.
•
Inclusion of additional mineral phases within the CaCO3.
Due to the hexagonal structure of the crystals, spaces between the cations allow elements with an ionic radii smaller than Ca2+, such as Fe, Mn, Cd, Co, Ni, Zn, and Cu to be incorporated with the crystal structure.
For the cases where elemental concentration increased with increasing bed inventory, the elements concerned were mostly those of low volatility (Be, Cr, and Mn), transition metals (Fe, Co, Ni, Cu, Zn, Nb, Mo, Rh, Cd, and W), and group 6 metals (Pb, and Sn).
Elements that have low volatility are most likely to partition to solid phases within the reactor, and therefore it would be expected that the concentration of the low volatility elements would increase for a greater bed inventory.
The transition metals have high melting points because of strong metallic bonds resulting from the presence of unpaired electrons, resulting in low volatility.
Most also have a charge of 2+, allowing substitution for Ca2+ within the CaCO3 lattice.
Iron oxides are also able to sequester transition metal cations and thus are able to immobilise other transition metal impurities [27], thus accounting for the general increase in transition metal concentration with bed inventory.
For Pb and Sn, of which Pb is considered to be a Group II element, and Sn is considered to be between Group I and Group II, both are emitted mostly in coal fly ashes in coal combustion [28], and therefore could be considered to partition to the solid phase in the calcium looping cycle.
Further, it was noted that non-volatile elements have been found more likely to condense out with particulate matter, resulting in an increase in trace element concentration with a decrease in sorbent size [3,29].
Sager [8] found that introduction of limestone increased Pb, Cd and Mn emissions, but these were immediately sorbed from the vapour phase onto particulates.
With regards to the elemental species that decrease in concentration with increasing bed inventory, Mg, Al and Si oxides are generally considered primary components of limestone in addition to Ca.
However, no Si was detected from ICP-MS analysis of unreacted limestone, which can be attributed to low Si detection limits on the ICP-MS in question; although EDS analysis shows decreasing Si concentrations with increasing bed inventory (Fig. 8).
MgO and Al2O3 can be considered to have extended ionic structures, whilst SiO2 has an extended covalent structure, meaning that all have high melting and boiling points, requiring large amounts of energy to volatilize them.
Therefore, they are considered to be of low volatility, but the increase of these elements in the flue gas analysis suggests that they are being removed from the sorbent under the high temperature conditions in the fluidised bed.
The literature suggests that limestone can be used to capture Ni, Pb and Cd, thus accounting for the increased concentration of these three elements with increasing bed inventory [30].
By contrast, it has also been suggested that interaction, and therefore capture of Ni with Ca-bearing materials is of minor significance [31] although it would appear that this is not the case in this study.
With regard to the lanthanides, when investigating limestone formation and rare earth element prevalence, Nagarajan et.al.
[32] found a negative correlation of rare earth elements and CaO.
It is suggested that in general the absorption of lanthanide cations is weak, and therefore they have low molar absorption coefficients, due to shielding of the 4f orbitals by the filled 5s and 5p sub-shells [33].
The low cation exchange capacity of the lanthanides may explain the small decreases in concentration in limestone, with increasing bed inventory.
Mass balance
A mass balance for the carbonator is provided as Table 3 calculated based on the incoming and outgoing concentrations of elements.
It shows that partitioning was always higher to the solid samples than in the flue gas samples, with very low concentrations portioning to the flue gas in all cases.
SEM-EDS analysis
SEM-EDS analysis of sorbent sampled from the carbonator after each test was undertaken to determine the percentage weight of the elemental species present.
The results as shown in Fig. 8 confirm that increasing bed inventory generally increases the presence of Al, Fe and Cu.
Al was identified in the unreacted sample at a weight% of 0.34, but this increased to similar values of 0.67% and 0.66% for 4.5kg and 6kg samples respectively, with a much higher value of 3.98% for 13kg sample of sorbent.
This increase in Al% weight with increasing sorbent mass may be because there is a greater amount of Al present originally, or because the limestone acts as a sorbent for Al, which is also considered the case for Fe and Cu.
Further, Al is likely to form Ca aluminates which are important constituents of cement.
The EDS results indicate similar quantities of C, O and Ca between tests, although values for the samples that had undergone testing are slightly lower for C and O, and slightly higher for Ca, compared to the unreacted limestone sample, implying that the small change was a result of chemical reactions.
Although the values are small, a steady decrease in Si weight% was recorded with increasing sorbent mass, implying removal of Si from limestone in the capture process.
However the lowest value was recorded for the unreacted limestone.
Elemental partitioning - experimental effect of SO2 concentration
Major elements
Fig. 9 shows that increasing SO2 concentration has some effect on major element concentration in solids.
Concentrations of Mg, Al, Si, P, K, Br, Sr, Y, Ba and Pb appear to decrease as SO2 concentration increases.
However, there appears to be little difference between concentrations for 1000 and 2000ppm, the greatest change in concentration being from 0 to 1000ppm SO2.
The trend in the batch experiments whereby concentrations of elements at 1000ppm were generally lower than at 0 and 2000ppm, is also seen for some elements in the looping cycle test including Cr, Mn and Fe, but not to the same extent as was seen in the batch tests.
Ti shows the same trend for these tests as for single column tests, with 1000ppm SO2 showing higher concentrations of Ti (135ppm for single column, 407ppm for looping test) compared to 0 and 2000ppm SO2 at values close to 10ppm Ti in both tests.
Zn is the only major element showing a clear increase in concentration with SO2, increasing from 9.4 to 14.1 to 66.8ppm for SO2 concentrations of 0, 1000 and 2000ppm respectively.
Fig. 10 shows how gaseous major element concentrations were affected by flue gas SO2 concentration.
Concentrations of all were very low at <1ppm, with Fe and Si present in the highest concentrations at 0.63 and 0.51ppm respectively.
For several elements, 1000ppm SO2 resulted in either the highest concentrations, as was the case for Na, Al, and Si, or the lowest concentrations, as was the case for Fe, when compared to 0 and 2000ppm SO2.
Minor elements
1000ppm SO2 concentration in looping cycle tests again appears to have some influence in reducing the concentration of some minor elements in the sorbent, when compared to 0 and 2000ppm SO2 (Fig. 11).
This is the case for the elements Co, Ni, Zr, Mo, Cd, Sn and Ce.
For the remainder of the elements shown, there was a small decrease in value with increasing SO2, other than for Cu which saw a small increase in value from 9.74 to 14.13ppm for 0ppm and 2000ppm SO2 respectively.
All minor element flue gas concentrations were <0.1ppm and therefore considered negligible.
Trace elements
Fig. 12 shows that increasing SO2 concentration does influence trace element sorbent concentration, and for B in particular, concentrations rose from 2ppm to 2.6ppm for 0 and 1000ppm SO2 respectively.
W showed a decrease with increasing SO2 concentration from 1.3ppm to 0.08ppm for 0 and 1000ppm SO2 respectively.
Although trends are evident for the trace elements, concentrations are low, reducing the reliability of the results and in turn affecting the reliability of the trends observed.
All trace element flue gas concentrations were <0.1ppm and therefore considered negligible.
The results show that looping cycle tests carried out in the presence of increasing concentrations of SO2 resulted in, for the most part, a decrease in elemental concentrations in both solid sorbent and flue gas samples.
A small number of elements appeared to increase in concentration with increasing SO2 concentration, including Zn, Cu, B and Yb.
Iron cation exchange sites may account for the increase in concentration of the transition metals Zn and Cu.
The larger molar volume of CaSO4 (46cm3/mol) compared to CaCO3 (37cm3/mol) may affect ion-exchange or covalent bonding within the pore structure, causing enhanced trace element release in the majority of cases.
The fact that emissions do not increase linearly with increasing sorbent mass or SO2 concentration implies that the chemistry of the system is complex.
Mass balance
A mass balance showing effects of increasing SO2 concentration on elemental partitioning for the carbonator is provided as Table 4, calculated based on the incoming and outgoing concentrations of elements.
A mass balance closure value of 1 indicates outgoing concentrations equal to incoming concentrations, and as shown by the low closure values, partitioning was always higher to the solid samples than in the flue gas samples, with very low concentrations portioning to the flue gas.
SEM-EDS analysis
Fig. 13 confirms that SO2 concentration appears to have had an effect on C, O and Ca weight%, with C and O decreasing with increasing SO2, and Ca increasing slightly with increasing SO2.
This suggests that the presence of SO2 decreases the extent to which CaO is able to convert to carbonate in the carbonator, the reactor from which the solid samples were taken.
It is widely accepted that the presence of SO2 during the carbonation reaction produces CaSO4 on the sorbent particle pore surfaces, which reduces the extent to which CO2 can diffuse into pore space and react to form CaCO3 [34].
As expected, the weight% of S present in the sorbent samples increased from 0.81% to 4.73%, to 10.32% for SO2 concentrations of 0, 1000 and 2000ppm respectively, with a value of 0% for unreacted limestone.
The value of 0% for unreacted limestone, but the presence of S identified during ICP-MS analysis (Table 2) may be due to the higher range that ICP-MS analysis can undertake.
In terms of other element species, an SO2 concentration of 1000ppm appears to result in higher weight% values for Al, Fe, Mg and Si, than for SO2 concentrations of 0ppm and 2000ppm SO2, with Fe, Mg and Si not present at other SO2 concentrations.
This result is unexpected, but it may be that, as mentioned previously in Section 3.2 the higher concentration of SO2, and in turn CaSO4, results in greatly reduced available pore space and therefore bonding between these species, when compared to 1000ppm SO2.
In turn, it may be that the presence of 1000ppm SO2 causes some corrosion of the stainless steel reactor itself, resulting in small deposits on the sorbent particles, although it would be expected that if this were the case, higher values would also be achieved for 2000ppm SO2.
Miller et al. [35] investigated the influence of SO2 on trace element behaviour during wood-bark combustion, and concluded that SO2 increased emissions of Cd, but reduced emissions of As and Hg.
Overall, it is apparent that the interactions that take place in the presence of SO2 are complex.
Thermodynamic modelling - sensitivity analysis
A sensitivity analysis was carried out using MTDATA, in order to determine the effect of the presence of certain trace elements on the formation of compounds.
The analysis was carried out using the statistical method 'non-parametric Kruskal-Wallis ANOVA by ranks', using STATISTICA software (CSS Statistica/W, Release 5.0 with Industrial units, Statsoft UK).
This method allows the comparison of two or more independent samples, and uses the sum of the difference between mean ranks of the samples, as the statistic.
This then provides an indication of whether the absence of an element input to the model has an effect on the stability of a certain compound in the model output.
Due to extended calculation times required, a sensitivity analysis was carried out firstly for nine trace elements (Ba, Cd, Cr, K, Mg, Ni, Sr, Ti, and Zn) and then for twenty-one trace elements (Li, Be, Mg, Al, K, Ti, V, Cr, Mn, Fe, Co, Ni, Zn, As, Se, Sr, Cd, Sb, Ba, Hg, and Pb), in addition to the four 'base' elements (Ca, C, O, and H) (always considered to be present from limestone and gas input), based on those which are considered to be most volatile, have the greatest negative impact on the environment and health, and which were present in the highest quantities in the limestone as identified by ICP-MS analysis, as outlined in Table 2.
MTDATA was run for all of 'base' elements and either the nine or twenty-one trace elements present, and the data recorded.
One trace element was then removed to see the effect on compound stability and formation.
For the next run, this trace element was included, and the next trace element removed, and so on.
The results for carbonation and calcination respectively, are shown in Tables 5 and 6.
The results identify that for both carbonation and calcination, Mg has a strong link with the presence of many compounds, including CaCO3.
In the case of the sensitivity analysis based on 9 elements, Mg appears to be the only element that has a link with the presence of compound formation in the model output.
However, in the case of the sensitivity analysis based on 21 elements, a greater number of elements have an link with the presence of compounds formed under carbonation conditions, including Mg and Al, having an effect on the greatest number of compounds.
The increased number of elements that have an effect on the presence of compounds under calcination conditions compared to carbonation conditions again may be due to the increased temperatures at which calcination is carried out, thus allowing the less volatile elements to react, due to an increased amount of thermal energy available for activation energies.
It is also apparent that increasing the number of trace elements present in the model, increases the chemical interactions that are likely to take place.
For example, when 21 trace elements are included in the model in addition to the four base elements, there are a greater number of compounds that are affected by the presence or absence of another trace element, compared to when only 9 elements are included in the model.
Thermodynamic modelling - effect of sorbent mass
MTDATA estimates of the likely compound formation and associated phases for varying quantities of sorbent, are shown in Figs.
14-16.
The results identify that under carbonation conditions the molar quantities of species in equilibrium with combustion gases and limestone, increase as sorbent mass in the reactor increases, and results appear consistent with experimental data for transition metals in particular (Figs.
4-6).
Values for gaseous Cd concentrations increased from 8.0E-05 for 4.5kg, to 6.0E-05 for 6kg, to 2.0E-04 for 13kg, at 740°C.
Similarly for Zn, molar gaseous emissions increased from 9.0E-04 for 4.5kg, to 7.0E-4 for 6kg, to 2.0E-03 for 13kg, at all temperatures.
These trends are consistent with the majority of the species present.
Gaseous KOH concentrations however reaches its maximum value at 720°C for 4.5kg, before then declining in value, whereas for 6kg and 13kg, the temperature at which the maximum value is reached at 740°C and 760°C respectively.
This shows the effect that increasing mass has on the thermodynamic equilibrium for a minority of species present.
Mg was identified as one of the primary impurities of limestone (Table 2).
Fig. 15 identifies that increasing sorbent mass increases the amount of solid MgO and MgCaO2 at the relevant temperatures at which they are stable, but increasing sorbent mass has no effect on the temperature at which both compounds are stable.
Fig. 16 shows a similar trend whereby increasing sorbent mass also increases the mass of solid CaCO3 and CaO present.
Thermodynamic modelling - effect of SO2 concentration
It is apparent from the results of MTDATA thermodynamic equilibrium modelling that increasing SO2 concentration during carbonation has an effect on the species present at equilibrium.
Fig. 17 shows that the addition of SO2 allows the formation of S-based compounds that would not otherwise form if SO2 was not introduced.
For example at 0ppm SO2 gaseous Ni and Zn are present, but at 1000ppm and 2000ppm SO2, ZnS is also formed, but only NiS is formed in place of Ni.
Experimental data (Fig. 11) identifies a general decrease in Ni concentration with increasing SO2 concentration, but it would be expected that this would be found as Ni rather than NiS, given the oxidative conditions in the reactor, and the requirement of reducing conditions for H2S, and in turn NiS formation.
Increasing SO2 concentration appears to have no effect on the stability of gaseous Cd or Ba(OH)2, although experimental data (Fig. 9) confirms a decrease with Ba with increasing SO2 concentration.
SO2 does appear to have some influence on the stability of gaseous Zn and ZnS, whereby increasing SO2 concentration appears to reduce the amount of gaseous Zn up until 780°C, above which temperature the amount of Zn reaches stability at -1.49/mol, regardless of the concentration of SO2 present.
Experimental data (Fig. 10) however confirms decreasing Zn concentration with increasing SO2 from 0ppm to 2000ppm SO2.Thermodynamic equilibrium data shows there is a small influence on the stability of gaseous K, with increasing SO2 concentration slightly reducing the amount of gaseous K.
For example, at 700°C and 0ppm SO2, calculations show there to be -4.59/mol, compared to -4.75/mol with 2000ppm SO2.
This however is the opposite of what experimental data (Fig. 10) suggests.
Gaseous ZnS formation increases with SO2 concentration, from -6.9/mol at 900°C for 1000ppm SO2 to -6.7/mol for 2000ppm SO2.
Again, the predicted formation of sulphides is unexpected given the O2 concentration in the carbonator (Fig. 4), and highlights the limitations of MTDATA compared to experimental data.
Fig. 18 shows that at approximately 700°C and with 0ppm SO2, the stability of solid MgO decreases, whilst in its place the stability of solid MgCaO2 increases to become the predominant Mg species.
Increasing concentrations of SO2 appear to increase the temperature at which the stability of MgO decreases and the stability of MgCaO2 increases.
For example at 2000ppm, the temperature at which MgO reaches its minimum and MgCaO2 starts increasing is at approximately 740°C.
Fig. 19 shows that increasing SO2 concentration has minimal effect on the stability of CaCO3 which reaches a minimum at approximately 860°C, but SO2 does have a small effect on the stability of CaO, with higher SO2 concentrations increasing the temperature at which CaO is first formed, from 780°C with 0ppm to 820°C with 2000ppm SO2.
The model suggests that solid CaS is formed in the presence of SO2 under conditions typical of carbonation, and higher concentrations of which result in higher amounts of CaS formed.
The results show that thermodynamic modelling is a useful means of supporting experimental data with regards to gaseous trace element compositions and elemental partitioning, over a range of temperature conditions relevant to Ca-based CO2 capture.
However, there may be several reasons for the various discrepancies noted, primarily that MTDATA calculates the reactions at equilibrium, and assumes that the reactions go to completion, when in reality, this may not be the case.
In addition, the model databases may not contain all of the compounds relevant to the required experimental conditions, and thermodynamic equilibrium data may not be available for all compounds at all required temperatures.
Diffusion rates through sorbent pores are also an important factor in CO2 capture efficiencies.
Work carried out by Bhatia and Perlmutter [36] proposed the 'Random Pore Model' which describes kinetically-controlled fluid-solid reactions, taking into account factors including boundary layers and product layer diffusion.
These influences are important when considering experimental reaction rates and in turn efficiencies, but unfortunately are not aspects that can be included when using thermodynamic equilibrium modelling to simulate reactions.
Conclusion
Gaseous element emissions sampling from Cranfield's pilot scale, 25kWth CO2 capture reactor using a limestone-derived CO2 sorbent, was undertaken for a sorbent mass of 4.5kg, 6g and 13kg, and also for SO2 concentration of 0ppm, 1000ppm and 2000ppm SO2 in the carbonator reactor.
Element emissions to the gaseous phase appeared to be influenced by sorbent mass, with the majority of species increasing in concentration as sorbent mass increased.
Increasing SO2 concentration also appeared to have an effect on gaseous trace element emissions, although the effect appeared to differ depending upon the element concerned, with some increasing in concentration, and other decreasing in concentration as SO2 concentration increased, suggesting complex underlying chemistry.
The mass balances however show that compared to the concentration of elements in the sorbent, the concentrations partitioning to the gaseous phase is very low.
Thermodynamic modelling was undertaken to verify the likely elemental speciation and partitioning under CO2 capture conditions, results of which have proved useful in supporting the experimental work.
The experimental results provide valuable information with regards to scale up of CO2 capture technology incorporating the Ca-looping cycle, and thermodynamic modelling can be considered a useful instrument in supporting this work, particularly with regards to elemental emissions.
Acknowledgments
The authors wish to acknowledge the Engineering and Physical Sciences Research Council (EPSRC Grant No: EP/G06279X/1) for financial support of the project, Sheffield University for carrying out ICP-MS analysis of Longcliffe SP52 limestone, and M. Roskilly for his technical support with the project.

Minor and trace element emissions from post-combustion CO2 capture from coal: Experimental and equilibrium calculations

Highlights
•
Tested pilot scale, 25kWth CO2 capture reactor, using Ca-based CO2 sorbent.
•
Flue gas trace element emission sampling based on EPA Method 29.
•
EDS analysis undertaken of Ca-based sorbent for elemental analysis.
•
Sensitivity analysis carried out on MTDATA software, for 9 and 21 trace elements.
•
Thermodynamic equilibrium modelling undertaken to support experimental work.
Abstract
Elemental partitioning, including gaseous elemental emissions from pilot scale (25kWth), post combustion CO2 capture using a Ca-based sorbent, have been investigated for naturally occurring elemental impurities found in limestone, that have the potential to be released to the environment under carbonation and calcination conditions.
Inductively Coupled Plasma-Mass Spectrometry (ICP-MS) analysis of Longcliffe SP52 limestone was undertaken to identify other impurities present, and the effect of sorbent mass and SO2 concentration on elemental partitioning in the carbonator between solid sorbent and gaseous phase was investigated, using a bubbler sampling system.
Samples were analysed using ICP-MS, which showed that sorbent mass and SO2 concentration in the carbonator effected the concentrations of gaseous trace elements sampled.
Thermodynamic modelling of the carbonation and calcination process was also undertaken, based on molar quantities of trace elements identified from ICP-MS analysis of limestone, which provided useful information with regards to element stability and partitioning under realistic CO2 capture conditions.

Introduction
Calcium looping is a method of CO2 capture which utilises fluidised bed technology, whereby a particulate bed is fluidised using a gas within a reactor.
In this case the particulate bed comprises a limestone-derived CO2 sorbent, and the fluidising gas comprises CO2-containing flue gas.
The calcium looping cycle makes use of the reversible carbonation reaction, and the subsequent CO2 capture and release process is therefore cyclic.
The equilibrium carbonation-calcination reaction is provided by Eq.
(1), whereby carbonation is exothermic and calcination is endothermic.
The reactors for carbonation (carbonator) and calcination (calciner) are separate, meaning that there is no gaseous exchange between the two, ensuring the efficiency of the process is optimised.(1)CaO+CO2↔CaCO3ΔH°r=±178kJmol-1
The use of limestone as a CO2 sorbent in fluidised bed carbon capture systems from coal, is gaining attention due to the high availability, low cost, and potential for high CO2 sorption efficiency of Ca based sorbents.
However, limestone is a naturally-occurring material, and always contains other species in the form of trace elements.
The high temperature conditions under which carbonation and calcination occur, present opportunities for trace elements to be released to the environment, thereby representing a potential source of environmental contamination, in addition to that produced from coal combustion.
A large amount of research has been carried out on the use of limestone in the calcium looping cycle as a method of CO2 capture, but to date only minimal work has been carried out on the potential pollutants that may be produced by the calcium looping cycle as a result of limestone impurities.
Environmental and human health problems can arise from emissions of toxic heavy metals, with Pb, As, Hg, Cd, Cr and Zn of particular cause for concern.
In addition, the presence of such trace elements may cause technical uncertainties within energy production systems, including corrosion of construction materials [1,2].
Several studies have shown that trace elements released from coal during energy processes tend to partition either between gaseous emissions, or ash residues, dependant on their volatility.
Based on this partitioning tendency, it has been suggested that trace elements can be classed into three main groups [3-5]:•
Group I elements - least volatile, partition into ash residues e.g.
Mn, Be, Co, Cr.
•
Group II elements - moderate volatility, partition between ash residues and gaseous phase.
As gases cool, vapour species condense onto particulate matter e.g.
As, Cd, Pb, Sb.
•
Group III elements - High volatility and unlikely to condense from vapour phase e.g., Hg, and Se.
Group III and Group II elements are considered to represent the greatest risk to the environment and human health due to their increased volatility compared to Group I elements.
These groups describe element behaviour, and therefore are considered valid regardless of the process in question.
Córdoba et al. [6] investigated the partitioning of elements in a pulverised coal combustion power plant fitted with wet limestone flue gas desulphurisation (FGD).
The most volatile elements including S and F were retained by the FGD-derived gypsum, whilst moderately volatile elements including As and B were for the most part retained in the fly ash.
Gaseous emissions were below European directive 2001/80/EC limits for large combustion plants and the pollutant release and transfer register (Pollutant Release and Transfer Registry (PRTR)) threshold values, with the exception of Hg emissions and particulate Se, As, Zn, Cu, Ni, and Cr.
The fate of trace elements was investigated in a 90kW oxy-combustion pilot plant fed with coal and limestone [7].
It was shown that 82% of elemental Hg was emitted in the exhaust gas, as was 81% of Cl.
It was further suggested that the relatively low temperatures, and high Ca content in the system from limestone use promoted condensation and sorption of sulphate, fluoride and chloride species.
Meij [4] analysed the concentrations and distributions of trace elements within coal-fired power plants fitted with FGD technology, employing limestone as an SO2 sorbent.
It was found that the predominant source of trace elements within the FGD plants were from limestone, suggesting that the use of limestone-derived CO2 sorbents may represent a further potential source of trace elements within power stations, and in turn may impact on downstream processes.
Further, Sager [8] also suggests that lime used for gas scrubbing within a power plant may introduce trace elements to the system, including As, Pb, Cd and Zn.
By contrast, Furimsky [9] concluded that limestone sorption of trace elements may have a diluting effect on trace element content in coal ash, whilst Cheng et al. [10] suggested lime (CaO) may be beneficial in reducing trace element emissions from coal combustion.
Metal oxides other than CaCO3 have been identified in limestone, including MgO, Al2O3, SiO2 and Fe2O3, in addition to trace elements [11,12] whilst Barber [13] concluded that Ca, Mg, Mn and Sr are mainly restricted to the carbonate fraction of limestone, and that 'mineralogical associations' of trace elements in limestone are complex.
With regards to elemental emissions from limestone use during the calcium looping cycle for CO2 capture, Dean [14] carried out experiments at the bench scale to investigate the effect of coal use on limestone-derived sorbent trace element inventory.
Experiments carried out without coal use showed no change in sorbent trace element inventory.
Batch experiments carried out using La Jagua coal showed an increase in the concentration of Ba, Cr, K, Mn, Sr, and Ti, whilst concentrations of B, Na and K remain the same, whilst for Cu there was a small decrease.
For continuous experiments comparing 2 different coals and refuse-derived fuel (RDF), an increase in Al content was observed suggesting some ash mixed in with the sorbent.
The concentrations of Ba, K, Sr and Zn remained the same for all fuels.
For Lea Hall coal, there was an increase in B over eight cycles from ∼20 to 40ppm, though not for La Jagua.
For Cu, sorbent concentrations in the first cycle (∼15ppm) remained the same over eight cycles for the two coals, but increased over five cycles in the presence of RDF from ∼20ppm to ∼180ppm.
Na remained the same for the sorbent cycled in the presence of La Jagua, but there was an increase in results for Lea Hall coal and RDF from ∼250 to ∼550ppm and from ∼200 to 400ppm respectively, expected given that the Na content of the La Jagua coal is an order of magnitude lower than of the other two fuels.
Ti remained the same for La Jagua over eight cycles, though saw an increase in the sorbent cycled in the presence of RDF over five cycles from ∼50ppm to ∼150ppm.
There appear to be no further studies available in the literature investigating elemental partitioning as a result of limestone use in the calcium looping cycle.
Several studies within the literature make use of thermodynamic equilibrium modelling software in predicting trace element release during energy production.
Thompson and Argent [15,16] have used thermodynamic equilibrium modelling to investigate trace element mobilisation under both combustion and air-blown gasification conditions.
One of the most widely used software packages, Metallurgical and Thermodynamic Databank (MTDATA), designed and produced by the National Physical Laboratory (NPL), UK, has been used to investigate trace element release including during combustion of wood bark [17], combustion of sewage sludge with Polish coal [18], in investigating trace element release from biomass-fuelled gasification systems [19], and in investigating trace element, including mercury, emissions from gasification and combustion conditions [20].
Goni et al. [21] utilised MTDATA to model fusibility during combustion of different coal blends, and confirmed that the equilibrium calculations were a valid tool in doing so.
Khodier et al. [1] used MTDATA to model the combustion and deposition process when co-firing miscanthus with coal, and concluded it a useful predictive tool to support experimental analytical techniques.
With regards to CCS in particular, MTDATA has been used to predict the impact of impurities on CO2 transport in relation to the pipeline transport of dense phase CO2 from a capture plant to a subsurface storage site [22], but otherwise, it can be concluded that there is limited data available in the literature with regards to using thermodynamic modelling to support experimentally-derived trace element emissions results from CO2 capture.
This objective of this study was to experimentally investigate the elemental partitioning between solid sorbent and gaseous release from the use of limestone-derived CO2 sorbent, within a 25kWth pilot scale CO2 capture reactor.
In order to support the experimental study, MTDATA has been used here to investigate the likely compounds to be formed under realistic reaction conditions, and also to examine the stability of those compounds under typical Ca looping conditions.
Experimental procedure
Pilot scale CO2 capture facility
Elemental partitioning, including emissions tests were carried out in a 25kWth pilot scale CO2 capture reactor, comprising a 4.3m high, 0.1m diameter, entrained flow bed carbonator, and a 1.2m high, 0.165m diameter, bubbling fluidised bed calciner, as shown in Fig. 1.
Two cyclones present at the exit of the carbonator, ensure solids recycling to the calciner, and minimise particulate emissions to the atmosphere.
Two loop seals, one located at the base of the first cyclone leading to the calciner, and the other located between the lower third of the two reactors, allow the controlled transfer of solids between the two reactors, whilst at the same time preventing gas transfer in order to maintain process efficiency.
A total of thirteen pressure tappings along the length of the carbonator, and seven pressure tappings along the length of the calciner, allow pressure analysis of the system.
Temperatures within the reactors were measured using K-type thermocouples with metal sheaths.
Gases were introduced into the reactor using rotameters to ensure accurate flow rates.
Continuous looping experiments were undertaken to investigate the effect of 4.5kg, 6kg and 13kg bed inventory, and SO2 concentrations of 0ppm, 1000ppm and 2000ppm for a bed inventory of 13kg, on elemental partitioning between solid sorbent and gaseous emissions.
Sorbent analysis
Inductively Coupled Plasma-Mass Spectrometry (ICP-MS) analysis was undertaken of Longcliffe SP52 limestone from the UK, to determine the quantities of elemental impurities present in the naturally occurring rock, for which the major, minor and trace elements are given in Table 1, and ICP-MS results are provided in Table 2.
Throughout the literature, there are varying definitions for major, minor and trace elements.
In this case, major elements are defined as those of concentration >10ppm, minor elements are of concentration between 1 and 10ppm, and trace elements are of concentration <1ppm.
The ICP-MS process uses high temperature argon plasma to generate positively-charged ions from the sample, allowing concentrations of such ions to then be measured and quantified.
Mg appears to be the element found in the highest concentration (5.75ppm), as expected given the presence of a dolomitic fraction (CaMg(CO3)2) in most naturally occurring limestone.
Other elements found at higher concentrations appear to be Na, Al, P, K, Mn, Fe, Sr, Ba and Pb, all found at concentrations of over 26ppb.
ICP-MS analysis confirmed the presence of Al and Fe in similar quantities (414.5 and 354.8ppm respectively).
Samples taken after testing were also analysed using ICP-MS and compared to a blank acid digestion sample, to identify changes in elemental concentration as a result of use in the calcium looping cycle process.
Ca-derived sorbents sampled before, during and after tests were analysed for changes in morphology using environmental scanning electron microscopy (SEM) and electron dispersive spectroscopy (EDS).
The SEM-EDS analysis provides useful data, but due to it having a smaller range than ICP-MS, cannot show data for elements below a certain concentration, and therefore shows data for fewer elements than ICP-MS.
Nonetheless, EDS data is useful in outlining trends for the elements which are analysed, and for supporting results achieved using ICP-MS.
Flue gas analysis
An ADC 7000 gas analyser was used to measure real time online levels of CO2 and O2 in combustion gases from the carbonator and calciner, during pilot scale CO2 capture tests.
CO2 and O2 concentrations from the carbonator for a typical test are provided in Fig. 2.
Elemental sampling from the reactor took place from the carbonator.
Although the carbonator operates at lower temperatures (600-700°C) than the calciner (800-950°C) and therefore elemental release may be slightly lower due to lower volatility, it was considered that compared to the calciner, the carbonator will be exposed to the flue gas in its entirety, and also in terms of location of the sampling equipment the carbonator was the most suitable reactor from which to sample.
The elemental sampling method was undertaken in accordance with US Environmental Protection Agency (EPA) Method 29: Determination of Metals Emissions from Stationary Sources [23].
The experimental set up, as outlined in Fig. 3, comprises a 'sampling train' consisting of several bubblers through which a stack sample of the flue gas is passed.
Several bubblers contain aqueous acidic dilution to allow collection of condensed trace elements in the flue gas which passes through.
A pump allows the gas to be sampled through the bubblers, and a dry gas meter allows the recording of the volume of gas which is sampled.
A glass filter prevents particulate matter from passing through the bubblers.
Prior to, and between each experiment, the glassware is acid washed in 10% HNO3 acid in order to prevent contamination.
Further details of the procedure are provided in EPA Method 29 [23].
The sampling system was started as the reactor was heating up, at approximately 30min prior to the beginning of the experiment.
This allowed the sampling of gases for a longer time period, and to take into account any emissions prior to the start of the actual experiment, which may occur as both the reactor material and Ca-based CO2 sorbent were heated.
Although the majority of element release is likely to occur at the start of each test, with emissions likely to decrease to a steady level as looping cycles continue, the sampling of the initial release is important to identify total emissions.
Sampling was stopped when the burner was turned off, and therefore at the end of experiment.
Results are therefore shown for average test durations of approximately 2h.
Elemental analysis of the acidic solution was carried out using ICP-MS.
Thermodynamic modelling
Chemical equilibrium models have proven useful in estimating flue gas compositions and elemental partitioning from energy processes [1].
The MTDATA software computes chemical equilibrium based on Gibbs free energy minimisation, allowing the identification of the most prevalent species in a reaction, under specific conditions, including temperature and pressure.
Results and discussion
Elemental partitioning - experimental effect of sorbent mass
Major elements
Fig. 4 shows the effect of bed inventory on major element concentration for solid samples taken from the carbonator.
Samples were acid digested and analysed using ICP-MS, and compared with acid digestion blanks in order to obtain the final values.
The effect of bed inventory is apparent, with concentrations of Fe in particular increasing from 319ppm to 4598ppm for bed inventories of 4.5kg and 13kg respectively.
Similarly, concentrations of Cr, Mn, Zn and Pb also increased with increasing inventory.
For Cr, no positive recording was made for the lowest bed inventory of 4.5kg, but recordings were made at 6kg and 13kg.
For Mg, Al, Si, Ti, Sr and Y concentrations appeared to slightly decrease with increasing bed inventory.
Results of ICP-MS flue gas analysis provided in Fig. 5 show that most of the major elements present in the carbonator flue gas increase with the mass of the limestone in the reactor.
However, some elements are only present for the largest bed inventory of 13kg e.g., Ti, Cr, and Mn.
However, all elements included within Fig. 5 can be considered to be at very low concentrations of <2ppm.
Those elements at the higher concentrations in the flue gas include Na, Si, K, Zn, and Br.
For Na in particular which was present in reduced amounts in the solid, this suggests that some of it may be partitioning to the flue gas for bed inventories 6 and 13kg.
For Na, Al, K and Fe, the lowest bed inventory of 4.5kg resulted in a concentration in the flue gas which was less than that of the blank sample, values which then increased for the higher bed inventories.
This suggests that a certain amount of these elements is being absorbed, perhaps by the sorbent in the case of Fe, or by the reactor itself.
In the case of P, decreased values compared to the blank were found for bed inventories of 4.5kg and 13kg, but an increased value for 6kg inventory.
However, the values are very low at <0.01ppm and this anomalous result may be due to analytical errors.
Overall, although concentrations in the flue gas are small at <2ppm, increasing bed inventory does appear to increase the concentration of the majority of major elements present in the flue gas.
Minor elements
Fig. 6 shows the concentrations of minor elements in the solid sorbent, where increasing bed inventory resulted in increasing values observed for Co, Ni, Cu, Mo, Cd, and Sn.
In the case of Cu and Sn, values were obtained that were lower than that of the blank for a bed inventory of 4.5kg, which then increased to values of 22 and 0.22ppm respectively for 13kg.
In the case of Gd, Dy and U, small decreases in concentration were found for an increasing inventory, however values are low at <1ppm and the changes observed are small.
The remaining elements (Zr, Le, Ce, Pt, and Nd) recorded an increase in concentration for 6kg over 4.5kg, and then a decrease from 6kg to 13kg.
All minor element flue gas concentrations were <0.1ppm and therefore considered negligible.
Trace elements
Fig. 7 shows solid concentrations of trace elements, of which only Rb, Nb and W were found at concentrations greater than 0.1ppm.
W showed the greatest change, with concentration increasing from 0.06 to 0.42 to 0.74ppm for inventories 4.5, 6 and 13kg respectively.
All trace element flue gas concentrations were found at a concentration of <0.1ppm and therefore considered negligible.
As expected for major, minor and trace elements, the concentration is generally higher by several orders of magnitude for solids samples compared to flue gas samples.
It is clear that in the cases where elemental concentration decreased with increasing bed inventory, this is mostly seen for period 3 elements (Mg, Al, and Si), and some lanthanide rare earth metals (Sm, Gd, Dy, and Rb) and U.
The geochemistry of limestone is complex, with various hydrous and anhydrous forms of the mineral available.
Naturally formed CaCO3 (termed 'CaCO3 I' for the purposes of this study) generally has a rhombohedral-hexagonal lattice system, and is stable at atmospheric pressures and Longcliffe SP52 can be assumed to be in this form.
Other forms of CaCO3 (II-V) are formed over ranges of both increasing temperature and pressure [24].
Each Ca2+ in the CaCO3 structure is bonded with six oxygen atoms, resulting in a charge of +1/3 per bond.
Similarly, the O in each CO3 group is bonded with two Ca2+ ions, meaning each CO3 group is coordinated to six Ca ion.
This provides alternating layers of Ca and CO3 groups [25].
Elemental impurities have three possible locations for bonding in CaCO3 [26]:•
Substitution via cation exchange for Ca2+ occurs for 90-95% of elements e.g., Fe, Mn, and Sr.
•
Adsorption onto crystal faces to balance charge inbalances e.g., Na, and K.
•
Inclusion of additional mineral phases within the CaCO3.
Due to the hexagonal structure of the crystals, spaces between the cations allow elements with an ionic radii smaller than Ca2+, such as Fe, Mn, Cd, Co, Ni, Zn, and Cu to be incorporated with the crystal structure.
For the cases where elemental concentration increased with increasing bed inventory, the elements concerned were mostly those of low volatility (Be, Cr, and Mn), transition metals (Fe, Co, Ni, Cu, Zn, Nb, Mo, Rh, Cd, and W), and group 6 metals (Pb, and Sn).
Elements that have low volatility are most likely to partition to solid phases within the reactor, and therefore it would be expected that the concentration of the low volatility elements would increase for a greater bed inventory.
The transition metals have high melting points because of strong metallic bonds resulting from the presence of unpaired electrons, resulting in low volatility.
Most also have a charge of 2+, allowing substitution for Ca2+ within the CaCO3 lattice.
Iron oxides are also able to sequester transition metal cations and thus are able to immobilise other transition metal impurities [27], thus accounting for the general increase in transition metal concentration with bed inventory.
For Pb and Sn, of which Pb is considered to be a Group II element, and Sn is considered to be between Group I and Group II, both are emitted mostly in coal fly ashes in coal combustion [28], and therefore could be considered to partition to the solid phase in the calcium looping cycle.
Further, it was noted that non-volatile elements have been found more likely to condense out with particulate matter, resulting in an increase in trace element concentration with a decrease in sorbent size [3,29].
Sager [8] found that introduction of limestone increased Pb, Cd and Mn emissions, but these were immediately sorbed from the vapour phase onto particulates.
With regards to the elemental species that decrease in concentration with increasing bed inventory, Mg, Al and Si oxides are generally considered primary components of limestone in addition to Ca.
However, no Si was detected from ICP-MS analysis of unreacted limestone, which can be attributed to low Si detection limits on the ICP-MS in question; although EDS analysis shows decreasing Si concentrations with increasing bed inventory (Fig. 8).
MgO and Al2O3 can be considered to have extended ionic structures, whilst SiO2 has an extended covalent structure, meaning that all have high melting and boiling points, requiring large amounts of energy to volatilize them.
Therefore, they are considered to be of low volatility, but the increase of these elements in the flue gas analysis suggests that they are being removed from the sorbent under the high temperature conditions in the fluidised bed.
The literature suggests that limestone can be used to capture Ni, Pb and Cd, thus accounting for the increased concentration of these three elements with increasing bed inventory [30].
By contrast, it has also been suggested that interaction, and therefore capture of Ni with Ca-bearing materials is of minor significance [31] although it would appear that this is not the case in this study.
With regard to the lanthanides, when investigating limestone formation and rare earth element prevalence, Nagarajan et.al.
[32] found a negative correlation of rare earth elements and CaO.
It is suggested that in general the absorption of lanthanide cations is weak, and therefore they have low molar absorption coefficients, due to shielding of the 4f orbitals by the filled 5s and 5p sub-shells [33].
The low cation exchange capacity of the lanthanides may explain the small decreases in concentration in limestone, with increasing bed inventory.
Mass balance
A mass balance for the carbonator is provided as Table 3 calculated based on the incoming and outgoing concentrations of elements.
It shows that partitioning was always higher to the solid samples than in the flue gas samples, with very low concentrations portioning to the flue gas in all cases.
SEM-EDS analysis
SEM-EDS analysis of sorbent sampled from the carbonator after each test was undertaken to determine the percentage weight of the elemental species present.
The results as shown in Fig. 8 confirm that increasing bed inventory generally increases the presence of Al, Fe and Cu.
Al was identified in the unreacted sample at a weight% of 0.34, but this increased to similar values of 0.67% and 0.66% for 4.5kg and 6kg samples respectively, with a much higher value of 3.98% for 13kg sample of sorbent.
This increase in Al% weight with increasing sorbent mass may be because there is a greater amount of Al present originally, or because the limestone acts as a sorbent for Al, which is also considered the case for Fe and Cu.
Further, Al is likely to form Ca aluminates which are important constituents of cement.
The EDS results indicate similar quantities of C, O and Ca between tests, although values for the samples that had undergone testing are slightly lower for C and O, and slightly higher for Ca, compared to the unreacted limestone sample, implying that the small change was a result of chemical reactions.
Although the values are small, a steady decrease in Si weight% was recorded with increasing sorbent mass, implying removal of Si from limestone in the capture process.
However the lowest value was recorded for the unreacted limestone.
Elemental partitioning - experimental effect of SO2 concentration
Major elements
Fig. 10 shows how gaseous major element concentrations were affected by flue gas SO2 concentration.
Concentrations of all were very low at <1ppm, with Fe and Si present in the highest concentrations at 0.63 and 0.51ppm respectively.
For several elements, 1000ppm SO2 resulted in either the highest concentrations, as was the case for Na, Al, and Si, or the lowest concentrations, as was the case for Fe, when compared to 0 and 2000ppm SO2.
Minor elements
1000ppm SO2 concentration in looping cycle tests again appears to have some influence in reducing the concentration of some minor elements in the sorbent, when compared to 0 and 2000ppm SO2 (Fig. 11).
This is the case for the elements Co, Ni, Zr, Mo, Cd, Sn and Ce.
For the remainder of the elements shown, there was a small decrease in value with increasing SO2, other than for Cu which saw a small increase in value from 9.74 to 14.13ppm for 0ppm and 2000ppm SO2 respectively.
All minor element flue gas concentrations were <0.1ppm and therefore considered negligible.
Trace elements
Fig. 12 shows that increasing SO2 concentration does influence trace element sorbent concentration, and for B in particular, concentrations rose from 2ppm to 2.6ppm for 0 and 1000ppm SO2 respectively.
W showed a decrease with increasing SO2 concentration from 1.3ppm to 0.08ppm for 0 and 1000ppm SO2 respectively.
Although trends are evident for the trace elements, concentrations are low, reducing the reliability of the results and in turn affecting the reliability of the trends observed.
All trace element flue gas concentrations were <0.1ppm and therefore considered negligible.
The results show that looping cycle tests carried out in the presence of increasing concentrations of SO2 resulted in, for the most part, a decrease in elemental concentrations in both solid sorbent and flue gas samples.
A small number of elements appeared to increase in concentration with increasing SO2 concentration, including Zn, Cu, B and Yb.
Iron cation exchange sites may account for the increase in concentration of the transition metals Zn and Cu.
The larger molar volume of CaSO4 (46cm3/mol) compared to CaCO3 (37cm3/mol) may affect ion-exchange or covalent bonding within the pore structure, causing enhanced trace element release in the majority of cases.
The fact that emissions do not increase linearly with increasing sorbent mass or SO2 concentration implies that the chemistry of the system is complex.
Mass balance
A mass balance showing effects of increasing SO2 concentration on elemental partitioning for the carbonator is provided as Table 4, calculated based on the incoming and outgoing concentrations of elements.
A mass balance closure value of 1 indicates outgoing concentrations equal to incoming concentrations, and as shown by the low closure values, partitioning was always higher to the solid samples than in the flue gas samples, with very low concentrations portioning to the flue gas.
SEM-EDS analysis
Fig. 13 confirms that SO2 concentration appears to have had an effect on C, O and Ca weight%, with C and O decreasing with increasing SO2, and Ca increasing slightly with increasing SO2.
This suggests that the presence of SO2 decreases the extent to which CaO is able to convert to carbonate in the carbonator, the reactor from which the solid samples were taken.
It is widely accepted that the presence of SO2 during the carbonation reaction produces CaSO4 on the sorbent particle pore surfaces, which reduces the extent to which CO2 can diffuse into pore space and react to form CaCO3 [34].
As expected, the weight% of S present in the sorbent samples increased from 0.81% to 4.73%, to 10.32% for SO2 concentrations of 0, 1000 and 2000ppm respectively, with a value of 0% for unreacted limestone.
The value of 0% for unreacted limestone, but the presence of S identified during ICP-MS analysis (Table 2) may be due to the higher range that ICP-MS analysis can undertake.
In terms of other element species, an SO2 concentration of 1000ppm appears to result in higher weight% values for Al, Fe, Mg and Si, than for SO2 concentrations of 0ppm and 2000ppm SO2, with Fe, Mg and Si not present at other SO2 concentrations.
This result is unexpected, but it may be that, as mentioned previously in Section 3.2 the higher concentration of SO2, and in turn CaSO4, results in greatly reduced available pore space and therefore bonding between these species, when compared to 1000ppm SO2.
In turn, it may be that the presence of 1000ppm SO2 causes some corrosion of the stainless steel reactor itself, resulting in small deposits on the sorbent particles, although it would be expected that if this were the case, higher values would also be achieved for 2000ppm SO2.
Miller et al. [35] investigated the influence of SO2 on trace element behaviour during wood-bark combustion, and concluded that SO2 increased emissions of Cd, but reduced emissions of As and Hg.
Overall, it is apparent that the interactions that take place in the presence of SO2 are complex.
Thermodynamic modelling - sensitivity analysis
A sensitivity analysis was carried out using MTDATA, in order to determine the effect of the presence of certain trace elements on the formation of compounds.
The analysis was carried out using the statistical method 'non-parametric Kruskal-Wallis ANOVA by ranks', using STATISTICA software (CSS Statistica/W, Release 5.0 with Industrial units, Statsoft UK).
This method allows the comparison of two or more independent samples, and uses the sum of the difference between mean ranks of the samples, as the statistic.
This then provides an indication of whether the absence of an element input to the model has an effect on the stability of a certain compound in the model output.
Due to extended calculation times required, a sensitivity analysis was carried out firstly for nine trace elements (Ba, Cd, Cr, K, Mg, Ni, Sr, Ti, and Zn) and then for twenty-one trace elements (Li, Be, Mg, Al, K, Ti, V, Cr, Mn, Fe, Co, Ni, Zn, As, Se, Sr, Cd, Sb, Ba, Hg, and Pb), in addition to the four 'base' elements (Ca, C, O, and H) (always considered to be present from limestone and gas input), based on those which are considered to be most volatile, have the greatest negative impact on the environment and health, and which were present in the highest quantities in the limestone as identified by ICP-MS analysis, as outlined in Table 2.
MTDATA was run for all of 'base' elements and either the nine or twenty-one trace elements present, and the data recorded.
One trace element was then removed to see the effect on compound stability and formation.
For the next run, this trace element was included, and the next trace element removed, and so on.
The results for carbonation and calcination respectively, are shown in Tables 5 and 6.
The results identify that for both carbonation and calcination, Mg has a strong link with the presence of many compounds, including CaCO3.
In the case of the sensitivity analysis based on 9 elements, Mg appears to be the only element that has a link with the presence of compound formation in the model output.
However, in the case of the sensitivity analysis based on 21 elements, a greater number of elements have an link with the presence of compounds formed under carbonation conditions, including Mg and Al, having an effect on the greatest number of compounds.
The increased number of elements that have an effect on the presence of compounds under calcination conditions compared to carbonation conditions again may be due to the increased temperatures at which calcination is carried out, thus allowing the less volatile elements to react, due to an increased amount of thermal energy available for activation energies.
It is also apparent that increasing the number of trace elements present in the model, increases the chemical interactions that are likely to take place.
For example, when 21 trace elements are included in the model in addition to the four base elements, there are a greater number of compounds that are affected by the presence or absence of another trace element, compared to when only 9 elements are included in the model.
Thermodynamic modelling - effect of sorbent mass
MTDATA estimates of the likely compound formation and associated phases for varying quantities of sorbent, are shown in Figs.
14-16.
The results identify that under carbonation conditions the molar quantities of species in equilibrium with combustion gases and limestone, increase as sorbent mass in the reactor increases, and results appear consistent with experimental data for transition metals in particular (Figs.
4-6).
Values for gaseous Cd concentrations increased from 8.0E-05 for 4.5kg, to 6.0E-05 for 6kg, to 2.0E-04 for 13kg, at 740°C.
Similarly for Zn, molar gaseous emissions increased from 9.0E-04 for 4.5kg, to 7.0E-4 for 6kg, to 2.0E-03 for 13kg, at all temperatures.
These trends are consistent with the majority of the species present.
Gaseous KOH concentrations however reaches its maximum value at 720°C for 4.5kg, before then declining in value, whereas for 6kg and 13kg, the temperature at which the maximum value is reached at 740°C and 760°C respectively.
This shows the effect that increasing mass has on the thermodynamic equilibrium for a minority of species present.
Mg was identified as one of the primary impurities of limestone (Table 2).
Fig. 15 identifies that increasing sorbent mass increases the amount of solid MgO and MgCaO2 at the relevant temperatures at which they are stable, but increasing sorbent mass has no effect on the temperature at which both compounds are stable.
Fig. 16 shows a similar trend whereby increasing sorbent mass also increases the mass of solid CaCO3 and CaO present.
Thermodynamic modelling - effect of SO2 concentration
It is apparent from the results of MTDATA thermodynamic equilibrium modelling that increasing SO2 concentration during carbonation has an effect on the species present at equilibrium.
Fig. 17 shows that the addition of SO2 allows the formation of S-based compounds that would not otherwise form if SO2 was not introduced.
For example at 0ppm SO2 gaseous Ni and Zn are present, but at 1000ppm and 2000ppm SO2, ZnS is also formed, but only NiS is formed in place of Ni.
Experimental data (Fig. 11) identifies a general decrease in Ni concentration with increasing SO2 concentration, but it would be expected that this would be found as Ni rather than NiS, given the oxidative conditions in the reactor, and the requirement of reducing conditions for H2S, and in turn NiS formation.
Increasing SO2 concentration appears to have no effect on the stability of gaseous Cd or Ba(OH)2, although experimental data (Fig. 9) confirms a decrease with Ba with increasing SO2 concentration.
SO2 does appear to have some influence on the stability of gaseous Zn and ZnS, whereby increasing SO2 concentration appears to reduce the amount of gaseous Zn up until 780°C, above which temperature the amount of Zn reaches stability at -1.49/mol, regardless of the concentration of SO2 present.
Experimental data (Fig. 10) however confirms decreasing Zn concentration with increasing SO2 from 0ppm to 2000ppm SO2.Thermodynamic equilibrium data shows there is a small influence on the stability of gaseous K, with increasing SO2 concentration slightly reducing the amount of gaseous K.
For example, at 700°C and 0ppm SO2, calculations show there to be -4.59/mol, compared to -4.75/mol with 2000ppm SO2.
This however is the opposite of what experimental data (Fig. 10) suggests.
Gaseous ZnS formation increases with SO2 concentration, from -6.9/mol at 900°C for 1000ppm SO2 to -6.7/mol for 2000ppm SO2.
Again, the predicted formation of sulphides is unexpected given the O2 concentration in the carbonator (Fig. 4), and highlights the limitations of MTDATA compared to experimental data.
Fig. 18 shows that at approximately 700°C and with 0ppm SO2, the stability of solid MgO decreases, whilst in its place the stability of solid MgCaO2 increases to become the predominant Mg species.
Increasing concentrations of SO2 appear to increase the temperature at which the stability of MgO decreases and the stability of MgCaO2 increases.
For example at 2000ppm, the temperature at which MgO reaches its minimum and MgCaO2 starts increasing is at approximately 740°C.
Fig. 19 shows that increasing SO2 concentration has minimal effect on the stability of CaCO3 which reaches a minimum at approximately 860°C, but SO2 does have a small effect on the stability of CaO, with higher SO2 concentrations increasing the temperature at which CaO is first formed, from 780°C with 0ppm to 820°C with 2000ppm SO2.
The model suggests that solid CaS is formed in the presence of SO2 under conditions typical of carbonation, and higher concentrations of which result in higher amounts of CaS formed.
The results show that thermodynamic modelling is a useful means of supporting experimental data with regards to gaseous trace element compositions and elemental partitioning, over a range of temperature conditions relevant to Ca-based CO2 capture.
However, there may be several reasons for the various discrepancies noted, primarily that MTDATA calculates the reactions at equilibrium, and assumes that the reactions go to completion, when in reality, this may not be the case.
In addition, the model databases may not contain all of the compounds relevant to the required experimental conditions, and thermodynamic equilibrium data may not be available for all compounds at all required temperatures.
Diffusion rates through sorbent pores are also an important factor in CO2 capture efficiencies.
Work carried out by Bhatia and Perlmutter [36] proposed the 'Random Pore Model' which describes kinetically-controlled fluid-solid reactions, taking into account factors including boundary layers and product layer diffusion.
These influences are important when considering experimental reaction rates and in turn efficiencies, but unfortunately are not aspects that can be included when using thermodynamic equilibrium modelling to simulate reactions.
Conclusion
Gaseous element emissions sampling from Cranfield's pilot scale, 25kWth CO2 capture reactor using a limestone-derived CO2 sorbent, was undertaken for a sorbent mass of 4.5kg, 6g and 13kg, and also for SO2 concentration of 0ppm, 1000ppm and 2000ppm SO2 in the carbonator reactor.
Element emissions to the gaseous phase appeared to be influenced by sorbent mass, with the majority of species increasing in concentration as sorbent mass increased.
Increasing SO2 concentration also appeared to have an effect on gaseous trace element emissions, although the effect appeared to differ depending upon the element concerned, with some increasing in concentration, and other decreasing in concentration as SO2 concentration increased, suggesting complex underlying chemistry.
The mass balances however show that compared to the concentration of elements in the sorbent, the concentrations partitioning to the gaseous phase is very low.
Thermodynamic modelling was undertaken to verify the likely elemental speciation and partitioning under CO2 capture conditions, results of which have proved useful in supporting the experimental work.
The experimental results provide valuable information with regards to scale up of CO2 capture technology incorporating the Ca-looping cycle, and thermodynamic modelling can be considered a useful instrument in supporting this work, particularly with regards to elemental emissions.
Acknowledgments
The authors wish to acknowledge the Engineering and Physical Sciences Research Council (EPSRC Grant No: EP/G06279X/1) for financial support of the project, Sheffield University for carrying out ICP-MS analysis of Longcliffe SP52 limestone, and M. Roskilly for his technical support with the project.

1. A method for producing a transition metal composite oxide, comprising: a 1 st step of preparing a 1 st internal formation metal salt aqueous solution and a 2 nd internal formation metal salt aqueous solution containing nickel, cobalt, and manganese and having different concentrations of the nickel, cobalt, and manganese from each other; step 2, supplying a chelating agent and an aqueous alkali solution into the reactor; a 3 rd step of continuously supplying and mixing the 1 st interior-forming metal salt aqueous solution, the chelating agent, and the alkali aqueous solution to the reactor, and culturing particles having a fixed concentration of nickel, cobalt, and manganese and containing the 1 st interior having a radius of r1(0.2 um. ltoreq. r 1. ltoreq.5 um); and a 4 th step of mixing and supplying the 1 st inside forming metal salt aqueous solution and the 2 nd inside forming metal salt aqueous solution in a mixing ratio gradually changing from 100 v%: 0 v% to 0 v%: 100 v%, while mixing the chelating agent and the alkali aqueous solution into the reactor, to form particles containing the 2 nd inside having a radius r2(r2 ≦ 10um) in the 1 st inside profile, characterized in that,
the concentration of the alkaline aqueous solution added in the reactor in the 2 nd step is adjusted according to the nickel concentration of the particles produced in the 3 rd step before the 3 rd step, and the concentration of the alkaline aqueous solution of the reaction solution in the 2 nd step is adjusted to 0.25g/L to 0.5g/L so that the pH of the solution in the reactor in the 2 nd step is adjusted to 11.8 to 12.3.2. The method for producing a transferred-metal composite oxide according to claim 1,
the nickel concentration in the 1 st internal formation metal salt aqueous solution is adjusted to 0.8 to 1 mol%.3. The method for producing a transferred-metal composite oxide according to claim 1,
in the steps 1 to 4, the aqueous solution of the metal salt for forming the inside of the 1 st compartment, the chelating agent, and the aqueous alkali solution are mixed in the reactor so that the particle size distribution of the particles formed after the reaction for 30 minutes has D50 of 4um or less.4. The method for producing a transferred-metal composite oxide according to claim 1,
further comprising the step 5: drying or heat-treating the transition metal composite oxide obtained by performing the 1 st to 4 th steps,
and the average diameter of the transfer metal composite oxide particles produced in the 5 th step is 5 to 10 um.5. A transition metal composite oxide produced by the production method according to any one of claims 1 to 4.6. The method for producing a transferred-metal composite oxide according to claim 5,
further comprising a 6 th step of mixing a lithium salt with the transition metal composite oxide produced in the 5 th step and performing a heat treatment.7. A lithium composite oxide produced by the method for producing a transition metal composite oxide according to claim 6.8. The lithium composite oxide produced by the method for producing a transition metal composite oxide according to claim 7,
represented by the following chemical formula 1
[ chemical formula 1]LiaaNixaCoyaMnzaO2+δ
(Xa is 0.5. ltoreq. Xa is less than or equal to 1.0 in the chemical formula 1).9. A method for producing a transition metal composite oxide,
the method comprises the following steps:
manufacturing a 1 st metal salt aqueous solution containing nickel, manganese and cobalt;
manufacturing an aqueous solution of a 2 nd metal salt containing nickel, manganese and cobalt;
mixing an alkaline aqueous solution and an aqueous ammonia solution in a reactor, adjusting the concentration of the alkaline aqueous solution according to the nickel concentration of the 1 st metal salt aqueous solution, and adjusting the pH value in the reaction solution to 11.8 to 12.3; and
supplying a 1 st mixed metal salt aqueous solution mixing the 1 st metal salt aqueous solution and the 2 nd metal salt aqueous solution, ammonia, and an alkaline aqueous solution into the reactor,
and a mixing ratio of the 1 st metal salt aqueous solution and the 2 nd metal salt aqueous solution in the 1 st mixed metal salt aqueous solution is 0 v% or more and 100 v% or less.10. The method for producing a transferred-metal composite oxide according to claim 8,
the content of nickel in the 1 st metal salt water solution is x1, the content of manganese is y1, the content of cobalt is z1,
the content of nickel in the 2 nd metal salt water solution is x2, the content of manganese is y2, the content of cobalt is z2,
x1+y1+z1＝1，x2+y2+z2＝1，
at least one of x1 ≠ x2, y1 ≠ y2, and z1 ≠ z2 is satisfied.11. The method for producing a transferred-metal composite oxide according to claim 9,
x1 is 0.8 or more and 1.0 or less.12. The method for producing a transferred-metal composite oxide according to claim 9,
the value of x2 is 0.8 or less.13. The method for producing a transferred-metal composite oxide according to claim 8,
when the 1 st mixed metal brine solution is provided, the mixing ratio of the 1 st metal brine solution and the 2 nd metal brine solution is gradually changed from 100 v%: 0 v% to 0 v%: 100 v%.14. The method for producing a transferred-metal composite oxide according to claim 8,
further comprising:
manufacturing a 3 rd metal salt aqueous solution including nickel, manganese and cobalt; and
providing a 2 nd mixed metal salt aqueous solution mixing the 1 st mixed metal salt aqueous solution and the 3 rd metal salt aqueous solution, ammonia, and an alkaline aqueous solution into a reactor,
and a mixing ratio of the 1 st mixed metal salt aqueous solution and the 3 rd mixed metal salt aqueous solution in the 2 nd mixed metal salt aqueous solution is 0 v% or more and 100 v% or less.15. The method for producing a transferred-metal composite oxide according to claim 13,
the content of nickel in the 1 st mixed metal salt water solution is x3, the content of manganese is y3, the content of cobalt is z3,
the content of nickel in the 3 rd metal salt water solution is x4, the content of manganese is y4, the content of cobalt is z4,
x3+ y3+ z3 is 1 and x4+ y4+ z4 is 1,
at least one of x3 ≠ x4, y3 ≠ y4, and z3 ≠ z4 is satisfied.16. The method for producing a transferred-metal composite oxide according to claim 13,
when the 2 nd mixed metal brine solution is provided, the mixing ratio of the 1 st mixed metal brine solution and the 3 rd mixed metal brine solution is gradually changed from 100 v%: 0 v% to 0 v%: 100 v%.The prevalence of tinnitus and the relationship with neuroticism in a middle-aged UK population

Abstract
Background
Previous research has suggested that a substantial proportion of the population are severely affected by tinnitus, however recent population data are lacking.
Furthermore, there is growing evidence that the perception of severity is closely related to personality factors such as neuroticism.
Objective
In a subset (N=172,621) of a large population sample of >500,000 adults aged 40 to 69years, (from the UK Biobank dataset) we calculated the prevalence of tinnitus and that which is perceived as bothersome, and examined the association between tinnitus and a putative predisposing personality factor, neuroticism.
Method
Participants were recruited through National Health Service registers and aimed to be inclusive and as representative of the UK population as possible.
The assessment included subjective questions concerning hearing and tinnitus.
Neuroticism was self-rated on 13 questions from the Eysenck Personality Inventory.
Associations between neuroticism and tinnitus were tested with logistic regression analyses.
Results
Prevalence of tinnitus was significantly higher for males, and increased with age, doubling between the youngest and oldest age groups (males 13% and 26%; females 9% and 19% respectively).
Of those with tinnitus, females were more likely to report bothersome tinnitus.
Neuroticism was associated with current tinnitus and bothersome tinnitus, with the items: 'loneliness', 'mood swings', 'worrier/anxious' and 'miserableness', as the strongest associations of bothersome tinnitus.
Conclusions
Neuroticism was identified as a novel association with tinnitus.
Individuals with tinnitus and higher levels of neuroticism are more likely to experience bothersome tinnitus, possibly as a reflection of greater sensitivity to intrusive experiences.

Introduction
Tinnitus is the perception of sounds in the head or ears, usually defined as a ringing, buzzing or whistling sound.
Tinnitus can be objective or subjective.
Objective tinnitus is caused by sounds generated by an internal biological activity.
However, subjective tinnitus is much more common and results from abnormal neural activities which are not formed by sounds [1].
Subjective tinnitus is a common and disturbing phenomenon, with a reported prevalence ranging from 7 to 20% [2-5] in the general population, and an estimated 10year incidence rate in adults aged over 48years of 13% [6].
Approximately 5% of the population is severely affected by their tinnitus [7], for example experiencing sleep disorders, concentration difficulties, and symptoms of anxiety and depression.
Tinnitus can affect a person's satisfaction with quality of life [8], and their physical, emotional, and social functioning [9], as well as leading to a higher incidence of anxiety and depression [10].
However, not everyone with tinnitus will experience the same amount of distress and impairment of quality of life.
It is likely that psychological factors - including personality characteristics - mediate the impact of tinnitus.
Personality type can influence the vulnerability to, and the severity of, the problem by influencing the tendency to be aware of it [11].
Personality characteristics previously reported to be associated with tinnitus include hysteria and hypochondriasis [9,12], introversion [13], withdrawal [9], and emotional isolation [14].
Additionally, particular cognitive strategies, for example, dysfunctional and catastrophic thoughts can increase patients' emotional distress and perceived tinnitus severity, and are thought to be closely related to personality factors [15].
Neuroticism is expressed as "individual differences in the tendency to experience negative, distressing emotions" [16] (p.
301).
At one extreme, individuals are characterized by high levels of vulnerability to experience negative emotions, including sadness, fear, anxiety, anger, frustration, and insecurity [17].
At the other end of the spectrum, individuals who score low in neuroticism are more emotionally stable and less reactive to stress.
Neuroticism has been associated with adverse outcomes in various health conditions, including increased likelihood of morbidity in those with testicular cancer [18], and an increased likelihood of arthritis, kidney/liver disease, and diabetes in the general population [19].
There is evidence that neurotic traits are stronger in tinnitus patients [20], particularly in those with higher levels of tinnitus annoyance, and recent evidence that neuroticism may predict the development of severe tinnitus in patients already experiencing some tinnitus [21].
In a cross-sectional sample of 530 participants (50% with chronic tinnitus), Bartels and colleagues [22] studied the role of type D personality (the tendency towards negative affectivity and social inhibition) on health-related quality of life and self-reported tinnitus-related distress.
Tinnitus patients with type D personality reported greater tinnitus-related distress and poorer health-related quality of life compared to those with other personality types.
The authors concluded that some personality characteristics are associated with having tinnitus and are likely to contribute to its perceived severity.
Methods
The UK Biobank is a large dataset which was established as a resource for the investigation of the genetic, environmental and lifestyle causes of common diseases [23].
It provides an excellent opportunity to study a range of factors associated with hearing-related problems, including tinnitus.
Recruitment of adults aged 40 to 69years was carried out through National Health Service registers and aimed to be inclusive and as representative as possible of the UK population.
Overall, 9.2million invitations were sent out and 503,325 participants were recruited over the course of 2006-2010, giving a response rate of 5.47%.
It is possible that the low response rate may have led to unknown biases; however, due to the size and coverage of the sample, UK Biobank suggests that associations observed in the sample should be generalizable to the UK population [23].
Participants completed a single assessment of approximately 90minute duration at one of the 22 assessment centres located in England, Scotland, and Wales.
The UK Biobank self-completed touch-screen questionnaire included data relating to a broad range of health-related issues, including hearing, lifestyle, occupation, family history, and psychological state.
The present study reports data related to tinnitus and personality.
Demographic questions
Demographic data were collected during the assessment, including age, sex and socioeconomic status.
The Townsend deprivation score was used in UK Biobank as a proxy for socioeconomic status.
This is a geographic based measure where census data are available and were used as a measure of deprivation using the 2001 census returns.
Tinnitus
Two self-report questions on tinnitus were included in the touchscreen questionnaire, answered by 172,621 participants.
The first question was "Do you get or have you had noises (such as ringing or buzzing) in your head or in one or both ears that lasts for more than five minutes at a time?" Participants who gave a yes response were then asked to categorise the duration from a predefined list.
Current tinnitus was identified if the participant responded: 'Yes, now most or all of the time', 'Yes, now a lot of the time' or 'Yes, now some of the time'.
The second question was concerned with severity of tinnitus: "How much do these noises worry, annoy or upset you when they are at their worst?" The response options were 'not at all', 'slightly', 'moderately', or 'severely'.
Bothersome tinnitus was defined as those who responded 'severely' or 'moderately'.
Participants could also respond 'do not know' or 'prefer not to answer' to these questions.
The UK Biobank assessment protocol did not include any tinnitus scale, nor did it measure the loudness of the tinnitus, however the subjective questions included have been deemed reliable in similar studies [2].
Hearing difficulty
Participants (n=497,984) were asked "Do you have any difficulty with your hearing?" Participants could respond 'yes', 'no', or 'I am completely deaf'.
Those that responded 'I am completely deaf' were excluded from answering subsequent hearing-related questions.
A second question asked: "Do you find it difficult to follow a conversation if there is background noise (such as TV, radio, children playing)?" Again, participants could respond 'yes', or 'no'.
Neuroticism
The Eysenck Personality Inventory (EPI) [24] is a questionnaire assessing personality type.
Eysenck describes personality according to three biologically-based independent dimensions of temperament measured on a continuum: extraversion/introversion, neuroticism/stability and psychoticism/socialisation.
Of these, the thirteen questions related to neuroticism/stability were included in UK Biobank (see Table 1).
A total of 501,776 participants responded to these questions.
The response options were: 'yes', 'no', 'do not know', 'prefer not to answer'.
These variables were combined to create an overall measure of 'neuroticism' on a continuous scale, as per the EPI scoring protocol, i.e. a score of 13 would indicate responding 'yes' to all 13 questions.
No formal diagnosis of neuroticism was made.
Analysis
Analysis was conducted in three phases.
First of all, prevalence rates of tinnitus and bothersome tinnitus were obtained for males and females separately.
Overall prevalence rates were age-standardised by reference to the 40-69year old age profile from the 2011 UK census; they were also standardised for deprivation using the Townsend index.
Standardisation of the national deciles of the Townsend score means that the range from 0 to 1 corresponds to the range from the least deprived to the most deprived.
The neuroticism score was similarly scaled to range between 0 and 1, by division of 13, to facilitate comparison with other effects.
Further analyses were performed using binary logistic regression to examine whether neuroticism is associated with tinnitus and tinnitus severity, while controlling for demographic factors (age, sex, deprivation) and hearing difficulty.
All analyses were conducted in R using the mice package for multiple imputation [25].
Initially a binary logistic multiple regression analysis was performed with current tinnitus as the dependent variable ('no tinnitus' as the reference category) with all participants being at risk.
A subsequent, conditional binary logistic multiple regression analysis was performed based on all participants with current tinnitus, with severity (tinnitus not bothersome as the reference category) as the dependent variable.
In both cases, age, sex, hearing difficulty, hearing difficulty in background noise, deprivation, and neuroticism were the predictor variables.
For all questions, participants had the option of choosing 'do not know' or 'prefer not to answer'.
Questions relating to hearing, including the two questions regarding tinnitus, were introduced at a later stage than other questions.
For example, all participants (n=501,776) were asked the personality questions but approximately one third of participants (n=172,621) were asked the questions relating to tinnitus.
Only this subset of participants was included in the analysis.
Inspection of patterns of missing data due to participants choosing not to respond to particular questions suggested that data were not Missing Completely at Random (MCAR) but that missing data might be in some way informative.
Missing responses for each of the individual personality type questions ranged from 1.9% to 4.9%.
Consequently, such missing responses were multiply imputed by chained equations using 20 imputation chains [26].
Results
Prevalence
Just over half of the sample were female (54.4%, n=273,488).
The mean age was 56years old (S.D.=8).
Prevalence rates for 40-69year olds, given in Fig. 1, are age-standardised in accordance with a reference population as reported in the UK 2011 census returns [27].
Overall, the prevalence of tinnitus was 16.2% and was higher for males than females (18.4% vs 14.1%).
Prevalence of bothersome tinnitus was 3.8% overall and slightly higher for males than females (4.1% vs 3.5%).
Estimates of age-specific prevalence rates of current tinnitus and current bothersome tinnitus are given in Fig. 2.
It is apparent that all rates of prevalence in this population are higher for males than for females, with a clearly increasing trend with age for both sexes.
The proportion of participants reporting current tinnitus who state that their tinnitus is bothersome remains fairly steady for males of all ages at 22%, whereas for women this proportion increases from 23% to 26% with increasing age.
Neuroticism as a risk factor for severe tinnitus
The results of the binary logistic regression can be seen in Table 2.
Odds ratios and 95% confidence intervals are shown for the predictors: sex, age, deprivation, hearing difficulty, hearing difficulty in background noise, and neuroticism.
The results confirm that males were more likely to experience tinnitus, but of all tinnitus sufferers, females were more likely to find their tinnitus bothersome.
Increasing age was associated with tinnitus, but not in terms of tinnitus being bothersome.
Deprivation was significantly associated with current tinnitus and bothersome tinnitus, with higher deprivation being associated with tinnitus/bothersome tinnitus.
Hearing difficulties in background noise were associated with tinnitus and bothersome tinnitus, but self-reported hearing difficulties were more strongly associated.
Neuroticism (measured on a continuous scale from 0 to 1) was found to be associated with tinnitus and bothersome tinnitus.
The odds ratio for neuroticism describes the effect on prevalence of tinnitus of the highest levels of neuroticism relative to the lowest level of neuroticism.
Comparing these two extremes of neuroticism accounted for more difference in the prevalence of tinnitus than any other factor with the exception of self-reported hearing difficulty.
With regard to whether participants found their tinnitus to be bothersome, differences in neuroticism appeared to have a much stronger effect than any other factor.
Following on from this, because neuroticism was measured on a continuous scale from 0 to 1, it was considered worthwhile examining the individual aspects of neuroticism, to see if any particular items were associated with tinnitus, more so than others.
Table 3 shows the adjusted odds ratios (controlling for age, sex, deprivation and hearing difficulty) for the individual neuroticism items as predictors for tinnitus.
When the individual items of neuroticism were examined, all but 'worrier/anxious', 'guilty feelings', and 'risk taking' were significantly associated with current tinnitus in the multivariate adjusted model.
However, of these three items, only one is not significant of having bothersome tinnitus ('guilty feelings').
All other items, apart from two ('irritability', and 'nervousness') were significantly associated with bothersome tinnitus.
The item with the greatest association with bothersome tinnitus was 'loneliness', followed by 'mood swings', 'worrier/anxious', and 'miserableness'.
Discussion
This study reports the prevalence of tinnitus in a large population sample, while further exploring the link between tinnitus severity and neuroticism (previously only examined in clinical studies).
After controlling for age, sex, hearing difficulty and deprivation, this study found that neuroticism was associated with tinnitus and bothersome tinnitus.
The effects were stronger for bothersome tinnitus.
As neuroticism is a stable personality trait [28] it is unlikely to develop as a result of experiencing tinnitus.
It is more likely that those who are more neurotic and experience tinnitus are more likely to perceive the tinnitus as being bothersome, compared to those who are less neurotic.
These findings do suggest that tinnitus patients with higher levels of neuroticism may be more prone to distress and consequently more severe tinnitus.
Loneliness was the most significant association with bothersome tinnitus which supports previous research showing that tinnitus sufferers are likely to experience loneliness [29], possibly as a result of withdrawing from social interactions [9] and thereby increasing feelings of isolation and loneliness.
Tyler and Baker [30] found that over a third (36.1%) of tinnitus patients reported emotional problems such as despair, frustration and depression, while nearly a fifth (16.6%) reported insecurity, fear and worry.
Furthermore, clinical studies have also shown higher rates of anxiety and depression among tinnitus patients compared with those without tinnitus [31], which supports the finding that 'mood swings', 'worrier/anxious' and 'miserableness' were significant associations of bothersome tinnitus in the current study.
The present study further expands on our earlier work [5] by providing a more detailed breakdown of tinnitus prevalence according to age and sex.
The prevalence of 16% overall, is slightly higher than the National Study of Hearing [2] which found that the prevalence of tinnitus was around 13%, (for adults aged 40 to 70years) and no significant differences between males and females were noted.
Other epidemiological studies, the Blue Mountains Hearing Study [32] and the Beaver Dam study [4], reported prevalence rates of tinnitus of 30% and 10.6% respectively, although the former included past tinnitus as well as current tinnitus.
The Beaver Dam study also found that tinnitus was higher for males than for females (11.9% vs 9.4%), and noted increasing trends in prevalence by age group for men, women and both sexes, consistent with the pattern found in the present study.
In the present study, 3.8% of current tinnitus sufferers reported bothersome tinnitus.
Even though males were more likely to report tinnitus, females were more likely to be bothered by their tinnitus.
This accords with previous research showing that female sex is a risk factor for bothersome tinnitus [33,34].
The under-reporting of bothersome tinnitus in men may be partly explained by a tendency for men to deny or play down the severity of the symptoms of ill health [35].
Self-reported hearing difficulty was the strongest association with tinnitus.
Previous research has shown that around 40% of 55-74year olds with hearing difficulties also report tinnitus [36], and the majority of tinnitus patients have some degree of hearing loss [37].
No single theory explaining the cause of tinnitus is universally accepted, but one theory suggests that tinnitus may result from an increase of central gain (to compensate for deprived sensory inputs) which amplifies neural noise in order to maintain neural homeostasis [38].
It is likely that the severity of tinnitus is influenced by a complex interaction involving auditory, psychological and emotional networks [39,40].
It is interesting that neuroticism is a strong association with current tinnitus, albeit not as strong as hearing difficulty.
Furthermore, of those who have tinnitus, neuroticism has a stronger effect on the perceived severity than hearing difficulty.
One explanation could be that those who are more neurotic may think their tinnitus is indicative of a more serious condition and therefore find the tinnitus more bothersome.
The finding that personality traits such as neuroticism can contribute to tinnitus awareness and distress is important when considering treatment approaches because personality traits are generally stable over time [41], although the absolute level of personality traits can change [42].
Thus it is possible that psychological interventions may be beneficial for tinnitus patients although the effects may be gradual [11].
The primary limitation of these analyses was that the UK Biobank test protocol did not include a validated tinnitus questionnaire.
More detailed data on lateralization, pitch, and loudness of the tinnitus may be informative as previous research suggested that the level of intrusiveness is moderated by the specific quality or type of tinnitus (e.g. fluctuating versus non-fluctuating) [43].
Furthermore, no information was available on the duration that the participant experienced tinnitus.
It is possible that long-term tinnitus sufferers may develop successful coping strategies and are better able to manage their condition, whereas those who have had tinnitus for a relatively short duration may not have learned coping strategies and thus are more bothered by tinnitus [43].
Although such data would be desirable for better characterisation of tinnitus, a full tinnitus questionnaire would not have been practical to administer within the crowded UK Biobank assessment protocol.
Conclusion
Analysing data from a large population of UK adults, we have shown that the prevalence of tinnitus increases with age, and that a high proportion of sufferers report tinnitus as being moderately or severely bothersome, although age was not related to the perceived severity of tinnitus.
Women were especially likely to report bothersome tinnitus.
This study confirms previous research suggesting that hearing loss is a strong predictor of tinnitus, but we have also shown that personality factors identified as neuroticism are associated with the experience of tinnitus, particularly in those reporting that their tinnitus is severe.
Although causality cannot be determined, the study supports an interpretation that otherwise stable personality factors and mood influence the ways in which tinnitus is experienced and managed.
From a clinical management point of view, clinicians would benefit from an understanding that tinnitus severity is a very complex interaction involving auditory, psychological and emotional networks.
Long-term psychological approaches may be effective in helping tinnitus sufferers.
Conflict of interest
We wish to confirm that there are no known conflicts of interest associated with this publication and there has been no significant financial support for this work that could have influenced its outcome.
We confirm that the manuscript has been read and approved by all named authors and that there are no other persons who satisfied the criteria for authorship but are not listed.
We further confirm that the order of authors listed in the manuscript has been approved by all of us.
We confirm that we have given due consideration to the protection of intellectual property associated with this work and that there are no impediments to publication, including the timing of publication, with respect to intellectual property.
In so doing we confirm that we have followed the regulations of our institutions concerning intellectual property.
We understand that the corresponding author is the sole contact for the Editorial process (including Editorial Manager and direct communications with the office).
She is responsible for communicating with the other authors about progress, submissions of revisions and final approval of proofs.
We confirm that we have provided a current, correct email address which is accessible by the corresponding author and which has been configured to accept email from abby.mccormack@nottingham.ac.uk.
Acknowledgement
This paper presents independent research funded in part by the National Institute for Health Research (NIHR).
The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health.

The prevalence of tinnitus and the relationship with neuroticism in a middle-aged UK population

Abstract
Background
Previous research has suggested that a substantial proportion of the population are severely affected by tinnitus, however recent population data are lacking.
Furthermore, there is growing evidence that the perception of severity is closely related to personality factors such as neuroticism.
Objective
In a subset (N=172,621) of a large population sample of >500,000 adults aged 40 to 69years, (from the UK Biobank dataset) we calculated the prevalence of tinnitus and that which is perceived as bothersome, and examined the association between tinnitus and a putative predisposing personality factor, neuroticism.
Method
Participants were recruited through National Health Service registers and aimed to be inclusive and as representative of the UK population as possible.
The assessment included subjective questions concerning hearing and tinnitus.
Neuroticism was self-rated on 13 questions from the Eysenck Personality Inventory.
Associations between neuroticism and tinnitus were tested with logistic regression analyses.
Results
Prevalence of tinnitus was significantly higher for males, and increased with age, doubling between the youngest and oldest age groups (males 13% and 26%; females 9% and 19% respectively).
Of those with tinnitus, females were more likely to report bothersome tinnitus.
Neuroticism was associated with current tinnitus and bothersome tinnitus, with the items: 'loneliness', 'mood swings', 'worrier/anxious' and 'miserableness', as the strongest associations of bothersome tinnitus.
Conclusions
Neuroticism was identified as a novel association with tinnitus.
Individuals with tinnitus and higher levels of neuroticism are more likely to experience bothersome tinnitus, possibly as a reflection of greater sensitivity to intrusive experiences.

Introduction
Tinnitus is the perception of sounds in the head or ears, usually defined as a ringing, buzzing or whistling sound.
Tinnitus can be objective or subjective.
Objective tinnitus is caused by sounds generated by an internal biological activity.
However, subjective tinnitus is much more common and results from abnormal neural activities which are not formed by sounds [1].
Subjective tinnitus is a common and disturbing phenomenon, with a reported prevalence ranging from 7 to 20% [2-5] in the general population, and an estimated 10year incidence rate in adults aged over 48years of 13% [6].
Approximately 5% of the population is severely affected by their tinnitus [7], for example experiencing sleep disorders, concentration difficulties, and symptoms of anxiety and depression.
Tinnitus can affect a person's satisfaction with quality of life [8], and their physical, emotional, and social functioning [9], as well as leading to a higher incidence of anxiety and depression [10].
However, not everyone with tinnitus will experience the same amount of distress and impairment of quality of life.
It is likely that psychological factors - including personality characteristics - mediate the impact of tinnitus.
Personality type can influence the vulnerability to, and the severity of, the problem by influencing the tendency to be aware of it [11].
Personality characteristics previously reported to be associated with tinnitus include hysteria and hypochondriasis [9,12], introversion [13], withdrawal [9], and emotional isolation [14].
Additionally, particular cognitive strategies, for example, dysfunctional and catastrophic thoughts can increase patients' emotional distress and perceived tinnitus severity, and are thought to be closely related to personality factors [15].
Neuroticism is expressed as "individual differences in the tendency to experience negative, distressing emotions" [16] (p.
301).
At one extreme, individuals are characterized by high levels of vulnerability to experience negative emotions, including sadness, fear, anxiety, anger, frustration, and insecurity [17].
At the other end of the spectrum, individuals who score low in neuroticism are more emotionally stable and less reactive to stress.
Neuroticism has been associated with adverse outcomes in various health conditions, including increased likelihood of morbidity in those with testicular cancer [18], and an increased likelihood of arthritis, kidney/liver disease, and diabetes in the general population [19].
There is evidence that neurotic traits are stronger in tinnitus patients [20], particularly in those with higher levels of tinnitus annoyance, and recent evidence that neuroticism may predict the development of severe tinnitus in patients already experiencing some tinnitus [21].
In a cross-sectional sample of 530 participants (50% with chronic tinnitus), Bartels and colleagues [22] studied the role of type D personality (the tendency towards negative affectivity and social inhibition) on health-related quality of life and self-reported tinnitus-related distress.
Tinnitus patients with type D personality reported greater tinnitus-related distress and poorer health-related quality of life compared to those with other personality types.
The authors concluded that some personality characteristics are associated with having tinnitus and are likely to contribute to its perceived severity.
Methods
The UK Biobank is a large dataset which was established as a resource for the investigation of the genetic, environmental and lifestyle causes of common diseases [23].
It provides an excellent opportunity to study a range of factors associated with hearing-related problems, including tinnitus.
Recruitment of adults aged 40 to 69years was carried out through National Health Service registers and aimed to be inclusive and as representative as possible of the UK population.
Overall, 9.2million invitations were sent out and 503,325 participants were recruited over the course of 2006-2010, giving a response rate of 5.47%.
It is possible that the low response rate may have led to unknown biases; however, due to the size and coverage of the sample, UK Biobank suggests that associations observed in the sample should be generalizable to the UK population [23].
Participants completed a single assessment of approximately 90minute duration at one of the 22 assessment centres located in England, Scotland, and Wales.
The UK Biobank self-completed touch-screen questionnaire included data relating to a broad range of health-related issues, including hearing, lifestyle, occupation, family history, and psychological state.
The present study reports data related to tinnitus and personality.
Demographic questions
Demographic data were collected during the assessment, including age, sex and socioeconomic status.
The Townsend deprivation score was used in UK Biobank as a proxy for socioeconomic status.
This is a geographic based measure where census data are available and were used as a measure of deprivation using the 2001 census returns.
Tinnitus
Two self-report questions on tinnitus were included in the touchscreen questionnaire, answered by 172,621 participants.
The first question was "Do you get or have you had noises (such as ringing or buzzing) in your head or in one or both ears that lasts for more than five minutes at a time?" Participants who gave a yes response were then asked to categorise the duration from a predefined list.
Current tinnitus was identified if the participant responded: 'Yes, now most or all of the time', 'Yes, now a lot of the time' or 'Yes, now some of the time'.
The second question was concerned with severity of tinnitus: "How much do these noises worry, annoy or upset you when they are at their worst?" The response options were 'not at all', 'slightly', 'moderately', or 'severely'.
Bothersome tinnitus was defined as those who responded 'severely' or 'moderately'.
Participants could also respond 'do not know' or 'prefer not to answer' to these questions.
The UK Biobank assessment protocol did not include any tinnitus scale, nor did it measure the loudness of the tinnitus, however the subjective questions included have been deemed reliable in similar studies [2].
Hearing difficulty
Participants (n=497,984) were asked "Do you have any difficulty with your hearing?" Participants could respond 'yes', 'no', or 'I am completely deaf'.
Those that responded 'I am completely deaf' were excluded from answering subsequent hearing-related questions.
A second question asked: "Do you find it difficult to follow a conversation if there is background noise (such as TV, radio, children playing)?" Again, participants could respond 'yes', or 'no'.
Neuroticism
The Eysenck Personality Inventory (EPI) [24] is a questionnaire assessing personality type.
Eysenck describes personality according to three biologically-based independent dimensions of temperament measured on a continuum: extraversion/introversion, neuroticism/stability and psychoticism/socialisation.
Of these, the thirteen questions related to neuroticism/stability were included in UK Biobank (see Table 1).
A total of 501,776 participants responded to these questions.
The response options were: 'yes', 'no', 'do not know', 'prefer not to answer'.
These variables were combined to create an overall measure of 'neuroticism' on a continuous scale, as per the EPI scoring protocol, i.e. a score of 13 would indicate responding 'yes' to all 13 questions.
No formal diagnosis of neuroticism was made.
Analysis
Analysis was conducted in three phases.
First of all, prevalence rates of tinnitus and bothersome tinnitus were obtained for males and females separately.
Overall prevalence rates were age-standardised by reference to the 40-69year old age profile from the 2011 UK census; they were also standardised for deprivation using the Townsend index.
Standardisation of the national deciles of the Townsend score means that the range from 0 to 1 corresponds to the range from the least deprived to the most deprived.
The neuroticism score was similarly scaled to range between 0 and 1, by division of 13, to facilitate comparison with other effects.
Further analyses were performed using binary logistic regression to examine whether neuroticism is associated with tinnitus and tinnitus severity, while controlling for demographic factors (age, sex, deprivation) and hearing difficulty.
All analyses were conducted in R using the mice package for multiple imputation [25].
Initially a binary logistic multiple regression analysis was performed with current tinnitus as the dependent variable ('no tinnitus' as the reference category) with all participants being at risk.
A subsequent, conditional binary logistic multiple regression analysis was performed based on all participants with current tinnitus, with severity (tinnitus not bothersome as the reference category) as the dependent variable.
In both cases, age, sex, hearing difficulty, hearing difficulty in background noise, deprivation, and neuroticism were the predictor variables.
For all questions, participants had the option of choosing 'do not know' or 'prefer not to answer'.
Questions relating to hearing, including the two questions regarding tinnitus, were introduced at a later stage than other questions.
For example, all participants (n=501,776) were asked the personality questions but approximately one third of participants (n=172,621) were asked the questions relating to tinnitus.
Only this subset of participants was included in the analysis.
Inspection of patterns of missing data due to participants choosing not to respond to particular questions suggested that data were not Missing Completely at Random (MCAR) but that missing data might be in some way informative.
Missing responses for each of the individual personality type questions ranged from 1.9% to 4.9%.
Consequently, such missing responses were multiply imputed by chained equations using 20 imputation chains [26].
Results
Prevalence
Just over half of the sample were female (54.4%, n=273,488).
The mean age was 56years old (S.D.=8).
Prevalence rates for 40-69year olds, given in Fig. 1, are age-standardised in accordance with a reference population as reported in the UK 2011 census returns [27].
Overall, the prevalence of tinnitus was 16.2% and was higher for males than females (18.4% vs 14.1%).
Prevalence of bothersome tinnitus was 3.8% overall and slightly higher for males than females (4.1% vs 3.5%).
Estimates of age-specific prevalence rates of current tinnitus and current bothersome tinnitus are given in Fig. 2.
It is apparent that all rates of prevalence in this population are higher for males than for females, with a clearly increasing trend with age for both sexes.
The proportion of participants reporting current tinnitus who state that their tinnitus is bothersome remains fairly steady for males of all ages at 22%, whereas for women this proportion increases from 23% to 26% with increasing age.
Neuroticism as a risk factor for severe tinnitus
The results of the binary logistic regression can be seen in Table 2.
Odds ratios and 95% confidence intervals are shown for the predictors: sex, age, deprivation, hearing difficulty, hearing difficulty in background noise, and neuroticism.
The results confirm that males were more likely to experience tinnitus, but of all tinnitus sufferers, females were more likely to find their tinnitus bothersome.
Increasing age was associated with tinnitus, but not in terms of tinnitus being bothersome.
Deprivation was significantly associated with current tinnitus and bothersome tinnitus, with higher deprivation being associated with tinnitus/bothersome tinnitus.
Hearing difficulties in background noise were associated with tinnitus and bothersome tinnitus, but self-reported hearing difficulties were more strongly associated.
Neuroticism (measured on a continuous scale from 0 to 1) was found to be associated with tinnitus and bothersome tinnitus.
The odds ratio for neuroticism describes the effect on prevalence of tinnitus of the highest levels of neuroticism relative to the lowest level of neuroticism.
Comparing these two extremes of neuroticism accounted for more difference in the prevalence of tinnitus than any other factor with the exception of self-reported hearing difficulty.
With regard to whether participants found their tinnitus to be bothersome, differences in neuroticism appeared to have a much stronger effect than any other factor.
Following on from this, because neuroticism was measured on a continuous scale from 0 to 1, it was considered worthwhile examining the individual aspects of neuroticism, to see if any particular items were associated with tinnitus, more so than others.
Table 3 shows the adjusted odds ratios (controlling for age, sex, deprivation and hearing difficulty) for the individual neuroticism items as predictors for tinnitus.
When the individual items of neuroticism were examined, all but 'worrier/anxious', 'guilty feelings', and 'risk taking' were significantly associated with current tinnitus in the multivariate adjusted model.
However, of these three items, only one is not significant of having bothersome tinnitus ('guilty feelings').
All other items, apart from two ('irritability', and 'nervousness') were significantly associated with bothersome tinnitus.
The item with the greatest association with bothersome tinnitus was 'loneliness', followed by 'mood swings', 'worrier/anxious', and 'miserableness'.
Discussion
This study reports the prevalence of tinnitus in a large population sample, while further exploring the link between tinnitus severity and neuroticism (previously only examined in clinical studies).
After controlling for age, sex, hearing difficulty and deprivation, this study found that neuroticism was associated with tinnitus and bothersome tinnitus.
The effects were stronger for bothersome tinnitus.
As neuroticism is a stable personality trait [28] it is unlikely to develop as a result of experiencing tinnitus.
It is more likely that those who are more neurotic and experience tinnitus are more likely to perceive the tinnitus as being bothersome, compared to those who are less neurotic.
These findings do suggest that tinnitus patients with higher levels of neuroticism may be more prone to distress and consequently more severe tinnitus.
Loneliness was the most significant association with bothersome tinnitus which supports previous research showing that tinnitus sufferers are likely to experience loneliness [29], possibly as a result of withdrawing from social interactions [9] and thereby increasing feelings of isolation and loneliness.
Tyler and Baker [30] found that over a third (36.1%) of tinnitus patients reported emotional problems such as despair, frustration and depression, while nearly a fifth (16.6%) reported insecurity, fear and worry.
Furthermore, clinical studies have also shown higher rates of anxiety and depression among tinnitus patients compared with those without tinnitus [31], which supports the finding that 'mood swings', 'worrier/anxious' and 'miserableness' were significant associations of bothersome tinnitus in the current study.
The present study further expands on our earlier work [5] by providing a more detailed breakdown of tinnitus prevalence according to age and sex.
The prevalence of 16% overall, is slightly higher than the National Study of Hearing [2] which found that the prevalence of tinnitus was around 13%, (for adults aged 40 to 70years) and no significant differences between males and females were noted.
Other epidemiological studies, the Blue Mountains Hearing Study [32] and the Beaver Dam study [4], reported prevalence rates of tinnitus of 30% and 10.6% respectively, although the former included past tinnitus as well as current tinnitus.
The Beaver Dam study also found that tinnitus was higher for males than for females (11.9% vs 9.4%), and noted increasing trends in prevalence by age group for men, women and both sexes, consistent with the pattern found in the present study.
In the present study, 3.8% of current tinnitus sufferers reported bothersome tinnitus.
Even though males were more likely to report tinnitus, females were more likely to be bothered by their tinnitus.
This accords with previous research showing that female sex is a risk factor for bothersome tinnitus [33,34].
The under-reporting of bothersome tinnitus in men may be partly explained by a tendency for men to deny or play down the severity of the symptoms of ill health [35].
The primary limitation of these analyses was that the UK Biobank test protocol did not include a validated tinnitus questionnaire.
More detailed data on lateralization, pitch, and loudness of the tinnitus may be informative as previous research suggested that the level of intrusiveness is moderated by the specific quality or type of tinnitus (e.g. fluctuating versus non-fluctuating) [43].
Furthermore, no information was available on the duration that the participant experienced tinnitus.
It is possible that long-term tinnitus sufferers may develop successful coping strategies and are better able to manage their condition, whereas those who have had tinnitus for a relatively short duration may not have learned coping strategies and thus are more bothered by tinnitus [43].
Although such data would be desirable for better characterisation of tinnitus, a full tinnitus questionnaire would not have been practical to administer within the crowded UK Biobank assessment protocol.
Conclusion
Analysing data from a large population of UK adults, we have shown that the prevalence of tinnitus increases with age, and that a high proportion of sufferers report tinnitus as being moderately or severely bothersome, although age was not related to the perceived severity of tinnitus.
Women were especially likely to report bothersome tinnitus.
This study confirms previous research suggesting that hearing loss is a strong predictor of tinnitus, but we have also shown that personality factors identified as neuroticism are associated with the experience of tinnitus, particularly in those reporting that their tinnitus is severe.
Although causality cannot be determined, the study supports an interpretation that otherwise stable personality factors and mood influence the ways in which tinnitus is experienced and managed.
From a clinical management point of view, clinicians would benefit from an understanding that tinnitus severity is a very complex interaction involving auditory, psychological and emotional networks.
Long-term psychological approaches may be effective in helping tinnitus sufferers.
Conflict of interest
We wish to confirm that there are no known conflicts of interest associated with this publication and there has been no significant financial support for this work that could have influenced its outcome.
We confirm that the manuscript has been read and approved by all named authors and that there are no other persons who satisfied the criteria for authorship but are not listed.
We further confirm that the order of authors listed in the manuscript has been approved by all of us.
We confirm that we have given due consideration to the protection of intellectual property associated with this work and that there are no impediments to publication, including the timing of publication, with respect to intellectual property.
In so doing we confirm that we have followed the regulations of our institutions concerning intellectual property.
We understand that the corresponding author is the sole contact for the Editorial process (including Editorial Manager and direct communications with the office).
She is responsible for communicating with the other authors about progress, submissions of revisions and final approval of proofs.
We confirm that we have provided a current, correct email address which is accessible by the corresponding author and which has been configured to accept email from abby.mccormack@nottingham.ac.uk.
Acknowledgement
This paper presents independent research funded in part by the National Institute for Health Research (NIHR).
The views expressed are those of the authors and not necessarily those of the NHS, the NIHR or the Department of Health.

Addition of low reactive clay into metakaolin-based geopolymer formulation: Synthesis, existence domains and properties
Geopolymer samples were prepared by dissolving potassium hydroxide pellets, supplied by VWR (Belgium) (85.2% pure), into a commercial potassium silicate solution (Si/K = 1.7) supplied by ChemLab (Belgium). Then, Me clay and/or MK were added. Samples were placed in a closed sealable polystyrene mold at room temperature (25 degC) according to the procedure previously established by Autef et al. [12].1. A positive active material for a lithium secondary battery, the positive active material comprising:
a core part and a shell part that both comprise a nickel-based composite oxide represented by Formula (1) below:
Lia[NixCoyMnz]O2(1)
where 0.8≤a≤1.2, 0.05≤x≤0.9, 0.1≤y≤0.8, 0.1≤z≤0.8, and x+y+z=1, wherein the content of nickel in the core part is larger than the content of nickel in the shell part,
wherein the content of nickel in the core part is in a range of 60 to 80 mole% with respect to the sum of Ni, Co, and Mn, and the content of nickel in the shell part is in a range of 33 to 55 mole% with respect to the sum of Ni, Co, and Mn, and
the core part comprises substantially needle-shape particles wherein a diameter of the core part is in a range of 1 to 10 µm, andcharacterized in thata diameter of the shell part is in a range of 5 to 20 µm.2. The positive active material according to claim 1, wherein the core part has a porous structure with open pores.3. The positive active material according to claim 2, wherein the shell part permeates into open pores of the core part.4. The positive active material according to any one of claims 1 to 3, wherein the core part has a curved surface structure.5. The positive active material according to any one of claims 1 to 4, wherein an amount of the core part is in a range of 40 to 90 parts by weight, based on 100 parts by weight of the positive active material, and an amount of the shell part is in a range of 10 to 60 parts by weight, based on 100 parts by weight of the positive active material.6. The positive active material according to any one of claims 1 to 5, wherein the core part comprises LiNi0.6Co0.2Mn0.2O2, and the shell part comprises LiNi1/3Co1/3Mn1/3O2.7. The positive active material according to any one of claims 1 to 6, wherein a content of nickel in the shell part is in a range of 33 to 49 mole% with respect to the sum of Ni, Co, and Mn.8. The positive active material according to any one of claims 1 to 7, wherein the core part and the shell part are substantially spherical.9. A method of preparing a positive active material for a lithium secondary battery, the method comprising:
a first process for mixing a first precursor solution comprising a nickel salt, a cobalt salt, and a manganese salt, at a molar ratio of x:y:z satisfying 0.05≤x≤0.9, 0.1≤y≤0.8, 0.1≤z≤0.8, and a first base, to prepare a first mixture and inducing a reaction in the first mixture to obtain a precipitate;
a second process for adding to the precipitate a second precursor solution comprising a nickel salt, a cobalt salt, and a manganese salt, at a molar ratio of x':y':z' satisfying 0.05≤x'≤0.9, 0.1≤y'≤0.8, 0.1≤z'≤0.8, and x'+y'+z'=1, and a second base, to obtain a second mixture and inducing a reaction in the second mixture to obtain a nickel-based composite hydroxide; and
mixing the composite metal hydroxide with a lithium salt and heat treating the mixed composite metal hydroxide to prepare the positive active material according to any one of claims 1 to 8,
wherein the content of nickel in the first precursor solution is adjusted to be larger than the content of the second precursor solution,
wherein the reaction time of the second mixture in the second process is adjusted to be longer than the reaction time of the first mixture in the first process, and
wherein a pH of the first mixture in the first process is adjusted to be in a range of 10 to 11, and a pH of the second mixture in the second process is adjusted to be in a range of 11.5 to 12.0.10. The method according to claim 9, wherein a reaction time of the first mixture in the first process is in a range of 5 to 7 hours, and a reaction time of the second mixture in the second process is in a range of 8 to 10 hours.11. The method according to any one of claims 9 to 10, wherein an amount of the nickel salt in the first process is in a range of 1 to 1.2 moles based on 1 mole of the cobalt salt.12. A lithium secondary battery (30) comprising a positive electrode (23), a negative electrode (22), a separator (24) and an electrolyte, wherein the positive electrode (23) comprises the positive active material according to any one of claims 1 to 8.13. A positive active material precursor for the positive active material according to any one of claims 1 to 8, the positive active material precursor comprising:
a nickel-based composite hydroxide represented by Formula (2) below
NixCoyMnz(OH)2(2)   wherein 0.05≤x≤0.9, 0.1≤y≤0.8, 0.1≤z≤0.8, and x+y+z=1,
preferably Ni0.6Co0.2Mn0.2(OH)2or Ni1/3Co1/3Mn1/3(OH)2.Understanding the stepwise capacity increase of high energy low-Co Li-rich cathode materials for lithium ion batteries
All the chemicals were purchased from Aldrich Sigma. The Li[CoxLi1/3-x/3Mn2/3-2x/3]O2 (x = 0, 0.087, 0.1, 0.118) composites were synthesized in two steps. First, stoichiometric amounts of Mn(NO3)2*4H2O and Co(NO3)2*6H2O were dissolved in distilled water and then slowly added dropwise to an equal volume of 0.2 M sodium carbonate solution under vigorous stirring. After 20 h aging at room temperature, the metal ions were fully co-precipitated as carbonates. In the second step, the collected carbonates were pre-treated at 500 degC for 5 h in air and then calcined with a stoichiometric amount of LiOH*H2O at 900 degC for another 12 h in air to produce the final cathode materials. A 3 wt% excess of LiOH*H2O was applied to the calcination process to compensate for Li volatilization at elevated temperature.Energetic electron observations of Rhea's magnetospheric interaction

Highlights
► We give a detailed review of an energetic electron dataset in a lunar-type interaction.
► A local transport process occurs in the wake of Rhea.
► The transport is associated to a flute type of instability driven by pressure gradients.
► We show that surface charging is an important element of the interaction.
Abstract
Saturn's moon Rhea is thought to be a simple plasma absorber, however, energetic particle observations in its vicinity show a variety of unexpected and complex interaction features that do not conform with our current understanding about plasma absorbing interactions.
Energetic electron data are especially interesting, as they contain a series of broad and narrow flux depletions on either side of the moon's wake.
The association of these dropouts with absorption by dust and boulders orbiting within Rhea's Hill sphere was suggested but subsequently not confirmed, so in this study we review data from all four Cassini flybys of Rhea to date seeking evidence for alternative processes operating within the moon's interaction region.
We focus on energetic electron observations, which we put in context with magnetometer, cold plasma density and energetic ion data.
All flybys have unique features, but here we only focus on several structures that are consistently observed.
The most interesting common feature is that of narrow dropouts in energetic electron fluxes, visible near the wake flanks.
These are typically seen together with narrow flux enhancements inside the wake.
A phase-space-density analysis for these structures from the first Rhea flyby (R1) shows that Liouville's theorem holds, suggesting that they may be forming due to rapid transport of energetic electrons from the magnetosphere to the wake, through narrow channels.
A series of possibilities are considered to explain this transport process.
We examined whether complex energetic electron drifts in the interaction region of a plasma absorbing moon (modeled through a hybrid simulation code) may allow such a transport.
With the exception of several features (e.g. broadening of the central wake with increasing electron energy), most of the commonly observed interaction signatures in energetic electrons (including the narrow structures) were not reproduced.
Additional dynamical processes, not simulated by the hybrid code, should be considered in order to explain the data.
For the small scale features, the possibility that a flute (interchange) instability acts on the electrons is discussed.
This instability is probably driven by strong gradients in the plasma pressure and the magnetic field magnitude: magnetometer observations show clearly signatures consistent with the (expected) plasma pressure loss due to ion absorption at Rhea.
Another potential driver of the instability could have been gradients in the cold plasma density, which are, however, surprisingly absent from most crossings of Rhea's plasma wake.
The lack of a density depletion in Rhea's wake suggests the presence of a local cold plasma source region.
Hybrid plasma simulations show that this source cannot be the ionized component of Rhea's weak exosphere.
It is probably related to accelerated photoelectrons from the moon's negatively charged surface, indicating that surface charging may play a very important role in shaping Rhea's magnetospheric interaction region.

Introduction
Rhea is Saturn's largest icy moon (radius: 1RRh=764km).
It orbits the planet on an equatorial and circular orbit at a distance of about 8.74Rs from its center (1Rs=60,268km).
Recent studies showed that Rhea is surrounded by a tenuous exosphere composed of oxygen and carbon dioxide (Teolis et al., 2010).
Despite that, the main interaction mode of Rhea with the magnetosphere was shown to be plasma absorption.
Magnetic field perturbations in Rhea's interaction region appear to be guided primarily by the formation of a plasma pressure cavity (wake) downstream of the moon and not from mass or momentum loading from the ionized products of this weak exosphere (Simon et al., 2012; Khurana et al., 2008; Roussos et al., 2008).
The interaction region of any plasma absorbing moon is typically located downstream of the moon with respect to the direction of the bulk plasma flow.
Depending of the upstream sonic Mach number, Ms, the opening angle of the plasma wake can vary along the magnetic field direction (low/high for large/small Ms) (Samir et al., 1983).
For very low Mach numbers (e.g. Saturn's moons) the outer flanks of the plasma cavity tend to become tangential to the surface of the plasma absorbing body (Khurana et al., 2008).
Perpendicular to the magnetic field (or to the flux tube enclosing the wake) the width of the cavity is approximately equal to the moon's diameter.
The extension of the wake downstream of the moon also appears to be associated with the sonic Mach number: for low Mach numbers, the cavity has a smaller extension than the typical, supersonic case of the lunar wake in the solar wind.
Simulations indicate that Rhea's cold plasma wake should refill completely within a distance of about 10RRh downstream (Simon et al., 2012; Roussos et al., 2008).
The strongest electric and magnetic field disturbances exist within the boundaries of the wake.
Pressure gradient (diamagnetic) currents flowing along the wake boundaries or within the wake lead to a compression of the magnetic field, B, behind the moon.
The requirement for the total pressure (magnetic and plasma) to remain constant is also an alternative (and equivalent) explanation for the presence of this B enhancement.
The same diamagnetic currents lead to magnetic field perturbations on the two sides of the flux tube corridor enclosing the plasma wake.
Khurana et al. (2008) calls these features "expansion fans", which at Rhea extend up to several moon radii on either side of the wake.
The magnetic field magnitude (∣B∣) drops within these regions (see also Fig. 5 of Roussos et al. (2008)).
While magnetometer data comply with this standard, lunar-type interaction concept, Cassini's energetic charged particle observations of the near-Rhea environment by MIMI/LEMMS (Krimigis et al., 2004) are rich in unusual interaction features (Jones et al., 2008).
The first close downstream flyby on November 26, 2005 (termed R1), revealed broad energetic electron (20-100keV) flux dropouts extending almost 7-8 Rhea radii (RRh) on each side of Rhea's wake.
Hints for a broad decrease in fluxes were also present in a more distant 2007 downstream, non-targetted flyby of Rhea that took place on August 30, 2007 (informally termed R1.5).
In addition to the broad regions of energetic electron flux dropouts, Cassini's MIMI/LEMMS detector recorded smaller scale dropouts each of which was few tens of km across.
These were detected just outside the wake boundaries (wake flanks), within 2RRh from the center of the moon, in both the saturnward and the antisaturnward sectors of the interaction region.
As the scale size of the broad depletion region was, for both flybys, comparable to the size of Rhea's Hill sphere (the region where the moon's gravity dominates that of Saturn), it was proposed that Rhea was surrounded by a disk containing large grains.
These grains served as an energetic electron sink.
In view of this interpretation it was also suggested that part of these trapped grain populations were concentrated into one or more narrow ringlets around Rhea, which could have been the source of the smaller scale dropouts seen by MIMI/LEMMS.
Other evidence, such as the broadening of these small dips with increasing electron energy (gyroradius effects) were not contrary to this possibility.
Subsequent analysis and optical observations, however, ruled out these scenarios (Tiscareno et al., 2010).
Features such as the similar spatial scales of the broad depletion regions with the size of Rhea's Hill sphere and the simultaneous presence of smaller scale dropouts near the moon appear to be coincidental.
Since it appears unlikely that the explanation of the observations involves any absorbing medium around Rhea, answers to the problem should invoke plasma or magnetospheric processes.
Since the first two flybys in 2005 and 2007, two more have taken place on March 2, 2010 (R2) and January 11, 2011 (R3).
The goal of this work is to primarily review and describe MIMI/LEMMS energetic electron observations from all Rhea close flybys, to put them in context with observations and findings from other Cassini instruments and to focus on common interaction signatures that can be identified among the different flybys.
We will then propose interpretations that will help us form a more clear picture about the structure and the dynamics of the moon's interaction region and about the nature of the processes that drive some of the unusual energetic particle observations.
Unique flyby features will not be explicitly analyzed in this work, but they will be identified mostly for reference in future studies.
Flyby information
Table 1 contains basic information about the first four close Cassini flybys of Rhea to date.
The first two (R1 and R1.5) were downstream of Rhea, with R1.5 at a relatively large distance compared to the other three.
R2 was above the moon's north pole, while R3 was above its south pole.
A sketch of the equatorial projections of Cassini's trajectories in Rhea's Interaction System (RHIS) are shown in Fig. 1.
The RHIS is centered at Rhea and has the positive x-axis along the plasma nominal corotation direction, the positive y-axis pointing towards Saturn, while +z completes the right-hand system, pointing north and approximately antiparallel to the direction of the background B.
The vertical offset from the xy-plane, given in Fig. 1, was less than 1.5RRh in all cases.
The average offsets are given in Table 1.
Here, the geometrical/corotational wake is defined on the basis of where energetic electron absorption may be observed.
Because of the rapid bounce motion of energetic electrons, the energetic electron wake is weakly limited by Cassini's z-position and can extend to very large distances, north and south of the moon.
Therefore the condition for absorption is satisfied when Cassini is downstream of Rhea's volume or magnetically connected to the moon, namely when ∣y∣⩽1RRh(x⩾0) or x2+y2⩽1RRh (x<0).
The region where x2+y2⩽1 corresponds to Rhea's flux-tube, a term that may sometimes be used in this study instead of "wake".
The aforementioned definition is valid for energetic electrons that drift towards +x.
Electrons with energies above the keplerian resonant energy drift towards -x and therefore the conditions given in the previous paragraph are reversed.
The keplerian resonant energy (Erk) is the kinetic energy at which the total drift rate of energetic electrons cancels the keplerian drift rate of a moon.
In various plots of this study we will indicate the wake boundaries only for electrons with E<Erk, since most of the available data are in this energy regime (Section 3).
When Erk estimations are given based on the assumption of a dipole magnetosphere (Thomsen and Van Allen, 1980), values should be taken as upper limits.
At Rhea's distance the actual magnetospheric field gradient is stronger than that of a dipole field and the actual Erk should be at lower energies.
The difference between the dipolar and the real Erk can be significant especially for particles that mirror near the equatorial plane (pitch angle around 90°).
We also stress that the energetic electron wake has a significantly different structure to that of a cold plasma wake.
Particle absorption at low energies is expected to be more pronounced for x>1RRh.
This could be important for flybys R2 and R3.
Refilling processes are also different in these two energy regimes, with high-energy depletions persisting longer (Khurana et al., 2008; Roussos et al., 2008).
These long-lived, high energy depletions can be observed many degrees in longitude ahead of the moon along its orbit, and are usually called "microsignatures" (Paranicas and Cheng, 1997; Paranicas et al., 2005; Roussos et al., 2007).
Instrumentation and data processing
Most of the data presented here are from Cassini's MIMI/LEMMS sensor.
MIMI/LEMMS is an energetic particle detector.
It has two oppositely pointing telescopes, the Low Energy and the High Energy Telescope (LET and HET respectively) (Krimigis et al., 2004).
Since for all Rhea flybys LEMMS electron fluxes have considerable signal to noise ratio only for energies below about 600keV, we analyzed data only from electron rate channels C0-C7 (LET) and E0-E2 (HET).
For similar reasons, ion channels considered for this study were A0-A4 (<500keV), all belonging to the LET.
Data from high energy resolution Pulse Height Analysis (PHA) channels are also available for the selected energy ranges.
The time resolution of the PHA channels is typically 1.5 times lower than that of the rate channels.
Based on LEMMS intercalibration with other sensors of MIMI (INCA, CHEMS), it is reasonable to assume that all LEMMS ion channels respond to protons (Dialynas et al., 2009).
The latest passband and geometry factor information for the electron and ion channels can be found in Armstrong et al. (2009) and Krupp et al. (2009).
Certain channels of LEMMS can accumulate counts with sixteen times higher temporal resolution than the typical rate channel time resolution (∼5.7s).
These are called priority channels.
The available priority channels for each flyby are given in Table 1.
We will distinguish rate and priority channels using the suffix "_PRIO" for the latter (e.g. C1_PRIO).
Priority channels are especially useful for the identification of short duration structures, such as the small scale flux dropouts near Rhea (Jones et al., 2008).
The LEMMS signal is corrected for the instrumental background as described in previous papers (Roussos et al., 2011; Kollmann et al., 2011).
Particle pitch angles are calculated using information from Cassini's magnetometer (MAG) (Dougherty et al., 2004).
For all flybys, electron density is evaluated through the detection of the upper hybrid (fUH) emission in radio spectrograms of the RPWS instrument (Persoon et al., 2005; Gurnett et al., 2004).
For flyby R1, RPWS Langmuir probe (LP) ion densities are also available, while CAPS/ELS electron densities will be shown for flyby R2 (Young et al., 2004).
Observations
The following subsections contain information about observations during the four closest Rhea flybys.
Focus will be given to MIMI/LEMMS energetic electron observations, which contain most of the interaction features.
R1: November 26, 2005
Jones et al. (2008) summarizes several observation aspects of this flyby.
Fig. 2 includes some of these observations as well as additional information not provided in the latter study.
The top panel shows an electron energy spectrogram based on the LEMMS PHA electron channels, between ∼25 and ∼250keV.
The second and third panels show "energy-cuts" through this electron spectrogram at the energies of nine rate channels of LEMMS.
The fourth and fifth panels show the plasma density and the total magnetic field measured by MAG, respectively.
We note that MAG data for all Rhea flybys (except R1.5) are extensively analyzed in a series of papers to which we refer the reader for more details (Simon et al., 2012; Khurana et al., 2008; Roussos et al., 2008).
We will now discuss some of the main features visible in these five panels.
Corotational wake
The signature of the wake crossing is a large dropout in particle fluxes of between one and four orders of magnitude, depending on electron energy.
The timings of the expected boundary crossings (dashed lines) agree well with predictions.
Fluxes do not reach background levels immediately inside these boundaries, but drop gradually over a distance of about 0.3RRh within them.
Fluxes are very low at the wake center, although some channels (C0-C4) record foreground with a weak signal to noise ratio of about 5-7.
The wake is also visible in the higher energy channels C6 and E2 (third panel).
Its edge-to-edge extension along the y-axis (RHIS system) is about 3RRh wider than Rhea's diameter.
The profile of channel C7 (510-832keV) features many flux dropouts.
None of these is centered on the location of the expected wake crossing.
Note that most C7 electrons have energies above the keplerian resonant energy at Rhea (Erk).
They arrive from the nominal wakeside direction and have not yet encountered the solid body of the moon at the locations of the R1 flyby.
Erk is ∼550keV.
An unexpected result is the lack of a reduction of the RPWS electron density (fourth panel from top).
We note that the electron density is dominated by electrons below about ∼1keV, which have different drift properties from energetic electrons and also have different source and loss processes.
We also overplot the ion density profile from the RPWS Langmuir Probe (LP), which appears to show a density dropout in the wake.
Furthermore, MAG data show an enhancement in the magnetic field magnitude, consistent with a loss of perpendicular plasma pressure in the wake.
This pressure loss is typically associated to a plasma density dropout, not visible in all datasets.
Since it is probably unreasonable to interpret MAG and RPWS fUH/LP observations as evidence of a large charge density in Rhea's wake (deviation from charge neutrality), one of the two density measurements is probably problematic.
The subject is discussed in Section 5.4.
Broad depletions and structures
As "broad depletions" we refer to the electron flux dropouts that do not appear to originate from absorption at the surface of the moon and have an extension larger than Rhea's diameter.
The distinction is not clear in all cases, as it depends also on how the data are interpreted.
For instance, the dropout seen in channel E2 of LEMMS appears too broad to be associated with absorption at Rhea's surface.
However, as Roussos et al. (2007) found, this could be the result of a diffusive broadening of the actual electron depletion formed at Rhea's surface, which is observed to occur much faster at higher energies than at low energies.
Alternative explanation for such cases is given in Section 5.2.
The most obvious broad depletions exist below about 100keV (channels C0-C4, e.g.
second panel of Fig. 2).
Their width is about ∼7RRh on each side (Jones et al., 2008).
Excluding channel E2, the remaining electron channels have also some rather broad electron flux dropouts that are about 2RRh wide in channel C7 (22:28-22:34) and channel C6 (22:44-22:47).
The latter dropout seems to be associated with smaller scale dropouts at lower energy channels (see Section 4.1.3), a sharp increase of ∣B∣ centered around 22:45 and a shallow dropout in electron density (marked between dotted-dashed lines).
This combination of signatures is typical of depleted flux tubes participating in the centrifugal interchange process (André et al., 2007).
Evidence that this may be the case is also visible in the CAPS/ELS electron dataset, where an enhancement of 1-5keV energetic electrons is seen (Fig. 2, Panel D of Jones et al. (2008)).
We also note a relatively sharp jump in electron densities at 22:25 (dotted line) and a density decrease on either side of the wake (from about 6cm-3 to 4cm-3), occurring slowly over a distance of 5RRh.
The significance of any of the features discussed here would also depend on whether they appear in the other Rhea flybys, otherwise we cannot rule out that they are fluctuations of the background magnetospheric parameters, unconnected to the moon.
Small scale depletions and structures
The term "small scale depletions" refers to the flux dropouts that have a width that is significantly smaller that Rhea's diameter and do not seem to be caused by electron absorption on the moon's surface.
These features are usually visible closer to the wake.
Fig. 3 shows data in a time interval spanning a few minutes before and after the wake crossing.
The format is similar to Fig. 2, but the second and third panels show data from the priority channels C1_PRIO and C5_PRIO, respectively.
Small scale depletions are well visible in the PHA energy-time spectrogram (top) and the data of C1_PRIO.
We use red1
For interpretation of color in Figs.
1-12, the reader is referred to the web version of this article.
1 arrows to mark some of them.
At the energy of C5_PRIO, small scale depletions are visible but less resolved than those of C1_PRIO.
This is partly attributed to the lower signal to noise ratio of that channel and partly to the fact that groups of small scale depletions at lower energies appear to coincide with a single, broader depletion above the energy of C5_PRIO.
Characteristic examples of merged low energy depletions at high energies are the dropouts in C5_PRIO (or C5) after 22:40:30 (last red arrow in the middle panel of Fig. 3).
The same feature is also visible in channels C6, C7 and E3 in Fig. 2.
The broadening and merging of small scale depletions appears to occur only at energies above 130keV, as inspection of the energy-time spectrogram of Fig. 3 shows.
As mentioned in Section 4.1.1, fluxes do not drop sharply to background inside the wake boundaries.
An interesting observation is that the flux within the wake exists in the form of short-lived spikes (Fig. 3, blue arrows).
The spikes are well resolved in C1_PRIO and C5_PRIO, although they are less regular in the latter.
An interesting aspect is that peak fluxes in the spikes are close to the flux lost in the small scale depletions, indicating a local transport process.
A more careful analysis is presented in Section 5.1.
R1.5: August 30, 2007
Flyby R1.5 occurred while Cassini was in the outbound leg of its orbit.
Few aspects of it are discussed in Jones et al. (2008), while Wilson et al. (2010) provide thermal ion moments for the encounter.
Here we present LEMMS, RPWS and MAG data in the same manner as for R1.
One interesting element of R1.5 is that it occurred downstream of Rhea, with LEMMS and CAPS having similar orientations as in R1.
In essence, the primary difference between R1 and R1.5 is the flyby distance.
Corotational wake
Fig. 4 shows the overview of LEMMS, MAG and RPWS observations, as in Fig. 2.
The corotational wake is again evident as a region with a large dropout in electron fluxes.
Flux levels in the wake are more recovered compared to the R1 case, since the distance of this flyby is greater and some refilling occurred.
The center of the wake has a small antiplanetward offset from the expected location.
Although this center cannot be exactly defined due to the variability of fluxes within it, we can detect a small difference in the offset between low energy channels (C0-C3: ∼0.5RRh) and C4 (∼1RRh).
This difference could exist because the C4 electrons drift more slowly than the C0-C3 ones and therefore are exposed to the effect that causes this outward motion for more time.
Quantification of this time-of-flight effect gives us an outward radial velocity of about 5kms-1, consistent in direction and order of magnitude with the local radial velocity measured by CAPS (Wilson et al., 2010).
Our inferred radial velocity value is a factor of two lower than the one given in Wilson et al. (2010), but this could be due to a series of factors affecting mainly our technique, such as the uncertainties in determining the wake center or the assumption of constant radial velocity for all the time-of-flight interval.
As in R1, the wake becomes broader with increasing electron energy.
Channels monitoring energies above about 500keV are featureless.
The cold electron density from RPWS does not show a wake signature, although densities are much more variable within the wake boundaries compared to R1.
The enhancement in ∣B∣ indicates that some considerable pressure loss still exists 8RRh downstream of the moon.
Cassini also crossed a depleted flux tube at about 01:36 (dotted-dashed lines).
At that location (shaded box, fourth panel), the fUH emission is barely above noise levels and not reliable enough to extract the electron densities.
Manual inspection of the RPWS spectrogram shows a very weak emission line which corresponds to a density 50% below the surrounding value.
Furthermore, the magnitude of the associated change in ∣B∣ is similar to the one observed in Rhea's wake.
This makes the lack of a wake signature in electron density even more puzzling.
Broad depletions and structures
Whether broad depletions are present in R1.5 is unclear.
Channels C0 and C1 show shallow dropouts in the time segment before the wake crossing and an even slower recovery afterwards.
Their spatial extent is similar to those seen in R1.
These dropouts (some of which are marked in the second panel of Fig. 4) are more apparent when fluxes are plotted in linear scale (see Supporting Online Material of Jones et al. (2008)), however it is difficult to understand whether they are associated with the interaction region of Rhea or are part of magnetospheric flux variations observed before and after the plotted interval (not shown here).
The dropout signatures in channels C5, C6 and E2 may be interpreted as broad depletions, or may simply be signatures of a diffuse wake, as explained in Section 4.1.1.
The electron density in RPWS also shows a broad and gradual dropout centered around the expected time of the wake crossing.
The slow dropout and the presence of a relatively sharp jump in densities at about 01:00 (dotted line) are qualitatively similar to features seen in R1.
The difference is that in R1.5 they are more pronounced in the saturnward side of the interaction region.
Small scale depletions and structures
LEMMS data show many small scale depletions, as it is already evident in the top two panels of Fig. 4, but they are less regular compared to those in R1.
Broadening of these features with increasing energy is present (more visible in line plots), but less obvious than in R1.
A zoom in the region where the dropouts are visible is shown in Fig. 5.
The format is similar to that of Fig. 3.
Small scale dropouts appear on the side of the wake (red arrows).
Fluxes within the wake are turbulent, with alternating dropouts and spikes, as in R1.
The flux enhancements are also apparent in the PHA spectrogram (black arrows) and in the rates of channels C0-C4 (Fig. 4).
Temporal changes in ∣B∣ and the RPWS electron density do not coincide consistently with small scale features in LEMMS.
R2: March 2, 2010
The fundamental difference of R2 (and R3) from R1 and R1.5 is that Cassini crossed field lines connected to the moon's surface.
Cassini also crossed regions slightly upstream of the corotational wake.
Furthermore, the pointing of LEMMS was also away from the field aligned direction (Table 1).
An overview of R2 observations is shown in Fig. 7.
CAPS/ELS electron densities are available for this flyby.
Corotational wake/flux tube crossing
The wake/flux tube signature is visible at the predicted location, for energies up to 100keV (top three panels).
The locations where Cassini's trajectory intersects the flux tube of Rhea also allows the observation of the absorption of electrons that are both above or below the keplerian resonant energy (e.g. channel C7).
A common feature with the previous flybys is that the wake seems to broaden with increasing energy.
This broadening is not symmetric about the wake center, but its more visible in the saturnward side.
It also appears that the broadening of the wake stems from the merging of small scale depletions, as we also see in R1.
The electron density measurements show again no density gradient as Cassini enters or exits the wake.
At the wake center, a sharp density increase is also visible.
The increase is sustained for a distance of about 2RRh along the y-direction, outside of the wake.
The density profile in this case is confirmed by comparing RPWS/fUH and CAPS/ELS densities.
CAPS/ELS had a full pitch angle coverage for this flyby, allowing for a more reliable determination of densities compared to R1 and R1.5 (Santolı´k et al., 2011).
LP densities are also available, but are not shown here because of their very low signal to noise ratio.
LP data also show that the spacecraft potential is few Volts negative.
This explains why CAPS/ELS densities have to be multiplied by a factor of 1.5 to match the levels of RPWS.
The enhancement in ∣B∣ corresponds to a partial plasma pressure loss, since only a fraction of the pressure-carrying heavy ions have probably been absorbed above Rhea's poles (mainly ions coming from the south).
Diamagnetic currents downstream of Rhea's volume are also be responsible for part of this change in the magnetic field (Simon et al., 2012).
At the time when the sharp electron density gradient occurs in the wake center, the magnetic field magnitude shows a spike, probably associated with pressure gradient driven currents at this boundary (dotted-dashed line).
Given the lack of ion moments and electron temperatures, it is unclear which species drive this current.
Furthermore, the boundary marked by the dotted-dashed line corresponds to the crossing of Cassini from the shadow of Rhea to the sunlit hemisphere of the moon.
If not coincidental, this observation hints that the lack of an electron density dropout in Rhea's wake is due to the presence of a cold plasma source, the strength of which is controlled by sunlight.
Such a source may be due to ionized particles from Rhea's weak exosphere (Teolis et al., 2010) or surface photoelectrons accelerated along the field lines from negative electrostatic potentials on Rhea's surface (Roussos et al., 2010).
We discuss these possibilities in Section 5.4.
Electron density streaks associated to surface charging have also been observed in moon-solar wind interaction simulations by Nakagawa and Kimura (2011).
Broad depletions and structures
As discussed in the previous subsection, there is no evidence of a symmetric broad depletion similar to the one observed in R1.
A broad flux dropout is seen only after Cassini exits the wake.
Apart from the lack of symmetry, the flux dropout has an additional difference with R1: the dropout becomes more intense with increasing energy.
For energies of channel C4 and above, the dropout signal extends to distances between 7 and 8RRh and at locations where x<-1RRh.
It is questionable how electrons at this location are influenced by a plasma absorbing moon, the interaction region of which is expected to extend only downstream.
Small scale depletions and structures
Fig. 7 is a zoom in the time interval around closest approach of R2, and also shows LEMMS priority channel data.
Notice that priority channels for R2 are different to those of R1 and R1.5.
Three small scale dropouts are seen in total, with the third one (after 17:42:20) becoming broader and deeper at higher energies (red arrows).
Flux foreground is visible within the wake, although the signal is not as pronounced as in the case of R1.
The clearest enhancement, again in the form of a spike, is visible in channel E2 (Fig. 6, middle panel, blue arrow).
Since around closest approach, LEMMS's C-channels measure electrons coming from the south (pitch angle greater than 90°), it is surprising that foreground is measured above Rhea's north pole.
One explanation may involve a fast transport process, as we infer for R1 (Section 4.1.3).
It is also possible that energetic electrons with the appropriate bounce phase are reflected by the enhanced magnetic field at the wake, before reaching the surface: the mirror magnetic field strength for particles with pitch angle of 110° is about 23.0-23.5nT.
This is close to the peak magnitude observed with MAG, meaning that a mirror point of the electron bounce motion can occur above Rhea's surface.
The enhancement of ∣B∣ may be even stronger closer to the surface compared to the 100km flyby altitude.
In that case, however, it is uncertain why fluxes within Rhea's wake appear as spikes and not as a continuous population.
As mentioned earlier, a clear spiky enhancement is seen in channel E2.
Since E2 belongs to the HET of LEMMS, it observes electrons with pitch angles opposite to those of the C-channels.
In that case, E2 electrons arrive from the north.
As no similar enhancement is seen the C-channels with overlapping energy ranges, we can also infer the presence of unidirectional energetic electron distributions.
Similar observations at Ganymede have been connected to transport by pitch angle diffusion (Williams and Mauk, 1997), a scenario that may also comply with observations by Santolı´k et al. (2011).
R3: January 11, 2011
R3 had its closest approach 80km above the south pole of the moon.
An overview of observations is shown in Fig. 8.
Due to instrumental problems (significant negative Cassini charging affecting CAPS/ELS fluxes, and commanding error for RPWS/LP), density measurements for this flyby are only shown from RPWS/fUH.
Corotational wake/flux tube crossing
A first look at the panels of Fig. 8 shows the interaction features to be quite symmetric around closest approach.
The wake/flux tube boundary crossings occur at the predicted times for energies up to about 100keV.
The wake broadens at higher energies, as seen in all other flybys.
Fluxes are above the instrumental background in certain LEMMS channels within the wake boundaries (blue arrows).
This is also discussed in Section 4.4.3.
R3 is the only flyby so far where electron densities show dropout signatures across both wake boundaries.
The density profile is, nevertheless, peculiar, as there is a large density enhancement just before wake ingress.
It is uncertain if this feature is connected to Rhea's interaction, as significant variability in densities is observed before and after the wake crossing.
If there is a connection, it again may be relevant to the findings of Nakagawa and Kimura (2011) (electron density streaks driven by surface charging).
One important difference with R2 is that the electron density drops after Cassini moves from the dark to the sunlit hemisphere of Rhea (dotted-dashed line).
The density shows a sharp gradient at this point and reaches a minimum a few minutes later, before starting to recover again.
The magnetic field magnitude (bottom panel) ∣B∣ shows the expected, persistent enhancement within the wake boundaries (Simon et al., 2012).
Outside those boundaries, MAG data show many short duration spikes, comparable in amplitude with the increased magnitude in the wake.
The similarity of these spikes to magnetic field signatures of depleted flux-tubes (André et al., 2007), may indicate that these features arise from injection events that coincidentally occurred during the flyby period (see also Simon et al., 2011).
A peak in the magnetic field is seen again close to the day-night boundary crossing (dotted-dashed line), as in R2 (Section 4.3.1), consistent with the presence of a plasma pressure gradient at this location.
Notice also that background ∣B∣ is similar to what is measured at all flybys except R1.
During the latter, ∣B∣ was on average 3-4nT stronger compared to all other flybys.
Broad depletions and structures
R3 shows little evidence of large scale depletions in energetic electrons.
The only persistent depletion observed away from Rhea starts around 04:46 and ends around 05:00.
It is better visible in channels C5-E2, but is very shallow compared to what has been observed in R1 and R2 and it is questionable whether it should be compared with the broad depletions seen during R1 and R2.
This possible broad dropout region also coexists with a considerable number of small scale depletions (red arrows), the merging of which may actually give rise to its existence.
From the other datasets, only the RPWS electron density contains signatures of a broad structure (04:48-05:00).
Again, however, this cannot be conclusively separated from the overall variability observed in that interval (e.g. sharp increase of electron density marked by the dotted line).
Small scale depletions and structures
Inspection of Fig. 8 reveals many small scale dropouts.
Here we do not include a figure with priority channel data, as all small scale structures are well resolved with rate and/or PHA channels.
What is unique for R3 is that small scale dropouts (red arrows) can be detected as far as ∼6RRh and ∼4RRh from the moon, before and after the wake crossing, respectively.
Closer to the wake, small scale dropouts are more symmetric in most channels and resemble what was observed in R1.
Few dropouts coincide with small scale enhancements in ∣B∣.
Fluxes are low but detectable within the wake.
Channel E2 has the most striking signature, with several spikes reaching flux levels similar to the fluxes outside the wake (blue arrows).
The pitch angle of E2 is 80°, meaning that it observes energetic electrons with a southward parallel velocity component (coming from Rhea).
This direction is occulted by Rhea, meaning that electrons should have reached at this location by the means of a fast transport process (fast diffusion/scattering) or by reflection at the enhanced ∣B∣ near Rhea's surface.
Again, we favor the fast transport solution, as it is not obvious how flux enhancements would be detected in spikes, if reflection at strong magnetic fields is involved.
Analysis and discussion
From the overview of the four flyby observations given in Section 4, we can understand that Rhea's interaction region has many complex features in the energetic particle, plasma density and the magnetic field datasets.
All flybys have unique features and only few structures are consistently observed.
These consistent features, observed in at least three out of the four flybys, are listed below:•
The width of Rhea's wake tends to increase with increasing electron energy.
•
The broadening appears to occur, at least partly, because small scale depletion regions at the wake flanks are merging at higher energies.
•
All flybys show narrow dropouts in energetic electron fluxes.
These dropouts are near the wake flanks, although in some cases (R3) they can be observed also than 5RRh far from Rhea.
•
All flybys show fluxes above the instrumental background within the wake boundaries.
This may be more easily explained for R1.5, as its flyby distance is large enough to allow for some diffusive fill-in to have occurred.
•
All flybys (including R1.5) show spiky flux enhancements in the wake.
The width of the spikes is comparable to that of the narrow dropouts outside the wake.
•
All flybys show the expected and persistent increase in ∣B∣ within the wake boundaries.
These are driven by the diamagnetic current system that is set up due to the presence of the wake, as demonstrated in several studies (Simon et al., 2012; Roussos et al., 2008).
In none of these flybys, however, do the plasma density measurements show the anticipated dropout at the wake.
As stated in the introduction, we will only focus on the common aspects.
Unique features of the interaction may well be connected to the different flyby geometries, the pointing of LEMMS and/or the state of the magnetosphere at each time.
Thus, we leave the analysis of such features for separate studies and we only briefly discuss some of their aspects in Section 6.
We will start with the observation that narrow flux dropouts and short-lived electron flux enhancements in the wake were seen in all cases.
The fluxes measured in these spikes appear comparable (in orders of magnitude) to the flux lost at various dropouts.
This is best visible at flyby R1 (Fig. 3, Section 4.1.3).
One possibility is that there is rapid transport between the magnetosphere and the wake.
To verify this claim, we have to perform a Phase-Space-Density analysis.
Phase Space Density (PSD) analysis
During flybys R2 and R3, flux enhancements in Rhea's wake were observed above the moon's north and south poles (Sections 4.3.3 and 4.4.3).
This suggests that transport time scales should be faster or comparable to half of the electron bounce period.
For a dipolar configuration, energetic electrons at Rhea's L-shell and with pitch angle pointing as in R2 and R3, this period is between about 8s (20keV) and 4s (300keV).
This simple calculation can give us an idea about the transport time scales of the process that we investigate.
If the process is so rapid, we can assume that no additional sources or losses act on the electrons, unless they get absorbed by Rhea during this transport.
In that case, Liouville's theorem should apply (PSD is constant along the trajectory).
Flyby R1 is probably the best one where Liouville's theorem can be tested.
The wake in R1.5 has contributions from diffusive fill-in.
For the R2 and R3 flux tube crossings, transport cannot be lossless since many electrons will get absorbed on Rhea's surface.
The same calculations can be performed with the use of an isotropic invariant, Λ, (instead of the first and second adiabatic invariants), as given in Harel et al. (1981) and Rymer et al. (2007).
Λ is insensitive to pitch angle scattering (Santolı´k et al., 2011), but gives the same results as the conservation of μ because of the small changes in ∣B∣ involved in our analysis.
Additional details about the extraction of PSDs are given in Appendix A, while sample results shown in Fig. 9.
Overall, comparisons show that average PSD losses (dotted line) and gains (dotted-dashed line) differ by less than half an order of magnitude.
This difference is very small, considering the assumptions, simplifications and the limitations (narrow LEMMS's pitch angle pointing) of the analysis.
Furthermore, we cannot be certain that spikes in the wake have a one-to-one correspondence with depletions at the flanks.
The order-of-magnitude agreement can then be considered as an indication that Liouville's theorem holds and hints that a lossless or sourceless transport of these energetic electrons from the magnetosphere to the wake may be occurring.
The question is what can be the driver of such a transport.
We first investigate the possibility of complex energetic electron drifts in Rhea's interaction region.
Lossless transport due to complex electron drifts in Rhea's interaction region
As plasma flows and the magnetic field are disturbed downstream of Rhea, energetic electron trajectories will be affected by these in ways drastically different to trajectories of cold test particles, since the intensity of magnetic (gradient and curvature) drifts increases with energy.
The relative strength of the observed ∣B∣ disturbances at Rhea with respect to the background ∣B∣ is between 6% and 13%, compared to 6-8% at Enceladus, where non-dipolar drifts appear to be important (Krupp et al., 2012).
Complex drifts may help energetic electrons access the wake (enhancements), or lead to the formation of forbidden regions (flux dropouts).
We therefore believe that the role of complex drifts at Rhea should be investigated.
We used the guiding-center approximation and the output of hybrid code simulations of Rhea's magnetospheric interaction (Simon et al., 2012), in order to visualize the effects of complex drifts on the energetic electrons near the moon.
Detailed information on the tracing procedure is given in Appendix B.
Tracing examples are shown in the panels of Fig. 10.
The top left panel shows the xy-map of ∣B∣ from the simulations, after we added a magnetospheric gradient of 0.23nT/RRh along the y-direction.
The other three panels show electron traces at energies indicated on top of each plot.
That is the initial energy of injected electrons.
We also express this energy as a percentage of the keplerian resonant energy, Erk, which for this simulation is at 100keV.
We show this percentage because the complexity of drifts depends also on how close we are to Erk, not only the energy of the injected electrons (Schulz and Eviatar, 1977).
As we approach Erk, the residence time of energetic electrons within Rhea's interaction region increases.
Then these electrons are exposed to the perturbed fields for longer time and distortions in their drift trajectories are amplified.
In the tracings shown in Fig. 10, 350 electrons were injected at the left boundary.
The top right panels show electrons at the 1% level of Erk (1keV).
They practically follow the plasma flow pattern as magnetic drifts are unimportant at low energies.
Electron trajectories "grazing" the wake boundaries expand into the wake due to flow perturbations.
The wake then becomes narrower than the moon's diameter.
Tracings at the 5% level of Erk are shown in the bottom left panel.
The initial electron drift towards -x is a bit slower due to the vmag∇B of Eqs.
(B.1) and (B) becoming stronger.
Firstly, electrons enter the "expansion fans" (drop in ∣B∣) and tend to drift towards -y.
As they continue to move towards +x they experience several competing drifts.
For instance, as they start to exit from the "expansion fans" further downstream, they see an opposite gradient in ∣B∣ from before, which now pushes them towards +y.
In addition, they feel a flow perturbation pushing them in the wake.
The latter has a negative y-component for y>0 and a positive y-component for y<0.
This explains the asymmetry in the trajectories that develops at the wake flanks.
The strong magnetic field within the wake boundaries, prevents most energetic electrons from accessing it and remains empty for most of its part.
At higher energies, magnetic drift terms compete even more with E×B drifts resulting in the very complex trajectories visible in the lower-right panel of Fig. 10.
The sensitivity of traces to numerical simulation noise was important for energies greater than 50keV, so for this study we restrict our analysis below that range.
From the illustrated results, the most interesting features are the following:•
Field gradients tend to exclude particles from the wake at electron energies that magnetic drifts start to become important.
This leads to a wake broadening with increasing energy, similar with what we observe with LEMMS.
On the other hand, LEMMS data indicate that this broadening may result from an apparent merging of small scale depletions seen in low energies.
The relative importance of such a contribution is unclear.
•
As expected, complex drifts lead to the appearance of forbidden regions and channels of lossless transport in the wake.
The structure of these regions does not agree with LEMMS observations.
LEMMS data also show a series of flux enhancements in the wake, while our simulations show only one channel of transport (in the best case).
The various inconsistencies between our data and the test particle simulations may be due to a series of reasons.
For instance, LEMMS observations during R1 (where we have the clearest indications of lossless transport) were at 170° pitch angle, while our simulations are for 90°, as mentioned in B.
Treatment of the full, 3D problem is more demanding and is considered as part of future studies.
Its worth noting, however, that CAPS/ELS observed small and large scale features identical to those observed with LEMMS at the overlapping energy range of the two instruments (20-30keV), while observing at 90° pitch angle during R1 (Jones et al., 2008).
This suggests that the mismatch between our drift simulations and LEMMS observations is probably not associated to the restriction of our calculations to 90° electrons.
Additional drifts may be connected with temporal fluctuations in the magnetic and electric fields at Rhea's distance.
Fluctuations in ∣B∣ are evident in all flyby plots.
The associated polarization drifts are not considered in this simulation, although we expect that they will have a diffusing (smoothing) effect on the spatial distribution of electron fluxes.
If that is the case, then the drifts that lead to the appearance of the small scale structures near Rhea should be associated to dynamics of the local interaction region, not described by the hybrid simulation code which provides us the background field parameters for the tracings.
One such dynamical feature could be a wake-driven instability, a topic investigated in the next two subsections.
Flute instability in Rhea's wake
The formation and evolution of Rhea's wake is in principle a case of "magnetized plasma expansion into the vacuum".
This subject has been investigated both theoretically and in space or laboratory experiments.
Many studies suggest, amongst others, that such an expansion can be the driver of plasma instabilities.
Borisov and Mall (2000) proposed that conditions in the lunar wake may allow the excitation of the flute (interchange) instability.
These conditions require a simultaneous decrease/increase of ∣B∣ and the plasma density.
These two parameters are typically anticorrelated, but around the wake boundaries there can be a positive correlation.
This has also been observed in other environments, as we discuss also below.
The flute instability is a Rayleigh-Taylor type of instability, where transport occurs quickly and in the form of narrow channels with typical scales lower than the characteristic ion gyroradius, ρi.
At Rhea, we indeed observe that small scale structures have spatial scales much smaller than the ion gyroradius (by a factor of 20 at least).
The instability's growth rate is also proportional to the intensity of the magnetic field and the plasma density gradients.
It is also expected to develop more quickly along the field-aligned direction.
Borisov and Mall (2000) calculated a linear growth rate between 0.1 and 0.3s-1 for Earth's Moon wake.
Apart from moon wakes, this instability has also been observed in expanding plasma clouds, such as the magnetotail barium releases from the satellite AMPTE (Active Magnetospheric Particle Tracers Explorers) (Bernhardt et al., 1987).
This expansion did not occur on a smooth spherical shell.
Many irregularities were observed on the shell's surface.
Wavelengths were smaller than 40km, about an order of magnitude smaller than the ion gyroradius.
Among the proposed mechanisms considered in order to explain the AMPTE results was the lower-hybrid drift instability (Bernhardt et al., 1987).
Alternatively, Hassam and Huba (1987) proposed a new form of flute instability in order to explain the data.
The growth rate of this instability increases rapidly if the ratio between the ion gyroradius and the spatial scale of number density depletion is much greater than unity.
This is also sometimes called "Large Larmor Radius Interchange Instability" (LLRII) (Ripin et al., 1987).
Using equations from Borisov and Mall (2000) and output from the hybrid simulations, we estimate that the flute instability has a linear growth rate of about 0.1-0.2s-1 just behind Rhea, giving a growth time of 5-10s, comparable to the time scales inferred particle transport in Rhea's wake through the observed, narrow channels (Section 5.1).
The rate reduces by an order of magnitude 10RRh downstream, and disappears at larger distances, since magnetic field and pressure gradients diminish.
If gyroradius effects are important (Hassam and Huba, 1987), the growth rate close to the moon may be even stronger.
Small scale depletions or enhancements seen at Rhea, may then represent signatures of such a type of an interchange process.
We also note that during R1, when Cassini was observing close to the field aligned direction, the number of the observed small scale structures was the greatest from all four flybys.
Gyroradius effects may also explain the absence of such signatures at Tethys (Jones et al., 2008).
Since at Tethys the magnetic field is stronger, the fraction between the scale of the ion gyroradius and the density gradients is smaller than it is at Rhea.
The value maybe small enough to prevent the instability from growing, before the wake refills.
Furthermore, magnetic field gradients are weaker at Tethys.
For a full plasma absorption it can be easily shown that the ratio of the ∣B∣ in the wake of Tethys with the value upstream is proportional to (1+β), where β is the plasma beta (Simon et al., 2012).
The value of β at Tethys (∼0.01) is more than an order of magnitude lower than at Rhea (>0.5), meaning that the corresponding gradients in ∣B∣ would be almost negligible, as also observed (Khurana et al., 2008), preventing a flute instability from developing.
From the above it is clear that critical for the formation of this instability are the strength of gradients in the various plasma and field parameters in Rhea's wake.
In that context, observations of cold plasma in Rhea's wake have several puzzling elements that need to be carefully considered.
Cold plasma in Rhea's wake and implications for the driver of the instability
Data from most Rhea flybys show that the electron number density profiles extracted from RPWS fUH observations do not show an absorption signature during the wake or flux tube crossings.
In the same time, magnetometer signatures are consistent with the expected plasma pressure loss.
As the observation in electrons is unexpected, we included, wherever that was possible, plasma density measurements from different Cassini sensors, namely the Langmuir Probe (ion density during R1) and the CAPS/ELS (electron density during R2), in an attempt to verify the measurements.
In the case of R1, LP measurements show that the ion density in the wake experiences a decrease of at least 50% (fourth panel from top, Fig. 2).
If that is valid, a significant electron-ion density imbalance is implied for Rhea's wake.
Numbers for this imbalance (∼1-4cm-3), would, however, be extreme: simulations hint that charge superabundances in moon wakes should be present mostly around the wake flanks and their amplitude should be a negligible fraction of the upstream plasma density (Nakagawa and Kimura, 2011; Farrell et al., 2008).
In addition, a large charge imbalance would lead to a very intense electrostatic potential in Rhea's wake.
Data from the lowest energy proton channels of LEMMS (A0, A1 - 25-56keV range) show flux enhancements in the vicinity of Rhea's wake (Fig. 11) that could be consistent with large negative electrostatic potentials (tens of keV) that accelerate ions along the magnetic field direction.
Using a simple calculation scheme from Nishino et al. (2010), we calculated that the excess electron density is in the order of 10-4cm-3.
That is four orders of magnitude lower than the value inferred from RPWS and MAG data.
The signal of higher energy ion channels of LEMMS is featureless, indicating that this value is an upper limit and the assumption for a macroscopic quasi-neutrality in the wake is reasonable.
The latter conclusion implies that one of the two RPWS plasma density measurements for the R1 wake crossing is more reliable.
We believe that this is the fUH electron density dataset.
Ion density extraction from the LP has several systematic errors, large uncertainties (more than 50% relative error for the low plasma densities observed at L-shells greater than 7) and its estimation is also based on a series of assumptions (Jacobsen et al., 2009).
Furthermore, the agreement between the fUH electron densities and the density profile of CAPS/ELS for flyby R2 (Fig. 6) adds further support to the validity of that specific density profile.
We therefore find that the cold plasma density in the wake is equal or comparable to the ambient magnetospheric plasma density.
This sets constraints on the exact nature of the instability that is inferred from energetic electron observations (Section 5.3).
The instability that drives the interchange motions cannot be controlled by density gradients.
The flute instability, however, can also be driven by plasma pressure (or magnetic field) gradients (Boyd and Sanderson, 2003).
Magnetic field gradients were also proposed as one of the possibilities to explain the flute modes in the expanding AMPTE cloud (Bernhardt et al., 1987).
MAG data from all flybys indicate that such gradients are indeed present and it is therefore reasonable to explore this concept in future studies.
Since density levels in the wake are comparable to the ambient ones, lossless transport may alternatively occur through the a Kelvin-Helmholtz (KH) instability, if velocity shear between the two domains is significant.
We still, however, do not believe that the small scale features in energetic electrons are signatures of such a process, even if the KH instability occurs.
The presence of those signatures almost upstream of Rhea and up to 6RRh from the moon (flybys R2 and R3) would imply the presence of KH vortices in a rather unexpected location and with an unrealistically large scale compared to the width of a region where velocity shear may potentially develop.
Plasma velocity vectors available for R1 and R1.5 also show nothing relevant (Wilson et al., 2010).
It is also important to understand how conditions in the wake allow for a plasma pressure gradient, while the plasma density is unchanged.
One scenario is that there is a cold plasma source composed of exospheric electrons and ions.
To estimate the plasma density from such a process, we repeated the hybrid simulations of Simon et al. (2012), adding an exosphere with the characteristics described in Teolis et al. (2010).
Peak exospheric densities at the R2 and R3 flyby altitudes or the surface did not exceed the level of 0.2cm-3, while for the R1 altitude this level was at 0.01cm-3, orders of magnitude lower than the ∼4-6cm-3 observed with Cassini.
The presence of an exosphere can therefore not explain the observations of ambient plasma density levels in Rhea's wake.
A second possibility is that the wake receives accelerated or escaped photoelectrons from Rhea's charged surface.
Unlike Earth's Moon, the wakeside hemisphere of Rhea (or part of it) can be immersed in sunlight and supply the wake directly with photoelectrons, given also that a large fraction of Rhea's surface is expected to be charged negatively (Roussos et al., 2010).
Data from flyby R2 support this idea, although observations in R3 do not show any electron density enhancement when Cassini crosses to the moon's sunlit hemisphere.
Using equations of Colwell et al. (2005) and assuming a photoelectron temperature of 1-2eV, we estimate a peak surface density (subsolar region) of photoelectrons between 35 and 55cm-3.
This value decays with increasing solar zenith angle, reaching the 50% level at 60°.
The surface potential profile will determine what fraction of the photoelectrons may escape (explaining possibly the complex electron density profile during R3), but in principle, the abundance of photoelectrons appears sufficient to supply the observed densities in Rhea's wake.
If this process is taking place, photoelectrons will accelerate ions in the wake in order to balance the excess negative charge.
Ambipolar electric field acceleration is more effective along the magnetic field.
This means that ions attracted towards the wake would carry little perpendicular pressure.
This may then explain why the magnetometer still observes an enhancement in ∣B∣.
The observation of very low count rates (or partial densities) for the perpendicular component of the ion velocity distribution during flybys R1 and R1.5 (Wilson et al., 2010) indicates that indeed the bulk of the ion density should be contained at small pitch angles.
If an exospheric source was dominant, count rates in CAPS would have been considerably higher.
Extraction of CAPS/IMS ion moments for flybys R2 and R3 (where the angular coverage of the velocity distribution is better) is therefore essential for exploring the aforementioned scenario.
Summary, open questions and outlook
Energetic particle, magnetometer and electron density data from the four closest Cassini flybys of Rhea to date were reviewed in an attempt to understand the processes under which a series of flux decreases appear in energetic electrons.
The most interesting observation concerns the simultaneous presence of narrow dropouts in energetic electron fluxes, visible usually near the wake flanks with narrow flux enhancements within the wake boundaries.
Liouville's theorem was found to hold for these features, when Phase-Space-Density losses or gains were compared at the locations of the dropouts and spikes, respectively.
This result is consistent with a process of lossless rapid transport of energetic electrons from the magnetosphere to the wake.
It appears more likely that the transport is driven by a form of instability, rather than complex energetic electron drifts due to disturbed flows and magnetic fields downstream of Rhea.
These drifts may primarily explain better the broadening of the central wake with increasing energy, although a contribution to this observed pattern may also come from the merging of small scale depletions at high electron energies (typically above 60-100keV, depending on the flyby).
The instability is probably driven by pressure and magnetic field gradients in the wake.
Density gradients in the wake are surprisingly absent, a fact that we relate to the importance of surface charging for Rhea's magnetospheric interaction.
Deviation from quasi-neutrality, is probably not more than 10-4cm-3, based on energetic ion observations in R1.
Regardless of the explanation, we believe that the presence of a cold plasma source near Rhea is implied from the observations.
This aspect of the interaction appears qualitatively similar to what was observed at the jovian moon Callisto with the Galileo orbiter (Gurnett et al., 2000).
Simulating the evolution of plasma pressure gradients (expanding plasma clouds) using particle codes (Winske, 1989) would reveal if for the conditions of Rhea's plasma wake (plasma beta near unity, zero density gradient, strong parallel pressure component in the "wake") a fast growing instability develops, with characteristics described in previous sections.
Alternatively, properties of the RPWS spectrograms may be compared to theoretical expectations for wake-driven instabilities as in Farrell et al. (1997).
The analysis presented in the previous sections was subject to a series of limitations.
The small number of Rhea flybys until now makes comparisons challenging, given that the flyby trajectory, the instrument pointing and the magnetospheric state are different in almost each case.
If the role of surface charging is important, as hinted by some observations, then also the magnetospheric local time of the flyby may become a controlling parameter for the interaction region's structure and dynamics.
Equally interesting for future studies is the identification of broad energetic electron dropouts - an observation not extensively analyzed in this study.
These depletions are clearly seen only in flybys R1 and R2.
A distinct difference between the dropouts in the two cases is the opposite energy dependence of the depletion depth and the lack of symmetry in the depletion profile, for R2, with respect to the position of the wake.
Less intense dropouts, but qualitatively similar to those in R1, are seen in flyby R1.5, and much weaker ones in R3.
Observed deviations from the typical, plasma-absorbing interaction region profile may explain some differences.
These deviations do not necessarily mean the main interaction mode at Rhea is not plasma absorption.
For instance, Simon et al. (2012) demonstrates that the combination of low magnetosonic Mach number and the high plasma beta values of Rhea's space environment amplifies interaction features which for other plasma absorbers (e.g. Tethys, Dione, Earth's Moon) are barely detectable.
One of these features is a stronger, flow-aligned magnetic field component perturbation, which gives rise to mass-loading-like interaction signatures.
Such structures may also complicate electron drifts, but to what extent it is uncertain.
Similarly, the high plasma beta makes surface charging more important for Rhea compared to the other Saturnian moons (Roussos et al., 2010).
Furthermore, recent developments in the study of Earth's Moon interaction with the solar wind indicate that the standard picture for a lunar-type interaction may be too simplified: processes, such as the entry of exospheric pick-up ions in the center of the wake, backscattering on the surface and self-pick up of ambient plasma ions, appear to also have an impact on the wake dynamics (Halekas et al., 2011).
For instance, the self-pick up process has been shown to lead to enhanced ULF wave activity in the lunar, at least 10% of the time (Nakagawa et al., 2012).
Whether a similar process is important the inferred instability at Rhea is questionable, as the latter appears to operate continuously.
Understanding the origin of the broad energetic electron flux dropouts was also significantly limited by the narrow pitch angle pointing of LEMMS.
So far, the only case where extended pitch angle coverage has been achieved was during the distant flyby of Voyager 1 through Rhea's absorption signature (microsignature), three degrees in longitude downstream of the moon.
Observations showed that the flux dropouts are present in a broad region, much greater than Rhea's diameter.
In addition, that depletion was wider for equatorial particles compared to that of the more field-aligned particles.
The pitch angle dependence observed with Voyager 1 could be considered in order to explore whether the possibility that the decreases form due to scattering from whistler waves is applicable (Santolı´k et al., 2011).
The ideal pitch angle coverage of CAPS/ELS for flybys R2 and R3 may also be used to study pitch angle distributions at the 20-30keV overlapping energy range with LEMMS.
Furthermore, distant flybys of Cassini from Rhea within few degrees from the moon should also be investigated, as the may contain additional information about the nature of the interaction.
Beyond these striking features identified in few flybys, there was a large number of additional unique observations.
For instance, energetic electron data in R1.5 confirm observations of Wilson et al. (2010) of a local, radial velocity component in the moon's interaction region (also observed during R1).
Signatures of fresh, interchange events were seen in R1 and R1.5, possibly also at R3.
Furthermore, localized gradients in ∣B∣ were seen during the crossing from the shadowed to the sunlit hemisphere of Rhea (R2 and R3).
What is unclear in most cases is why these features do not appear consistently (or with consistent characteristics), and/or whether they are related to Rhea's interaction, or they are simply magnetospheric.
Additional, multi-instrument investigations and interaction simulations are essential all possible scenarios and their implications for the structure dynamics of Rhea's magnetospheric interaction.
Acknowledgments
The German contribution of the MIMI/LEMMS Instrument was in part financed by the German BMWi through the German Space Agency DLR under Contracts 50 OH 0103, 50 OH 0801, 50 OH 0802, 50 OH 1101 and by the Max Planck Society.
G.H.J. is supported by UK Science and Technology Facilities Council Advanced and Postdoctoral Fellowships, respectively.
H.K. acknowledges financial support by the Deutsche Forschungsgemeinschaft (DFG) under Grant MO539/19-1.
The hybrid simulations were performed on JUROPA supercomputers at the Jülich Supercomputing Centre, Forschungszentrum Jülich (Germany).
We thank Andreas Lagg and Markus Fränz (MPS) for extensive software support, Martha Kusterer and Jon Vandegriff (both JHUAPL) for reducing the MIMI data, Gethyn Lewis and Lin Gilbert (both MSSL/UCL) for reducing the CAPS/ELS data and Michelle Thomsen (LANL) for useful discussions.
Details on the Phase Space Density calculations
Here we provide some additional details about conversion of fluxes to Phase Space Densities, as this was required to test Liouville's theorem for data collected near and within Rhea's wake (Section 5.1).
Since PSDs were calculated at constant first adiabatic invariant, fluxes had to be evaluated at a different energy for each point that we had Cassini observations.
In order to derive the electron intensity j(E(t),t) at the changed energy, we used two methods.
In the first, we used high energy resolution spectral information from the PHA channels to extract the flux at the new energy E(t).
A potential problem of this method may arise from the relatively poor time resolution of the PHA channels with respect to the few second duration of the small scale enhancements and dropouts (the central features of this analysis).
This could lead to undersampling and an underestimation of the enhancement or dropout amplitudes.
For that reason, in the second method we used measurements at the 36 and at 229keV levels of the high time resolution LEMMS channels (channels C1_PRIO and C5_PRIO), and interpolated linearly between these energies on a log-log scale in order to find j(E(t),t).
The disadvantage of this method is that the assumed power-law spectral shape is only a rough approximation, given the shape of the energy-flux spectra taken with the PHA channels in that region (Fig. 9, bottom panel).
After we extract j(E(t),t) (with either of the two methods), we convert the differential intensity j to Phase Space Density f=j/p2, with p the kinetic momentum.
We then measure the amplitude of the PSD losses/enhancements at the locations of the narrow dropouts/spikes.
Comparison of the values is shown in Fig. 9 (top panel).
The indicated values were calculated using the first method described above.
The second method gave nearly identical results, meaning that the influence of the channel effects discussed in the previous paragraph is insignificant.
Details on the energetic electron tracings
Here we provide additional information regarding the calculations presented in Section 5.2, where we show guiding center tracing of energetic electrons in a simulated interaction region of Rhea.
The use of the guiding center approximation is justified since the gyroradius of energetic electrons is much smaller than Rhea's diameter (15-35km for electrons between 20 and 100keV) or the scale size of the various macroscopic interaction features (wake, expansion fans).
Furthermore, field parameters in the simulation are static.
The total drift velocity vector of equatorially mirroring electrons is given by the following equation:(B.1)v=vcorotE×B+vRheaE×B+vmag∇B+vRhea∇B
The first two terms are energy and charge independent E×B drifts, and represent the corotation and plasma flow perturbations, respectively.
They dominate at energies below about 5-10keV, depending on the percentage of subcorotation at Rhea and the intensity of local magnetic field gradients.
The other two contributions are energy dependent and represent drifts along the local ∇B×B direction.
The third term represents drifts due to magnetospheric field gradients and it is always opposing corotation (drift towards -x).
The last term may lead to drifts in all directions, depending on the location in Rhea's interaction region.
The sum of the last two components becomes dominant at energies above about 80-100keV for equatorially mirroring particles.
For intermediate energies, contributions from all terms are considerable.
We solved Eq.
(B.1) using numerical integration.
Numerical integration of Eq.
(B.1) requires inputs for each of the four terms.
For the calculation of the third term, vmag∇B, we require a description of the equatorial magnetospheric field.
For this purpose, we applied a linear fit to the magnetospheric ∣B∣ profile as a function of y-position in the RHIS.
A linear fit is applicable since the region of interest is very narrow (∼15RRh wide) compared to the magnetospheric scales and at Rhea's location ∣B∣ changes slowly with distance, as it is also visible in Figs.
2-8.
The field gradient is between 0.19 and 0.27nT/RRh in the y-direction.
For reference, in a dipole field the value is about 0.13nT/RRh.
From that, we used an average of 0.23nT/RRh in the simulations.
Inputs for the magnetic field perturbations and the plasma flow at Rhea are extracted from new hybrid code simulations of Rhea's magnetospheric interaction (Simon et al., 2012).
These simulations are similar to those of Roussos et al. (2008), utilizing an updated version of the hybrid simulation code, called AIKEF (Adaptive Ion-Kinetic Electron-Fluid) (Müller et al., 2011; Kriegel et al., 2011).
The sum of the first two terms of Eq.
(B.1) vcorotE×B,vRheaE×B is directly extracted from these simulations.
The simulated magnetic field perturbations are important for the last term of Eq.
(B.1) vRhea∇B.
Since gradient drift terms are proportional to the electron energy, high energy electron tracings can become extremely sensitive even to weak numerical noise in the ∣B∣ output.
To reduce the noise we averaged the output from the final steps of the simulation and also applied a Lee-filter in the xy-map of ∣B∣.
Derivatives in ∣B∣ were calculated numerically using a three-point interpolation.
The spatial scales across which derivatives were estimated were at least three times greater than the scale size of noise features (∼0.1RRh).
This ensured that the residual noise had the minimum possible effect in our tracings.
Since we simulated equatorial mirroring particle drifts, we also set Bx and By to zero (Roussos et al., 2008).
After the ∣B∣ map is constructed, we calculate the two gradient drift terms vmag∇B,vRhea∇B using a relativistically correct formula (Northrop, 1963):(B.2)v∇B=(γ2-1)γmec22q|B|3(B×∇B)Here, γ is the Lorentz factor (containing information about the electron energy), me is the electron mass, q is the electron charge and c is the speed of light.
The use of relativistically adjusted equation for electrons is necessary since for energies between 20 and 100keV, for instance, β=uc (or γ) gets values between 0.27 (1.17) and 0.55 (1.48), respectively.
This corresponds to a correction factor for the electron kinetic energy between 5% (20keV) and 22% (100keV).
Integration of Eq.
(B.1) was carried out with a fourth order Runge-Kutta method.
A series of test runs were performed in order to find the maximum allowed time step per electron energy.
Integration was done on the equatorial (xy) grid of the hybrid simulation.
The hybrid code uses adaptive grid size, but output was interpolated on a uniform cartesian grid with a resolution of about 0.084RRh in each direction.
Field and flow values between the grid points were estimated by interpolation.
At each time step, conservation of μ was enforced, adjusting the initial energy of the electrons, when these moved across regions of different ∣B∣.
Simulations of Rhea's magnetospheric interaction, with the presence of an exosphere
These simulations were carried out in order to determine the peak ionospheric densities in Rhea's interaction region and understand whether the weak exosphere of Rhea is responsible for the lack of a plasma density dropout in the wake.
For the simulation, the setup was similar to what is described in Simon et al. (2012), who, however, did not include an exosphere in the calculations.
The neutral exosphere we added has a profile similar to that given in Saur and Strobel (2005) and Simon et al. (2011) and its composed from molecular oxygen.
Neutral densities are scaled by the total number of oxygen particles (N=2.5×1029), given in Teolis et al. (2010), corresponding to a surface density of 3.4×105 cm-3.
For the ionization of neutrals, we assumed that charge-exchange dominates with a rate of 1.7×10-8cm-3s-1 (Simon et al., 2011).
Photoionization was not included since the rate is negligible compared to that of charge exchange.
The simulation shows that since the ionospheric particle production rate is low, the ionosphere does not act as a barrier to the upstream plasma flow.
The corotation electric field penetrates down to the surface of Rhea and all exospheric particles are immediately picked-up, before they accumulate to large numbers around Rhea.
At steady-state, peak ionospheric densities at the surface of Rhea are at least an order of magnitude lower compared to the electron densities measured by RPWS (Fig. C.12).

Energetic electron observations of Rhea's magnetospheric interaction

Highlights
► We give a detailed review of an energetic electron dataset in a lunar-type interaction.
► A local transport process occurs in the wake of Rhea.
► The transport is associated to a flute type of instability driven by pressure gradients.
► We show that surface charging is an important element of the interaction.
Abstract
Saturn's moon Rhea is thought to be a simple plasma absorber, however, energetic particle observations in its vicinity show a variety of unexpected and complex interaction features that do not conform with our current understanding about plasma absorbing interactions.
Energetic electron data are especially interesting, as they contain a series of broad and narrow flux depletions on either side of the moon's wake.
The association of these dropouts with absorption by dust and boulders orbiting within Rhea's Hill sphere was suggested but subsequently not confirmed, so in this study we review data from all four Cassini flybys of Rhea to date seeking evidence for alternative processes operating within the moon's interaction region.
We focus on energetic electron observations, which we put in context with magnetometer, cold plasma density and energetic ion data.
All flybys have unique features, but here we only focus on several structures that are consistently observed.
The most interesting common feature is that of narrow dropouts in energetic electron fluxes, visible near the wake flanks.
These are typically seen together with narrow flux enhancements inside the wake.
A phase-space-density analysis for these structures from the first Rhea flyby (R1) shows that Liouville's theorem holds, suggesting that they may be forming due to rapid transport of energetic electrons from the magnetosphere to the wake, through narrow channels.
A series of possibilities are considered to explain this transport process.
We examined whether complex energetic electron drifts in the interaction region of a plasma absorbing moon (modeled through a hybrid simulation code) may allow such a transport.
With the exception of several features (e.g. broadening of the central wake with increasing electron energy), most of the commonly observed interaction signatures in energetic electrons (including the narrow structures) were not reproduced.
Additional dynamical processes, not simulated by the hybrid code, should be considered in order to explain the data.
For the small scale features, the possibility that a flute (interchange) instability acts on the electrons is discussed.
This instability is probably driven by strong gradients in the plasma pressure and the magnetic field magnitude: magnetometer observations show clearly signatures consistent with the (expected) plasma pressure loss due to ion absorption at Rhea.
Another potential driver of the instability could have been gradients in the cold plasma density, which are, however, surprisingly absent from most crossings of Rhea's plasma wake.
The lack of a density depletion in Rhea's wake suggests the presence of a local cold plasma source region.
Hybrid plasma simulations show that this source cannot be the ionized component of Rhea's weak exosphere.
It is probably related to accelerated photoelectrons from the moon's negatively charged surface, indicating that surface charging may play a very important role in shaping Rhea's magnetospheric interaction region.

Introduction
Rhea is Saturn's largest icy moon (radius: 1RRh=764km).
It orbits the planet on an equatorial and circular orbit at a distance of about 8.74Rs from its center (1Rs=60,268km).
Recent studies showed that Rhea is surrounded by a tenuous exosphere composed of oxygen and carbon dioxide (Teolis et al., 2010).
Despite that, the main interaction mode of Rhea with the magnetosphere was shown to be plasma absorption.
Magnetic field perturbations in Rhea's interaction region appear to be guided primarily by the formation of a plasma pressure cavity (wake) downstream of the moon and not from mass or momentum loading from the ionized products of this weak exosphere (Simon et al., 2012; Khurana et al., 2008; Roussos et al., 2008).
The interaction region of any plasma absorbing moon is typically located downstream of the moon with respect to the direction of the bulk plasma flow.
Depending of the upstream sonic Mach number, Ms, the opening angle of the plasma wake can vary along the magnetic field direction (low/high for large/small Ms) (Samir et al., 1983).
For very low Mach numbers (e.g. Saturn's moons) the outer flanks of the plasma cavity tend to become tangential to the surface of the plasma absorbing body (Khurana et al., 2008).
Perpendicular to the magnetic field (or to the flux tube enclosing the wake) the width of the cavity is approximately equal to the moon's diameter.
The extension of the wake downstream of the moon also appears to be associated with the sonic Mach number: for low Mach numbers, the cavity has a smaller extension than the typical, supersonic case of the lunar wake in the solar wind.
Simulations indicate that Rhea's cold plasma wake should refill completely within a distance of about 10RRh downstream (Simon et al., 2012; Roussos et al., 2008).
The strongest electric and magnetic field disturbances exist within the boundaries of the wake.
Pressure gradient (diamagnetic) currents flowing along the wake boundaries or within the wake lead to a compression of the magnetic field, B, behind the moon.
The requirement for the total pressure (magnetic and plasma) to remain constant is also an alternative (and equivalent) explanation for the presence of this B enhancement.
The same diamagnetic currents lead to magnetic field perturbations on the two sides of the flux tube corridor enclosing the plasma wake.
Khurana et al. (2008) calls these features "expansion fans", which at Rhea extend up to several moon radii on either side of the wake.
The magnetic field magnitude (∣B∣) drops within these regions (see also Fig. 5 of Roussos et al. (2008)).
While magnetometer data comply with this standard, lunar-type interaction concept, Cassini's energetic charged particle observations of the near-Rhea environment by MIMI/LEMMS (Krimigis et al., 2004) are rich in unusual interaction features (Jones et al., 2008).
The first close downstream flyby on November 26, 2005 (termed R1), revealed broad energetic electron (20-100keV) flux dropouts extending almost 7-8 Rhea radii (RRh) on each side of Rhea's wake.
Hints for a broad decrease in fluxes were also present in a more distant 2007 downstream, non-targetted flyby of Rhea that took place on August 30, 2007 (informally termed R1.5).
In addition to the broad regions of energetic electron flux dropouts, Cassini's MIMI/LEMMS detector recorded smaller scale dropouts each of which was few tens of km across.
These were detected just outside the wake boundaries (wake flanks), within 2RRh from the center of the moon, in both the saturnward and the antisaturnward sectors of the interaction region.
As the scale size of the broad depletion region was, for both flybys, comparable to the size of Rhea's Hill sphere (the region where the moon's gravity dominates that of Saturn), it was proposed that Rhea was surrounded by a disk containing large grains.
These grains served as an energetic electron sink.
In view of this interpretation it was also suggested that part of these trapped grain populations were concentrated into one or more narrow ringlets around Rhea, which could have been the source of the smaller scale dropouts seen by MIMI/LEMMS.
Other evidence, such as the broadening of these small dips with increasing electron energy (gyroradius effects) were not contrary to this possibility.
Subsequent analysis and optical observations, however, ruled out these scenarios (Tiscareno et al., 2010).
Features such as the similar spatial scales of the broad depletion regions with the size of Rhea's Hill sphere and the simultaneous presence of smaller scale dropouts near the moon appear to be coincidental.
Since it appears unlikely that the explanation of the observations involves any absorbing medium around Rhea, answers to the problem should invoke plasma or magnetospheric processes.
Since the first two flybys in 2005 and 2007, two more have taken place on March 2, 2010 (R2) and January 11, 2011 (R3).
The goal of this work is to primarily review and describe MIMI/LEMMS energetic electron observations from all Rhea close flybys, to put them in context with observations and findings from other Cassini instruments and to focus on common interaction signatures that can be identified among the different flybys.
We will then propose interpretations that will help us form a more clear picture about the structure and the dynamics of the moon's interaction region and about the nature of the processes that drive some of the unusual energetic particle observations.
Unique flyby features will not be explicitly analyzed in this work, but they will be identified mostly for reference in future studies.
Flyby information
Table 1 contains basic information about the first four close Cassini flybys of Rhea to date.
The first two (R1 and R1.5) were downstream of Rhea, with R1.5 at a relatively large distance compared to the other three.
R2 was above the moon's north pole, while R3 was above its south pole.
A sketch of the equatorial projections of Cassini's trajectories in Rhea's Interaction System (RHIS) are shown in Fig. 1.
The RHIS is centered at Rhea and has the positive x-axis along the plasma nominal corotation direction, the positive y-axis pointing towards Saturn, while +z completes the right-hand system, pointing north and approximately antiparallel to the direction of the background B.
The vertical offset from the xy-plane, given in Fig. 1, was less than 1.5RRh in all cases.
The average offsets are given in Table 1.
Here, the geometrical/corotational wake is defined on the basis of where energetic electron absorption may be observed.
Because of the rapid bounce motion of energetic electrons, the energetic electron wake is weakly limited by Cassini's z-position and can extend to very large distances, north and south of the moon.
Therefore the condition for absorption is satisfied when Cassini is downstream of Rhea's volume or magnetically connected to the moon, namely when ∣y∣⩽1RRh(x⩾0) or x2+y2⩽1RRh (x<0).
The region where x2+y2⩽1 corresponds to Rhea's flux-tube, a term that may sometimes be used in this study instead of "wake".
The aforementioned definition is valid for energetic electrons that drift towards +x.
Electrons with energies above the keplerian resonant energy drift towards -x and therefore the conditions given in the previous paragraph are reversed.
The keplerian resonant energy (Erk) is the kinetic energy at which the total drift rate of energetic electrons cancels the keplerian drift rate of a moon.
In various plots of this study we will indicate the wake boundaries only for electrons with E<Erk, since most of the available data are in this energy regime (Section 3).
When Erk estimations are given based on the assumption of a dipole magnetosphere (Thomsen and Van Allen, 1980), values should be taken as upper limits.
At Rhea's distance the actual magnetospheric field gradient is stronger than that of a dipole field and the actual Erk should be at lower energies.
The difference between the dipolar and the real Erk can be significant especially for particles that mirror near the equatorial plane (pitch angle around 90°).
We also stress that the energetic electron wake has a significantly different structure to that of a cold plasma wake.
Particle absorption at low energies is expected to be more pronounced for x>1RRh.
This could be important for flybys R2 and R3.
Refilling processes are also different in these two energy regimes, with high-energy depletions persisting longer (Khurana et al., 2008; Roussos et al., 2008).
These long-lived, high energy depletions can be observed many degrees in longitude ahead of the moon along its orbit, and are usually called "microsignatures" (Paranicas and Cheng, 1997; Paranicas et al., 2005; Roussos et al., 2007).
Instrumentation and data processing
Most of the data presented here are from Cassini's MIMI/LEMMS sensor.
MIMI/LEMMS is an energetic particle detector.
It has two oppositely pointing telescopes, the Low Energy and the High Energy Telescope (LET and HET respectively) (Krimigis et al., 2004).
Since for all Rhea flybys LEMMS electron fluxes have considerable signal to noise ratio only for energies below about 600keV, we analyzed data only from electron rate channels C0-C7 (LET) and E0-E2 (HET).
For similar reasons, ion channels considered for this study were A0-A4 (<500keV), all belonging to the LET.
Data from high energy resolution Pulse Height Analysis (PHA) channels are also available for the selected energy ranges.
The time resolution of the PHA channels is typically 1.5 times lower than that of the rate channels.
Based on LEMMS intercalibration with other sensors of MIMI (INCA, CHEMS), it is reasonable to assume that all LEMMS ion channels respond to protons (Dialynas et al., 2009).
The latest passband and geometry factor information for the electron and ion channels can be found in Armstrong et al. (2009) and Krupp et al. (2009).
The LEMMS signal is corrected for the instrumental background as described in previous papers (Roussos et al., 2011; Kollmann et al., 2011).
Particle pitch angles are calculated using information from Cassini's magnetometer (MAG) (Dougherty et al., 2004).
For all flybys, electron density is evaluated through the detection of the upper hybrid (fUH) emission in radio spectrograms of the RPWS instrument (Persoon et al., 2005; Gurnett et al., 2004).
For flyby R1, RPWS Langmuir probe (LP) ion densities are also available, while CAPS/ELS electron densities will be shown for flyby R2 (Young et al., 2004).
Observations
The following subsections contain information about observations during the four closest Rhea flybys.
Focus will be given to MIMI/LEMMS energetic electron observations, which contain most of the interaction features.
R1: November 26, 2005
Jones et al. (2008) summarizes several observation aspects of this flyby.
Fig. 2 includes some of these observations as well as additional information not provided in the latter study.
The top panel shows an electron energy spectrogram based on the LEMMS PHA electron channels, between ∼25 and ∼250keV.
The second and third panels show "energy-cuts" through this electron spectrogram at the energies of nine rate channels of LEMMS.
The fourth and fifth panels show the plasma density and the total magnetic field measured by MAG, respectively.
We note that MAG data for all Rhea flybys (except R1.5) are extensively analyzed in a series of papers to which we refer the reader for more details (Simon et al., 2012; Khurana et al., 2008; Roussos et al., 2008).
We will now discuss some of the main features visible in these five panels.
Corotational wake
The signature of the wake crossing is a large dropout in particle fluxes of between one and four orders of magnitude, depending on electron energy.
The timings of the expected boundary crossings (dashed lines) agree well with predictions.
Fluxes do not reach background levels immediately inside these boundaries, but drop gradually over a distance of about 0.3RRh within them.
Fluxes are very low at the wake center, although some channels (C0-C4) record foreground with a weak signal to noise ratio of about 5-7.
The wake is also visible in the higher energy channels C6 and E2 (third panel).
Its edge-to-edge extension along the y-axis (RHIS system) is about 3RRh wider than Rhea's diameter.
The profile of channel C7 (510-832keV) features many flux dropouts.
None of these is centered on the location of the expected wake crossing.
Note that most C7 electrons have energies above the keplerian resonant energy at Rhea (Erk).
They arrive from the nominal wakeside direction and have not yet encountered the solid body of the moon at the locations of the R1 flyby.
Erk is ∼550keV.
An unexpected result is the lack of a reduction of the RPWS electron density (fourth panel from top).
We note that the electron density is dominated by electrons below about ∼1keV, which have different drift properties from energetic electrons and also have different source and loss processes.
We also overplot the ion density profile from the RPWS Langmuir Probe (LP), which appears to show a density dropout in the wake.
Furthermore, MAG data show an enhancement in the magnetic field magnitude, consistent with a loss of perpendicular plasma pressure in the wake.
This pressure loss is typically associated to a plasma density dropout, not visible in all datasets.
Since it is probably unreasonable to interpret MAG and RPWS fUH/LP observations as evidence of a large charge density in Rhea's wake (deviation from charge neutrality), one of the two density measurements is probably problematic.
The subject is discussed in Section 5.4.
Broad depletions and structures
As "broad depletions" we refer to the electron flux dropouts that do not appear to originate from absorption at the surface of the moon and have an extension larger than Rhea's diameter.
The distinction is not clear in all cases, as it depends also on how the data are interpreted.
For instance, the dropout seen in channel E2 of LEMMS appears too broad to be associated with absorption at Rhea's surface.
However, as Roussos et al. (2007) found, this could be the result of a diffusive broadening of the actual electron depletion formed at Rhea's surface, which is observed to occur much faster at higher energies than at low energies.
Alternative explanation for such cases is given in Section 5.2.
The most obvious broad depletions exist below about 100keV (channels C0-C4, e.g.
second panel of Fig. 2).
Their width is about ∼7RRh on each side (Jones et al., 2008).
Excluding channel E2, the remaining electron channels have also some rather broad electron flux dropouts that are about 2RRh wide in channel C7 (22:28-22:34) and channel C6 (22:44-22:47).
The latter dropout seems to be associated with smaller scale dropouts at lower energy channels (see Section 4.1.3), a sharp increase of ∣B∣ centered around 22:45 and a shallow dropout in electron density (marked between dotted-dashed lines).
This combination of signatures is typical of depleted flux tubes participating in the centrifugal interchange process (André et al., 2007).
Evidence that this may be the case is also visible in the CAPS/ELS electron dataset, where an enhancement of 1-5keV energetic electrons is seen (Fig. 2, Panel D of Jones et al. (2008)).
We also note a relatively sharp jump in electron densities at 22:25 (dotted line) and a density decrease on either side of the wake (from about 6cm-3 to 4cm-3), occurring slowly over a distance of 5RRh.
The significance of any of the features discussed here would also depend on whether they appear in the other Rhea flybys, otherwise we cannot rule out that they are fluctuations of the background magnetospheric parameters, unconnected to the moon.
Small scale depletions and structures
The term "small scale depletions" refers to the flux dropouts that have a width that is significantly smaller that Rhea's diameter and do not seem to be caused by electron absorption on the moon's surface.
These features are usually visible closer to the wake.
Fig. 3 shows data in a time interval spanning a few minutes before and after the wake crossing.
The format is similar to Fig. 2, but the second and third panels show data from the priority channels C1_PRIO and C5_PRIO, respectively.
Small scale depletions are well visible in the PHA energy-time spectrogram (top) and the data of C1_PRIO.
We use red1
For interpretation of color in Figs.
1-12, the reader is referred to the web version of this article.
1 arrows to mark some of them.
At the energy of C5_PRIO, small scale depletions are visible but less resolved than those of C1_PRIO.
This is partly attributed to the lower signal to noise ratio of that channel and partly to the fact that groups of small scale depletions at lower energies appear to coincide with a single, broader depletion above the energy of C5_PRIO.
Characteristic examples of merged low energy depletions at high energies are the dropouts in C5_PRIO (or C5) after 22:40:30 (last red arrow in the middle panel of Fig. 3).
The same feature is also visible in channels C6, C7 and E3 in Fig. 2.
The broadening and merging of small scale depletions appears to occur only at energies above 130keV, as inspection of the energy-time spectrogram of Fig. 3 shows.
As mentioned in Section 4.1.1, fluxes do not drop sharply to background inside the wake boundaries.
An interesting observation is that the flux within the wake exists in the form of short-lived spikes (Fig. 3, blue arrows).
The spikes are well resolved in C1_PRIO and C5_PRIO, although they are less regular in the latter.
An interesting aspect is that peak fluxes in the spikes are close to the flux lost in the small scale depletions, indicating a local transport process.
A more careful analysis is presented in Section 5.1.
R1.5: August 30, 2007
Flyby R1.5 occurred while Cassini was in the outbound leg of its orbit.
Few aspects of it are discussed in Jones et al. (2008), while Wilson et al. (2010) provide thermal ion moments for the encounter.
Here we present LEMMS, RPWS and MAG data in the same manner as for R1.
One interesting element of R1.5 is that it occurred downstream of Rhea, with LEMMS and CAPS having similar orientations as in R1.
In essence, the primary difference between R1 and R1.5 is the flyby distance.
Corotational wake
Fig. 4 shows the overview of LEMMS, MAG and RPWS observations, as in Fig. 2.
The corotational wake is again evident as a region with a large dropout in electron fluxes.
Flux levels in the wake are more recovered compared to the R1 case, since the distance of this flyby is greater and some refilling occurred.
The center of the wake has a small antiplanetward offset from the expected location.
Although this center cannot be exactly defined due to the variability of fluxes within it, we can detect a small difference in the offset between low energy channels (C0-C3: ∼0.5RRh) and C4 (∼1RRh).
This difference could exist because the C4 electrons drift more slowly than the C0-C3 ones and therefore are exposed to the effect that causes this outward motion for more time.
Quantification of this time-of-flight effect gives us an outward radial velocity of about 5kms-1, consistent in direction and order of magnitude with the local radial velocity measured by CAPS (Wilson et al., 2010).
Our inferred radial velocity value is a factor of two lower than the one given in Wilson et al. (2010), but this could be due to a series of factors affecting mainly our technique, such as the uncertainties in determining the wake center or the assumption of constant radial velocity for all the time-of-flight interval.
As in R1, the wake becomes broader with increasing electron energy.
Channels monitoring energies above about 500keV are featureless.
The cold electron density from RPWS does not show a wake signature, although densities are much more variable within the wake boundaries compared to R1.
The enhancement in ∣B∣ indicates that some considerable pressure loss still exists 8RRh downstream of the moon.
Cassini also crossed a depleted flux tube at about 01:36 (dotted-dashed lines).
At that location (shaded box, fourth panel), the fUH emission is barely above noise levels and not reliable enough to extract the electron densities.
Manual inspection of the RPWS spectrogram shows a very weak emission line which corresponds to a density 50% below the surrounding value.
Furthermore, the magnitude of the associated change in ∣B∣ is similar to the one observed in Rhea's wake.
This makes the lack of a wake signature in electron density even more puzzling.
Broad depletions and structures
Whether broad depletions are present in R1.5 is unclear.
Channels C0 and C1 show shallow dropouts in the time segment before the wake crossing and an even slower recovery afterwards.
Their spatial extent is similar to those seen in R1.
These dropouts (some of which are marked in the second panel of Fig. 4) are more apparent when fluxes are plotted in linear scale (see Supporting Online Material of Jones et al. (2008)), however it is difficult to understand whether they are associated with the interaction region of Rhea or are part of magnetospheric flux variations observed before and after the plotted interval (not shown here).
The dropout signatures in channels C5, C6 and E2 may be interpreted as broad depletions, or may simply be signatures of a diffuse wake, as explained in Section 4.1.1.
The electron density in RPWS also shows a broad and gradual dropout centered around the expected time of the wake crossing.
The slow dropout and the presence of a relatively sharp jump in densities at about 01:00 (dotted line) are qualitatively similar to features seen in R1.
The difference is that in R1.5 they are more pronounced in the saturnward side of the interaction region.
Small scale depletions and structures
LEMMS data show many small scale depletions, as it is already evident in the top two panels of Fig. 4, but they are less regular compared to those in R1.
Broadening of these features with increasing energy is present (more visible in line plots), but less obvious than in R1.
A zoom in the region where the dropouts are visible is shown in Fig. 5.
The format is similar to that of Fig. 3.
Small scale dropouts appear on the side of the wake (red arrows).
Fluxes within the wake are turbulent, with alternating dropouts and spikes, as in R1.
The flux enhancements are also apparent in the PHA spectrogram (black arrows) and in the rates of channels C0-C4 (Fig. 4).
Temporal changes in ∣B∣ and the RPWS electron density do not coincide consistently with small scale features in LEMMS.
R2: March 2, 2010
The fundamental difference of R2 (and R3) from R1 and R1.5 is that Cassini crossed field lines connected to the moon's surface.
Cassini also crossed regions slightly upstream of the corotational wake.
Furthermore, the pointing of LEMMS was also away from the field aligned direction (Table 1).
An overview of R2 observations is shown in Fig. 7.
CAPS/ELS electron densities are available for this flyby.
Corotational wake/flux tube crossing
The wake/flux tube signature is visible at the predicted location, for energies up to 100keV (top three panels).
The locations where Cassini's trajectory intersects the flux tube of Rhea also allows the observation of the absorption of electrons that are both above or below the keplerian resonant energy (e.g. channel C7).
A common feature with the previous flybys is that the wake seems to broaden with increasing energy.
This broadening is not symmetric about the wake center, but its more visible in the saturnward side.
It also appears that the broadening of the wake stems from the merging of small scale depletions, as we also see in R1.
The electron density measurements show again no density gradient as Cassini enters or exits the wake.
At the wake center, a sharp density increase is also visible.
The increase is sustained for a distance of about 2RRh along the y-direction, outside of the wake.
The density profile in this case is confirmed by comparing RPWS/fUH and CAPS/ELS densities.
CAPS/ELS had a full pitch angle coverage for this flyby, allowing for a more reliable determination of densities compared to R1 and R1.5 (Santolı´k et al., 2011).
LP densities are also available, but are not shown here because of their very low signal to noise ratio.
LP data also show that the spacecraft potential is few Volts negative.
This explains why CAPS/ELS densities have to be multiplied by a factor of 1.5 to match the levels of RPWS.
The enhancement in ∣B∣ corresponds to a partial plasma pressure loss, since only a fraction of the pressure-carrying heavy ions have probably been absorbed above Rhea's poles (mainly ions coming from the south).
Diamagnetic currents downstream of Rhea's volume are also be responsible for part of this change in the magnetic field (Simon et al., 2012).
At the time when the sharp electron density gradient occurs in the wake center, the magnetic field magnitude shows a spike, probably associated with pressure gradient driven currents at this boundary (dotted-dashed line).
Given the lack of ion moments and electron temperatures, it is unclear which species drive this current.
Furthermore, the boundary marked by the dotted-dashed line corresponds to the crossing of Cassini from the shadow of Rhea to the sunlit hemisphere of the moon.
If not coincidental, this observation hints that the lack of an electron density dropout in Rhea's wake is due to the presence of a cold plasma source, the strength of which is controlled by sunlight.
Such a source may be due to ionized particles from Rhea's weak exosphere (Teolis et al., 2010) or surface photoelectrons accelerated along the field lines from negative electrostatic potentials on Rhea's surface (Roussos et al., 2010).
We discuss these possibilities in Section 5.4.
Electron density streaks associated to surface charging have also been observed in moon-solar wind interaction simulations by Nakagawa and Kimura (2011).
Broad depletions and structures
As discussed in the previous subsection, there is no evidence of a symmetric broad depletion similar to the one observed in R1.
A broad flux dropout is seen only after Cassini exits the wake.
Apart from the lack of symmetry, the flux dropout has an additional difference with R1: the dropout becomes more intense with increasing energy.
For energies of channel C4 and above, the dropout signal extends to distances between 7 and 8RRh and at locations where x<-1RRh.
It is questionable how electrons at this location are influenced by a plasma absorbing moon, the interaction region of which is expected to extend only downstream.
Small scale depletions and structures
Fig. 7 is a zoom in the time interval around closest approach of R2, and also shows LEMMS priority channel data.
Notice that priority channels for R2 are different to those of R1 and R1.5.
Three small scale dropouts are seen in total, with the third one (after 17:42:20) becoming broader and deeper at higher energies (red arrows).
Flux foreground is visible within the wake, although the signal is not as pronounced as in the case of R1.
The clearest enhancement, again in the form of a spike, is visible in channel E2 (Fig. 6, middle panel, blue arrow).
Since around closest approach, LEMMS's C-channels measure electrons coming from the south (pitch angle greater than 90°), it is surprising that foreground is measured above Rhea's north pole.
One explanation may involve a fast transport process, as we infer for R1 (Section 4.1.3).
It is also possible that energetic electrons with the appropriate bounce phase are reflected by the enhanced magnetic field at the wake, before reaching the surface: the mirror magnetic field strength for particles with pitch angle of 110° is about 23.0-23.5nT.
This is close to the peak magnitude observed with MAG, meaning that a mirror point of the electron bounce motion can occur above Rhea's surface.
The enhancement of ∣B∣ may be even stronger closer to the surface compared to the 100km flyby altitude.
In that case, however, it is uncertain why fluxes within Rhea's wake appear as spikes and not as a continuous population.
As mentioned earlier, a clear spiky enhancement is seen in channel E2.
Since E2 belongs to the HET of LEMMS, it observes electrons with pitch angles opposite to those of the C-channels.
In that case, E2 electrons arrive from the north.
As no similar enhancement is seen the C-channels with overlapping energy ranges, we can also infer the presence of unidirectional energetic electron distributions.
Similar observations at Ganymede have been connected to transport by pitch angle diffusion (Williams and Mauk, 1997), a scenario that may also comply with observations by Santolı´k et al. (2011).
R3: January 11, 2011
R3 had its closest approach 80km above the south pole of the moon.
An overview of observations is shown in Fig. 8.
Due to instrumental problems (significant negative Cassini charging affecting CAPS/ELS fluxes, and commanding error for RPWS/LP), density measurements for this flyby are only shown from RPWS/fUH.
Corotational wake/flux tube crossing
A first look at the panels of Fig. 8 shows the interaction features to be quite symmetric around closest approach.
The wake/flux tube boundary crossings occur at the predicted times for energies up to about 100keV.
The wake broadens at higher energies, as seen in all other flybys.
Fluxes are above the instrumental background in certain LEMMS channels within the wake boundaries (blue arrows).
This is also discussed in Section 4.4.3.
R3 is the only flyby so far where electron densities show dropout signatures across both wake boundaries.
The density profile is, nevertheless, peculiar, as there is a large density enhancement just before wake ingress.
It is uncertain if this feature is connected to Rhea's interaction, as significant variability in densities is observed before and after the wake crossing.
If there is a connection, it again may be relevant to the findings of Nakagawa and Kimura (2011) (electron density streaks driven by surface charging).
One important difference with R2 is that the electron density drops after Cassini moves from the dark to the sunlit hemisphere of Rhea (dotted-dashed line).
The density shows a sharp gradient at this point and reaches a minimum a few minutes later, before starting to recover again.
The magnetic field magnitude (bottom panel) ∣B∣ shows the expected, persistent enhancement within the wake boundaries (Simon et al., 2012).
Outside those boundaries, MAG data show many short duration spikes, comparable in amplitude with the increased magnitude in the wake.
The similarity of these spikes to magnetic field signatures of depleted flux-tubes (André et al., 2007), may indicate that these features arise from injection events that coincidentally occurred during the flyby period (see also Simon et al., 2011).
A peak in the magnetic field is seen again close to the day-night boundary crossing (dotted-dashed line), as in R2 (Section 4.3.1), consistent with the presence of a plasma pressure gradient at this location.
Notice also that background ∣B∣ is similar to what is measured at all flybys except R1.
During the latter, ∣B∣ was on average 3-4nT stronger compared to all other flybys.
Broad depletions and structures
R3 shows little evidence of large scale depletions in energetic electrons.
The only persistent depletion observed away from Rhea starts around 04:46 and ends around 05:00.
It is better visible in channels C5-E2, but is very shallow compared to what has been observed in R1 and R2 and it is questionable whether it should be compared with the broad depletions seen during R1 and R2.
This possible broad dropout region also coexists with a considerable number of small scale depletions (red arrows), the merging of which may actually give rise to its existence.
From the other datasets, only the RPWS electron density contains signatures of a broad structure (04:48-05:00).
Again, however, this cannot be conclusively separated from the overall variability observed in that interval (e.g. sharp increase of electron density marked by the dotted line).
Small scale depletions and structures
Inspection of Fig. 8 reveals many small scale dropouts.
Here we do not include a figure with priority channel data, as all small scale structures are well resolved with rate and/or PHA channels.
What is unique for R3 is that small scale dropouts (red arrows) can be detected as far as ∼6RRh and ∼4RRh from the moon, before and after the wake crossing, respectively.
Closer to the wake, small scale dropouts are more symmetric in most channels and resemble what was observed in R1.
Few dropouts coincide with small scale enhancements in ∣B∣.
Fluxes are low but detectable within the wake.
Channel E2 has the most striking signature, with several spikes reaching flux levels similar to the fluxes outside the wake (blue arrows).
The pitch angle of E2 is 80°, meaning that it observes energetic electrons with a southward parallel velocity component (coming from Rhea).
This direction is occulted by Rhea, meaning that electrons should have reached at this location by the means of a fast transport process (fast diffusion/scattering) or by reflection at the enhanced ∣B∣ near Rhea's surface.
Again, we favor the fast transport solution, as it is not obvious how flux enhancements would be detected in spikes, if reflection at strong magnetic fields is involved.
Analysis and discussion
From the overview of the four flyby observations given in Section 4, we can understand that Rhea's interaction region has many complex features in the energetic particle, plasma density and the magnetic field datasets.
All flybys have unique features and only few structures are consistently observed.
These consistent features, observed in at least three out of the four flybys, are listed below:•
The width of Rhea's wake tends to increase with increasing electron energy.
•
The broadening appears to occur, at least partly, because small scale depletion regions at the wake flanks are merging at higher energies.
•
All flybys show narrow dropouts in energetic electron fluxes.
These dropouts are near the wake flanks, although in some cases (R3) they can be observed also than 5RRh far from Rhea.
•
All flybys show fluxes above the instrumental background within the wake boundaries.
This may be more easily explained for R1.5, as its flyby distance is large enough to allow for some diffusive fill-in to have occurred.
•
All flybys (including R1.5) show spiky flux enhancements in the wake.
The width of the spikes is comparable to that of the narrow dropouts outside the wake.
•
All flybys show the expected and persistent increase in ∣B∣ within the wake boundaries.
These are driven by the diamagnetic current system that is set up due to the presence of the wake, as demonstrated in several studies (Simon et al., 2012; Roussos et al., 2008).
In none of these flybys, however, do the plasma density measurements show the anticipated dropout at the wake.
As stated in the introduction, we will only focus on the common aspects.
Unique features of the interaction may well be connected to the different flyby geometries, the pointing of LEMMS and/or the state of the magnetosphere at each time.
Thus, we leave the analysis of such features for separate studies and we only briefly discuss some of their aspects in Section 6.
We will start with the observation that narrow flux dropouts and short-lived electron flux enhancements in the wake were seen in all cases.
The fluxes measured in these spikes appear comparable (in orders of magnitude) to the flux lost at various dropouts.
This is best visible at flyby R1 (Fig. 3, Section 4.1.3).
One possibility is that there is rapid transport between the magnetosphere and the wake.
To verify this claim, we have to perform a Phase-Space-Density analysis.
Phase Space Density (PSD) analysis
During flybys R2 and R3, flux enhancements in Rhea's wake were observed above the moon's north and south poles (Sections 4.3.3 and 4.4.3).
This suggests that transport time scales should be faster or comparable to half of the electron bounce period.
For a dipolar configuration, energetic electrons at Rhea's L-shell and with pitch angle pointing as in R2 and R3, this period is between about 8s (20keV) and 4s (300keV).
This simple calculation can give us an idea about the transport time scales of the process that we investigate.
If the process is so rapid, we can assume that no additional sources or losses act on the electrons, unless they get absorbed by Rhea during this transport.
In that case, Liouville's theorem should apply (PSD is constant along the trajectory).
Flyby R1 is probably the best one where Liouville's theorem can be tested.
The wake in R1.5 has contributions from diffusive fill-in.
For the R2 and R3 flux tube crossings, transport cannot be lossless since many electrons will get absorbed on Rhea's surface.
The same calculations can be performed with the use of an isotropic invariant, Λ, (instead of the first and second adiabatic invariants), as given in Harel et al. (1981) and Rymer et al. (2007).
Λ is insensitive to pitch angle scattering (Santolı´k et al., 2011), but gives the same results as the conservation of μ because of the small changes in ∣B∣ involved in our analysis.
Additional details about the extraction of PSDs are given in Appendix A, while sample results shown in Fig. 9.
Overall, comparisons show that average PSD losses (dotted line) and gains (dotted-dashed line) differ by less than half an order of magnitude.
This difference is very small, considering the assumptions, simplifications and the limitations (narrow LEMMS's pitch angle pointing) of the analysis.
Furthermore, we cannot be certain that spikes in the wake have a one-to-one correspondence with depletions at the flanks.
The order-of-magnitude agreement can then be considered as an indication that Liouville's theorem holds and hints that a lossless or sourceless transport of these energetic electrons from the magnetosphere to the wake may be occurring.
The question is what can be the driver of such a transport.
We first investigate the possibility of complex energetic electron drifts in Rhea's interaction region.
Lossless transport due to complex electron drifts in Rhea's interaction region
As plasma flows and the magnetic field are disturbed downstream of Rhea, energetic electron trajectories will be affected by these in ways drastically different to trajectories of cold test particles, since the intensity of magnetic (gradient and curvature) drifts increases with energy.
The relative strength of the observed ∣B∣ disturbances at Rhea with respect to the background ∣B∣ is between 6% and 13%, compared to 6-8% at Enceladus, where non-dipolar drifts appear to be important (Krupp et al., 2012).
Complex drifts may help energetic electrons access the wake (enhancements), or lead to the formation of forbidden regions (flux dropouts).
We therefore believe that the role of complex drifts at Rhea should be investigated.
We used the guiding-center approximation and the output of hybrid code simulations of Rhea's magnetospheric interaction (Simon et al., 2012), in order to visualize the effects of complex drifts on the energetic electrons near the moon.
Detailed information on the tracing procedure is given in Appendix B.
Tracing examples are shown in the panels of Fig. 10.
The top left panel shows the xy-map of ∣B∣ from the simulations, after we added a magnetospheric gradient of 0.23nT/RRh along the y-direction.
The other three panels show electron traces at energies indicated on top of each plot.
That is the initial energy of injected electrons.
We also express this energy as a percentage of the keplerian resonant energy, Erk, which for this simulation is at 100keV.
We show this percentage because the complexity of drifts depends also on how close we are to Erk, not only the energy of the injected electrons (Schulz and Eviatar, 1977).
As we approach Erk, the residence time of energetic electrons within Rhea's interaction region increases.
Then these electrons are exposed to the perturbed fields for longer time and distortions in their drift trajectories are amplified.
In the tracings shown in Fig. 10, 350 electrons were injected at the left boundary.
The top right panels show electrons at the 1% level of Erk (1keV).
They practically follow the plasma flow pattern as magnetic drifts are unimportant at low energies.
Electron trajectories "grazing" the wake boundaries expand into the wake due to flow perturbations.
The wake then becomes narrower than the moon's diameter.
Tracings at the 5% level of Erk are shown in the bottom left panel.
The initial electron drift towards -x is a bit slower due to the vmag∇B of Eqs.
(B.1) and (B) becoming stronger.
Firstly, electrons enter the "expansion fans" (drop in ∣B∣) and tend to drift towards -y.
As they continue to move towards +x they experience several competing drifts.
For instance, as they start to exit from the "expansion fans" further downstream, they see an opposite gradient in ∣B∣ from before, which now pushes them towards +y.
In addition, they feel a flow perturbation pushing them in the wake.
The latter has a negative y-component for y>0 and a positive y-component for y<0.
This explains the asymmetry in the trajectories that develops at the wake flanks.
The strong magnetic field within the wake boundaries, prevents most energetic electrons from accessing it and remains empty for most of its part.
At higher energies, magnetic drift terms compete even more with E×B drifts resulting in the very complex trajectories visible in the lower-right panel of Fig. 10.
The sensitivity of traces to numerical simulation noise was important for energies greater than 50keV, so for this study we restrict our analysis below that range.
From the illustrated results, the most interesting features are the following:•
Field gradients tend to exclude particles from the wake at electron energies that magnetic drifts start to become important.
This leads to a wake broadening with increasing energy, similar with what we observe with LEMMS.
On the other hand, LEMMS data indicate that this broadening may result from an apparent merging of small scale depletions seen in low energies.
The relative importance of such a contribution is unclear.
•
As expected, complex drifts lead to the appearance of forbidden regions and channels of lossless transport in the wake.
The structure of these regions does not agree with LEMMS observations.
LEMMS data also show a series of flux enhancements in the wake, while our simulations show only one channel of transport (in the best case).
The various inconsistencies between our data and the test particle simulations may be due to a series of reasons.
For instance, LEMMS observations during R1 (where we have the clearest indications of lossless transport) were at 170° pitch angle, while our simulations are for 90°, as mentioned in B.
Treatment of the full, 3D problem is more demanding and is considered as part of future studies.
Its worth noting, however, that CAPS/ELS observed small and large scale features identical to those observed with LEMMS at the overlapping energy range of the two instruments (20-30keV), while observing at 90° pitch angle during R1 (Jones et al., 2008).
This suggests that the mismatch between our drift simulations and LEMMS observations is probably not associated to the restriction of our calculations to 90° electrons.
Additional drifts may be connected with temporal fluctuations in the magnetic and electric fields at Rhea's distance.
Fluctuations in ∣B∣ are evident in all flyby plots.
The associated polarization drifts are not considered in this simulation, although we expect that they will have a diffusing (smoothing) effect on the spatial distribution of electron fluxes.
If that is the case, then the drifts that lead to the appearance of the small scale structures near Rhea should be associated to dynamics of the local interaction region, not described by the hybrid simulation code which provides us the background field parameters for the tracings.
One such dynamical feature could be a wake-driven instability, a topic investigated in the next two subsections.
Flute instability in Rhea's wake
The formation and evolution of Rhea's wake is in principle a case of "magnetized plasma expansion into the vacuum".
This subject has been investigated both theoretically and in space or laboratory experiments.
Many studies suggest, amongst others, that such an expansion can be the driver of plasma instabilities.
Borisov and Mall (2000) proposed that conditions in the lunar wake may allow the excitation of the flute (interchange) instability.
These conditions require a simultaneous decrease/increase of ∣B∣ and the plasma density.
These two parameters are typically anticorrelated, but around the wake boundaries there can be a positive correlation.
This has also been observed in other environments, as we discuss also below.
The flute instability is a Rayleigh-Taylor type of instability, where transport occurs quickly and in the form of narrow channels with typical scales lower than the characteristic ion gyroradius, ρi.
At Rhea, we indeed observe that small scale structures have spatial scales much smaller than the ion gyroradius (by a factor of 20 at least).
The instability's growth rate is also proportional to the intensity of the magnetic field and the plasma density gradients.
It is also expected to develop more quickly along the field-aligned direction.
Borisov and Mall (2000) calculated a linear growth rate between 0.1 and 0.3s-1 for Earth's Moon wake.
Apart from moon wakes, this instability has also been observed in expanding plasma clouds, such as the magnetotail barium releases from the satellite AMPTE (Active Magnetospheric Particle Tracers Explorers) (Bernhardt et al., 1987).
This expansion did not occur on a smooth spherical shell.
Many irregularities were observed on the shell's surface.
Wavelengths were smaller than 40km, about an order of magnitude smaller than the ion gyroradius.
Among the proposed mechanisms considered in order to explain the AMPTE results was the lower-hybrid drift instability (Bernhardt et al., 1987).
Alternatively, Hassam and Huba (1987) proposed a new form of flute instability in order to explain the data.
The growth rate of this instability increases rapidly if the ratio between the ion gyroradius and the spatial scale of number density depletion is much greater than unity.
This is also sometimes called "Large Larmor Radius Interchange Instability" (LLRII) (Ripin et al., 1987).
Using equations from Borisov and Mall (2000) and output from the hybrid simulations, we estimate that the flute instability has a linear growth rate of about 0.1-0.2s-1 just behind Rhea, giving a growth time of 5-10s, comparable to the time scales inferred particle transport in Rhea's wake through the observed, narrow channels (Section 5.1).
The rate reduces by an order of magnitude 10RRh downstream, and disappears at larger distances, since magnetic field and pressure gradients diminish.
If gyroradius effects are important (Hassam and Huba, 1987), the growth rate close to the moon may be even stronger.
Small scale depletions or enhancements seen at Rhea, may then represent signatures of such a type of an interchange process.
We also note that during R1, when Cassini was observing close to the field aligned direction, the number of the observed small scale structures was the greatest from all four flybys.
Gyroradius effects may also explain the absence of such signatures at Tethys (Jones et al., 2008).
Since at Tethys the magnetic field is stronger, the fraction between the scale of the ion gyroradius and the density gradients is smaller than it is at Rhea.
The value maybe small enough to prevent the instability from growing, before the wake refills.
Furthermore, magnetic field gradients are weaker at Tethys.
For a full plasma absorption it can be easily shown that the ratio of the ∣B∣ in the wake of Tethys with the value upstream is proportional to (1+β), where β is the plasma beta (Simon et al., 2012).
The value of β at Tethys (∼0.01) is more than an order of magnitude lower than at Rhea (>0.5), meaning that the corresponding gradients in ∣B∣ would be almost negligible, as also observed (Khurana et al., 2008), preventing a flute instability from developing.
From the above it is clear that critical for the formation of this instability are the strength of gradients in the various plasma and field parameters in Rhea's wake.
In that context, observations of cold plasma in Rhea's wake have several puzzling elements that need to be carefully considered.
Cold plasma in Rhea's wake and implications for the driver of the instability
Data from most Rhea flybys show that the electron number density profiles extracted from RPWS fUH observations do not show an absorption signature during the wake or flux tube crossings.
In the same time, magnetometer signatures are consistent with the expected plasma pressure loss.
As the observation in electrons is unexpected, we included, wherever that was possible, plasma density measurements from different Cassini sensors, namely the Langmuir Probe (ion density during R1) and the CAPS/ELS (electron density during R2), in an attempt to verify the measurements.
In the case of R1, LP measurements show that the ion density in the wake experiences a decrease of at least 50% (fourth panel from top, Fig. 2).
If that is valid, a significant electron-ion density imbalance is implied for Rhea's wake.
Numbers for this imbalance (∼1-4cm-3), would, however, be extreme: simulations hint that charge superabundances in moon wakes should be present mostly around the wake flanks and their amplitude should be a negligible fraction of the upstream plasma density (Nakagawa and Kimura, 2011; Farrell et al., 2008).
In addition, a large charge imbalance would lead to a very intense electrostatic potential in Rhea's wake.
Data from the lowest energy proton channels of LEMMS (A0, A1 - 25-56keV range) show flux enhancements in the vicinity of Rhea's wake (Fig. 11) that could be consistent with large negative electrostatic potentials (tens of keV) that accelerate ions along the magnetic field direction.
Using a simple calculation scheme from Nishino et al. (2010), we calculated that the excess electron density is in the order of 10-4cm-3.
That is four orders of magnitude lower than the value inferred from RPWS and MAG data.
The signal of higher energy ion channels of LEMMS is featureless, indicating that this value is an upper limit and the assumption for a macroscopic quasi-neutrality in the wake is reasonable.
The latter conclusion implies that one of the two RPWS plasma density measurements for the R1 wake crossing is more reliable.
We believe that this is the fUH electron density dataset.
Ion density extraction from the LP has several systematic errors, large uncertainties (more than 50% relative error for the low plasma densities observed at L-shells greater than 7) and its estimation is also based on a series of assumptions (Jacobsen et al., 2009).
Furthermore, the agreement between the fUH electron densities and the density profile of CAPS/ELS for flyby R2 (Fig. 6) adds further support to the validity of that specific density profile.
We therefore find that the cold plasma density in the wake is equal or comparable to the ambient magnetospheric plasma density.
This sets constraints on the exact nature of the instability that is inferred from energetic electron observations (Section 5.3).
The instability that drives the interchange motions cannot be controlled by density gradients.
The flute instability, however, can also be driven by plasma pressure (or magnetic field) gradients (Boyd and Sanderson, 2003).
Magnetic field gradients were also proposed as one of the possibilities to explain the flute modes in the expanding AMPTE cloud (Bernhardt et al., 1987).
MAG data from all flybys indicate that such gradients are indeed present and it is therefore reasonable to explore this concept in future studies.
Since density levels in the wake are comparable to the ambient ones, lossless transport may alternatively occur through the a Kelvin-Helmholtz (KH) instability, if velocity shear between the two domains is significant.
We still, however, do not believe that the small scale features in energetic electrons are signatures of such a process, even if the KH instability occurs.
The presence of those signatures almost upstream of Rhea and up to 6RRh from the moon (flybys R2 and R3) would imply the presence of KH vortices in a rather unexpected location and with an unrealistically large scale compared to the width of a region where velocity shear may potentially develop.
Plasma velocity vectors available for R1 and R1.5 also show nothing relevant (Wilson et al., 2010).
It is also important to understand how conditions in the wake allow for a plasma pressure gradient, while the plasma density is unchanged.
One scenario is that there is a cold plasma source composed of exospheric electrons and ions.
To estimate the plasma density from such a process, we repeated the hybrid simulations of Simon et al. (2012), adding an exosphere with the characteristics described in Teolis et al. (2010).
Peak exospheric densities at the R2 and R3 flyby altitudes or the surface did not exceed the level of 0.2cm-3, while for the R1 altitude this level was at 0.01cm-3, orders of magnitude lower than the ∼4-6cm-3 observed with Cassini.
The presence of an exosphere can therefore not explain the observations of ambient plasma density levels in Rhea's wake.
A second possibility is that the wake receives accelerated or escaped photoelectrons from Rhea's charged surface.
Unlike Earth's Moon, the wakeside hemisphere of Rhea (or part of it) can be immersed in sunlight and supply the wake directly with photoelectrons, given also that a large fraction of Rhea's surface is expected to be charged negatively (Roussos et al., 2010).
Data from flyby R2 support this idea, although observations in R3 do not show any electron density enhancement when Cassini crosses to the moon's sunlit hemisphere.
Using equations of Colwell et al. (2005) and assuming a photoelectron temperature of 1-2eV, we estimate a peak surface density (subsolar region) of photoelectrons between 35 and 55cm-3.
This value decays with increasing solar zenith angle, reaching the 50% level at 60°.
The surface potential profile will determine what fraction of the photoelectrons may escape (explaining possibly the complex electron density profile during R3), but in principle, the abundance of photoelectrons appears sufficient to supply the observed densities in Rhea's wake.
If this process is taking place, photoelectrons will accelerate ions in the wake in order to balance the excess negative charge.
Ambipolar electric field acceleration is more effective along the magnetic field.
This means that ions attracted towards the wake would carry little perpendicular pressure.
This may then explain why the magnetometer still observes an enhancement in ∣B∣.
The observation of very low count rates (or partial densities) for the perpendicular component of the ion velocity distribution during flybys R1 and R1.5 (Wilson et al., 2010) indicates that indeed the bulk of the ion density should be contained at small pitch angles.
If an exospheric source was dominant, count rates in CAPS would have been considerably higher.
Extraction of CAPS/IMS ion moments for flybys R2 and R3 (where the angular coverage of the velocity distribution is better) is therefore essential for exploring the aforementioned scenario.
Summary, open questions and outlook
Energetic particle, magnetometer and electron density data from the four closest Cassini flybys of Rhea to date were reviewed in an attempt to understand the processes under which a series of flux decreases appear in energetic electrons.
The most interesting observation concerns the simultaneous presence of narrow dropouts in energetic electron fluxes, visible usually near the wake flanks with narrow flux enhancements within the wake boundaries.
Liouville's theorem was found to hold for these features, when Phase-Space-Density losses or gains were compared at the locations of the dropouts and spikes, respectively.
This result is consistent with a process of lossless rapid transport of energetic electrons from the magnetosphere to the wake.
It appears more likely that the transport is driven by a form of instability, rather than complex energetic electron drifts due to disturbed flows and magnetic fields downstream of Rhea.
These drifts may primarily explain better the broadening of the central wake with increasing energy, although a contribution to this observed pattern may also come from the merging of small scale depletions at high electron energies (typically above 60-100keV, depending on the flyby).
The instability is probably driven by pressure and magnetic field gradients in the wake.
Density gradients in the wake are surprisingly absent, a fact that we relate to the importance of surface charging for Rhea's magnetospheric interaction.
Deviation from quasi-neutrality, is probably not more than 10-4cm-3, based on energetic ion observations in R1.
Regardless of the explanation, we believe that the presence of a cold plasma source near Rhea is implied from the observations.
This aspect of the interaction appears qualitatively similar to what was observed at the jovian moon Callisto with the Galileo orbiter (Gurnett et al., 2000).
Simulating the evolution of plasma pressure gradients (expanding plasma clouds) using particle codes (Winske, 1989) would reveal if for the conditions of Rhea's plasma wake (plasma beta near unity, zero density gradient, strong parallel pressure component in the "wake") a fast growing instability develops, with characteristics described in previous sections.
Alternatively, properties of the RPWS spectrograms may be compared to theoretical expectations for wake-driven instabilities as in Farrell et al. (1997).
The analysis presented in the previous sections was subject to a series of limitations.
The small number of Rhea flybys until now makes comparisons challenging, given that the flyby trajectory, the instrument pointing and the magnetospheric state are different in almost each case.
If the role of surface charging is important, as hinted by some observations, then also the magnetospheric local time of the flyby may become a controlling parameter for the interaction region's structure and dynamics.
Equally interesting for future studies is the identification of broad energetic electron dropouts - an observation not extensively analyzed in this study.
These depletions are clearly seen only in flybys R1 and R2.
A distinct difference between the dropouts in the two cases is the opposite energy dependence of the depletion depth and the lack of symmetry in the depletion profile, for R2, with respect to the position of the wake.
Less intense dropouts, but qualitatively similar to those in R1, are seen in flyby R1.5, and much weaker ones in R3.
Observed deviations from the typical, plasma-absorbing interaction region profile may explain some differences.
These deviations do not necessarily mean the main interaction mode at Rhea is not plasma absorption.
For instance, Simon et al. (2012) demonstrates that the combination of low magnetosonic Mach number and the high plasma beta values of Rhea's space environment amplifies interaction features which for other plasma absorbers (e.g. Tethys, Dione, Earth's Moon) are barely detectable.
One of these features is a stronger, flow-aligned magnetic field component perturbation, which gives rise to mass-loading-like interaction signatures.
Such structures may also complicate electron drifts, but to what extent it is uncertain.
Similarly, the high plasma beta makes surface charging more important for Rhea compared to the other Saturnian moons (Roussos et al., 2010).
Furthermore, recent developments in the study of Earth's Moon interaction with the solar wind indicate that the standard picture for a lunar-type interaction may be too simplified: processes, such as the entry of exospheric pick-up ions in the center of the wake, backscattering on the surface and self-pick up of ambient plasma ions, appear to also have an impact on the wake dynamics (Halekas et al., 2011).
For instance, the self-pick up process has been shown to lead to enhanced ULF wave activity in the lunar, at least 10% of the time (Nakagawa et al., 2012).
Whether a similar process is important the inferred instability at Rhea is questionable, as the latter appears to operate continuously.
Understanding the origin of the broad energetic electron flux dropouts was also significantly limited by the narrow pitch angle pointing of LEMMS.
So far, the only case where extended pitch angle coverage has been achieved was during the distant flyby of Voyager 1 through Rhea's absorption signature (microsignature), three degrees in longitude downstream of the moon.
Observations showed that the flux dropouts are present in a broad region, much greater than Rhea's diameter.
In addition, that depletion was wider for equatorial particles compared to that of the more field-aligned particles.
The pitch angle dependence observed with Voyager 1 could be considered in order to explore whether the possibility that the decreases form due to scattering from whistler waves is applicable (Santolı´k et al., 2011).
The ideal pitch angle coverage of CAPS/ELS for flybys R2 and R3 may also be used to study pitch angle distributions at the 20-30keV overlapping energy range with LEMMS.
Furthermore, distant flybys of Cassini from Rhea within few degrees from the moon should also be investigated, as the may contain additional information about the nature of the interaction.
Beyond these striking features identified in few flybys, there was a large number of additional unique observations.
For instance, energetic electron data in R1.5 confirm observations of Wilson et al. (2010) of a local, radial velocity component in the moon's interaction region (also observed during R1).
Signatures of fresh, interchange events were seen in R1 and R1.5, possibly also at R3.
Furthermore, localized gradients in ∣B∣ were seen during the crossing from the shadowed to the sunlit hemisphere of Rhea (R2 and R3).
What is unclear in most cases is why these features do not appear consistently (or with consistent characteristics), and/or whether they are related to Rhea's interaction, or they are simply magnetospheric.
Additional, multi-instrument investigations and interaction simulations are essential all possible scenarios and their implications for the structure dynamics of Rhea's magnetospheric interaction.
Acknowledgments
The German contribution of the MIMI/LEMMS Instrument was in part financed by the German BMWi through the German Space Agency DLR under Contracts 50 OH 0103, 50 OH 0801, 50 OH 0802, 50 OH 1101 and by the Max Planck Society.
G.H.J. is supported by UK Science and Technology Facilities Council Advanced and Postdoctoral Fellowships, respectively.
H.K. acknowledges financial support by the Deutsche Forschungsgemeinschaft (DFG) under Grant MO539/19-1.
The hybrid simulations were performed on JUROPA supercomputers at the Jülich Supercomputing Centre, Forschungszentrum Jülich (Germany).
We thank Andreas Lagg and Markus Fränz (MPS) for extensive software support, Martha Kusterer and Jon Vandegriff (both JHUAPL) for reducing the MIMI data, Gethyn Lewis and Lin Gilbert (both MSSL/UCL) for reducing the CAPS/ELS data and Michelle Thomsen (LANL) for useful discussions.
Details on the Phase Space Density calculations
Here we provide some additional details about conversion of fluxes to Phase Space Densities, as this was required to test Liouville's theorem for data collected near and within Rhea's wake (Section 5.1).
Since PSDs were calculated at constant first adiabatic invariant, fluxes had to be evaluated at a different energy for each point that we had Cassini observations.
In order to derive the electron intensity j(E(t),t) at the changed energy, we used two methods.
In the first, we used high energy resolution spectral information from the PHA channels to extract the flux at the new energy E(t).
A potential problem of this method may arise from the relatively poor time resolution of the PHA channels with respect to the few second duration of the small scale enhancements and dropouts (the central features of this analysis).
This could lead to undersampling and an underestimation of the enhancement or dropout amplitudes.
For that reason, in the second method we used measurements at the 36 and at 229keV levels of the high time resolution LEMMS channels (channels C1_PRIO and C5_PRIO), and interpolated linearly between these energies on a log-log scale in order to find j(E(t),t).
The disadvantage of this method is that the assumed power-law spectral shape is only a rough approximation, given the shape of the energy-flux spectra taken with the PHA channels in that region (Fig. 9, bottom panel).
After we extract j(E(t),t) (with either of the two methods), we convert the differential intensity j to Phase Space Density f=j/p2, with p the kinetic momentum.
We then measure the amplitude of the PSD losses/enhancements at the locations of the narrow dropouts/spikes.
Comparison of the values is shown in Fig. 9 (top panel).
The indicated values were calculated using the first method described above.
The second method gave nearly identical results, meaning that the influence of the channel effects discussed in the previous paragraph is insignificant.
Details on the energetic electron tracings
Here we provide additional information regarding the calculations presented in Section 5.2, where we show guiding center tracing of energetic electrons in a simulated interaction region of Rhea.
The use of the guiding center approximation is justified since the gyroradius of energetic electrons is much smaller than Rhea's diameter (15-35km for electrons between 20 and 100keV) or the scale size of the various macroscopic interaction features (wake, expansion fans).
Furthermore, field parameters in the simulation are static.
The total drift velocity vector of equatorially mirroring electrons is given by the following equation:(B.1)v=vcorotE×B+vRheaE×B+vmag∇B+vRhea∇B
The first two terms are energy and charge independent E×B drifts, and represent the corotation and plasma flow perturbations, respectively.
They dominate at energies below about 5-10keV, depending on the percentage of subcorotation at Rhea and the intensity of local magnetic field gradients.
The other two contributions are energy dependent and represent drifts along the local ∇B×B direction.
The third term represents drifts due to magnetospheric field gradients and it is always opposing corotation (drift towards -x).
The last term may lead to drifts in all directions, depending on the location in Rhea's interaction region.
The sum of the last two components becomes dominant at energies above about 80-100keV for equatorially mirroring particles.
For intermediate energies, contributions from all terms are considerable.
We solved Eq.
(B.1) using numerical integration.
Numerical integration of Eq.
(B.1) requires inputs for each of the four terms.
For the calculation of the third term, vmag∇B, we require a description of the equatorial magnetospheric field.
For this purpose, we applied a linear fit to the magnetospheric ∣B∣ profile as a function of y-position in the RHIS.
A linear fit is applicable since the region of interest is very narrow (∼15RRh wide) compared to the magnetospheric scales and at Rhea's location ∣B∣ changes slowly with distance, as it is also visible in Figs.
2-8.
The field gradient is between 0.19 and 0.27nT/RRh in the y-direction.
For reference, in a dipole field the value is about 0.13nT/RRh.
From that, we used an average of 0.23nT/RRh in the simulations.
Inputs for the magnetic field perturbations and the plasma flow at Rhea are extracted from new hybrid code simulations of Rhea's magnetospheric interaction (Simon et al., 2012).
These simulations are similar to those of Roussos et al. (2008), utilizing an updated version of the hybrid simulation code, called AIKEF (Adaptive Ion-Kinetic Electron-Fluid) (Müller et al., 2011; Kriegel et al., 2011).
The sum of the first two terms of Eq.
(B.1) vcorotE×B,vRheaE×B is directly extracted from these simulations.
The simulated magnetic field perturbations are important for the last term of Eq.
(B.1) vRhea∇B.
Since gradient drift terms are proportional to the electron energy, high energy electron tracings can become extremely sensitive even to weak numerical noise in the ∣B∣ output.
To reduce the noise we averaged the output from the final steps of the simulation and also applied a Lee-filter in the xy-map of ∣B∣.
Derivatives in ∣B∣ were calculated numerically using a three-point interpolation.
The spatial scales across which derivatives were estimated were at least three times greater than the scale size of noise features (∼0.1RRh).
This ensured that the residual noise had the minimum possible effect in our tracings.
Since we simulated equatorial mirroring particle drifts, we also set Bx and By to zero (Roussos et al., 2008).
After the ∣B∣ map is constructed, we calculate the two gradient drift terms vmag∇B,vRhea∇B using a relativistically correct formula (Northrop, 1963):(B.2)v∇B=(γ2-1)γmec22q|B|3(B×∇B)Here, γ is the Lorentz factor (containing information about the electron energy), me is the electron mass, q is the electron charge and c is the speed of light.
The use of relativistically adjusted equation for electrons is necessary since for energies between 20 and 100keV, for instance, β=uc (or γ) gets values between 0.27 (1.17) and 0.55 (1.48), respectively.
This corresponds to a correction factor for the electron kinetic energy between 5% (20keV) and 22% (100keV).
Integration of Eq.
(B.1) was carried out with a fourth order Runge-Kutta method.
A series of test runs were performed in order to find the maximum allowed time step per electron energy.
Integration was done on the equatorial (xy) grid of the hybrid simulation.
The hybrid code uses adaptive grid size, but output was interpolated on a uniform cartesian grid with a resolution of about 0.084RRh in each direction.
Field and flow values between the grid points were estimated by interpolation.
At each time step, conservation of μ was enforced, adjusting the initial energy of the electrons, when these moved across regions of different ∣B∣.
Simulations of Rhea's magnetospheric interaction, with the presence of an exosphere
These simulations were carried out in order to determine the peak ionospheric densities in Rhea's interaction region and understand whether the weak exosphere of Rhea is responsible for the lack of a plasma density dropout in the wake.
For the simulation, the setup was similar to what is described in Simon et al. (2012), who, however, did not include an exosphere in the calculations.
The neutral exosphere we added has a profile similar to that given in Saur and Strobel (2005) and Simon et al. (2011) and its composed from molecular oxygen.
Neutral densities are scaled by the total number of oxygen particles (N=2.5×1029), given in Teolis et al. (2010), corresponding to a surface density of 3.4×105 cm-3.
For the ionization of neutrals, we assumed that charge-exchange dominates with a rate of 1.7×10-8cm-3s-1 (Simon et al., 2011).
Photoionization was not included since the rate is negligible compared to that of charge exchange.
The simulation shows that since the ionospheric particle production rate is low, the ionosphere does not act as a barrier to the upstream plasma flow.
The corotation electric field penetrates down to the surface of Rhea and all exospheric particles are immediately picked-up, before they accumulate to large numbers around Rhea.
At steady-state, peak ionospheric densities at the surface of Rhea are at least an order of magnitude lower compared to the electron densities measured by RPWS (Fig. C.12).

Reduced graphene oxide/carbon nanotubes sponge: A new high capacity and long life anode material for sodium-ion batteries
Graphene oxide (GO) was obtained from commercial graphite by a modified Hummers method, which has been reported in our previous work [28]. For the synthesis of rGO sponge (GS) and GCNTS, in a typical process, CNTs (Nanotech Port Co. Ltd, Shenzhen, China) aqueous dispersion was added into a vial containing GO aqueous dispersion. In this step, the weight ratio of GO to CNTs was changed by adjusting the volumes of GO solution and CNTs solution. The mixed solution was stirred under magnetic stirring, and then the vial was placed into a freezer. After the mixed solution was frozen, the vial was moved to a freeze-dryer (-53 degC, pressure < 10 Pa) for 3 days to obtain the GCNTS. Finally, GCNTS was calcined at 800 degC for 3 h in nitrogen atmosphere. GS was also prepared for comparison according to similar procedure without the use of CNTs.The influence of the high Fe and Cr contents of Alloy 800 on its inter- and intragranular oxidation tendency in 480degC hydrogenated steam
Alloy 800 was purchased from Rolled Alloys Inc. as a 1.3 mm thick plate; Table 1 reports the composition of the material. The material was cut into flat coupons measuring 1 cm by 1 cm. Coupons were solution annealed at 1050 degC in Ar-5% H2 gas for one hour and immediately water quenched to ensure complete dissolution of carbides and homogeneous composition.
Samples were ground with 400, 800, and 1200 grit paper. Following this, coupons were fine polished using diamond paste (9 μm, 3 μm, and 1 μm) and then with alumina suspension to a 0.05 μm finish. Coupons were ultrasonically cleaned in ethanol and de-ionized water for 10 minutes each between polishing stages and dried with air. A desiccator was used to store samples until time of exposure.
Palladium-Schiff base-triphenylphosphine catalyzed oxidation of alcohols
All the chemicals used were of analytical grade. Solvents were purified and dried according to standard procedures.21 Anhydrous PdCl2 was purchased from Merck and was used without further purification. [PdCl2 (PPh3)2] was prepared by the reaction of anhydrous PdCl2 (CDH) and triphenylphosphine in tetrahydrofuran (Merck) under reflux for 5 h.22 The Schiff bases were prepared in 70-80% yield by condensation reactions of 2-hydrazinopyridine (Aldrich) with the corresponding 5-substituted salicylaldehyde (Loba) in methanolic media.23
Complexes PdL1-PdL5 were prepared by stirring a mixture of [PdCl2 (PPh3)2] in 3 ml of 0.1 M sodium acetate and the respective ligands in 15 ml alcohol in a 1:1 ratio for 5 h. The red solid was filtered off, washed with ethanol and dried in vacuo.Enhancing phase stability and kinetics of lithium-rich layered oxide for an ultra-high performing cathode in Li-ion batteries
We synthesized LLO doped with F, which exhibited enhancing phase stability and Li ion kinetics by our computational investigation, and measured its electrochemical performance. F-doped LLO formulated as Li1.17Ni0.17Co0.17Mn0.50O2-zFz (0 <= z <= 0.1) was prepared as follows: the starting materials of the precursor Ni0.2Co0.2Mn0.6(OH)2 were prepared from a 2:2:6 molar ratio of Ni/Co/Mn sulfate-containing solution using a co-precipitation reactor. The precipitate was continuously rotated at 600 rpm at 50 degC, and the pH was maintained at 11.5 by controlling the concentration of NH4OH. The product was repeatedly rinsed with de-ionized water, filtered and dried overnight. Li2CO3, LiF, and the precursor powder corresponding to the desired stoichiometry were mixed and thoroughly grounded. The resulting mixtures were pelletized and calcined at 700 degC in a stream of dried air for 10 h.Superior performance of mesoporous tin oxide over nano and bulk forms in the activation of a carbonyl group: conversion of bio-renewable feedstock
The nano tin oxide was synthesized by the method as reported in the literature.34 In a typical procedure, 11.3 g of SnCl4*5H2O was dissolved in 25 ml of distilled water followed by the addition of an aqueous NaOH solution (7.5 g NaOH + 25 ml of H2O). To the above solution, 2.5 g of CTAB was added with continuous stirring for 1 h. The precipitated solution was hydrothermally treated at 150 degC for 4 h using a Teflon-lined autoclave. The obtained precipitate was filtered, washed with distilled water and dried at 120 degC. The as-formed white solid was calcined to 400 and 500 degC. Bulk tin oxide was also prepared by the same procedure as given above without using the template. The tin oxide samples were designated as M-Y, where M stands for the meso, nano or bulk type material and Y is the calcination temperature.Facile method to synthesize a carbon layer embedded into titanium dioxide nanotubes with metal oxide decoration for electrochemical applications
TiO2 nanotube layers were prepared by electrochemical anodization of Ti. For this, Ti foils (0.1 mm thickness, 99.6% purity) were firstly degreased by sonication in ethanol, followed by thoroughly rinsing with deionized (DI) water and drying with a nitrogen stream. The Ti foils were used as the working electrode, and a platinum gauze served as the counter electrode. After the anodization in an electrolyte of ethylene glycol (EG) containing 0.135 M NH4F and 1 M H2O at 50 V for 6 h, the samples were washed in a bath of DI water for 1 min to remove the organic electrolyte on the sample surface and then annealed in an argon (Ar) atmosphere at 450 degC for 1 h. To prepare the anatase TiO2 NTs without carbon grafting, the as-prepared samples were dipped into ethanol for 24 h, and then annealed in air at 450 degC for 3 h.Aerial oxidation of p-isopropyltoluene over manganese containing mesoporous MCM-41 and Al-MCM-41 molecular sieves
The parent MCM-41 and Al-MCM-41 catalysts were synthesized according to the previous reports [20], [21] and [22] using C16H33(CH3)3N+Br- as the template. 0.3 M manganese acetate solution was prepared and mixed with 3 g of MCM-41 or Al-MCM-41 under constant stirring. The solution was dried under reduced pressure, and calcined in air at 550 degC for 6 h.Preparation of nitrogen doped TiO2 photocatalyst by oxidation of titanium nitride with H2O2

In a typical synthesis, 500 mg TiN (99% Hefei Kaier Nano Company (China)) was dispersed in 80 mL of the different concentrations of H2O2 solution, and then the mixture was subject to 700 W ultrasonication for 10 min. After vigorously stirred for 2 h, the solid was transferred into a Teflon-lined autoclave (100 mL) and then heated at 170 degC for 24 h. After the hydrothermal treatment, the precipitate was collected and washed with distilled water and absolute alcohol for three times, respectively. The product was dried in air at 110 degC for 24 h. The as-synthesized samples were denoted as NT-X (X denotes the weight concentration of H2O2 in solution, wt%).
Self-assembled graphene-wrapped SnO2 nanotubes nanohybrid as a high-performance anode material for lithium-ion batteries

The Sn nanorod templates were prepared via a simple NaBH4 reduction process using SnCl4 as a reactant and poly(diallyldimethylammonium chloride) (PDDA) as a soft template according to the previous report [41]. Then, the SnO2-NTs/G nanohybrid was synthesized through a facile electrostatic attraction directed self-assembly approach using Sn nanorods as templates. Typically, the pristine Sn nanorods and graphene oxide (GO) sheets (4:1 by weight) were dispersed in distilled water, stirred for 12 h, and then the GO matrix was reduced by NaBH4 (weight ratio of 10:1 for NaBH4/GO). After the reaction was finished, the resulting product was washed with distilled water and ethanol, yielding the SnO2-NTs/G nanohybrid.
Ambient dissolution-recrystallization towards large-scale preparation of V2O5 nanobelts for high-energy battery applications
In a typical synthesis, 100.0 g commercial V2O5 powder (Alfa Aesar) was mixed with a 1.5 L of NaCl (2.0 M) aqueous solution under vigorous stirring at room temperature (~25 degC). After 72 h, the solution changed to brownish suspension. The product was washed and separated by centrifugation-redispersion cycles with distilled water and ethanol, and then dried in an oven at 80 degC for 12 h. After that, the product was obtained in high yield (~99.2 g).1. A precursor for the preparation of a lithium transition metal oxide that is used for the preparation of a lithium transition metal oxide as a cathode active material for a lithium secondary battery through a reaction with a lithium-containing compound, wherein the transition metal has a composition of NixCoyMn1-(x+y)wherein 0.3 ≤ x ≤ 0.9, 0.1 ≤ y ≤ 0.6, and x+y ≤ 1, and wherein the precursor also contains sulfate ion (SO4)-containing salt ions derived from a transition metal salt for the preparation of the precursor at a content of 0.1 to 0.7% by weight based on the total weight of the precursor.2. The precursor according to claim 1, wherein the transition metal salt is sulfate.3. The precursor according to claim 2, wherein the sulfate is one or more selected from the group consisting of nickel sulfate, cobalt sulfate and manganese sulfate.6. A lithium transition metal oxide which is prepared by sintering the precursor of claim 1 and a lithium-containing material.Sulfur tolerant redox stable layered perovskite SrLaFeO4-δ as anode for solid oxide fuel cells

SLFO4 precursor was synthesized using a solid-state reaction method. Initially, La2O3 and SrCO3/Fe2O3 were preheated at 1000 and 300 degC for 6-8 h, respectively, to remove the absorbed moisture, and then stoichiometric amounts of La2O3, SrCO3, and Fe2O3 powders were thoroughly mixed by ball milling for 24 h and pressed into pellets at a pressure of 4 MPa. The pellets were calcined in air at 1100 degC for 10 h as SLFO4 precursor. The precursor was reduced in H2 for 2 h and subsequently grounded to obtain SLFO4 - δ final powders.
In situ investigations of Fe3+ induced complexation of adsorbed Mefp-1 protein film on iron substrate

Graphical abstract
Highlights
•
Pure iron was used as substrate for study of mussel adhesive protein adsorption.
•
Multianalytical in situ studies of the protein film forming and complexation.
•
Spectroscopy analysis was assisted by ab initial computational modeling.
•
Details of the film formation, structure, property, and compaction mechanism.
•
Tri-catechol/Fe3+ complexation leads to compaction of the surface protein film.
Abstract
A range of in situ analytical techniques and theoretical calculations were applied to gain insights into the formation and properties of the Mefp-1 film on iron substrate, as well as the protein complexation with Fe3+ ions.
Adsorption kinetics of Mefp-1 and the complexation were investigated using QCM-D.
The results suggest an initially fast adsorption, with the molecules oriented preferentially parallel to the surface, followed by a structural change within the film leading to molecules extending toward solution.
Exposure to a diluted FeCl3 solution results in enhanced complexation within the adsorbed protein film, leading to water removal and film compaction.
In situ Peak Force Tapping AFM was employed for determining morphology and nano-mechanical properties of the surface layer.
The results, in agreement with the QCM-D observations, demonstrate that addition of Fe3+ induces a transition from an extended and soft protein layer to a denser and stiffer one.
Further, in situ ATR-FTIR and Confocal Raman Micro-spectroscopy (CRM) techniques were utilized to monitor compositional/structural changes in the surface layer due to addition of Fe3+ ions.
The spectroscopic analyses assisted by DFT calculations provide evidence for formation of tri-Fe3+/catechol complexes in the surface film, which is enhanced by Fe3+ addition.

Introduction
In recent years, mussel adhesive proteins (MAPs) excreted from marine mussels have gained great interest in the scientific community for their film forming ability on various substrates.
Currently, at least five different proteins (Mefp-1 to Mefp-5: denotation for Mytilus edulis foot protein hereafter) from the blue mussels have been isolated and characterized [1,2].
Mefp-1 was the first one to be purified, and it is regarded as the primary component of the cuticle of Mytilid treads.
Mefp-1 contains a high level of peptides with catecholic functional groups present in the primary structure as peptidyl (3,4-di-hydroxypheny)-l-alanine (DOPA) [3-7,2,1].
The coherent accumulation of Mefp-1 and Fe3+ has been confirmed in the cuticle of Mytilid byssus, and Fe3+ is an integral part of the cuticle that provides exceptional stability due to formation of Fe3+/DOPA complexes [8-11].
DOPA is a bi-dentate, each Fe3+ ion can bind one, two, or three DOPA ligands, depending mainly on the pH and the Fe3+/DOPA ratio [12,13].
Complexation has been reported to occur at Fe3+ concentration as low as 10μM, and the Fe3+ ions provide significant and fully reversible intermolecular bridging of Mefp-1 layers on mica surfaces, forming multivalent tri-Fe3+/DOPA complexes [13].
Metal-mediated bridging of Mefp-1 films has considerable potential for its non-toxic and multifunctional properties.
Due to the increasing interest of industries, including biomedical and medical oriented industry, the detailed understanding of the complexation process of Mefp-1 and Fe3+ on different material surfaces becomes more crucial.
Some investigations of the Mefp-1/Fe3+ complexation have been performed in vitro [11,14,15], and Fe3+ induced bridging in Mefp-1 films on a model surface has been demonstrated by using ex situ techniques [13].
However, to the author's knowledge, no study has described Mefp-1/Fe3+ complexation on a base metal substrate using in situ analytical techniques.
We have recently explored the possibility to use Mefp-1 for corrosion protection of carbon steel [16-18], and a deep understanding of the protein film formation and complexation process on iron substrate is of great importance for improving the protection performance of the adsorbed protein film.
This study considers two issues related to Mefp-1 film formation on such substrates.
The first is the adsorption kinetics of Mefp-1 on an oxidized iron substrate, which is more complex but more relevant for corrosion protection applications than inorganic or organic model surfaces used in other studies [19].
The other issue considered in this work is the composition and properties of the Fe3+ bridged Mefp-1 film formed on the iron substrate.
To gain relevant information, we have utilized quartz crystal microbalance with dissipation monitor (QCM-D), in situ AFM, in situ attenuated total reflectance Fourier-transform infrared spectroscopy (ATR-FTIR), in situ Confocal Raman Micro-spectroscopy (CRM) for the measurements, and also density functional theory (DFT) calculations to assist the interpretation of spectroscopic data.
The QCM-D offers information on adsorption kinetics, adsorbed mass including trapped water, and the viscous and elastic properties of adsorbed layers [20].
This technique has previously been used for Mefp-1 studies on model silica surfaces [19,21], whereas an oxidized iron surface was used in this study.
In situ Peak Force tapping AFM measurements were performed to investigate the morphology and nano-mechanical properties of the adsorbed film.
Further, in situ ATR-FTIR and in situ CRM were performed on the iron substrate to obtain chemical information of the adsorbed Mefp-1 film before and after exposure to aqueous solutions containing Fe3+ ions.
Materials and methods
Protein and solutions
The Mefp-1 with a purity of 92% (impurity is mainly aggregation/degradation products of Mefp-1) was supplied by Biopolymer Products AB (Gothenburg, Sweden).
The details of the extraction and purification processes of Mefp-1 are described elsewhere [22].
Mefp-1 is a large and basic hydrophilic protein, having a molecular weight of about 108kDa and a high isoelectric point about 10 [23,24].
It is known that Mefp-1 is highly positive charged at neutral pH [25].
The Mefp-1 was delivered as a 25mg/ml aqueous solution containing 1wt.% citric acid and stored as received in dark at+4°C before use.
A solution containing 1wt.% citric acid, at pH 7-8, was prepared as the buffer solution, and the pH was adjusted by sodium hydroxide solution.
Just 3min prior to measurement, Mefp-1 was added from its stock solution to the buffer solution to give a concentration of 0.1mg/mL.
The 10μM FeCl3 solution was prepared by dissolving a small amount of FeCl3⋅6H2O in pure water.
All chemicals used were of analytical purity and aqueous solutions were made using pure water with resistivity of 18MΩcm as prepared by Milli-Q system.
Substrates
The substrate in QCM-D and in situ AFM measurements were quartz crystals coated with a 100nm thick iron film (Q-Sense, Gothenburg, Sweden).
These sensors have a fundamental resonance frequency of 4.95MHz.
The in situ ATR-FTIR measurements utilized an iron film ca.
30nm thick that was deposited on a ZnSe internal reflection element (IRE) by high-vacuum evaporation using a Univex 300 vacuum evaporator (Leybold Vaccum).
The evaporator was equipped with a quartz crystal microbalance thickness monitor to control the thickness of deposited layer.
For deposition of pure iron film, ultra-pure iron (99.99%) target were used.
A bare ZnSe crystal was also used as substrate for comparison.
For in situ CRM measurements, a pure iron sheet sample was wet ground with SiC grinding paper successively to 1200 grits and then cleaned ultrasonically with ethanol.
The nominal composition of the pure iron (Stena Stal AB, Slovak Republic) is 99.87wt.% Fe, 0.06wt.% C, 0.06wt.% N, and 0.014wt.% S.
To obtain a relatively stable substrate during the measurements, all iron substrates were pre-conditioned in a 10wt.% NaOH solution overnight to form a thin oxidized surface layer, followed by rinsing with 99.5% pure ethanol and drying with a gentle stream of nitrogen gas prior to use.
The alkaline treatment of the iron surface results in formation of hematite (α-Fe2O3), which will be shown and discussed in relation to Fig. 9.
The hematite surface contains different types of surface hydroxyl groups that differ by their coordination to the substrate [26].
The hematite surface is amphoteric due to the possibility of protonization and deprotonization of the surface hydroxyl groups.
In neutral solution, the ξ-potential has been found to be slightly positive [27,28].
The water contact angle on our hematite surface is 69±3°, and the open circuit potential relative to Ag/AgCl is -0.72V, which is similar to that of iron due to the small thickness of the oxide layer.
QCM with dissipation monitoring, QCM-D
A QCM-D instrument, which is a highly sensitive balance based on the measurement of changes in the resonance frequency of a quartz crystal oscillator, was used to study the film formation and complexation processes.
The instrument used was a Q-sense E4 microbalance (Q-sense, Gothenburg).
The temperature of the measuring cell was controlled at 25±0.02°C, and the resonant frequency of the oscillator (f) and the energy dissipation value (D) were recorded simultaneously as a function of time.
The baseline was determined using the buffer solution without protein.
The dissipation is a measure of the amplitude decay rate of the crystal oscillation when the voltage is switched off, which is defined as D=Ed/(2πEs), where Ed is the energy dissipated during one period of oscillation and Es is the stored energy.
The combination of dissipation measurement with the frequency monitoring allows the determination the adsorbed mass as well as viscoelastic properties of the protein layers, from which changes in the orientation or conformational state of the protein adsorbed may be inferred [29].
The Sauerbrey equation [30] is commonly used to obtain the sensed mass (m) from the change in resonance frequency (Δf).
It assumes that the adsorbed film is evenly distributed, rigid and thin, and for this situation the change in resonance frequency is only due to the mass of the adsorbed film, including water hydrodynamically trapped in the film.
However, the Mefp-1 film is soft (viscoelastic) and contains a high content of water [31], and therefore, evaluation based on the Voigt model is more accurate and informative since it utilizes frequency and dissipation data from multiple harmonics (overtones) and allows estimations of thickness, shear elastic modulus, and shear viscosity of the adsorbed film.
The model assumes that the bulk solution is purely viscous and Newtonian, and the adsorbed film is uniform, the viscoelastic properties are frequency independent in the utilized frequency range (15-25MHz), and that there is no slip between the adsorbed layer and the crystal during shearing [20,32].
The frequency change and the dissipation change are given as:(1)Δf=Im(β)2πtqρqand(2)ΔD=-Re(β)πftqρqwhereβ=ξf2πfηf-iμf2πf1-αexp(2ξ1tf)1+αexp(2ξ1tf)·α=ξ1ξ22πfηf-iμf2πfηb+1ξ1ξ22πfηf-iμf2πfηb-1,ξ1=-(2πf)2ρfμf+i2πfηf,ξ2=i2πfρbηbwhere μ is the elastic shear modulus, η is the shear viscosity, f is the oscillation frequency, t is the thickness, and ρ is the density of the adsorbed film.
The subscripts q, f, and b refer to crystal quartz, film, and bulk solution, respectively.
In this work, results at two overtones (third and fifth) were fitted to the Voigt model using QTools software v3.0.12.518.
The QCM-D measurements were repeated two times, and the same trend was observed.
In situ AFM imaging and nano-mechanical measurement
In situ Peak Force tapping mode AFM measurements were performed on iron QCM crystals (coated with iron) with a pre-adsorbed Mefp-1 film, immersed in buffer solution and FeCl3 solution, respectively.
The measurements provide nanometer resolution images of topography and nano-mechanical properties of the adsorbed protein films.
The AFM instrument (Multimode, Nanoscope V, Bruker) is equipped with a fluid cell with inlets and outlets allowing solution exchange.
Rectangular silicon cantilevers with spring constant of 0.7N/m and resonant frequency range of 120-180kHz (SCANASYST-FLUID+, BRUKER) were employed for the measurements.
The Peak Force AFM images were obtained using a controlled feedback force of 1.1nN, and quantitative information of several surface mechanical properties was obtained simultaneously [33,34].
The principle of the technique has been explained in the literatures [35,36].
In brief, (as shown in Fig. 1), the Peak Force AFM measurement allows determination of surface deformation as the difference between the tip-sample separation at the peak force and at a given percentage of the peak force (15% in this study).
The tip-sample adhesion force is determined from the difference between the zero force and the minimum force experienced during tip retraction, and the dissipated energy during one approach-retraction measurement cycle is evaluated from the area between the trace and retrace curve.
Moreover, combined with the Derjaguin-Muller-Toporov (DMT) model [37,38], the elastic modulus of the surface can be obtained from the force curve where the sample and the tip are in contact [35].
To obtain the correct DMT modulus values, the tip radius needs to be known.
This value can be obtained by calibration against a surface with known elastic modulus.
In our case, we used a polystyrene surface with a Young's modulus of 2.7GPa.
According to the calibration result, the spring constant was 0.7N/m and the tip radius was 10nm.
During measurements, the topographical image and the above-mentioned nano-mechanical property maps are obtained simultaneously from the force vs. separation curves, each collected in about 0.5ms.
The Nanoscope Analysis v.120 software (Bruker) was used for image analysis.
A second order polynomial flattening algorithm was performed to remove the titling slope of topography images, and the surface roughness parameters Rq and Ra were determined.
Rq is the standard deviation of the Z-values of the surface, calculated as:(3)Rq=Σ(Zi)2NAnd Ra is the arithmetic average of the absolute values of the surface height deviations:(4)Ra=1N∑j=1N|Zj|
All other images remained as originally obtained.
For the quantitative comparison of the nano-mechanical properties of the adsorbed Mefp-1 film before and after Fe3+ enhanced complexation, mean Z-values of the property data were calculated by the software.
The mean value for a quantity across an image is the average of the values obtained at each pixel and it is calculated as:(5)Mean value=1N∑j=1NZjwhere Zj is the local value at pixel j and N is the number of pixels.
In situ ATR-FTIR
The ATR-FTIR measurements were performed using a Varian 7000 spectrometer equipped with a linearized Mercury cadmium telluride (MCT) detector.
The ATR measurements were performed using a custom made grazing angle total reflectance accessory (GATRTM from Harrick), with a removable hemispherical IRE and with incident beam angle of 65°.
The absorbance spectra were obtained as an average of 600 scans across the wavenumber region 4500-600cm-1, with a resolution of 8cm-1.
The ATR-FTIR spectra were recorded using the Varian Resolution Pro software 4.3 as single beam spectra before and after introduction of buffer solution and then also after different times during the exposure to solutions containing Mefp-1 or FeCl3.
The spectra are presented in the absorbance format, -log(R/R0), where R is the reflected intensity of the sample and R0 is the reflected intensity of the background, and obtained by taking the ratio of a single beam at a certain exposure time in Mefp-1 and FeCl3vs.
a single beam spectrum obtained in the buffer solution.
In situ CRM analysis
In situ CRM measurements were performed using an iron substrate in the Mefp-1 solution and also using such surfaces with a pre-adsorbed Mefp-1 film in the FeCl3 solution.
The CRM instrument was a WITec alpha300 system (WITec GmbH, Germany), equipped with a 532nm laser for excitation, and an oil immersion objective with 100×magnification and numerical aperture (NA) of 1.25.
The optical parameters give a lateral resolution of 260nm.
During a CRM measurement, a droplet of the liquid was placed on the substrate and covered by a microscope glass slide, which gave an approximate thickness of the liquid layer of 10μm.
Raman spectra were recorded with an integration time of 0.2s, and the data were analyzed using the WITec Project 2.06 software.
DFT calculations
Density functional theory (DFT) calculations within the Kohn-Sham formalism [39] were performed for a number of model compounds that included the catechol unit of the DOPA residues as well as parts of the protein backbone.
We also studied complexes between a Fe3+ ion and one or more catechol units and water molecules.
All computations were performed using software Gaussian 09 suite [40].
The model compounds were subjected to geometry optimizations followed by computations of time dependence-DFT electronic absorption spectra, IR spectra, and pre-resonance Raman spectra.
In order to produce reliable resonance Raman spectra, computations need to reproduce any electronic absorption bands that lie in the vicinity of the excitation wavelength used in the CRM experiment.
We tested different exchange-correlation functionals and found the recently developed long range corrected ωB97X functional [41] to be best suited to reproduce the charge-transfer bands found in complexes between Fe3+ and catechols.
The LACVP+basis set, which corresponds to the 6-31+G(d) basis for first and second row atoms and an augmented LANL2DZ ECP-basis set for heavier atoms, was used for all computations.
Vibrational spectra were computed within the harmonic approximation, and the obtained vibrational frequencies need to be scaled down in comparison with experimental spectra.
This is the standard procedure when comparing theoretical and experimental spectra, since the computed frequencies are always too large due to the lack of anharmonicity corrections.
In addition, the harmonic potentials are sensitive to electron correlation, and the scaling factor will differ depending upon the exchange-correlation functional and the basis set used.
For the ωB97X/LACVP∗+ level of theory, a scaling factor for the vibrational frequency of 0.94-0.95 was found to give good agreement between theory and experiment for most vibrational bands.
The exception was vibrations involving Fe-O bonds for which a scaling factor closer to unit was more appropriate.
Results and discussion
Kinetic aspects of layer formation and complexation
The adsorption of Mefp-1 on an oxidized iron surface was followed in real time using QCM-D as illustrated in Fig. 2.
The sensed mass determined with QCM-D includes the mass of the adsorbed protein and that of water that is hydrodynamically coupled to the layer.
This mass will be referred to as the "sensed mass" in this article to distinguish it from the adsorbed mass of the protein.
The exposure to the Mefp-1 solution leads to initial rapid adsorption, as expected for a protein showing high affinity for the adsorbent.
After this initial rapid adsorption, a slower increase in the sensed mass is observed, suggesting a continuous build-up of the Mefp-1 layer.
Since the substrate is a pre-conditioned iron surface that is not as stable as a model silica surface, the adsorption of Mefp-1 was interrupted after 10min to prevent non-negligible corrosion of the substrate within the period of measurement.
The protein solution was removed by injection of a protein free 10μm FeCl3 solution into the cell, which allows the investigation of Fe3+ enhanced complexation of the pre-adsorbed protein layer.
As shown in Fig. 2, the exposure of the adsorbed layer to the FeCl3 solution leads to a decrease in the sensed mass, which can arise from removal of some loosely bound Mefp-1 and/or some of the coupled water in the protein layer.
Further information can be obtained from the change in the dissipation (ΔD) values.
For a soft film, the decay time of the oscillation in the quartz resonator is small, and the dissipation value is high, whereas for a rigid film, the dissipation value is smaller.
The plot of the change in dissipation (ΔD) as a function of the change in frequency (Δf) is quite informative [42] and can be used for visualizing structural changes in adsorbed layers.
Such plots in Fig. 3 illustrate the changes in the Mefp-1 adsorption step and the subsequent rinsing step with FeCl3.
During Mefp-1 adsorption (Fig. 3a), ΔD initially increases linearly with decreasing frequency.
However, the slope of the curve increases when Δf exceeds about 25Hz.
This clear increase in the slope suggests that the film becomes more extended as the adsorption approaches its equilibrium value.
The transition range in the ΔD-Δf curve corresponds to the transition range between the fast and slow adsorption regime marked with arrow "a" in Fig. 3a.
This structural change may be explained as being due to increased repulsion between neighboring molecules at higher surface coverage.
During the exposure to the 10μM FeCl3 solution, Δf becomes less negative with time (Fig. 3b), which indicates that complexation between DOPA groups in Mefp-1 and Fe3+ causes significant removal of water from the adsorbed protein layer.
The ΔD-Δf curve obtained in the FeCl3 solution (Fig. 3b) shows that ΔD first decreases linearly with deceasing Δf, indicating formation of a more compact layer.
This process is rapid as seen by the drop in the thickness and the sensed mass shown in Fig. 2.
However, after the point marked as "b" in Fig. 3b, ΔD increases somewhat, a feature that could be due to increasing net positive charge of the Mefp-1/Fe3+ surface film as compared to the Mefp-1 layer prior to Fe3+ addition and thus caused by increased electrostatic repulsion within the layer.
Based on the increased mass and the assumption that the surface was relatively stable within the measurement time, the Voigt modeling gives values of the average film thickness of the order of 7nm and 4nm for the Mefp-1 layer before and after exposure to the FeCl3 solution, respectively.
However, in reality, the adsorbed Mefp-1 layer on iron substrate is not uniform at micro scale, as revealed in the AFM measurements below.
Structural and nano-mechanical properties of the adsorbed Mefp-1 films
In situ nano-mechanical mapping was performed on an iron substrate with a pre-adsorbed Mefp-1 layer, immersed in the citric acid buffer solution and the 10μM FeCl3 solution, respectively.
The topography images are shown in Fig. 4, presenting the starting surface in the buffer solution at pH 7-8 (Fig. 4a).
Since the iron surface was pre-conditioned in 10% NaOH, the surface mainly consists of Fe2O3 as confirmed by the CRM measurement.
The Rq value is 3.91nm over a 500nm×500nm scanned area.
The topograpical images of the Mefp-1 layer adsorbed on the iron surface in the buffer solution and in the FeCl3 solution are shown in Fig. 4b and c, respectively.
The adsorption of the protein leads to a smearing of the features seen on the surface prior to the Mefp-1 adsorption, compare Fig. 4a and b.
By comparing the Z-range of the surface with and without the Mefp-1 film (Table 1), it is seen that the adsorption of Mefp-1 does not lead to any significant change in the height variation, which indicates a full coverage of the Mefp-1 film on the surface.
Fig. 4c presents the topography of the Mefp-1 film exposed to the FeCl3 solution; by comparing with Fig. 4b and the Z-range in Table 1, it is observed that the introduction of Fe3+ induced a negligible decrease in the height variation, which remains predominantly due to the roughness of the substrate surface.
In summary, all these results indicate that the Fe3+ enhanced complexation increases the stiffness of the Mefp-1 film, which results in a denser film.
In this senses, the Peak Force tapping AFM observations are consistent with the decreased dissipation value and thickness observed in the QCM-D measurements.
Chemical information of Mefp-1 films before and after complexation
Mefp-1 film formation on iron substrate
ATR-FTIR was employed for obtaining in situ the chemical information of Mefp-1 films formed on the oxidized iron surface.
After the addition of the Mefp-1 into the buffer solution, the IR spectra were recorded after different time of exposure, as shown in Fig. 6a.
The negative band in the region around 3050-3650cm-1 is the combined result of negative water bands due to decreased water content in the vicinity of the iron oxide covered ZnSe crystal and positive O-H and N-H stretching bands from Mefp-1 following its adsorption on the surface.
The spectra also show positive bands in the region around 2800-3200cm-1 due to amino acid zwitterions (H3N+-CH-CO2) and C-H stretching vibrations from the adsorbed Mefp-1 molecules[43,44].
The part of the spectra below 1700cm-1 is similar to the IRAS spectrum of non-crosslinked Mefp-1 absorbed on carbon steel that was analyzed in our earlier study [17].
The bands that are similar for the two surfaces, oxidized iron and ZnSe, can be assigned to vibrations of the protein backbone and of the DOPA side chain without a need to consider the influence of the oxidized iron surface.
On the basis of our DFT calculations and our previous work we make the following assignments for the main bands.
The very strong band around 1640cm-1 is associated with the C-O stretching of amide groups of the backbone (Amide I).
The strong and broad peak around 1550cm-1 has contributions from two different types of transitions, i.e.
N-H bending of the backbone (Amide II) and aromatic C-C stretching of the catechol aromatic ring of the DOPA residue.
However, it should be noted that there are some clear differences between the spectrum of Mefp-1 adsorbed on oxidized iron surface (Fig. 6b) as compared to that obtained on the ZnSe crystal (Fig. 6c).
The spectrum of Mefp-1 on the iron substrate displays a strong peak at 1485cm-1 (Fig. 6b) that is entirely absent for Mefp-1 on the ZnSe surface (Fig. 6c).
Further, the shoulder at 1423cm-1 of the peak at 1450cm-1 (1454cm-1 on ZnSe) is only found for Mefp-1 on the iron substrate.
A more pronounced difference is the strong peak at 1258cm-1 present for Mefp-1 on the iron substrate, which is replaced by a much weaker peak at 1243cm-1 for Mefp-1 on the ZnSe surface.
Regarding the three peaks that are absent on ZnSe surface, i.e., the strong peaks at 1485cm-1 and 1258cm-1 and the shoulder at 1423cm-1, it can be noted that these peaks were also absent or much diminished in ATR-IR spectra for Mefp-1 absorbed on polystyrene and poly(octadecyl methaacrylate) reported by Baty et al. [45].
The 1485cm-1 and 1258cm-1 peaks were, however, present in our IRAS spectrum for Mefp-1 absorbed on carbon steel, but they disappeared upon oxidation of the DOPA residues [17].
The shoulder at 1423cm-1 could not be observed in the IRAS spectrum but that may be due to the lower resolution of that spectrum.
Thus, based on these observations, it seems clear that at least the 1485cm-1 and 1258cm-1 peaks are associated with catechol rings and their appearance requires the presence of Fe3+ in some form.
We performed DFT computations on different model compounds with catechol groups complexed to Fe3+, and our results suggest that the 1485cm-1 and 1258cm-1 bands arise from a significant fraction of catechols (Cat) of the DOPA residues being complexed to Fe3+ in the classical Fe(Cat)3 fashion.
The computed IR spectrum for such a complex is dominated by two strong bands that, after scaling the vibrational frequency by a factor of 0.945, reproduce the locations of the 1485cm-1 and 1258cm-1 bands of the experimental spectrum (Fig. 7).
The computed intensity of each of these bands is one order of magnitude higher than that of bands that traditionally are considered as very strong, such as a carbonyl stretching band.
This extreme intensity enhancement is an effect of the large charge polarization of the [Fe(Cat)3]3- complex due to the strongly positive Fe3+ being surrounded by three negatively charged catechols.
Because of the charge polarization, vibrations that result in unsymmetrical distortions of the complex will induce large dipole moment changes and therefore have very large IR intensities.
The very large intensities of these peaks have the consequence that even if only a fraction of the catechol units are complexed to Fe3+ these transitions will show up in the IR spectrum.
Thus, it seems that the most proper explanation for the 1485cm-1 and 1258cm-1 bands is symmetrical coordination of three catechols to Fe3+.
In complexes with two catechols of the type [Fe(Cat)2(H2O)2]-, the theoretical spectrum has only one strong peak, at around 1260cm-1, but with an intensity that is a factor five lower than that in the Fe(Cat)3 complex.
The [Fe(Cat)(H2O)4]+ complex has no strongly enhanced peak.
The shoulder at 1423cm-1 cannot be explained from this type of symmetrical complexes but has been observed earlier in IR spectra of Mefp-1 upon addition of Fe3+[46].
Thus, it may arise from complexation between Fe3+ and catechols, but potentially from less symmetrical complexes.
In conclusion, our analysis strongly indicates that Fe(Cat)3 complexes are formed in the adsorbed Mefp-1 film due to the release of Fe3+ from the oxidized iron surface.
But the existence of other types of complexes between Fe3+ and catechol-groups cannot be discarded as such complexes may not result in clear fingerprints in the IR spectrum.
Fe3+ complexation of the adsorbed Mefp-1 film
Fig. 8 shows the in situ ATR-FTIR spectra of the oxidized iron surface with pre-adsorbed Mefp-1 film obtained after exposure to the 10μM FeCl3 solution for different times.
Comparing with the 15min Mefp-1 spectrum (Fig. 6a), the intensity of negative water band is relatively larger after exposing the Mefp-1 layer to the FeCl3 solution (Fig. 8a), which indicates that water is released from the pre-adsorbed Mefp-1 layer.
Therefore, it is reasonable to conclude that the addition of the Fe3+ makes the Mefp-1 film more compact, in agreement with the QCM-D and AFM results.
It should be noted that the spectra obtained with the addition of FeCl3 are remarkably similar to those without the FeCl3 addition but after sufficiently long exposure in the Mefp-1 solution, which indicates that Fe3+ could also be generated from the oxidized iron surface.
We note that the broad and unresolved 1540cm-1 peak is more clearly split into two separate peaks at 1550cm-1 and 1536cm-1 (Fig. 8b).
This is not surprising since the DFT analysis showed that the 1540cm-1 peak had contributions from both the backbone and the catechol unit, and that these bands may be influenced differently by the change in aqueous solvation when the water content of the protein film becomes reduced.
It should also be recognized that the effect of subtracting the water spectrum may influence the two spectra slightly differently in this region.
However, more importantly the peaks, at 1485cm-1 and 1260cm-1 (Fig. 8b), which we have associated with the complexation between catechol and Fe3+, remain largely unaltered both in terms of frequencies and intensities.
This supports our earlier hypothesis that Fe3+ could be generated from the oxidized iron surface during the exposure to Mefp-1 solution and forms [Fe(Cat)3]3- complexes..
The shoulder at 1423cm-1 (marked with arrow in Fig. 8b) seems to be slightly enhanced upon addition of FeCl3, and this could indicate some differences in the geometry of the complexes.
Both QCM-D and AFM results show compaction of the Mefp-1 film with introduction of the FeCl3 solution, whereas the ATR-FTIR spectra suggest the formation of same type of complexes with and without FeCl3 addition.
It could be that without the addition of FeCl3 most complexes are formed near the iron oxide surface since the Fe3+ that is released from the surface is immediately bound to the catechols.
However, with the addition of the FeCl3 solution, a much more enhanced uniform complexation takes place throughout the adsorbed protein film, leading to those changes detected by the QCM-D and AFM measurements.
In situ CRM analysis
CRM measurements were also performed with the oxidized iron surface.
Although CRM has lower signal sensitivity, the Raman spectra provide information over a larger wavenumber range than the ATR-FTIR spectra.
Moreover, the DOPA peaks present a resonance enhancement upon Fe3+ complexation in Raman spectra.
A single CRM spectrum of the oxidized iron surface is shown in Fig. 9, the main peaks which indicate that the dominant surface oxide is α-Fe2O3 spectrum [47], but the presence of minor amount of other oxides and/or hydroxides cannot be ruled out.
When the oxidized iron surface was exposed to the Mefp-1 solution, there is no visible Mefp-1 band on the CRM spectrum after 1min exposure (Fig. 10a).
After 10min exposure, some organic related bands can be observed in the range of 1200-1600cm-1 (Fig. 10a), suggesting an increase in the adsorbed Mefp-1 amount.
However, the bands are relatively broad, characteristic peaks of the Mefp-1 were not distinguishable, which is mainly due to the concentration of the protein solution being low (0.1mg/ml), so the amount of Mefp-1 adsorbed on the oxidized iron surface was not sufficient to be detected clearly by the CRM.
After 10min of exposure to the Mefp-1 solution, the oxidized iron surface was rinsed with the buffer solution and then exposed to a 10μM FeCl3 solution, which in principle may lead to a further decrease in the amount of the adsorbed Mefp-1.
However, in the FeCl3 solution, the resonance-enhanced peaks typical for Fe(Cat)3 complexes begin to appear in the Raman spectrum after a few minutes (Fig. 10b).
The same peaks could also be discerned from our reference calculation of pre-resonance Raman spectra of the [Fe(Cat)3]3- complex (Fig. 11).
In agreement with earlier assignments, we found that the resonance-enhanced peaks in the region of 460-700cm-1 are related to vibrations of the C-O groups bonded to the central Fe3+.
The peak at 547cm-1 has been assigned to δ,ν-chelate, and the peaks at 591 and 638cm-1 to νFe-O [48,49].
There are also several resonance-enhanced bands in the region of 1200-1500cm-1 that are due to vibrational motions of the C- and H-atoms of the catechol units.
Most pronounced is the band at 1496cm-1 that is the symmetric counterpart to the asymmetric vibrations that give rise to the peak at 1482cm-1 in the IR spectrum.
The resonance-enhanced bands are several orders of magnitude stronger than the normal bands, and in agreement with earlier studies, bands associated with vibrations of the protein backbone are too weak to be observed [18,50].
Interestingly, if the oxidized iron surface is kept immersed in the Mefp-1 solution, the Raman spectrum is gradually transformed into that obtained when the surface is exposed to the FeCl3 solution, and already after an hour's exposure, the two spectra are basically indistinguishable (Fig. 10c).
This confirms our hypothesis derived from the IR measurements that Fe3+ is slowly released from the oxidized iron surface in presence of a Mefp-1 solution, and this results in the formation of Fe(Cat)3-type complexes, similar to those observed when the pre-adsorbed Mefp-1 film is exposed to the diluted FeCl3 solution.
Conclusions
Through a pre-conditioning to produce a stable oxidized iron surface, we have studied the film formation of Mefp-1 on the iron substrate and Fe3+ enhanced complexation of the pre-adsorbed film, by using in situ techniques including QCM-D, Peak Force tapping AFM, ATR-FTIR, and CRM combined with DFT calculations.
The results from different in situ measurements provide a consistent picture, and the spectroscopic analyses are supported by theoretical calculations.
The combined data set leads to the following conclusions:
The QCM-D measurements demonstrate that Mefp-1 initially adsorbs rapidly onto the iron substrate, but the adsorption slows down as full surface coverage is approached.
A structural change in the protein layer toward a more extended structure occurs as equilibrium is approached.
The exposure of the pre-adsorbed Mefp-1 film to a 10μM FeCl3 solution causes a significant removal of trapped water in the film and results in a denser and stiffer surface film.
The Fe3+ induced structural changes in the pre-adsorbed Mefp-1 film result in marked changes in the nano-mechanical properties of the protein films, most notably an increased layer stiffness as quantified by the Peak Force tapping AFM measurements.
The spectroscopic results from the in situ measurements strongly suggest the formation of tri-Fe3+/DOPA complexes in the protein film during the exposure to both the Mefp-1 solution and the FeCl3 solution.
In the Mefp-1 solution, complexes are mostly formed near the substrate since the Fe3+ released from the surface is immediately trapped by the catechols in the adsorbed protein film.
However, the introduction of the FeCl3 solution to the pre-adsorbed Mefp-1 film leads to a more enhanced complexation throughout the protein film.
Acknowledgements
Financial supports from Vinnova in the project "Green Corrosion Protection Treatment", from 'Swedish Science Council', and from SSF in the program on "Microstructure, Corrosion and Friction Control" are gratefully acknowledged.
Biopolymer AB, Gothenburg, Sweden, is acknowledged for supplying the mussel adhesive protein and for valuable discussions.
Dr.
Andra Dedinaite and Xiaoyan Liu at the division of surface and corrosion science of KTH are acknowledged for the valuable discussion about the QCM-D analysis.
Dr.
Birgit Brandner at Chemistry, Materials and Surfaces of SP is acknowledged for help with CRM measurements.
Supplementary material
mentary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jcis.2013.05.016.
Supplementary material
Supplementary data 1
Details of QCM-D data fitting

In situ investigations of Fe3+ induced complexation of adsorbed Mefp-1 protein film on iron substrate

Graphical abstract
Highlights
•
Pure iron was used as substrate for study of mussel adhesive protein adsorption.
•
Multianalytical in situ studies of the protein film forming and complexation.
•
Spectroscopy analysis was assisted by ab initial computational modeling.
•
Details of the film formation, structure, property, and compaction mechanism.
•
Tri-catechol/Fe3+ complexation leads to compaction of the surface protein film.
Abstract
A range of in situ analytical techniques and theoretical calculations were applied to gain insights into the formation and properties of the Mefp-1 film on iron substrate, as well as the protein complexation with Fe3+ ions.
Adsorption kinetics of Mefp-1 and the complexation were investigated using QCM-D.
The results suggest an initially fast adsorption, with the molecules oriented preferentially parallel to the surface, followed by a structural change within the film leading to molecules extending toward solution.
Exposure to a diluted FeCl3 solution results in enhanced complexation within the adsorbed protein film, leading to water removal and film compaction.
In situ Peak Force Tapping AFM was employed for determining morphology and nano-mechanical properties of the surface layer.
The results, in agreement with the QCM-D observations, demonstrate that addition of Fe3+ induces a transition from an extended and soft protein layer to a denser and stiffer one.
Further, in situ ATR-FTIR and Confocal Raman Micro-spectroscopy (CRM) techniques were utilized to monitor compositional/structural changes in the surface layer due to addition of Fe3+ ions.
The spectroscopic analyses assisted by DFT calculations provide evidence for formation of tri-Fe3+/catechol complexes in the surface film, which is enhanced by Fe3+ addition.

Introduction
In recent years, mussel adhesive proteins (MAPs) excreted from marine mussels have gained great interest in the scientific community for their film forming ability on various substrates.
Currently, at least five different proteins (Mefp-1 to Mefp-5: denotation for Mytilus edulis foot protein hereafter) from the blue mussels have been isolated and characterized [1,2].
Mefp-1 was the first one to be purified, and it is regarded as the primary component of the cuticle of Mytilid treads.
Mefp-1 contains a high level of peptides with catecholic functional groups present in the primary structure as peptidyl (3,4-di-hydroxypheny)-l-alanine (DOPA) [3-7,2,1].
The coherent accumulation of Mefp-1 and Fe3+ has been confirmed in the cuticle of Mytilid byssus, and Fe3+ is an integral part of the cuticle that provides exceptional stability due to formation of Fe3+/DOPA complexes [8-11].
DOPA is a bi-dentate, each Fe3+ ion can bind one, two, or three DOPA ligands, depending mainly on the pH and the Fe3+/DOPA ratio [12,13].
Complexation has been reported to occur at Fe3+ concentration as low as 10μM, and the Fe3+ ions provide significant and fully reversible intermolecular bridging of Mefp-1 layers on mica surfaces, forming multivalent tri-Fe3+/DOPA complexes [13].
Metal-mediated bridging of Mefp-1 films has considerable potential for its non-toxic and multifunctional properties.
Due to the increasing interest of industries, including biomedical and medical oriented industry, the detailed understanding of the complexation process of Mefp-1 and Fe3+ on different material surfaces becomes more crucial.
Some investigations of the Mefp-1/Fe3+ complexation have been performed in vitro [11,14,15], and Fe3+ induced bridging in Mefp-1 films on a model surface has been demonstrated by using ex situ techniques [13].
However, to the author's knowledge, no study has described Mefp-1/Fe3+ complexation on a base metal substrate using in situ analytical techniques.
We have recently explored the possibility to use Mefp-1 for corrosion protection of carbon steel [16-18], and a deep understanding of the protein film formation and complexation process on iron substrate is of great importance for improving the protection performance of the adsorbed protein film.
This study considers two issues related to Mefp-1 film formation on such substrates.
The first is the adsorption kinetics of Mefp-1 on an oxidized iron substrate, which is more complex but more relevant for corrosion protection applications than inorganic or organic model surfaces used in other studies [19].
The other issue considered in this work is the composition and properties of the Fe3+ bridged Mefp-1 film formed on the iron substrate.
To gain relevant information, we have utilized quartz crystal microbalance with dissipation monitor (QCM-D), in situ AFM, in situ attenuated total reflectance Fourier-transform infrared spectroscopy (ATR-FTIR), in situ Confocal Raman Micro-spectroscopy (CRM) for the measurements, and also density functional theory (DFT) calculations to assist the interpretation of spectroscopic data.
The QCM-D offers information on adsorption kinetics, adsorbed mass including trapped water, and the viscous and elastic properties of adsorbed layers [20].
This technique has previously been used for Mefp-1 studies on model silica surfaces [19,21], whereas an oxidized iron surface was used in this study.
In situ Peak Force tapping AFM measurements were performed to investigate the morphology and nano-mechanical properties of the adsorbed film.
Further, in situ ATR-FTIR and in situ CRM were performed on the iron substrate to obtain chemical information of the adsorbed Mefp-1 film before and after exposure to aqueous solutions containing Fe3+ ions.
Materials and methods
Protein and solutions
The Mefp-1 with a purity of 92% (impurity is mainly aggregation/degradation products of Mefp-1) was supplied by Biopolymer Products AB (Gothenburg, Sweden).
The details of the extraction and purification processes of Mefp-1 are described elsewhere [22].
Mefp-1 is a large and basic hydrophilic protein, having a molecular weight of about 108kDa and a high isoelectric point about 10 [23,24].
It is known that Mefp-1 is highly positive charged at neutral pH [25].
The Mefp-1 was delivered as a 25mg/ml aqueous solution containing 1wt.% citric acid and stored as received in dark at+4°C before use.
A solution containing 1wt.% citric acid, at pH 7-8, was prepared as the buffer solution, and the pH was adjusted by sodium hydroxide solution.
Just 3min prior to measurement, Mefp-1 was added from its stock solution to the buffer solution to give a concentration of 0.1mg/mL.
The 10μM FeCl3 solution was prepared by dissolving a small amount of FeCl3⋅6H2O in pure water.
All chemicals used were of analytical purity and aqueous solutions were made using pure water with resistivity of 18MΩcm as prepared by Milli-Q system.
Substrates
The substrate in QCM-D and in situ AFM measurements were quartz crystals coated with a 100nm thick iron film (Q-Sense, Gothenburg, Sweden).
These sensors have a fundamental resonance frequency of 4.95MHz.
The in situ ATR-FTIR measurements utilized an iron film ca.
30nm thick that was deposited on a ZnSe internal reflection element (IRE) by high-vacuum evaporation using a Univex 300 vacuum evaporator (Leybold Vaccum).
The evaporator was equipped with a quartz crystal microbalance thickness monitor to control the thickness of deposited layer.
For deposition of pure iron film, ultra-pure iron (99.99%) target were used.
A bare ZnSe crystal was also used as substrate for comparison.
For in situ CRM measurements, a pure iron sheet sample was wet ground with SiC grinding paper successively to 1200 grits and then cleaned ultrasonically with ethanol.
The nominal composition of the pure iron (Stena Stal AB, Slovak Republic) is 99.87wt.% Fe, 0.06wt.% C, 0.06wt.% N, and 0.014wt.% S.
To obtain a relatively stable substrate during the measurements, all iron substrates were pre-conditioned in a 10wt.% NaOH solution overnight to form a thin oxidized surface layer, followed by rinsing with 99.5% pure ethanol and drying with a gentle stream of nitrogen gas prior to use.
The alkaline treatment of the iron surface results in formation of hematite (α-Fe2O3), which will be shown and discussed in relation to Fig. 9.
The hematite surface contains different types of surface hydroxyl groups that differ by their coordination to the substrate [26].
The hematite surface is amphoteric due to the possibility of protonization and deprotonization of the surface hydroxyl groups.
In neutral solution, the ξ-potential has been found to be slightly positive [27,28].
The water contact angle on our hematite surface is 69±3°, and the open circuit potential relative to Ag/AgCl is -0.72V, which is similar to that of iron due to the small thickness of the oxide layer.
QCM with dissipation monitoring, QCM-D
A QCM-D instrument, which is a highly sensitive balance based on the measurement of changes in the resonance frequency of a quartz crystal oscillator, was used to study the film formation and complexation processes.
The instrument used was a Q-sense E4 microbalance (Q-sense, Gothenburg).
The temperature of the measuring cell was controlled at 25±0.02°C, and the resonant frequency of the oscillator (f) and the energy dissipation value (D) were recorded simultaneously as a function of time.
The baseline was determined using the buffer solution without protein.
The dissipation is a measure of the amplitude decay rate of the crystal oscillation when the voltage is switched off, which is defined as D=Ed/(2πEs), where Ed is the energy dissipated during one period of oscillation and Es is the stored energy.
The combination of dissipation measurement with the frequency monitoring allows the determination the adsorbed mass as well as viscoelastic properties of the protein layers, from which changes in the orientation or conformational state of the protein adsorbed may be inferred [29].
The Sauerbrey equation [30] is commonly used to obtain the sensed mass (m) from the change in resonance frequency (Δf).
It assumes that the adsorbed film is evenly distributed, rigid and thin, and for this situation the change in resonance frequency is only due to the mass of the adsorbed film, including water hydrodynamically trapped in the film.
However, the Mefp-1 film is soft (viscoelastic) and contains a high content of water [31], and therefore, evaluation based on the Voigt model is more accurate and informative since it utilizes frequency and dissipation data from multiple harmonics (overtones) and allows estimations of thickness, shear elastic modulus, and shear viscosity of the adsorbed film.
The model assumes that the bulk solution is purely viscous and Newtonian, and the adsorbed film is uniform, the viscoelastic properties are frequency independent in the utilized frequency range (15-25MHz), and that there is no slip between the adsorbed layer and the crystal during shearing [20,32].
The frequency change and the dissipation change are given as:(1)Δf=Im(β)2πtqρqand(2)ΔD=-Re(β)πftqρqwhereβ=ξf2πfηf-iμf2πf1-αexp(2ξ1tf)1+αexp(2ξ1tf)·α=ξ1ξ22πfηf-iμf2πfηb+1ξ1ξ22πfηf-iμf2πfηb-1,ξ1=-(2πf)2ρfμf+i2πfηf,ξ2=i2πfρbηbwhere μ is the elastic shear modulus, η is the shear viscosity, f is the oscillation frequency, t is the thickness, and ρ is the density of the adsorbed film.
The subscripts q, f, and b refer to crystal quartz, film, and bulk solution, respectively.
In this work, results at two overtones (third and fifth) were fitted to the Voigt model using QTools software v3.0.12.518.
The QCM-D measurements were repeated two times, and the same trend was observed.
In situ AFM imaging and nano-mechanical measurement
In situ Peak Force tapping mode AFM measurements were performed on iron QCM crystals (coated with iron) with a pre-adsorbed Mefp-1 film, immersed in buffer solution and FeCl3 solution, respectively.
The measurements provide nanometer resolution images of topography and nano-mechanical properties of the adsorbed protein films.
The AFM instrument (Multimode, Nanoscope V, Bruker) is equipped with a fluid cell with inlets and outlets allowing solution exchange.
Rectangular silicon cantilevers with spring constant of 0.7N/m and resonant frequency range of 120-180kHz (SCANASYST-FLUID+, BRUKER) were employed for the measurements.
The Peak Force AFM images were obtained using a controlled feedback force of 1.1nN, and quantitative information of several surface mechanical properties was obtained simultaneously [33,34].
The principle of the technique has been explained in the literatures [35,36].
In brief, (as shown in Fig. 1), the Peak Force AFM measurement allows determination of surface deformation as the difference between the tip-sample separation at the peak force and at a given percentage of the peak force (15% in this study).
The tip-sample adhesion force is determined from the difference between the zero force and the minimum force experienced during tip retraction, and the dissipated energy during one approach-retraction measurement cycle is evaluated from the area between the trace and retrace curve.
Moreover, combined with the Derjaguin-Muller-Toporov (DMT) model [37,38], the elastic modulus of the surface can be obtained from the force curve where the sample and the tip are in contact [35].
To obtain the correct DMT modulus values, the tip radius needs to be known.
This value can be obtained by calibration against a surface with known elastic modulus.
In our case, we used a polystyrene surface with a Young's modulus of 2.7GPa.
According to the calibration result, the spring constant was 0.7N/m and the tip radius was 10nm.
During measurements, the topographical image and the above-mentioned nano-mechanical property maps are obtained simultaneously from the force vs. separation curves, each collected in about 0.5ms.
The Nanoscope Analysis v.120 software (Bruker) was used for image analysis.
A second order polynomial flattening algorithm was performed to remove the titling slope of topography images, and the surface roughness parameters Rq and Ra were determined.
Rq is the standard deviation of the Z-values of the surface, calculated as:(3)Rq=Σ(Zi)2NAnd Ra is the arithmetic average of the absolute values of the surface height deviations:(4)Ra=1N∑j=1N|Zj|
All other images remained as originally obtained.
For the quantitative comparison of the nano-mechanical properties of the adsorbed Mefp-1 film before and after Fe3+ enhanced complexation, mean Z-values of the property data were calculated by the software.
The mean value for a quantity across an image is the average of the values obtained at each pixel and it is calculated as:(5)Mean value=1N∑j=1NZjwhere Zj is the local value at pixel j and N is the number of pixels.
In situ ATR-FTIR
The ATR-FTIR measurements were performed using a Varian 7000 spectrometer equipped with a linearized Mercury cadmium telluride (MCT) detector.
The ATR measurements were performed using a custom made grazing angle total reflectance accessory (GATRTM from Harrick), with a removable hemispherical IRE and with incident beam angle of 65°.
The absorbance spectra were obtained as an average of 600 scans across the wavenumber region 4500-600cm-1, with a resolution of 8cm-1.
The ATR-FTIR spectra were recorded using the Varian Resolution Pro software 4.3 as single beam spectra before and after introduction of buffer solution and then also after different times during the exposure to solutions containing Mefp-1 or FeCl3.
The spectra are presented in the absorbance format, -log(R/R0), where R is the reflected intensity of the sample and R0 is the reflected intensity of the background, and obtained by taking the ratio of a single beam at a certain exposure time in Mefp-1 and FeCl3vs.
a single beam spectrum obtained in the buffer solution.
In situ CRM analysis
In situ CRM measurements were performed using an iron substrate in the Mefp-1 solution and also using such surfaces with a pre-adsorbed Mefp-1 film in the FeCl3 solution.
The CRM instrument was a WITec alpha300 system (WITec GmbH, Germany), equipped with a 532nm laser for excitation, and an oil immersion objective with 100×magnification and numerical aperture (NA) of 1.25.
The optical parameters give a lateral resolution of 260nm.
During a CRM measurement, a droplet of the liquid was placed on the substrate and covered by a microscope glass slide, which gave an approximate thickness of the liquid layer of 10μm.
Raman spectra were recorded with an integration time of 0.2s, and the data were analyzed using the WITec Project 2.06 software.
DFT calculations
Density functional theory (DFT) calculations within the Kohn-Sham formalism [39] were performed for a number of model compounds that included the catechol unit of the DOPA residues as well as parts of the protein backbone.
We also studied complexes between a Fe3+ ion and one or more catechol units and water molecules.
All computations were performed using software Gaussian 09 suite [40].
The model compounds were subjected to geometry optimizations followed by computations of time dependence-DFT electronic absorption spectra, IR spectra, and pre-resonance Raman spectra.
In order to produce reliable resonance Raman spectra, computations need to reproduce any electronic absorption bands that lie in the vicinity of the excitation wavelength used in the CRM experiment.
We tested different exchange-correlation functionals and found the recently developed long range corrected ωB97X functional [41] to be best suited to reproduce the charge-transfer bands found in complexes between Fe3+ and catechols.
The LACVP+basis set, which corresponds to the 6-31+G(d) basis for first and second row atoms and an augmented LANL2DZ ECP-basis set for heavier atoms, was used for all computations.
Vibrational spectra were computed within the harmonic approximation, and the obtained vibrational frequencies need to be scaled down in comparison with experimental spectra.
This is the standard procedure when comparing theoretical and experimental spectra, since the computed frequencies are always too large due to the lack of anharmonicity corrections.
In addition, the harmonic potentials are sensitive to electron correlation, and the scaling factor will differ depending upon the exchange-correlation functional and the basis set used.
For the ωB97X/LACVP∗+ level of theory, a scaling factor for the vibrational frequency of 0.94-0.95 was found to give good agreement between theory and experiment for most vibrational bands.
The exception was vibrations involving Fe-O bonds for which a scaling factor closer to unit was more appropriate.
Results and discussion
Kinetic aspects of layer formation and complexation
For a soft film, the decay time of the oscillation in the quartz resonator is small, and the dissipation value is high, whereas for a rigid film, the dissipation value is smaller.
The plot of the change in dissipation (ΔD) as a function of the change in frequency (Δf) is quite informative [42] and can be used for visualizing structural changes in adsorbed layers.
Such plots in Fig. 3 illustrate the changes in the Mefp-1 adsorption step and the subsequent rinsing step with FeCl3.
During Mefp-1 adsorption (Fig. 3a), ΔD initially increases linearly with decreasing frequency.
However, the slope of the curve increases when Δf exceeds about 25Hz.
This clear increase in the slope suggests that the film becomes more extended as the adsorption approaches its equilibrium value.
The transition range in the ΔD-Δf curve corresponds to the transition range between the fast and slow adsorption regime marked with arrow "a" in Fig. 3a.
This structural change may be explained as being due to increased repulsion between neighboring molecules at higher surface coverage.
During the exposure to the 10μM FeCl3 solution, Δf becomes less negative with time (Fig. 3b), which indicates that complexation between DOPA groups in Mefp-1 and Fe3+ causes significant removal of water from the adsorbed protein layer.
The ΔD-Δf curve obtained in the FeCl3 solution (Fig. 3b) shows that ΔD first decreases linearly with deceasing Δf, indicating formation of a more compact layer.
This process is rapid as seen by the drop in the thickness and the sensed mass shown in Fig. 2.
However, after the point marked as "b" in Fig. 3b, ΔD increases somewhat, a feature that could be due to increasing net positive charge of the Mefp-1/Fe3+ surface film as compared to the Mefp-1 layer prior to Fe3+ addition and thus caused by increased electrostatic repulsion within the layer.
Based on the increased mass and the assumption that the surface was relatively stable within the measurement time, the Voigt modeling gives values of the average film thickness of the order of 7nm and 4nm for the Mefp-1 layer before and after exposure to the FeCl3 solution, respectively.
However, in reality, the adsorbed Mefp-1 layer on iron substrate is not uniform at micro scale, as revealed in the AFM measurements below.
Structural and nano-mechanical properties of the adsorbed Mefp-1 films
In situ nano-mechanical mapping was performed on an iron substrate with a pre-adsorbed Mefp-1 layer, immersed in the citric acid buffer solution and the 10μM FeCl3 solution, respectively.
The topography images are shown in Fig. 4, presenting the starting surface in the buffer solution at pH 7-8 (Fig. 4a).
Since the iron surface was pre-conditioned in 10% NaOH, the surface mainly consists of Fe2O3 as confirmed by the CRM measurement.
The Rq value is 3.91nm over a 500nm×500nm scanned area.
The topograpical images of the Mefp-1 layer adsorbed on the iron surface in the buffer solution and in the FeCl3 solution are shown in Fig. 4b and c, respectively.
The adsorption of the protein leads to a smearing of the features seen on the surface prior to the Mefp-1 adsorption, compare Fig. 4a and b.
By comparing the Z-range of the surface with and without the Mefp-1 film (Table 1), it is seen that the adsorption of Mefp-1 does not lead to any significant change in the height variation, which indicates a full coverage of the Mefp-1 film on the surface.
Fig. 4c presents the topography of the Mefp-1 film exposed to the FeCl3 solution; by comparing with Fig. 4b and the Z-range in Table 1, it is observed that the introduction of Fe3+ induced a negligible decrease in the height variation, which remains predominantly due to the roughness of the substrate surface.
In summary, all these results indicate that the Fe3+ enhanced complexation increases the stiffness of the Mefp-1 film, which results in a denser film.
In this senses, the Peak Force tapping AFM observations are consistent with the decreased dissipation value and thickness observed in the QCM-D measurements.
Chemical information of Mefp-1 films before and after complexation
Mefp-1 film formation on iron substrate
ATR-FTIR was employed for obtaining in situ the chemical information of Mefp-1 films formed on the oxidized iron surface.
After the addition of the Mefp-1 into the buffer solution, the IR spectra were recorded after different time of exposure, as shown in Fig. 6a.
The negative band in the region around 3050-3650cm-1 is the combined result of negative water bands due to decreased water content in the vicinity of the iron oxide covered ZnSe crystal and positive O-H and N-H stretching bands from Mefp-1 following its adsorption on the surface.
The spectra also show positive bands in the region around 2800-3200cm-1 due to amino acid zwitterions (H3N+-CH-CO2) and C-H stretching vibrations from the adsorbed Mefp-1 molecules[43,44].
The part of the spectra below 1700cm-1 is similar to the IRAS spectrum of non-crosslinked Mefp-1 absorbed on carbon steel that was analyzed in our earlier study [17].
The bands that are similar for the two surfaces, oxidized iron and ZnSe, can be assigned to vibrations of the protein backbone and of the DOPA side chain without a need to consider the influence of the oxidized iron surface.
On the basis of our DFT calculations and our previous work we make the following assignments for the main bands.
The very strong band around 1640cm-1 is associated with the C-O stretching of amide groups of the backbone (Amide I).
The strong and broad peak around 1550cm-1 has contributions from two different types of transitions, i.e.
N-H bending of the backbone (Amide II) and aromatic C-C stretching of the catechol aromatic ring of the DOPA residue.
However, it should be noted that there are some clear differences between the spectrum of Mefp-1 adsorbed on oxidized iron surface (Fig. 6b) as compared to that obtained on the ZnSe crystal (Fig. 6c).
The spectrum of Mefp-1 on the iron substrate displays a strong peak at 1485cm-1 (Fig. 6b) that is entirely absent for Mefp-1 on the ZnSe surface (Fig. 6c).
Further, the shoulder at 1423cm-1 of the peak at 1450cm-1 (1454cm-1 on ZnSe) is only found for Mefp-1 on the iron substrate.
A more pronounced difference is the strong peak at 1258cm-1 present for Mefp-1 on the iron substrate, which is replaced by a much weaker peak at 1243cm-1 for Mefp-1 on the ZnSe surface.
Regarding the three peaks that are absent on ZnSe surface, i.e., the strong peaks at 1485cm-1 and 1258cm-1 and the shoulder at 1423cm-1, it can be noted that these peaks were also absent or much diminished in ATR-IR spectra for Mefp-1 absorbed on polystyrene and poly(octadecyl methaacrylate) reported by Baty et al. [45].
The 1485cm-1 and 1258cm-1 peaks were, however, present in our IRAS spectrum for Mefp-1 absorbed on carbon steel, but they disappeared upon oxidation of the DOPA residues [17].
The shoulder at 1423cm-1 could not be observed in the IRAS spectrum but that may be due to the lower resolution of that spectrum.
Thus, based on these observations, it seems clear that at least the 1485cm-1 and 1258cm-1 peaks are associated with catechol rings and their appearance requires the presence of Fe3+ in some form.
We performed DFT computations on different model compounds with catechol groups complexed to Fe3+, and our results suggest that the 1485cm-1 and 1258cm-1 bands arise from a significant fraction of catechols (Cat) of the DOPA residues being complexed to Fe3+ in the classical Fe(Cat)3 fashion.
The computed IR spectrum for such a complex is dominated by two strong bands that, after scaling the vibrational frequency by a factor of 0.945, reproduce the locations of the 1485cm-1 and 1258cm-1 bands of the experimental spectrum (Fig. 7).
The computed intensity of each of these bands is one order of magnitude higher than that of bands that traditionally are considered as very strong, such as a carbonyl stretching band.
This extreme intensity enhancement is an effect of the large charge polarization of the [Fe(Cat)3]3- complex due to the strongly positive Fe3+ being surrounded by three negatively charged catechols.
Because of the charge polarization, vibrations that result in unsymmetrical distortions of the complex will induce large dipole moment changes and therefore have very large IR intensities.
The very large intensities of these peaks have the consequence that even if only a fraction of the catechol units are complexed to Fe3+ these transitions will show up in the IR spectrum.
Thus, it seems that the most proper explanation for the 1485cm-1 and 1258cm-1 bands is symmetrical coordination of three catechols to Fe3+.
In complexes with two catechols of the type [Fe(Cat)2(H2O)2]-, the theoretical spectrum has only one strong peak, at around 1260cm-1, but with an intensity that is a factor five lower than that in the Fe(Cat)3 complex.
The [Fe(Cat)(H2O)4]+ complex has no strongly enhanced peak.
The shoulder at 1423cm-1 cannot be explained from this type of symmetrical complexes but has been observed earlier in IR spectra of Mefp-1 upon addition of Fe3+[46].
Thus, it may arise from complexation between Fe3+ and catechols, but potentially from less symmetrical complexes.
In conclusion, our analysis strongly indicates that Fe(Cat)3 complexes are formed in the adsorbed Mefp-1 film due to the release of Fe3+ from the oxidized iron surface.
But the existence of other types of complexes between Fe3+ and catechol-groups cannot be discarded as such complexes may not result in clear fingerprints in the IR spectrum.
Fe3+ complexation of the adsorbed Mefp-1 film
Fig. 8 shows the in situ ATR-FTIR spectra of the oxidized iron surface with pre-adsorbed Mefp-1 film obtained after exposure to the 10μM FeCl3 solution for different times.
Comparing with the 15min Mefp-1 spectrum (Fig. 6a), the intensity of negative water band is relatively larger after exposing the Mefp-1 layer to the FeCl3 solution (Fig. 8a), which indicates that water is released from the pre-adsorbed Mefp-1 layer.
Therefore, it is reasonable to conclude that the addition of the Fe3+ makes the Mefp-1 film more compact, in agreement with the QCM-D and AFM results.
It should be noted that the spectra obtained with the addition of FeCl3 are remarkably similar to those without the FeCl3 addition but after sufficiently long exposure in the Mefp-1 solution, which indicates that Fe3+ could also be generated from the oxidized iron surface.
We note that the broad and unresolved 1540cm-1 peak is more clearly split into two separate peaks at 1550cm-1 and 1536cm-1 (Fig. 8b).
This is not surprising since the DFT analysis showed that the 1540cm-1 peak had contributions from both the backbone and the catechol unit, and that these bands may be influenced differently by the change in aqueous solvation when the water content of the protein film becomes reduced.
It should also be recognized that the effect of subtracting the water spectrum may influence the two spectra slightly differently in this region.
However, more importantly the peaks, at 1485cm-1 and 1260cm-1 (Fig. 8b), which we have associated with the complexation between catechol and Fe3+, remain largely unaltered both in terms of frequencies and intensities.
This supports our earlier hypothesis that Fe3+ could be generated from the oxidized iron surface during the exposure to Mefp-1 solution and forms [Fe(Cat)3]3- complexes..
The shoulder at 1423cm-1 (marked with arrow in Fig. 8b) seems to be slightly enhanced upon addition of FeCl3, and this could indicate some differences in the geometry of the complexes.
Both QCM-D and AFM results show compaction of the Mefp-1 film with introduction of the FeCl3 solution, whereas the ATR-FTIR spectra suggest the formation of same type of complexes with and without FeCl3 addition.
It could be that without the addition of FeCl3 most complexes are formed near the iron oxide surface since the Fe3+ that is released from the surface is immediately bound to the catechols.
However, with the addition of the FeCl3 solution, a much more enhanced uniform complexation takes place throughout the adsorbed protein film, leading to those changes detected by the QCM-D and AFM measurements.
In situ CRM analysis
CRM measurements were also performed with the oxidized iron surface.
Although CRM has lower signal sensitivity, the Raman spectra provide information over a larger wavenumber range than the ATR-FTIR spectra.
Moreover, the DOPA peaks present a resonance enhancement upon Fe3+ complexation in Raman spectra.
A single CRM spectrum of the oxidized iron surface is shown in Fig. 9, the main peaks which indicate that the dominant surface oxide is α-Fe2O3 spectrum [47], but the presence of minor amount of other oxides and/or hydroxides cannot be ruled out.
When the oxidized iron surface was exposed to the Mefp-1 solution, there is no visible Mefp-1 band on the CRM spectrum after 1min exposure (Fig. 10a).
After 10min exposure, some organic related bands can be observed in the range of 1200-1600cm-1 (Fig. 10a), suggesting an increase in the adsorbed Mefp-1 amount.
However, the bands are relatively broad, characteristic peaks of the Mefp-1 were not distinguishable, which is mainly due to the concentration of the protein solution being low (0.1mg/ml), so the amount of Mefp-1 adsorbed on the oxidized iron surface was not sufficient to be detected clearly by the CRM.
After 10min of exposure to the Mefp-1 solution, the oxidized iron surface was rinsed with the buffer solution and then exposed to a 10μM FeCl3 solution, which in principle may lead to a further decrease in the amount of the adsorbed Mefp-1.
However, in the FeCl3 solution, the resonance-enhanced peaks typical for Fe(Cat)3 complexes begin to appear in the Raman spectrum after a few minutes (Fig. 10b).
The same peaks could also be discerned from our reference calculation of pre-resonance Raman spectra of the [Fe(Cat)3]3- complex (Fig. 11).
In agreement with earlier assignments, we found that the resonance-enhanced peaks in the region of 460-700cm-1 are related to vibrations of the C-O groups bonded to the central Fe3+.
The peak at 547cm-1 has been assigned to δ,ν-chelate, and the peaks at 591 and 638cm-1 to νFe-O [48,49].
There are also several resonance-enhanced bands in the region of 1200-1500cm-1 that are due to vibrational motions of the C- and H-atoms of the catechol units.
Most pronounced is the band at 1496cm-1 that is the symmetric counterpart to the asymmetric vibrations that give rise to the peak at 1482cm-1 in the IR spectrum.
The resonance-enhanced bands are several orders of magnitude stronger than the normal bands, and in agreement with earlier studies, bands associated with vibrations of the protein backbone are too weak to be observed [18,50].
Interestingly, if the oxidized iron surface is kept immersed in the Mefp-1 solution, the Raman spectrum is gradually transformed into that obtained when the surface is exposed to the FeCl3 solution, and already after an hour's exposure, the two spectra are basically indistinguishable (Fig. 10c).
This confirms our hypothesis derived from the IR measurements that Fe3+ is slowly released from the oxidized iron surface in presence of a Mefp-1 solution, and this results in the formation of Fe(Cat)3-type complexes, similar to those observed when the pre-adsorbed Mefp-1 film is exposed to the diluted FeCl3 solution.
Conclusions
Through a pre-conditioning to produce a stable oxidized iron surface, we have studied the film formation of Mefp-1 on the iron substrate and Fe3+ enhanced complexation of the pre-adsorbed film, by using in situ techniques including QCM-D, Peak Force tapping AFM, ATR-FTIR, and CRM combined with DFT calculations.
The results from different in situ measurements provide a consistent picture, and the spectroscopic analyses are supported by theoretical calculations.
The combined data set leads to the following conclusions:
The QCM-D measurements demonstrate that Mefp-1 initially adsorbs rapidly onto the iron substrate, but the adsorption slows down as full surface coverage is approached.
A structural change in the protein layer toward a more extended structure occurs as equilibrium is approached.
The exposure of the pre-adsorbed Mefp-1 film to a 10μM FeCl3 solution causes a significant removal of trapped water in the film and results in a denser and stiffer surface film.
The Fe3+ induced structural changes in the pre-adsorbed Mefp-1 film result in marked changes in the nano-mechanical properties of the protein films, most notably an increased layer stiffness as quantified by the Peak Force tapping AFM measurements.
The spectroscopic results from the in situ measurements strongly suggest the formation of tri-Fe3+/DOPA complexes in the protein film during the exposure to both the Mefp-1 solution and the FeCl3 solution.
In the Mefp-1 solution, complexes are mostly formed near the substrate since the Fe3+ released from the surface is immediately trapped by the catechols in the adsorbed protein film.
However, the introduction of the FeCl3 solution to the pre-adsorbed Mefp-1 film leads to a more enhanced complexation throughout the protein film.
Acknowledgements
Financial supports from Vinnova in the project "Green Corrosion Protection Treatment", from 'Swedish Science Council', and from SSF in the program on "Microstructure, Corrosion and Friction Control" are gratefully acknowledged.
Biopolymer AB, Gothenburg, Sweden, is acknowledged for supplying the mussel adhesive protein and for valuable discussions.
Dr.
Andra Dedinaite and Xiaoyan Liu at the division of surface and corrosion science of KTH are acknowledged for the valuable discussion about the QCM-D analysis.
Dr.
Birgit Brandner at Chemistry, Materials and Surfaces of SP is acknowledged for help with CRM measurements.
Supplementary material
mentary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.jcis.2013.05.016.
Supplementary material
Supplementary data 1
Details of QCM-D data fitting

Osteogenic and bactericidal surfaces from hydrothermal titania nanowires on titanium substrates
The Ti disk samples were prepared from a 0.9 mm thick ASTM grade 1 Ti sheet (Ti metals Ltd, UK). They were polished to a mirror image and ultrasonically cleaned in water and ethanol. Nanowires were created by immersing the Ti disks in 1 M NaOH in a PTFE lined steel vessel (Acid Digestion vessel 4748, Parr Instrument Company, USA), at a temperature of 240 degC for 2, 2.5 or 3 h. The vessel was removed after each time point from the oven and allowed to cool to room temperature. The samples were rinsed in water and ethanol, sequentially and they were subsequently heat treated at 300 degC for 1 h prior to ion exchange in HCl. To convert the sodium titanate nanowires to TiO2 the samples were immersed in 0.6 M HCl for 1 h, rinsed in water and ethanol, and finally heat treated at 600 degC for 2 h.1. A precursor for the preparation of a lithium transition metal oxide that is used for the preparation of a lithium transition metal oxide as a cathode active material for a lithium secondary battery through a reaction with a lithium-containing compound, wherein the transition metal has a composition Ni<sub>x</sub>Co<sub>y</sub>Mn<sub>1-(x+y)</sub> wherein 0.3≤x≤0.9, 0.1≤y≤0.6 and x+y<1, and wherein the precursor also contains sulphate ion (SO<sub>4</sub>)-containing salt ions derived from a transition metal salt for the preparation of the precursor at a content of 0.1 to 0.7% by weight based on the total weight of the precursor,
with the exception of Ni0.6Co0.2Mn0.2O0.12(OH)1.87(SO4)0.005.2. The precursor according to claim 1, wherein the transition metal salt is sulphate.3. The precursor according to claim 2, wherein the sulphate is one or more selected from the group consisting of nickel sulphate, cobalt sulphate and manganese sulphate.4. The precursor according to claim 1, wherein the salt ions includes nitrate ions (NO3).5. The precursor according to claim 1, wherein the salt ions have a content of 0.2 to 0.6% by weight, based on the total weight of the precursor.6. A lithium transition metal oxide, which is prepared by sintering a precursor and a lithium-containing material,
whereby the precursor has the transition metal composition Ni<sub>x</sub>Co<sub>y</sub>Mn<sub>1-(x+y)</sub> wherein 0.3≤x≤0.9, 0.1≤y≤0.6 and x+y<1, and wherein the precursor also contains sulphate ion (SO<sub>4</sub>)-containing salt ions derived from a transition metal salt for the preparation of the precursor at a content of 0.1 to 0.7% by weight based on the total weight of the precursor.7. The oxide according to claim 6, wherein the lithium-containing material is lithium carbonate (Li2CO3) and/or lithium hydroxide (LiOH).8. A cathode active material for a lithium secondary battery, comprising the lithium transition metal oxide of claim 6.9. A lithium secondary battery comprising the cathode active material of claim 8.
Enhanced electrospray ionization mass spectrometric detection of hexamethylene triperoxide diamine (HMTD) after oxidation to tetramethylene diperoxide diamine dialdehyde (TMDDD)
All chemicals used for synthesis were of reagent grade. Lithium acetate dihydrate (99.999%), acetic acid (99.99%) as well as the LC/MS grade acetonitrile were purchased from Sigma-Aldrich (St. Louis, MO, USA). Sodium acetate (99%; HPLC grade) was obtained from Acros Organics (Geel, Belgium). Ammonium acetate (LC/MS grade) was from Fisher Scientific (Fair Lawn, NJ, USA). Potassium acetate (analytical grade) was purchased from POCH (Gliwice, Poland). Milli-Q water was used throughout all the experiments. Standard solutions of HMTD and TMDDD were prepared daily in acetonitrile.
HMTD was prepared according to a published procedure.[24] A 25-mL round-bottomed flask was charged at 0 degC with 0.7 g of hexamethylenetetramine and 2.3 mL of 50% H2O2. The mixture was continuously mixed with a magnetic stirrer. After complete dissolution, citric acid (1.15 g) was added in four portions every 15 min. The mixture was left overnight while it was allowed to warm to room temperature. The white precipitate was filtered, thoroughly washed with water, and an approximately 100-mg fraction of the product was dried in a vacuum desiccator.
TMDDD was synthesized according to the procedure published by Wierzbicki et al.[24] with minor modifications. A portion of HMTD was dissolved in boiling (freshly distilled) n-butyl acetate (1:80 ratio). The solution was filtered into a three-necked flask equipped with a condenser, a thermometer and a capillary (1 mm diameter). This solution was purged with a stream (10-15 mL s-1) of oxygen at 120 degC for approximately 1 h until complete conversion of HMTD. The solution was allowed to cool to room temperature and white crystals were collected on the following day. The product was recrystallized from n-butyl acetate (yield 10%).1. A positive active material precursor for a rechargeable lithium battery comprising:a nickel-based composite precursor comprising a secondary particle each comprising a plurality of primary particles that are aggregated together,the nickel-based composite precursor having a central portion and a surface portion, andthe central portion of the nickel-based composite precursor comprising a phosphate.
a nickel-based composite precursor comprising a secondary particle each comprising a plurality of primary particles that are aggregated together,
the nickel-based composite precursor having a central portion and a surface portion, and
the central portion of the nickel-based composite precursor comprising a phosphate.2. The positive active material precursor ofclaim 1, wherein, the nickel-based composite precursor may be selected from metal hydroxide or metal oxide, wherein the metal is at least one selected from Mn, Ni, Co, Al, Fe, V, Ti, and Cr.3. The positive active material precursor ofclaim 1, wherein:the surface portion of the nickel-based composite precursor comprises a first surface portion contacting the central portion of the nickel-based composite precursor and a second surface portion surrounding the first surface portion, andat least one selected from the first surface portion and the second surface portion does not comprise phosphate.
the surface portion of the nickel-based composite precursor comprises a first surface portion contacting the central portion of the nickel-based composite precursor and a second surface portion surrounding the first surface portion, and
at least one selected from the first surface portion and the second surface portion does not comprise phosphate.4. The positive active material precursor ofclaim 1, wherein the central portion of the nickel-based composite precursor has a volume of about 20 volume % to about 70 volume % relative to a total volume of the secondary particle.5. A positive active material for a rechargeable lithium battery comprising:a nickel-based composite oxide comprising a secondary particle each comprising a plurality of primary particles that are aggregated together,the nickel-based composite oxide having a central portion and a surface portion, and the central portion comprising a lithium transition metal phosphate.
a nickel-based composite oxide comprising a secondary particle each comprising a plurality of primary particles that are aggregated together,
the nickel-based composite oxide having a central portion and a surface portion, and the central portion comprising a lithium transition metal phosphate.6. The positive active material ofclaim 5, wherein the central portion of the nickel-based composite oxide has a volume of about 20 volume % to about 70 volume % relative to a total volume of the secondary particle.7. The positive active material ofclaim 5, wherein a concentration of the lithium transition metal phosphate in the central portion of the nickel-based composite oxide is higher than or equal to a concentration of the lithium transition metal phosphate in the surface portion of the nickel-based composite oxide.8. The positive active material ofclaim 5, wherein the lithium transition metal phosphate is present at grain boundaries of the secondary particle of the nickel-based composite oxide or between adjacent ones of the plurality of the primary particles of the nickel-based composite oxide.9. The positive active material ofclaim 5, wherein the lithium transition metal phosphate comprises a nickel-based composite oxide secondary particle present in an amount in a range of about 0.01 wt % to about 2 wt % based on a total amount of the positive active material.10. The positive active material ofclaim 5, wherein the nickel-based composite oxide is represented by Chemical Formula 1:LiaNixCOyMnzM1−x−y−zO2[Chemical Formula 1]wherein, in Chemical Formula 1,0.8≤a≤1.2, 0.5≤x≤1, 0≤y≤0.5, 0≤z≤0.5, 0.5≤x+y+z≤1, and M is at least one selected from Mn, Ni, Co, Al, Fe, V, Ti, and Cr.
LiaNixCOyMnzM1−x−y−zO2[Chemical Formula 1]
wherein, in Chemical Formula 1,
0.8≤a≤1.2, 0.5≤x≤1, 0≤y≤0.5, 0≤z≤0.5, 0.5≤x+y+z≤1, and M is at least one selected from Mn, Ni, Co, Al, Fe, V, Ti, and Cr.11. The positive active material ofclaim 5, wherein the nickel-based composite oxide further comprises a coating layer comprising the lithium transition metal phosphate on the surface portion of the nickel-based composite oxide.12. The positive active material ofclaim 11, wherein the coating layer has a thickness of less than or equal to about 3 μm.13. A method of preparing a positive active material for a rechargeable lithium battery, the method of comprising:preparing a mixed solution comprising a transition metal solution comprising a nickel raw material, a manganese raw material, a cobalt raw material, a phosphorus-containing compound including at least one selected from a phosphoric acid and a phosphate, and, optionally, further comprising a metal raw material,introducing the mixed solution, a chelating agent, and a precipitating agent into a reactor to prepare a secondary particle precursor by a co-precipitation method, and then drying the secondary particle precursor to prepare a nickel-based composite precursor, andmixing the nickel-based composite precursor and a lithium raw material and calcining the mixture to prepare the positive active material ofclaim 5.
preparing a mixed solution comprising a transition metal solution comprising a nickel raw material, a manganese raw material, a cobalt raw material, a phosphorus-containing compound including at least one selected from a phosphoric acid and a phosphate, and, optionally, further comprising a metal raw material,
introducing the mixed solution, a chelating agent, and a precipitating agent into a reactor to prepare a secondary particle precursor by a co-precipitation method, and then drying the secondary particle precursor to prepare a nickel-based composite precursor, and
mixing the nickel-based composite precursor and a lithium raw material and calcining the mixture to prepare the positive active material ofclaim 5.14. The method ofclaim 13, wherein the nickel-based composite precursor may be selected from metal hydroxide or metal oxide, wherein the metal is at least one selected from Mn, Ni, Co, Al, Fe, V, Ti, and Cr.15. The method ofclaim 13, wherein the phosphorus-containing compound comprises at least one selected from NH4H2PO4, H3PO4, and (NH4)2HPO4.16. The method ofclaim 13, wherein the phosphorus-containing compound is mixed in a mole ratio in a range of about 0.01 mole to about 1 mole based on 100 moles of the nickel-based composite precursor.17. A rechargeable lithium battery comprising:a positive electrode comprising the positive active material ofclaim 5;a negative electrode; andan electrolyte.
a positive electrode comprising the positive active material ofclaim 5;
a negative electrode; and
an electrolyte.The influence of oxygen on the microstructural, optical and photochromic properties of polymer-matrix, tungsten-oxide nanocomposite films
The matrix chosen was a commercially-available, 127 μm thick, FEP film (CS Hyde, Lake Villa, IL). This material was placed in a glass reaction vessel with 300 mg of tungsten carbonyl precursor (Sigma, St. Louis, MO). The reaction vessel was evacuated to 100 m Torr to remove air from the reaction chamber as well as to remove volatiles from the polymer matrix. While under vacuum, the entire system was placed in a furnace and heated to 150 degC to vaporize the precursor. The vessel was held at temperature for at least 3 hs to allow precursor diffusion into the polymer. Subsequent heating to 175 degC produced thermal decomposition of the precursor within the polymer and this temperature was held for approximately 4 h to permit particle formation and growth. The system was allowed to cool gradually to room temperature. This process resulted in a polymer film reinforced with discrete, tungsten-oxide nanoparticles distributed throughout the bulk of the material. Since the matrix is not degraded during particle synthesis, the process described can be performed repeatedly to coarsen existing particles and to nucleate new ones. For the work reported here, PMNCs are designated according to the number of processing cycles a given material experienced - 1x for one processing cycle, 2x for two cycles, etc. There are physical limits to the number of processing cycles that a material can undergo related to the precursor diffusion into the material, however the practical working limit relates primarily to the time required for synthesis. The maximum number of cycles used in the work presented here is four.Chemical weathering and provenance evolution of Holocene-Recent sediments from the Western Indus Shelf, Northern Arabian Sea inferred from physical and mineralogical properties

Abstract
We present a multi-proxy mineral record based on X-ray diffraction and diffuse reflectance spectrophotometry analysis for two cores from the western Indus Shelf in order to reconstruct changing weathering intensities, sediment transport, and provenance variations since 13ka.
Core Indus-10 is located northwest of the Indus Canyon and exhibits fluctuations in smectite/(illite+chlorite) ratios that correlate with monsoon intensity.
Higher smectite/(illite+chlorite) and lower illite crystallinity, normally associated with stronger weathering, peaked during the Early-Mid Holocene, the period of maximum summer monsoon.
Hematite/goethite and magnetic susceptibility do not show clear co-variation, although they both increase at Indus-10 after 10ka, as the monsoon weakened.
At Indus-23, located on a clinoform just west of the canyon, hematite/goethite increased during a period of monsoon strengthening from 10 to 8ka, consistent with increased seasonality and/or reworking of sediment deposited prior to or during the glacial maximum.
After 2ka terrigenous sediment accumulation rates in both cores increased together with redness and hematite/goethite, which we attribute to widespread cultivation of the floodplain triggering reworking, especially after 200years ago.
Over Holocene timescales sediment composition and mineralogy in two localities on the high-energy shelf were controlled by varying degrees of reworking, as well as climatically modulated chemical weathering.
Highlights
► We examine a high resolution multi-proxy physical properties from two marine cores.
► Little correlation between physical proxies and climate in early Holocene ► Reworking probable cause of poor correlation in Early Holocene ► Possible anthropogenic influence on sedimentation in the last 200 years

Introduction
Understanding how climate change affects continental environments and weathering processes is a key component of quantifying solid Earth-climate interactions.
Analysis of marine sediments holds out the prospect of being able to reconstruct this evolution over a number of timescales (e.g., Debrabant et al., 1993; Fagel et al., 1994; Colin et al., 1999; Abrajevitch et al., 2009).
Geochemical analysis of Arabian Sea sediment has attracted significant interest from those seeking to reconstruct changing chemical weathering and provenance during the Cenozoic, over various timescales through analysis of elemental ratios or isotopic systems (e.g., Papavassiliou and Cosgrove, 1982; Suczek and Ingersoll, 1985; Fagel et al., 1994; Sirocko et al., 2000; Staubwasser and Sirocko, 2001; Clift and Blusztajn, 2005; Colin et al., 2006; Limmer et al., 2012).
However, there has been limited interest in the physical and mineralogical properties of sediments in the Northern Arabian Sea over any timescale, especially during the Holocene to Recent, despite the relatively well-defined variability in climate and sea level known for this period.
This lack of data is despite the fact that physical and mineralogical properties have been shown to be effective in tracing sediment transport in several areas worldwide (e.g., Balsam et al., 1995; Liu et al., 2003; Adler et al., 2009).
The lack of a mineralogical record limits our ability to understand the impact that changing monsoon intensity has had on sediment composition sourced from a drainage basin with a strong erosional signal.
Mineralogy has been studied on the deep sea Indus Fan, but this has been starved of sediment since 11.5ka (Prins et al., 2000) and contains no record spanning the Holocene, where we have the best climate records.
Prior to 1947 under more natural conditions the Indus River delivered between 250 and 675milliontonnes of sediment to the Indus Delta annually (Milliman et al., 1984; Giosan et al., 2006) while since that time discharge has fallen sharply because of damming (Inam et al., 2007).
However, it is possible that some anthropogenic impact on sediment flux occurred earlier starting with the agriculture of the Harappan Civilization, ~5ka (Possehl, 1999).
The Indus Shelf provides a potential archive for tracking changes in sediment discharge from the river and relating this to changing conditions in the onshore flood plain.
The sediment record since the Last Glacial Maximum (LGM) is the target of this study reflecting the fact that this period has the best defined monsoon climate record derived from oceanographic upwelling records (Gupta et al., 2003), from speleothems (Fleitmann et al., 2003) and from lacustrine records in western India (Enzel et al., 1999).
The aim of this paper is to understand how evolving climate and sea level may have affected chemical weathering, as well sediment transport pathways and sources to the western shelf of the Indus delta (Fig. 1).
We do this through examination of the physical properties and clay mineralogy of sediments deposited in that region.
We focus on two Holocene-Recent cores taken from the Indus Shelf, which have already been the subject of a separate geochemical, weathering response and provenance study by Limmer et al. (2012).
That study highlighted the importance of long-shore currents and the reworking of older sediment on the shelf, as well as direct supply from the Indus River in accounting for the sediment flux to the shelf during the Holocene.
Applying a multi-proxy approach yields information about the transport, depositional history and origin of sediment on the Western Indus Shelf that cannot be obtained through analysis of individual sample methods.
Moreover, application of continuous sensing methods allows rapid, cm-scale fluctuations to be examined in a way not possible with conventional geochemistry.
The relative ease and low cost of sample preparation means a much larger and more complete dataset can be produced relatively quickly.
We test the hypothesis that clay mineralogy, magnetic susceptibility and diffuse reflectance spectrophotometry can record changes in terrigenous sediment supply from the Indus River.
We present a complete record of western Indus Shelf sedimentation based on whole core analysis and sampling.
We use this record to investigate how sedimentation has changed in response to changes in monsoon strength and sea level.
Background
Regional setting
The Indus Shelf, located in the northern Arabian Sea spans approximately 180km from the delta to the slope edge and stretches southeast from the Murray Ridge and Makran Coast to the Rann of Kutch in India (Fig. 1).
The shelf is dominated by the submarine Indus delta and a large sinuous canyon structure, the Swatch (Giosan et al., 2006), separating the eastern and western shelves.
The Holocene delta is built over an 11km-thick passive margin sediment pile dating back to the Late Cretaceous and which has been the recipient of large volumes of sediment from the Indus River since at least the Early Miocene (Clift et al., 2001).
The region is tectonically quiescent, except for the transform plate boundary to the west, along the Murray Ridge.
Sediment supply is dominantly from the Himalayas and Karakoram (Clift et al., 2004; Garzanti et al., 2005).
Clay mineralogy and magnetic properties have been used to reconstruct palaeoenvironmental conditions and provenance of marine sediments throughout Asia (e.g., Bouquillon et al., 1989; Sirocko and Lange, 1991; Prins et al., 2000; Thamban et al., 2002; Boulay et al., 2005; Wan et al., 2007; Liu et al., 2009).
However, these studies have often been focussed in deepwater settings, away from the influence of major river systems or sediment processes on high-energy shelves.
The cores in this study are located only 20km away from the coast line in ~70m of water, and are strongly influenced by the Indus River as well as the high wave energy of the Indus Shelf.
As a result the two sites could potentially provide detailed records of Holocene environmental conditions recorded by the Indus River and delta system.
Clay mineralogy
Clay mineralogy has been widely used as a proxy for reconstructing climate, weathering and provenance in marine sediments.
Changes in clay mineralogy have been linked to changes in weathering in the terrestrial environment of several Asian continental margins (e.g., Clift et al., 2002; Liu et al., 2003; Boulay et al., 2005; Wan et al., 2007; Colin et al., 2010) and as a record of terrestrial climate change in the deep Indian Ocean (Fagel et al., 1994).
The major assumption made with palaeoclimatic interpretation of clay minerals is that there is no post-depositional diagenesis (Chamley, 1989; Hillier, 1995) which is likely to be true in a young, shallow-buried core such as we consider here.
Typically studies quantify changes in the relative abundances of kaolinite, chlorite, smectite and illite as weathering proxies.
In the Arabian Sea variations in clay mineralogy have been interpreted to reflect changes in the relative strength of dust plumes from Arabia, India and Pakistan, as well as changes in the input by major fluvial systems in the region (Sirocko and Lange, 1991; Fagel, 2007).
Indus River assemblages are dominated by smectite, illite and to a lesser extent chlorite (Rao and Rao, 1995; Alizai et al., 2012).
Smectite is a product of chemical weathering and its presence is typically associated with alteration of volcanic minerals (Chamley, 1989; Liu et al., 2003), although it also forms in low-lying poorly-drained soils (Wilson, 1999).
In contrast, kaolinite often forms in warm, wet tropical environments subject to strong leaching (Chamley, 2001; Thamban et al., 2002).
Both illite and chlorite can form through the breakdown of muscovite and biotite from or through the erosion of sedimentary rocks or minerals (Boulay et al., 2003).
Chlorite and illite are usually preserved in soils and sediments at high-latitude or cooler climatic conditions where they are inherited from parent materials (Biscaye, 1965).
Chemical weathering in these settings is weak but physical erosion is often stronger (e.g., Campbell and Claridge, 1982).
An additional property that has been used in clay mineral studies is the crystallinity index of illite, which is thought to be sensitive to alteration and thus to climate change (Chamley, 1989; de Visser and Chamley, 1990; Pandarinath, 2009).
Low illite crystallinity is associated with greater chemical weathering (Pandarinath, 2009) or lower burial diagenesis (Kisch, 1983).
High temperature and rainfall causes stronger weathering and thus wider XRD peaks and lower crystallinity.
A stronger monsoon might favour greater hydrolyzation and stronger weathering causing a reduction in illite crystallinity (Lamy et al., 1998; Alizai et al., 2012).
Diffuse reflectance spectrophotometry (DRS)
DRS is applied to estimate mineral composition along a core using the reflectance of light (Balsam et al., 1999; St-Onge et al., 2007).
The most simplistic and commonly used analysis of DRS data is that supplied by the Commission Internationale de l'Eclairage (CIE) (Debret et al., 2011) where the parameters L*, a* and b* are calculated (Balsam et al., 1999; Nederbragt et al., 2006; St-Onge et al., 2007).
L*, often referred to as grey scale, represents lightness where 0 is dark and values of 100 are pale (Croft and Pye, 2004).
L* has been widely used as a proxy for carbonate content (Rogerson et al., 2006).
A* represents the red to green colour spectrum and is often used as an indicator of red mineral abundance, for example hematite (Croft and Pye, 2004; St-Onge et al., 2007).
B* values represent the colour spectrum from yellow to blue (Croft and Pye, 2004; Almogi-Labin et al., 2009) and have been shown to respond to changes in organic matter (Debret et al., 2006; St-Onge et al., 2007).
These values are calculated from the XYZ tri-stimulus coordinates (Rodgers et al., 2008).
XYZ values are calculated from the material reflectance, the spectrum used and the standard observer values (Rodgers et al., 2008) and were also defined by the CIE (Croft and Pye, 2004).
Because L* values have been shown to be affected by not only carbonate composition but also clay mineralogy (Balsam et al., 1999), we do not consider this to be suitable for this study.
Another use of DRS is to estimate the relative proportion of the iron-bearing minerals hematite and goethite.
Increasing values of redness (a*) and hematite/goethite ratios have been shown to relate to drought phases because hematite forms under warm, arid conditions and goethite under cool, wet conditions (Schwertmann, 1971; Zang et al., 2007).
Hematite/goethite ratios are often estimated using the first derivative peak of hematite, which occurs at 575nm and the first derivative peak of goethite, which occurs at 565nm (Helmke et al., 2002).
A simplistic method is to divide the reflectance value at 565nm by the reflectance value at 435nm, which corresponds to peak reflectance of hematite and goethite.
Magnetic susceptibility
Magnetic susceptibility (χ) is a measure of magnetic mineral concentration (De Menocal et al., 1991).
Magnetite, the main control of magnetic susceptibility, is associated with terrigenous sediment supply (Kumar et al., 2005).
It is therefore possible to use magnetic susceptibility as a proxy for terrigenous sediment supply (De Menocal et al., 1991).
It is widely used in a variety of marine and terrestrial settings as an indicator of changing environmental conditions and thus climate in the Asian monsoon region (e.g., De Menocal et al., 1991; Kumar et al., 2005; Thamban et al., 2005; Yancheva et al., 2007; Sun et al., 2009).
It has been shown that increasing values of magnetic susceptibility correlate with periods of strengthening monsoon, as reconstructed from oxygen isotope records, revealing a link between Eolian transport and strong East Asian Winter Monsoon winds (An et al., 2001).
Records of Holocene monsoon evolution in Asia
A comprehensive review of climate records and the mechanisms for climate change are presented in Staubwasser and Weiss (2006).
This section discusses periods of short-lived (500years) changes in climate during the Holocene.
Many authors have reported a trend for a period of strengthening of the SW Asian monsoon during the Early Holocene between ~10ka and ~8ka, followed by a gradual drying, especially between 6 and 2ka (Enzel et al., 1999; Sarkar et al., 2000; Staubwasser et al., 2002; Fleitmann et al., 2003).
Several periods of abrupt weakening of the SW Asian monsoon were identified by Gupta et al. (2003), using planktonic foraminifera at 8.2ka, 6.2-5.8ka, 4.6-4.2ka, 3.2ka and 2ka-1.2ka.
These have since been linked short-lived solar minima events Gupta et al. (2005).
It is this robust knowledge of climate history that allows us to use the clay mineral data presented here to understand the environmental response to the climate forcing.
Methodology
The techniques used to ascertain the mineralogy and physical properties of the sediment are magnetic susceptibility, DRS, X-ray diffraction (XRD), and bulk density porosity.
XRD and DRS yield semi-quantitative data in the form of estimates of mineral composition.
We use bulk density with radiocarbon dating to calculate mass accumulation rates at the two core sites.
The advantages of DRS and magnetic methods over bulk elemental and isotopic analyses are the relatively short sample preparation times and the speed of analysis.
Apart from XRD all techniques are non-destructive and yield data for the entire core length.
We chose two cores obtained during the Winter 2008/09 aboard cruise 64PE300 of the RV Pelagia on the western shelf of the Indus Delta in the Arabian Sea (Fig. 1).
Core Indus-10 is 9.06m long was recovered from 71m water depth and is located 120km northwest of the modern Indus Canyon.
Core Indus-23 is 7.67m long and is located ~100km southeast of Indus-10 in 70m water depth (Fig. 1).
Grain size variation is low, with mean grain size values ranging from 20 to 50μm (Limmer et al., 2012).
These provide excellent material for our chosen methods and also contained material suitable for 14C AMS dating.
Radiocarbon dating from these cores demonstrates these cores date back to the Early Holocene (Fig. 2) (Limmer et al., 2012).
At Indus-10 there is a depositional hiatus approximately 830cm below the present seafloor (cmbsf) (Fig. 3).
A depositional hiatus near the base of the core is a transgressive surface, with Holocene material overlying sediments deposited during the Pleistocene and exposed during the last glacial, sea level lowstand.
At Indus-23 a possible depositional hiatus in the Mid Holocene is identified at ~640cmbsf below the present day seafloor (Fig. 4) (Limmer et al., 2012).
Clay mineralogy
Semi-quantitative clay mineralogy was conducted on 80 samples, 40 from each core.
Each sample was placed in a beaker with 500ml deionized water, sonicated with a probe for 5min and stirred to disperse the material.
Some samples were rinsed with deionised water several times to remove soluble salts and thereby aid dispersion.
The dispersed sample was then transferred to a labelled, Atterberg cylinder topped up with deionized water, shaken and left for 16h.
The volume above a 20cm sedimentation depth, which according to Stoke's law contains the <2μm fraction was then syphoned into a labelled bottle.
Specimens for XRD analysis were then made using the filter peel method (vacuum filtration) by transferring clay to a glass slide placed onto the filtrate, which was left to dry at room temperature (Hillier, 2003).
The samples were run through a range of 2-45° 2θ on a Siemens D5000 X-Ray Diffractometer (XRD), with Co Kα radiation selected by a diffracted beam monochromator at The James Hutton Institute, Aberdeen, UK.
Three diffraction patterns were recorded on the same specimen, air-dried, solvated in ethylene glycol by vapour pressure at 60°C for 24h and heating to 300°C for 1h (Hillier, 2003).
Heating to 300°C enhances the illite peak due to collapse of in any expandable clays such as smectite or mixed-layer illite-smectite (Moore and Reynolds, 1989; Hillier, 2003).
Analysis of the diffraction data was conducted by measuring peak intensity as peak area using Bruker Diffrac Plus EVA-12.0 software.
Estimates of mineral composition were made by a reference intensity ratio method based on factors calculated with the Newmod programme as described in Hillier (2003).
Illite crystallinity was measured using the full width at half maximum (FHWM) of the 001 basal illite peak and integral breadth (I Breadth) of the same peak (Kübler and Jaboyedoff, 2000).
Both measurements are measured as values of ∆2°θ and show identical trends (Alizai et al., 2012).
Because our cores are <9m long, post-depositional burial diagenesis should not be a significant factor in clay mineral composition.
Where clay mineral values are greater than 10% uncertainty is estimated as better than 5% weight at the 95% confidence level (Hillier, 2003).
Clay mineral estimates are shown in Table 1.
Diffuse reflectance spectrophotometry (DRS)
Core scanning was conducted at the British Ocean Sediment Core Research Facility (BOSCORF) at the National Oceanography Centre, Southampton (NOCS) using a Geotek MSCL-XYZ with a Konica-Minolta CM2006d for spectrophotometry measurements.
The archived halves were lightly scraped and carefully rewrapped in polyethylene to limit air bubble formation.
Scanning was conducted at 1-cm resolution in order to generate a sufficiently high resolution record, while at the same time allowing for the collection of data in a reasonable duration.
Wavelengths ranging from 360nm to 740nm at 10nm intervals were recorded.
This provided us with the full visible light spectrum, eliminating the problems of water absorption associated with greater wavelengths, while minimizing sample preparation (Jarrard and Vanden Berg, 2006).
Values were measured as percentage reflectance compared to a barium sulphate calibration plate as a white standard (Balsam et al., 2000).
Magnetic susceptibility
Magnetic susceptibility values were measured on U-channels through all sections of Indus-10 and Indus-23 using a Barrington Instruments MS2 Magnetic Susceptibility metre at NOCS (Roberts and Lewin-Harris, 2000).
Each U-channel was run three times and continuous measurements recorded every centimetre.
Bulk density
Data was collected using a Geotek MSCL-S at 1cm resolution at NOCS.
The cores were stored at room temperature the night before analysis and the data then processed using unpublished conductivity, temperature and depth (CTD) data obtained from the same cruise.
We use bulk density in order to calculate mass accumulation rates (MAR) for both cores.
MAR was calculated assuming linear sedimentation rates between dated samples and the bulk density values.
Age model
A total of 19 radiocarbon dates were obtained from the two core sites (Table 2), 15 of which were published in Limmer et al. (2012) using the marine standard calibration of Hughen et al. (2009).
Analysis of the original 11 samples was completed at National Ocean Sciences Accelerator Mass Spectrometry facility (NOSAMS, Woods Hole Oceanographic Institution, USA) using their standard method, as discussed by McNichol et al. (1995).
Four additional dates obtained from shell samples were prepared for radiocarbon analysis at the NERC Radiocarbon Facility (Environment) at East Kilbride, UK.
Shells were ultrasonicated in deionized water and the outer 25% by weight of each shell was removed by controlled hydrolysis with dilute HCl.
A known weight of homogenised, pre-treated shell was hydrolysed to CO2 using 85% orthophosphoric acid at room temperature.
The resulting CO2 was cryogenically trapped and a subsample of CO2 was collected for independent δ13C measurement.
The remaining CO2 was converted to graphite by Fe/Zn reduction.
The δ13C value was measured on a dual inlet stable isotope mass spectrometer (VG OPTIMA) and is representative of δ13C in the original, pre-treated sample material (quoted precision is the machine error).
Graphite targets were analysed at the AMS Laboratory, East Kilbride and the results were corrected to δ13C Vienna Pee Dee Belemnite standard (VPDB‰ -25) using the δ13C values.
For this study all ages were remodelled using Calib 6.0 calibration software (Hughen et al., 2009) and the Marine09 dataset (Reimer et al., 2009).
A local marine reservoir correction (ΔR) of +232±26years was used based on the average reservoir age of two samples from a similar location to the Indus core sites, as shown on the 14Chrono Marine Reservoir Database (http://calib.qub.ac.uk/marine/).
We note that two samples, one at Indus-10 (373cmbsf) and one at Indus-23 (578cmbsf) do not fit the general age model.
In both cases these shells were unarticulated and located close to sandier sections of the cores implying that they could be reworked.
They are not included in calculating sedimentation rates or the age model (Fig. 2).
All the dated samples are included in Table 2 with the new ages highlighted in bold.
Results
Clay mineralogy
The relative abundances of four minerals smectite, illite, chlorite and kaolinite were plotted together with the illite crystallinity proxies of I Breadth and FHWM for Indus-10 (Fig. 3).
Beneath the depositional hiatus illite is slightly more abundant than smectite, with values of 44% for illite compared to values of 41% for smectite.
Above the depositional hiatus, between 480cmbsf and 760cmbsf, smectite is generally the more abundant mineral, although the change is very modest.
Smectite values range from 44 to 53%, while illite values range from 34 to 43%.
Above 480cmbsf illite is generally more abundant than smectite (39-51% compared to 38-44%).
Both minerals are far more abundant than kaolinite (3-6%) and chlorite (7-12%).
The illite crystallinity proxies both show sharp increases above the depositional hiatus between 760cmbsf and 835cmbsf from 0.26 to 0.37 FHWM ∆2°θ and 0.40 to 0.57 I-Breadth ∆2°θ (i.e. lower crystallinity).
Above 480cmbsf illite crystallinity remains low apart from two declines in FHWM ∆2°θ from 0.40 to 0.31 and in I-Breadth ∆2°θ from 0.60 to 0.48 at 240cmbsf.
There is also a slight increase in crystallinity at 130cmbsf where FHWM ∆2°θ drops from 0.36 to 0.33 and I-Breadth ∆2°θ from 0.53 to 0.47.
At Indus-23 (Fig. 3) illite crystallinity values remain very stable throughout the core with values that are generally lower (i.e., more crystalline, less altered) than those observed in Indus-10.
FHWM ∆2°θ ranges 0.33-0.36 and I-Breadth ∆2°θ 0.45-0.53 at Indus-23.
Illite (46-52%) seems more dominant than smectite (35-45%) throughout Indus-23.
Smectite and illite are the dominant mineral groups, although chlorite contents do increase (from ~7 to 11%) when smectite values generally decline (~53 to 36%) in both cores.
Magnetic susceptibility, DRS and bulk density
Magnetic susceptibility, redness, hematite/goethite ratios and bulk density values were plotted with the sedimentary log for Indus-10 (Fig. 5).
There is no clear correlation between sediment type and any of these geophysical proxies, apart from at the depositional hiatuses.
Four areas where variations in patterns shift sharply are highlighted.
Above the depositional hiatus at ~850cmbsf magnetic susceptibility increases from 75×10-6 SI to approximately 260×10-6 SI at 700cmbsf, while redness and hematite/goethite ratios values decline sharply from ~2.8% to 0.8% and 1.8 to 0.8 respectively.
At 550cmbsf magnetic susceptibility, redness and hematite/goethite values all increase sharply while bulk density values decrease.
Similar shifts also occur at ~440cmbsf and at ~220cmbsf respectively.
Magnetic susceptibility, redness, hematite/goethite and bulk density variations were plotted with the sedimentary log for Indus-23 (Fig. 6).
As at Indus-10 lithology does not appear to be a key control on proxy values, not least because there is not much lithological variation.
Magnetic susceptibility values decline sharply at 680cmbsf from 300×10-6 SI to almost zero before rebounding back and reaching values 380×10-6 SI at 660cmbsf across the depositional hiatus.
At the same point redness values initially decreases from ~1.3% to ~0.4% to close to zero at 660cmbsf.
At ~640cmbsf magnetic susceptibility values increase, redness and hematite/goethite values peak at 2.2% and 1.9.
Similar patterns are observed at 350cmbsf.
Temporal variations in clay mineralogy
Clay mineral ratios are calculated with the chemical weathered clays (smectite and kaolinite) divided by the physically eroded clay minerals (chlorite and illite) in order to generate a proxy for the relative intensity of chemical weathering although kaolinite-based values are of doubtful value because of the generally low concentrations of this mineral.
Values for smectite/illite and smectite/(chlorite+illite) are generally lower in Indus-23 than Indus-10 (Fig. 7).
We employ smectite/(chlorite+illite) because it has already been demonstrated to be effective in similar Quaternary sediments from the Mekong Delta (Colin et al., 2010).
However, the smectite/chlorite value is generally higher in Indus-23 than in Indus-10.
Below the hiatus in Indus-23 (~7.5ka), both cores show very similar patterns in clay mineralogy, although there is an increase in all ratios in Indus-10 between 13ka and 9ka.
In Indus-10 all smectite-based ratios increase to very high values (e.g., 1.2 for smectite/illite and 1.1 for smectite/(chlorite+illite)) after ~2.5ka before declining sharply at 2ka (e.g., 0.4 for both smectite/illite and smectite/(chlorite+illite)).
Smectite/(chlorite+illite) again increase to 0.8 after ~0.5ka (Fig. 8).
A much higher resolution record is available for the last 1.5ka from core Indus-23 (Fig. 8).
A net increase in smectite/illite (0.7 to 1.3), smectite/(chlorite+illite) (0.5 to 1.0) and smectite/chlorite (4 to 6) is observed.
Much of this change appears to have happened in the past 200years.
An opposing trend is observed in kaolinite/illite.
The smectite/kaolinite ratio follows the pattern of smectite-based ratios (e.g., smectite/chlorite).
In contrast, the Indus-10 record for the last 1.5kyr shows a significant scatter in all clay mineral ratios at ~0.6ka that is not observed at Indus-23, while the kaolinite/illite increases between 1.2ka and 0.6ka.
Discussion
The variations in clay mineralogy at both core sites are consistent with previously published data for the Arabian Sea (Sirocko and Lange, 1991) and Indus Fan (Kolla et al., 1981) in that smectite and illite are the dominant mineral groups.
The data from Indus-10 and Indus-23 agree with recent work from the onshore delta and floodplains where chlorite was observed to account for less than 10% of the total clay assemblage (Alizai et al., 2012).
Consistently low kaolinite values of 3-5% are also consistent with previously published work in the region that show that kaolinite concentrations are generally the highest in the central Arabian Sea (Sirocko and Lange, 1991).
In order to compare our clay mineral assemblages with those from the onshore (Alizai et al., 2012), a ternary plot of kaolinite, smectite and (chlorite+illite) was constructed (Fig. 9).
This plot shows that both the river mouth site at Keti Bandar and Indus-10, located in the northwest of the study area have very similar clay compositions.
However, sediments from Indus-23 have consistently higher smectite contents.
This indicates that sediment deposited at Indus-23 was eroded from a source that is generally more chemically weathered than that deposited at the river mouth.
Such a source could include older sediments deposited before the Holocene, or that this part of the shelf was receiving sediment from a region where more smectite-producing rocks are exposed.
Indeed the contrast between Indus-23 and the river mouth is quite surprising given their close proximity.
Clay mineralogy as provenance tool: comparing onshore to offshore
We further compare the temporal evolution in neodymium isotope values with the variation in clay mineralogy in the two cores considered here, as well as from the river mouth (Alizai et al., 2012) (Fig. 11).
Since ~11ka the two core sites show less negative εNd values compared to sediment deposited near the river mouth at Keti Bandar, although the discrepancy is most pronounced at Indus-10.
This difference indicates that the sediment at the core sites is not totally dominated by direct flux from the Indus River at any given time.
Indus-10 has received sediment from the Bela Ophiolite the west of the study area via longshore drift during the Early Holocene (Limmer et al., 2012), although the clay mineralogy does not reflect this change.
The one εNd value from Indus-23 during the Early Holocene suggests that the sediment had more in common with Indus River sediment from the LGM rather than the active river, a hypothesis that is consistent with the less weathered state of the material.
Together these data may indicate reworking of sediments on the shelf, for example those deposited and exposed during the LGM.
In general however, the smectite/(chlorite+illite) ratios of the Indus sediments show higher values (more weathering) between 10 and 5ka, when the monsoon was stronger.
As the climate dried falling smectite/(chlorite+illite) ratios are consistent with the notion of less chemical weathering, and supportive of a dominant flux from the Indus River to the core sites.
The shift towards less negative εNd values in Indus-10 at ~3ka is probably linked to reworking of older shelf sediments (Limmer et al., 2012).
During the recent past, especially the last 200years the overall pattern both in the on and offshore record is a net increase in the smectite/(chlorite+illite) ratios, following a period of falling ratios between 6 and 2ka at Indus-10 and a shift towards more negative εNd values.
This more recent change is unlikely to be linked to natural climate changes but coincides with the onset of large scale agriculture across the Indus basin.
Clay mineralogy as a weathering proxy
Because of their contrasting origins kaolinite/illite ratios have been used as proxies for hydrolysis compared to physical erosion intensity in marine sediments (Chamley, 1989).
Kaolinite/illite has also been used as a proxy for humidity (Thamban et al., 2002).
Another commonly used ratio is smectite/(chlorite+illite), which is interpreted to reflect the proportion of chemically weathered material compared to physically weathered material (e.g., Boulay et al., 2005; Alizai et al., 2012).
As a result, this ratio is a useful weathering indicator in sub-tropical and arid environments where there is little kaolinite present.
Figs. 7 and 8 plotted clay minerals associated with chemical weathering against clay minerals associated with physical weathering over time in order to determine how intensity has changed with time.
If greater values of kaolinite and smectite reflect increased chemical weathering then smectite/illite and kaolinite/illite would show identical patterns through time, which is not the case.
This is because smectite is favoured by drier, more seasonal conditions, while kaolinite is formed by leaching in wet, tropical environments (Thiry, 2000).
We further test the ability of clay minerals to act as weathering proxies by comparing them with K/Al, which is the weathering proxy that is least affected by grain size or carbonate content, at least on the Indus Shelf (Limmer et al., 2012) (Fig. 12).
For all three clay mineral ratios there is a poorly defined negative relationship between weathering intensity from the geochemistry and the weathering intensity in the clay mineralogy, although some of this relationship may be caused by the effects of grain size, this could also be attributed to the loss of potassium and the increase in both kaolinite and smectite during chemical weathering.
Such proxies need to be used with some care because K is enriched under moderate degrees of chemical weathering, but then becomes depleted under more intense weathering, as K-feldspars break down (Blaxland, 1974; Nesbitt et al., 1997).
We suggest that the anti-correlation noted in Fig. 12C in particular reflects the relatively moderate degree of alteration in Indus River sediments.
The generally higher values for smectite relative to illite in Indus-10 could also reflect the mixing of Bela Ophiolite-derived sediments into Indus-10 rather than increased chemical weathering.
Response to Holocene sea level rise and monsoon intensification
Temporal variations in magnetic susceptibility, mass accumulation rates, clay mineralogy, redness, illite crystallinity (FHWM) and hematite/goethite ratios are plotted against the Qunf Cave climate record of Fleitmann et al. (2003) (Fig. 13).
The clay mineral proxy smectite/(chlorite+illite), as well as illite crystallinity, shows clearly that strong monsoons are associated with periods of more intense weathering.
A similar coherent relationship between smectite/(chlorite+illite) and monsoon strength has been observed elsewhere in Asia (Colin et al., 1999).
Interestingly, the start of the period of stronger weathering between 11 and 8ka is also a time when hematite/goethite and redness increased.
Normally these trends would be associated with drier rather than wetter conditions, yet are superior climate record clearly rules this out in this case.
We suggest that the change in hematite/goethite reflects erosion and reworking of pre-existing hematite-rich soils formed during the arid LGM period and our reworked during the period of intensifying monsoon in the Early Holocene.
Alternatively, the high hematite abundance may be associated with increased seasonality (i.e. monsoonal conditions) (Schwertmann, 1971; Ji et al., 2005; Naidu, 2006).
Magnetic susceptibility decline sharply in Indus-23 between 10ka and 8.5ka during a period when the monsoon had reached maximum strength, as observed in the oxygen isotope records of Fleitmann et al. (2003).
At Indus-10 however there is no such drop in magnetic susceptibility at this time, indeed the reverse is observed.
If susceptibility tracks the flux of magnetite-bearing clastic flux to the ocean then its reduction at Indus-23 at a time when run-off might have increased is unexpected and suggests flux of sediment away from that site.
Another possibility is that the magnetic mineralogy of the sediments at Indus-23 has been altered after deposition, possibly through diagenesis.
Indus-10 behaves as might be expected in the context of a strengthening monsoon.
Both cores show an increase in FHWM (i.e. decreased illite crystallinity) during the Early Holocene indicating deposition of more weathered sediment at a time when the monsoon rains were strengthening.
This is consistent with monsoonal moisture required for chemical weathering, although some of the weathered material being deposited might be reworked from older LGM-sediment driven by heavier monsoon rains.
4ka is known to be a time of major climate change (Fleitmann et al., 2003; Gupta et al., 2003; Staubwasser and Weiss, 2006), which appears to have triggered a weathering response that is preserved in the sediment of Indus-10 (~560cm core depth; Fig. 5).
Magnetic susceptibility and smectite/(chlorite+illite) values drop as the monsoon weakened.
It is this drought phase that has been correlated with the demise of the Indus-Harrapan civilization (Staubwasser et al., 2003; Madella and Fuller, 2006).
Unfortunately the resolution of our records does not allow this climate event to be better resolved, although it is clear that it forms part of a long-term trend to weaker chemical weathering, as shown by increasing redness, higher hematite/goethite ratios and falling smectite/(chlorite+illite) values in Indus-10 between 8 and 2ka.
We conclude that clay mineralogy and DRS data can be correlated with palaeoclimatic data over periods >1000years at Indus-10.
εNd values indicate that some of the sediment deposited in the Early Holocene at Indus-10 was sourced from the Makran region, presumably through longshore drift, as well as from the Indus River mouth (Limmer et al., 2012).
This transport could account for the higher smectite abundance at Indus-10 during the Early Holocene (Fig. 7), although the parallel trend seen at Keti Bandar instead suggests that there is a common climatic control to weathering in both the Makran and the main Indus basin.
What is more surprising is the contrast between sediment from Indus-23 and the drill site at Keti Bandar, despite their relative proximity.
Nonetheless, since 1.5ka smectite/(illite+chlorite) values at the two sites are indistinguishable, suggesting that more recently all sediment west of the canyon is dominantly derived from the river mouth.
During this time the monsoon is believed to have slightly strengthen, during what has been known as the Roman Warm Period (Chauhan et al., 2009), although there is no weathering response to this.
The two sites only show moderate differences in magnetic susceptibility, redness and hematite/goethite since 2ka.
Mass accumulation rates (MAR) increased at Indus-10 during the Holocene.
Although rising sea level created accommodation space on the shelf this process was completed by ~6ka, before the increasing MAR as noted.
The hiatus in the section at Indus-23 makes it impossible to derive a meaningful long-term MAR reconstruction.
We have no way of knowing if the higher rates at Indus-10 are just a local anomaly or if sediment delivery rates really increased across the shelf.
MAR increased at both sites after ~2ka and particularly since 500years ago when there is a modest increase in summer monsoon intensity (von Rad et al., 2002; Fleitmann et al., 2003).
However, this increase is out of proportion to the modest climate change and more likely indicates anthropogenic activity, such as cultivation or irrigation of the floodplain, which is widely linked to enhanced erosion and sediment delivery to deltas worldwide (Syvitski et al., 2005).
Conclusions
A study of the geophysical properties and clay mineralogy of sediments from the Indus Shelf was conducted at two sites in ~70m water depths west of the Indus River mouth and canyon.
Changing clay mineralogy reflects a climatically driven evolution in chemical weathering in the onshore basin.
The coherent variation in smectite/(illite+chlorite) and illite crystallinity with monsoon intensity at Indus-10 is consistent with a climatic control to chemical weathering processes since 14ka.
However, we do not know if this is a direct response to stronger summer monsoon rains, or stronger reworking of pre-existing, more weathered material.
Increasing hematite/goethite in the Early Holocene indicates that reworking for older LGM sediment was important at least at Indus 23.
Hematite/goethite values show a gradual fall at Indus-10 from 11 to 2ka, suggestive of reduced seasonality as the summer monsoon weakened.
At Indus-10 susceptibility has increased first rapidly after 10.5ka, and then steadily after 8ka until 5.5ka, mostly during a period of strong summer monsoon.
Falling susceptibility of the 5.5ka correlates with the weaker monsoon, although increases after 2.5ka are hard to relate to climate records.
Sedimentation rates gradually increased during the Holocene, most notably in the last 200years, which is attributed to anthropogenic modification of the floodplain, boosting sediment delivered to the ocean, as a result of reworking of older weathered materials.
Strengthening of the monsoon during this recent period has been minor and is unlikely to be the trigger of the trend since ~2ka.
We conclude that there are links between climatic variation and clay mineralogy on the Indus Shelf, but that reworking can make this hard to relate to weathering processes in the Indus flood plains at all locations.
The most promising weathering proxies are smectite/(illite+chlorite) and illite crystallinity, but it is unclear whether the positive correlation between weathering intensity and summer monsoon strength is driven by direct response, or more by erosion and reworking of material weathered during earlier time periods.
Acknowledgements
The work was funded by a NERC studentship grant NE/G002029/1 for DL.
The work was also supported by a grant from the International Association of Sedimentologists (IAS) for clay mineral analysis and a NERC facility grant for additional radiocarbon dates (NRCF allocation 1529.0311).
We thank Christophe Colin and Nathalie Fagel for their constructive reviews.
DL thanks Guy Rothwell and Suzanne MacLachlan at BOSCORF for training on the Geotek MSCL-XYZ and technical assistance and Jeremy Sothcott for his help in preparing cores for bulk density measurements.
Anwar Alizai and Raja Ganeshram are thanked for useful discussions.
DL also thanks Ian Philips for his assistance with the XRD analysis.
DL and CK thank Nicola Pressling for the use of the Barrington MS2 Magnetic Susceptibility metre.
PC thanks the Hanse Wissenschaftskolleg, Germany for the time to think about monsoon river and delta systems.

Numerical model of swash motion and air entrapment within coarse-grained beaches

Abstract
The paper presents a numerical model for bore-driven swash on permeable coarse-grained beaches.
The surface flow module is modelled using the non-linear shallow water equations (NLSWEs), solved using the Godunov-based finite volume ADER scheme, which is suitable for handling steep bores as well as large source terms in the NLSWEs.
The subsurface flow comprises: (i) infiltration and exfiltration modelled as vertical piston-like flow, (ii) horizontal pore-air movement within the unsaturated region of the beach, (iii) the horizontal groundwater flow.
Model predictions of the surface and subsurface flow are in good agreement with measurements from large-scale laboratory experiments for swash on permeable, immobile beaches.
In these simulations air velocity was sufficiently small to justify using Darcy's resistance law for the air flow, whereas the quadratic Forchheimer law had to be used for the infiltration and groundwater flow.
The validated numerical model provides insight into the surface-subsurface water exchange for bore-driven swash on coarse-grained beaches.
The impact of air entrapped between the wetting front and the groundwater level on the water exchange is examined in particular.
Highlights
► We model key mechanisms of the water exchange between swash and subsurface.
► Focus is on filtration and pore-air pressure build-up within the unsaturated beach.
► Validation is carried out with measurements from large-scale laboratory experiments.
► Swash model is consistent with experimental results on two coarse-grained beaches.
► The insights contribute to better understanding and modelling of key swash processes.

Introduction
Bore-driven swash on unsaturated coarse-grained beaches generates several interacting physical processes: surface flow over the beach face, infiltration into the initially unsaturated part of the beach, air entrapment below the wetting front (Steenhauer et al., 2011a) and groundwater flow.
Surface flow is commonly modelled using the Non-Linear Shallow Water Equations (NLSWEs); examples of such models are reviewed in Brocchini and Dodd (2008).
Many swash models account for groundwater flow (Clarke et al., 2004; Lara et al., 2006; Van Gent, 1994; Wurjanto and Kobayashi, 1993), but few include simulation of water infiltration into the unsaturated part of the beach (Dodd et al., 2008; Packwood, 1983; Steenhauer et al., 2011b), and, to the authors' knowledge, none simulates air movement within the beach.
However, the field investigations of Horn (2002, 2006) and a recent laboratory study by Steenhauer et al. (2011a) have shown that air entrapment can significantly impact on infiltration and exfiltration processes in natural beaches.
This paper presents a numerical model of bore-driven swash, including subsurface flow, for coarse-grained beaches.
The model accounts for all relevant physical processes, namely the surface flow, infiltration and exfiltration, pore-air movement and groundwater flow.
Model predictions are compared with the detailed surface and subsurface flow measurements of Steenhauer et al. (2011a).
The paper is organised as follows.
Section 2 describes the numerical model for bore-driven swash on permeable coarse-grained beaches, focussing in particular on the simulation of pore-air pressure and the groundwater behaviour.
In Section 3 experimental results are compared to numerical results giving an assessment of the capability of the model formulations.
The main conclusions are summarised in Section 4.
Model description
The conceptual model of bore-driven swash on coarse-grained beaches is based on the experimental findings of Steenhauer et al. (2011a), who used a large-scale swash rig to measure the time- and cross-shore-varying surface and subsurface flow characteristics for large-scale bore-driven swash on steep, coarse-grained beaches.
A definition sketch and the relevant flow quantities are shown in Fig. 1.•
The surface flow is described by the NLSWEs.
The corresponding module calculates flow depth, h, and depth-averaged horizontal velocity, u.
•
The infiltration/exfiltration is modelled as one-dimensional, vertical, piston-like movement.
In the uprush (Fig. 1, top panel) the surface water covers the beach surface so the 'piston' extends between the beach surface and the wetting front (a sharp boundary between the saturated and the unsaturated zones within the beach).
In the backwash the surface water depth falls to zero, but the 'piston' continues to move in the subsurface.
It is possible that by that time pore-air pressure below the 'piston' has sufficiently built up to reverse its direction and push it upwards, hence causing exfiltration.
Otherwise, the 'piston' continues to move downwards and another sharp interface called the 'wetting tail' forms on the top of the 'piston' (Fig. 1, bottom panel).
The height of the 'piston' is called the saturated thickness and is denoted hf.
The levels of the wetting front and tail are denoted zf and zt respectively, and the volumetric flux (discharge per unit plan area) is denoted q, with positive sign in the case of downward flow.
•
The air flow module simulates one-dimensional migration of air in the horizontal direction, i.e. the balance equations are integrated over the thickness of the unsaturated zone, ha.
The air is considered compressible.
The air pressure, density and discharge are denoted pa, ρa and Qa respectively.
•
The groundwater flow module simulates one-dimensional movement of groundwater in the horizontal direction.
The balance equations are integrated over the groundwater thickness, hη.
In the lower region of the beach the 'piston' has merged with the groundwater, so the whole depth of the beach is saturated.
In this region we have introduced an imaginary boundary (a dashed line in Fig. 1), where the (nearly) vertical flow (above the boundary) is converted into quasi-horizontal groundwater flow (below the boundary).
The imaginary boundary meets with the groundwater table at the most shoreward fully saturated cross-section of the beach, where the wetting front has just reached the groundwater table.
Beyond this location groundwater flow has a free surface, i.e. it is unconfined.
The surface and subsurface models are coupled: they exchange volume and momentum via corresponding source terms in the volume and momentum balance equations.
The flow variables in the surface and subsurface models are calculated at every spatial step and at every time-step across the swash zone.
The model uses the horizontal x-coordinate and vertical z-coordinate for both surface and subsurface flow models with the origin set at the initial shoreline position (Fig. 1).
The conceptual model describes all relevant processes by one-dimensional models.
As mentioned earlier, the suitability of the NLSWEs for modelling the surface flow in the swash zone is well established.
The significant part of bore excursion extends over an initially unsaturated beach.
The experimental results of Steenhauer et al. (2011a) have shown that infiltration into the unsaturated zone is predominantly vertical.
The same study has also demonstrated that air pressure below the wetting front is approximately constant across ha (height of the unsaturated beach, Fig. 1), so that the direction of air movement is predominantly horizontal.
Our model assumes that air movement remains horizontal beyond the location of maximum swash run-up, where this assumption is less justified.
However, the vertical air movement is represented as a source term.
Similarly, groundwater movement is indeed reasonably close to being one-dimensional in the region where the wetting front has not yet reached the groundwater table, but the assumption of one-dimensional horizontal flow is less justified for the region where the beach is fully saturated.
Although the assumptions of flow one-dimensionality are not fully justified everywhere in the model, the model captures the main features of swash on coarse-grained beaches well, including the air pressure build-up and subsequent shoreward migration.
It must be pointed out that our model, as well as the experimental results of Steenhauer et al. (2011a), apply for conditions where the incident bore does not vary in the long-shore direction, which means that air trapped below the wetting front and above the groundwater table, cannot escape in the long-shore direction.
Surface flow
The surface flow model is based on the NLSWEs.
The continuity equation and the momentum equation of the NLSWEs are expressed in conservative form:(1)∂h∂t+∂uh∂x=-q(2)∂uh∂t+∂u2h∂x+∂(12gh2)∂x=-τ0ρ-ghtanγwhere h is the water depth, u is the depth-averaged horizontal velocity, q (m/s) is the volume flux through the beach surface, g is the gravitational acceleration, ρ is the density and γ is the bed slope (Fig. 1).
The volume flux q transfers volume between surface and subsurface flows, and hence provides coupling of the surface and subsurface simulation modules.
The details of this coupling are presented later.
The volume flux q also transfers horizontal momentum between the surface and subsurface flow.
However, in this model it was assumed that coarse grains at the beach surface fully absorb the fluid momentum, so the corresponding term in the momentum equation is omitted.
The first term on the right-hand side of the momentum equation denotes the bed shear stress and is replaced by a simple quadratic parametrisation as(3)τ0ρ=12fu|u|where f is the friction coefficient.
The NLSWEs are solved using the Godunov-based finite volume ADER scheme (Castro and Toro, 2008; Titarev and Toro, 2002; Toro and Titarev, 2006).
The details of the scheme and numerical solution for the surface flow are presented in Steenhauer et al. (2011b).
Vertical filtration
When a bore arrives at an initially unsaturated region of the beach it generates nearly vertical piston-like downwards flow, which continues throughout the whole swash cycle.
Depending on the x-location and the stage within the swash cycle, the 'piston' can extend between (Fig. 1): (i) the beach surface and the wetting front; (ii) the wetting tail and the wetting front; (iii) the beach surface and the imaginary boundary (the dashed line in Fig. 1); (iv) the wetting tail and the imaginary boundary.
In order to evaluate the hydraulic gradient which drives the vertical filtration we first define the fluid potential for the upper and lower end of the 'piston'.
These can be expressed, in units of pressure, as:(4)ψt={pA+ρgh+ρgzbat the beach surfacepA+ρgzt-pctat the wetting tail(5)ψf={pa+ρgzf-pcfat the wetting frontρgHηat the imaginary boundarywhere ψt and ψf are the fluid potential at the upper and the lower end of the saturated region respectively, pA is atmospheric pressure, pcf and pct is the capillary head at the wetting front and tail respectively, pa is the pore-air pressure below the wetting front, Hη is the hydraulic head of the horizontal groundwater flow, ρ is density of water, g is acceleration due to gravity and the meaning of other symbols is shown in Fig. 1.
The hydraulic gradient, I, is now expressed as(6)I=ψt-ψfρghfand it can be related to the flux q via a quadratic parametrisation of the hydraulic resistance(7)I=aKq+bKq|q|where aK and bK are the Forchheimer coefficients.
The part of the model which calculates the movement of the wetting front (and the wetting tail) in cases (i) and (ii) is called the filtration module.
The wetting front advances by filling the voids in the beach with water, so the corresponding continuity equation is(8)θdzfdt=-qwhere q is the volume flux of the vertical flow, zf is the position of the wetting front (Fig. 1, top panel) and θ defines the volume within the unsaturated part of the beach available for infiltration relative to the total volume, i.e. the effective porosity.
To simulate the propagation of the wetting front, Eqs.
(4)-(7) are used to express the right-hand side of Eq.
(8) in terms of the unknown wetting front position zf.
The resulting ODE is solved for zf using the fourth order Runge-Kutta method.
Further details can be found in Steenhauer et al. (2011b).
In the case when vertical flow occurs between the wetting tail and the wetting front (case (ii) above, and Fig. 1, lower panel) continuity requires that they move at the same speed.
The position of the wetting tail, zt then follows from the displacement of the wetting front (Eq. (8)).
In the case when vertical flow occurs between the beach surface and the wetting front (case (i) above, and Fig. 1, upper panel) the vertical flow is coupled with the surface flow through the volumetric flux q.
Depending on the comparison of the fluid potential at the top and the base of the vertical flow, the flow direction can be downwards or upwards.
The latter for instance occurs if pore-air pressure within the unsaturated region of the beach reaches a sufficiently high magnitude to create negative hydraulic gradient (Eq.
(6)) and hence reverse the flow (i.e. change the sign of q in Eq.
(8)).
The pressure build-up then drives the wetting front towards the bed surface causing exfiltration, i.e. water is returned to the surface flow through the volume flux shown in Eq. (1).
The filtration module does not handle the cases when the imaginary boundary defines the lower end of the vertical flow (cases (iii) and (iv) above).
These are the cases when the vertical flow recharges the horizontal groundwater flow.
The flow rate is calculated using Eqs.
(4)-(7), and incorporated in the groundwater flow simulation (Section 2.4).
In case (iv) the groundwater flow is coupled with the surface flow through the volumetric flux q.
Air pressure
The air pressure module is based on a mass balance equation for one-dimensional horizontal pore-air movement in the unsaturated region of the beach.
The control volume covers a computational cell i, shown in Fig. 2 for two typical situations: in the presence and absence of a wetting front.
The latter situation occurs at cross-shore locations where the surface water depth is zero, for example at locations beyond the location of maximum run-up.
Air pressure and density everywhere inside the unsaturated part of a cell is assumed to be constant.
The mass balance equation for air during a single computational step Δt=tn+1-tn is expressed as(9)∫nn+1d(Viaρia)=∫tntn+1ρi-1aQi-1/2adt-∫tntn+1ρiaQi+1/2adt-∫tntn+1ρiaQibdtwhere ρia and Via denote the density and volume of air within the cell i, the term ρi-1aQi-1/2a is the mass flux at the interface between computational cells i-1 and i and ρiaQib is the mass flux through the top of the computational cell i (i.e. the bed surface), which exists only in the absence of the wetting front (Fig. 2, right).
Assuming Darcy resistance and homogeneous material, the horizontal flux can be written in terms of pressure as(10)Qi+1/2a=k(hia+hi+1a)2Δxνa1ρi+1/2a[pia-pi+1a+ρi+1/2ag(zi-zi+1)]where k is the intrinsic permeability, νa is the viscosity of air, Δx is the length of the cell, zi is the middle level of the unsaturated beach region in cell i, hia is the height of unsaturated cell i, ρia is the density of air within the unsaturated cell i, with ρi+1/2a=12(ρia+ρi+1a) and pia is the pore-air pressure within the unsaturated cell i.
The approximation of flux Qi-1/2a on the right-hand side of the mass balance Eq.
(9) is analogous.
The extrapolation of Eq.
(10) for heterogeneous beach material is straightforward.
The vertical flux between cell i and the atmosphere is expressed using Darcy's law as(11)Qib=2kΔxhiaνaρia[(pia-pA)+ρiag(zi-zib)]where zib is the level of the bed surface and pA is the atmospheric pressure at the bed surface.
Eq. (10) expresses the relationship between the air flux and the pressure gradient, which is assumed valid at any point in time.
The average flux over time interval (tn,tn+1) is evaluated as(12)∫tntn+1ρiaQi+1/2adt=[(1-w)ρia,nQi+1/2a,n+wρia,nQi+1/2a,n+1]Δtwhere 0≤w≤1 denotes the weighting coefficient, Qi+1/2a,n is found from Eq.
(10) taking flow quantities at tn, and Qi+1/2a,n+1 is found from Eq.
(10) taking solely pressure pa at time tn+1 (and other quantities at time tn).
The other terms on the right-hand side of the mass balance Eq.
(9) are weighted between tn and tn+1 analogously.
The term on the left-hand side of the mass balance Eq.
(9) represents the difference in mass between tn and tn+1 incorporating a change in volume (i.e. height of unsaturated zone) and a change in air density.
It is expressed as(13)∫nn+1d(ρiaVia)=ρia,n+1Via,n+1-ρia,nVia,n=ρia,n+1θΔxhia,n+1-ρia,nθΔxhia,nwhere θ denotes the effective porosity, Δx is the length of cell i, and hia,n is the height of unsaturated zone in cell i at time n.
The height of unsaturated cell at time n+1 on the right-hand side of Eq.
(13), is evaluated as hia,n+1=hia,n-dhif,n*-dhiη,n*, where dhif,n* and dhiη,n* are predictions of the movement of the wetting front and groundwater level during tn+1-tn (discussed in more detail in Section 2.5 and Appendix C).
Air density ρia,n+1 in Eq.
(13) is found through the relationship for an ideal gas undergoing a reversible (i.e. no entropy generation) adiabatic process given by(14)pia,n(ρia,n)γ=pia,n+1(ρia,n+1)γ=constantwhere γ is the heat capacity ratio, for which a value of 1.4 is commonly taken for a diatomic gas (such as air, which consists mainly of nitrogen and oxygen) at room temperature.
Expressions for all terms in the mass balance Eq.
(9) are combined into a linear system presented in Appendix A.
The set of equations is solved using Gaussian elimination.
Groundwater
The groundwater module simulates the nearly horizontal movement of groundwater within the beach.
The groundwater module is used in two steps.
The first step is the prediction of groundwater levels within the unconfined region of the beach to enable the determination of the air pressure values in the air flow module.
This step uses a quick procedure of the groundwater module described in Appendix C.
Once the air flow module has been updated the second step is to determine the values for the groundwater levels within the confined and unconfined region of the beach as explained in the following.
Groundwater flow in a computational cell i can be either unconfined, i.e. with a free surface (Fig. 2), or confined (Fig. 3).
The latter case occurs when the wetting front has merged with the groundwater level.
From this moment an imaginary boundary (dashed line in Fig. 3) is introduced in the model.
The boundary represents the zone where the vertical percolation (above the boundary) is converted into quasi-horizontal flow in the groundwater (below the boundary).
The continuity equation for a cell i can be written as(15)Qi-1/2η-Qi+1/2η={-qiΔxfor a confined beachθ∂Hiη∂tΔxfor an unconfined beachwhere Qi-1/2η denotes the discharge at the interface between cells i-1 and i, qi denotes vertical flux (infiltration/exfiltration) through the imaginary upper boundary, Hiη is the hydraulic head within the cell i and θ is the effective porosity (Fig. 1).
The horizontal flux Qi-1/2η is written in terms of hydraulic head Hη as(16)Qi-1/2η=Ki-1/2(hiη+hi-1η)2Δx(Hi-1η-Hiη)where hiη is the height of the groundwater in cell i and Ki-1/2 is the hydraulic conductivity obtained from the linearised resistance law, i.e.(17)Ki-1/2=1aK+bKcK|up,i-1/2|with aK and bK the Forchheimer coefficients, up,i-1/2=Qi-1/2η12(hiη+hi-1η) the horizontal flux (in unit m/s), and cK a coefficient determined by model calibration (Section 3.1 and Appendix B).
The approximation of flux Qi+1/2η on the left-hand side of Eq.
(15) is analogous.
The average discharge over time interval (tn,tn+1) is evaluated as(18)1Δt∫tntn+1Qi-1/2ηdt=[(1-w)Qi-1/2η,n+wQi-1/2η,n+1]where 0≤w≤1 denotes the weighting coefficient, and Qi-1/2η,n is found from Eq. (16).
In the case of confined groundwater flow there are two more quantities that have to be evaluated: the imaginary upper boundary of the model and the vertical flux (Fig. 3).
After the arrival of the wetting front, the region of the beach is considered confined and from this moment the boundary, initially coinciding with the groundwater level, moves towards the bed surface.
Reasonable results for the level of the imaginary boundary were obtained by assuming that the imaginary boundary moves with a vertical upward velocity equal to the vertical flux through the imaginary boundary q, i.e. during a time step Δt the boundary moves by zic,n+1-zic,n=qin+1Δt.
This is smaller, by factor θ, than the height of the fluid volume that has recharged the horizontal groundwater during the same step.
In other words only a fraction of vertical flow (equal θ) is immediately converted into horizontal flow, while the remaining part (1-θ) remains vertical.
Although not based on a physical law, this is considered reasonable.
The vertical flux into the confined groundwater cell is driven by the difference in hydraulic head at the beach surface, Hih, and at the upper boundary of groundwater flow, Hiη.
It was necessary to use Forchheimer parameterisation of the subsurface flow resistance, as initial attempts with a Darcian model failed to adequately reproduce the water exchange between the surface flow and a coarse-grained beach during a bore-driven swash event.
Due to the quadratic law describing vertical flux through the imaginary boundary it was necessary to carry out the calculation of the hydraulic head within the confined region of the beach in two iterations.
In the first iteration the system of the confined region is solved with the Darcian vertical flux, and with the same weighting coefficient w as before, i.e. as(19)qi=Ki(1-w)Hih,n-Hiη,nzib-zic+wHih,n+1-Hiη,n+∗zib-zicwhere Hih,n and Hih,n+1 are the hydraulic head at the bed surface based on the surface flow known at tn and tn+1 respectively, (Appendix B), zb is the bed level and zc is the level of the imaginary boundary within the confined region discussed above (bold dashed line in Fig. 3).
This yields the first iteration for the hydraulic head within the beach at time level tn+1, Hiη,n+*.
The system of the confined region is then solved with the vertical flux, qi calculated from the Forchheimer resistance law (Eq.
(7)) using Hiη,n+* from the first iteration, i.e. as(20)qi=-aK2bK+12[(aKbK)2+4(zib-zic)bK[(1-w)(Hih,n-Hiη,n)+w(Hih,n+1-Hiη,n+∗)]]12
By continuity the vertical flux q is equal to the volume flux q through the beach surface in Eq.
(1).
This provides coupling between surface flow and groundwater flow at cross-shore locations where the groundwater flow is confined.
If the hydraulic head at the beach surface is greater than the hydraulic head within the confined region of the beach, the surface flow recharges groundwater and q is positive.
Otherwise, if the groundwater head becomes greater than the surface head, exfiltration occurs, i.e. q is negative.
In the backwash when the swash depth becomes zero, the beach starts to drain.
As no water enters the beach, the wetting tail, zt, initially at the level of the bed surface, starts to propagate downwards (Fig. 3).
Before the wetting front has merged with the groundwater the wetting tail moves at the same speed as the wetting front.
Afterwards the level of the wetting tail is given by zit,n+1=zit,n-(qin+1Δt)/θ, where q is the vertical flux through the imaginary boundary of the groundwater flow calculated using the wetting tail level, zit, and the corresponding hydraulic head instead of zib and Hih in Eqs.
(19) and (20).
Eq. (15) combined with Eqs.
(16), (18) and (20) is written for each cell in the confined and unconfined region of the beach.
Together these equations form a linear system.
Details of the system are presented in Appendix B.
The set of equations is solved using Gaussian elimination.
The most seaward groundwater cell is located at the position of the initial shoreline.
Groundwater flow between the beach toe and the most seaward cell is incorporated using the procedure described in the last paragraph of Appendix C.
Coupling of flow modules
Fig. 4 illustrates the coupling between the surface and subsurface flow solutions in a single computational time step (Δt=tn+1-tn).
In summary, the model for swash on permeable beaches takes the following steps:•
The surface flow variables above the bed (water depth h and velocity u) are calculated.
•
Trial values for the saturated thickness, hf,n*, and for the height of the groundwater level, hη,n*, are predicted based on the updated surface flow depth, hn+1, and the pore-air pressure in the unsaturated region of the beach, pa,n, at the previous time-level, tn.
The wetting front and groundwater estimate are obtained from the filtration module and from a quick procedure of the groundwater module respectively (Appendix C).
•
The pore-air pressure in the unsaturated region of the beach, pa,n+1, is determined by taking into account the updated surface depth, hn+1 and the wetting front and groundwater predictions, hf,n* and hηn*, respectively.
•
The filtration module solves for the corrected value of saturated thickness, hf,n+1 (and thus wetting front and tail level zf,n+1 and zt,n+1), and the vertical volume flux qn+1, with the hydraulic gradient calculated from the updated surface water depth, hn+1, and air pressure within the unsaturated region of the beach, pa,n+1.
•
The groundwater module solves the corrected hydraulic head within the beach based on the updated pore-air pressure in the unsaturated region of the beach, pa,n+1, and the updated surface water depth at the beach face, hn+1, and also solves the vertical volume flux, qn+1, across the confined region of the beach.
Comparison of numerical model and laboratory experiments
This section starts with the description of the numerical model set-up used to simulate the laboratory experiments of Steenhauer et al. (2011a).
The main body of the section is divided into three parts.
Section 3.2 assesses the overall ability of the numerical model to predict swash and subsurface flow within the beach by comparing the measured and model-predicted shoreline trajectory, water profiles, vertical hydraulic gradients and volume time-series.
Sections 3.3 and 3.4 present measured and model-predicted pressure and groundwater behaviour respectively.
Numerical set-up
The numerical model described in Section 2 is used to simulate the large-scale laboratory experiments presented in Steenhauer et al. (2011a).
Fig. 5 illustrates the set-up in a Cartesian system with x and z coordinates in the horizontal and vertical directions respectively, and with the origin at the initial shoreline position.
The experiments involved two beach materials with nominal sediment diameters of 1.5mm and 8.5mm.
The dam-break set-up is simulated through a water reservoir with initial water depth in the reservoir 600mm for experimental series R60PER015 of the 1.5mm beach and experimental series R60PER085 of the 8.5mm beach.
An initial water level of 62mm in front of the beach and a corresponding groundwater level of 62mm within the beach were used in both cases.
The initial shoreline position is defined through the initial water depth in front of the reservoir.
The slope of the beach was 1:10.
A detailed description of the experimental set-up, surface and subsurface measurements and results are presented in Steenhauer et al. (2011a).
The following lists the main physical input parameters for the numerical model presented in Table 1: i) Friction coefficient: The model uses a constant bed friction coefficient.
The value of the friction coefficient was established by tuning its value in order to match the measured and predicted maximum run-up for swash on an impermeable beach.
The measured values come from the experiments of Kikkert et al. (2012).
ii) Forchheimer coefficients: Stand-alone experiments using a constant head apparatus were undertaken (Steenhauer et al., 2011a) to measure the hydraulic resistance of the granular material.
Forchheimer coefficients and hydraulic conductivity were extracted from this dataset.
The calibration coefficient cK used in the groundwater module in Eq.
(17) has a value of 0.9.
iii) Air conductivity: The intrinsic permeability of the porous material, used in the subsurface flow model to determine the conductivity of air, was calculated from the linear Forchheimer coefficient (k=νaaKg with νa denoting the viscosity of water and g the gravitational coefficient).
iv) Porosity: A constant value for the effective porosity is used for both sediment types and is based on volume balance analysis of the surface and subsurface flow measurements (Steenhauer et al., 2011a).
v) Capillary fringe: For the 1.5mm beach the model used a capillary fringe of 20mm obtained by model calibration.
This value is lower than the 50mm reported in Steenhauer et al. (2011a), based on direct observation of the dark-coloured fringe through the glass wall of the flume.
The capillary fringe was added at the initial groundwater level and the wetting tail, whereas at the wetting front it is assumed zero.
For the 8.5mm beach the capillary fringe was zero (Steenhauer et al., 2011a).
The groundwater module was used only for the 8.5mm beach, where the groundwater response was directly observed at bore arrival.
In the 1.5mm beach the groundwater levels were not affected during the swash event, because the wetting front did not reach the groundwater within the swash cycle, due to the low permeability of the beach.
This means that the simulation for the 1.5mm beach is carried out with a constant groundwater level equal to the initial groundwater level.
All other modules were used for both beaches.
Surface and subsurface flow hydrodynamics
This section evaluates the ability of the numerical model to predict the swash and subsurface flow for permeable coarse-grained beaches.
Shoreline position
Fig. 6 presents the measured and model-predicted shoreline position for the 1.5mm and 8.5mm beaches.
Time t=0 corresponds to the opening of the reservoir gate.
The shoreline position is defined as the location near the tip where the water depth is 5mm (Kikkert et al., 2012).
Good agreement is generally seen between the model-predicted and measured shoreline position, in terms of the arrival of the bore and the movement of the tip on the slope in the uprush and early backwash.
In the later stages of the backwash on the 8.5mm beach the simulated rate of shoreline retreat is smaller than experimental values.
This is due to the limitations of the relatively simple groundwater model.
Water profiles
The measured and predicted water profiles at several selected times during the swash cycle are presented in Fig. 7 for the 1.5mm beach and in Fig. 8 for the 8.5mm beach.
In the case of the 1.5mm beach, wetting front measurements covered the region between x=1023mm and 4242mm, i.e. up to around maximum run-up; for the 8.5mm beach, wetting front measurements were made between x=1313mm and 3179mm, i.e. up to around maximum run-up.
Note that the wetting tail of the infiltration profile could not be extracted from the experiments, so only the model-predicted wetting tail is presented in the figures.
Overall the figures show good agreement between predictions and measurements.
The distinctly different behaviour of the subsurface flow for the two beaches is predicted well by the model.
The overall shape of the subsurface flow profile is captured, as is the general timing and level of the wetting front in both uprush and backwash phases.
The numerical predictions for the 1.5mm beach reveal that the hydraulic gradient becomes negative and reverses the initially downward-propagating wetting front causing exfiltration, especially visible at t=9.55s in Fig. 7, where in the region between approximately x=1200 and 1500mm the wetting front has almost returned back to the bed surface.
This agrees with the experimental findings of Steenhauer et al. (2011a), who could not directly record the upwards movement of the wetting front, but were able to infer it from the wetting front and pressure measurements.
The numerical simulation provides more detailed information on the build-up of pore-air pressure, which reaches a sufficiently high magnitude to not only decrease infiltration rates, but to reverse the flow, thus returning some of the infiltrated water back to the surface flow.
Exfiltration takes place at the lower end of the beach between approximately x=1200 and 1500mm with rates generally between -2 and -8.5mm/s.
In the case of the 8.5mm beach the numerical predictions over-estimate the groundwater level, particularly in the backwash.
This is probably due to the relatively simple parameterisation of the groundwater flow.
However, for both beaches, the comparison between the measured and model-predicted surface and subsurface water profiles is satisfying.
Vertical hydraulic gradients
Vertical hydraulic gradients governing the rates of infiltration and exfiltration assess the exchange of water between the surface and the subsurface.
Fig. 9 presents time-series of the hydraulic gradient at several cross-shore locations for the 1.5mm and the 8.5mm beaches.
Note that the results are given relative to time t0, which is the bore arrival time at each cross-shore location, and only while the wetting front is still moving towards the groundwater table.
Positive gradients are associated with infiltration.
For the 1.5mm beach the agreement between model predictions and measurements is quite good after t=2s.
At the early stages of swash, however, there is a disagreement: the experimental results show an approximately constant gradient whereas the simulation results suggest a steep increase upon the bore arrival, followed by a gradual decline.
At the initial stages of infiltration, when the height of surface water is much greater than the penetration depth, hydraulic gradients are expected to be higher than later on.
Furthermore, when the penetration depth is small the experimental error in evaluating gradients is large.
For these reasons the discrepancy between the simulation and the experimental results is probably due to experimental error.
For the 8.5mm beach the modelling produces a reasonably accurate trend at x=1980mm, while at x=2780mm the model fails to predict the initial sharp rise and the decline at the later stages of the swash.
In this case the discrepancy between experimental and numerical results is probably caused by the modelling, most likely by the limited capability of the groundwater module.
Overall the time-series of the hydraulic gradients display the steep increase at the time of bore arrival.
For both beaches hydraulic gradients gradually decrease with time, as the saturated zone above the wetting front becomes thicker.
Moreover, hydraulic gradients are significantly reduced by the build-up of pore-air pressure in the unsaturated region of the 1.5mm beach (Steenhauer et al., 2011a).
This pressure build-up is discussed in Section 3.3.
Volume time-series
Fig. 10 presents the cumulative volume of infiltrated water as a function of time for the 1.5mm and the 8.5mm beaches.
The solid line corresponds to the numerical predictions and the symbols correspond to measured volumes of infiltration using two independent methods: i) based on the surface flow depth measurements and ii) based on the measured subsurface profiles (Steenhauer et al., 2011a).
The agreement between model predictions and measurements is good for the uprush and early backwash, with the time of maximum run-up at approximately t=5.33s for the 1.5mm beach and at approximately t=4.52s for the 8.5mm beach.
Pressure within the beach
Build-up of pressure was more significant within the 1.5mm beach.
This section therefore compares the model-predicted and measured pressure within the 1.5mm beach.
Fig. 11 presents the numerical and experimental pressure head time-series at several cross-shore locations.
The measurements at each x-location consisted of several pressure transducers positioned at various depths below the wetting front during the whole of the swash cycle (Steenhauer et al., 2011a) and presented in Fig. 11 as dashed lines.
The pressure head within the unsaturated region of the beach is expressed in mm of water, with Δpa the pressure relative to atmospheric pressure pA.
The arrival of the pressure front, i.e. the time when pressure first changes, is predicted very well at all locations.
The pressure front caused by air entrapment as shown in Steenhauer et al. (2011a) propagates through the unsaturated region of the beach in the shoreward direction.
There is also good agreement with regard to the magnitude of the pressure head: the numerical predictions display a similar rise and fall in pressure head as observed in the measurements.
The effects of exfiltration and the associated air escape from the unsaturated region of the 1.5mm beach are seen at x=1980mm (top panel of Fig. 11) as short periods of rapid decline in pressure close to the end of the swash event, at approximately t=9.3s in the model and slightly earlier in the experiments.
The rapid decline results from the additional pressure outlet occurring at the lower end of the beach (approximately between x=1200 and 1500mm, visible in Fig. 7).
The outlet is formed when the negative hydraulic gradient, created by the encapsulated air, has managed to push the wetting front back to the beach surface.
After the outlet is opened air is not only being released at locations beyond maximum run-up, but also at locations lower down the beach.
The pressure build-up occurring within the unsaturated region of the 1.5mm beach relates to a corresponding variation in air density relative to the air density at atmospheric pressure, Δρa/ΔρA, which ranges between 0 and 1.2%.
The interrelated flow processes of the subsurface region in the 1.5mm beach are complex.
Nevertheless, the model, despite some discrepancies, is able to capture the pressure behaviour affected by the water exchange between the swash and the subsurface quite well.
Groundwater behaviour
In contrast to the 1.5mm beach, within the 8.5mm beach infiltration was rapid, and the wetting front reached the groundwater level across the majority of the swash zone during the uprush.
Groundwater response was hence simulated only in the case of the 8.5mm beach and discussed in the following.
Fig. 12 shows the surface and hydraulic head profiles obtained from the model and experiments at several selected times.
The agreement between the measured and model-predicted head profiles is good.
Fig. 13 presents time-series of the hydraulic head within the beach, Hη, at two cross-shore locations.
The measured values for the hydraulic head are based on the pressure results of the lowest pressure transducer within the beach (Steenhauer et al., 2011a).
For a short time at the start of the increase in hydraulic head, both the measured and model-predicted head display a small rate of hydraulic head increase resulting from the horizontal hydraulic gradients.
They are mainly generated by the increased surface water levels at the lower end of the beach directly recharging groundwater flow.
Furthermore, air entrapment between the wetting front and the groundwater surface increases pore-air pressure and hence creates a horizontal pressure gradient which induces air flow as well as groundwater flow in the shoreward direction (Steenhauer et al., 2011a).
This period of slow increase lasts until the moment when the wetting front reaches the groundwater, marked by the steep increase in hydraulic head.
The groundwater level then rapidly rises towards the bed surface, before subsequently decreasing as the beach starts to drain in the backwash (Steenhauer et al., 2011a).
Although the overall agreement for the 8.5mm sediment is quite good, several discrepancies are evident.
The time that the wetting front reaches the groundwater occurs slightly earlier in the numerical predictions than in the experimental results, e.g.
at approximately t=2.8s versus t=2.9s for x=1180mm and at approximately t=3.8s versus t=4s for x=1980mm of Fig. 13.
This is probably due to the over-prediction of the shoreline velocities and water depths in the surface flow model (visible in Fig. 12).
As a result infiltration is not only induced sooner by the earlier arrival of the bore, but also the hydraulic gradient that drives the flow is over-estimated, enhancing the wetting front propagation towards the groundwater table.
Furthermore the drainage of the backwash is not well simulated.
Overall the beach in the experiments shows a quicker drainage than is observed in the numerical predictions.
This, as mentioned before, is due to the relatively simple parametrisation of the groundwater flow.
Overall the agreement in the uprush and early backwash between the model and experimental results for hydraulic head within the beach is reasonably good.
Steenhauer et al. (2011a) defined the saturation boundary as the cross-shore limit of the fully saturated beach, i.e. the cross-shore location where the wetting front just merges with the groundwater.
The predicted and measured time-series of the saturation boundary are shown in Fig. 14.
Early in the backwash the position of the saturation boundary becomes equivalent to what is more commonly referred to as the exit point (Steenhauer et al., 2011a).
The discrepancy between the numerical and experimental results of the position of the saturation boundary is again most likely due to the relatively simple parametrisation of the groundwater flow.
However, the overall agreement between model-predicted and measured saturation boundary position is satisfactory.
Conclusions
The swash model of Steenhauer et al. (2011b) simulating the movement of a steep bore over a permeable coarse-grained beach and the movement of the wetting front within the beach has been extended to include the behaviour of air entrapment and groundwater.
The new model has been validated by comparing model predictions with measurements from the large-scale laboratory experiments of Steenhauer et al. (2011a).
The model has been used to gain more insights into key mechanisms of the water exchange between surface and subsurface, which cannot be easily studied via experiments alone.
The following summarises the key findings of the paper:•
A numerical model for swash on permeable beaches has been developed.
The model includes a novel approach to simulating the behaviour of air entrapped between the wetting front and the groundwater.
This air pressure module solves for the horizontal pore-air movement within the unsaturated region of the beach, based on a mass balance equation of air using Darcy's law to parametrise flow resistance.
•
The numerical predictions of surface and subsurface flow behaviour for a bore-driven swash on two permeable coarse-grained beaches (1.5mm and 8.5mm sediment) are in good agreement with large-scale laboratory swash measurements.
The numerical results capture the main swash flow features and wetting front profiles in uprush and backwash, and give good predictions of hydraulic gradient and infiltrated volume time-series across the swash zone for both beaches.
The numerical predictions of the pore-air pressure build-up within the two sediments are in good agreement with the large-scale laboratory measurements.
The time when pressure first changes (i.e. the arrival of the pressure front) is well-predicted across the swash zone, as are the magnitudes of pressure head during uprush and backwash.
The numerical model captures well the groundwater response to surface-subsurface water exchange, with reasonable agreement between the model-predicted and measured exit point during the backwash.
Discrepancies between model and experimental results are primarily due to the relatively simple parametrisation of the bed shear stress for the surface flow and the approximation of the subsurface flow as (coupled) one-dimensional processes.
•
The numerical predictions for a bore-driven swash on two steep coarse-grained beaches are thus consistent with the experimental results of Steenhauer et al. (2011a) and the model is considered validated for the level of accuracy required for engineering applications.
•
The numerical results give further insight into the role of air entrapment, which significantly impedes infiltration into the 1.5mm beach, and even reverses the flow at the lower end of the beach during the backwash, generating exfiltration with rates between -2 and -8.5mm/s.
•
The numerical study shows that when the upwards-driven wetting front has reached the level of the bed surface of the 1.5mm beach, a pathway is created to release the air, at a higher pressure than atmospheric pressure, entrapped within the beach.
Entrapped air is then not only released through the unsaturated region of the beach beyond the shoreline position, but also through the additional flow paths created at the lower end of the beach, where the beach has returned to its initially unsaturated state.
•
The importance of modelling infiltration/exfiltration in and out of the unsaturated region, as well as pore-air pressure build-up within the unsaturated region of the beach, is shown by the numerical study.
So far these physical processes have been neglected in swash models.
The insights from the numerical work significantly contribute to the better understanding and modelling of key physical processes for swash and similar flows.
Acknowledgements
This work was carried out as part of Kate Steenhauer's PhD study in the School of Engineering at the University of Aberdeen, with funding provided by the University of Aberdeen.
The work was conducted in conjunction with EPSRC-funded research (grant number EP/E011330/1) at the University of Aberdeen.
The system of equations for the air flow module is combined into a linear system expressed for one computational cell i as:(21)wa1pi-1a,n+1-(a1+a3+a2+a5/w)pia,n+1+wa3pi+1a,n+1=Bwhere the individual component coefficients of system (21) are given as:(22)a1=ΔtΔxkρi-1a,nνaρi-1/2a,nhi-1a,n+hia,n2a2=ΔtΔxkρia,nνaρia,n2hia,na3=ΔtΔxkρi+1a,nνaρi+1/2a,nhia,n+hi+1a,n2a4=Δxθρia,nhia,na5=Δxθρia,n(hia,n-dhif,n∗-dhiη,n∗)(pia,n)(1γ-1)(pia,n)1γB=-Wa1pi-1a,n+(a1+a3+a2)Wpia,n-Wa3pi+1a,n-a1ρi-1/2a,ng(zi-1n-zin)+a3ρi+1/2a,ng(zin-zi+1n)+a2[ρia,ng(zin+1-zib)-pA]-a4where 0≤w≤1 denotes the weighting coefficient, where W=(1-w), k is the intrinsic permeability, νa is the viscosity of air, ρa is the air density within the unsaturated region of the beach, with ρi+1/2a=12(ρia+ρi+1a), zib is the level of the bed surface, zi is the level at the middle of the unsaturated cell i, hia is the height of the unsaturated cell i, pA is the atmospheric pressure, pia is the pressure within the unsaturated cell i, θ denotes the effective porosity, Δx is the length of cell i, γ is the heat capacity ratio and the change in wetting front and groundwater level, dhif,n* and dhiη,n*, respectively, is based on a prediction (trial value) evaluated earlier within the same time-step (see the coupling of flow modules in Fig. 4).
The value for dhif,n* is obtained from the infiltration module.
The method for obtaining the value for dhiη,n* is based on a quick procedure of the groundwater module described in Appendix C.
The system of equations for the groundwater module is combined into a linear system expressed for one computational cell i as:(23)Ki-1/2wa1Hi-1η,n+1-(Ki-1/2wa1+Ki+1/2wa2+a4)Hiη,n+1+Ki+1/2wa2Hi+1η,n+1=-B1+B2where the individual coefficients of system (23) for the confined and unconfined region of the beach are given as:(24)a1=hiη,n+hi-1η,n2Δxa2=hiη,n+hi+1η,n2Δxa3=Δxzib-zica4={Kiwa3:Iteration1for confined beach(Darcy flux forq)0:Iteration2for confined beach(Forchheimer law forq)θΔxΔt:Unconfined beachB1=Ki-1/2Wa1Hi-1η,n-(Ki-1/2Wa1+Ki+1/2Wa2)Hiη,n+Ki+1/2Wa2Hi+1η,nB2={-Kia3[(WHih,n+wHih,n+1)-WHiη,n]:Iteration1forconfinedbeach-Δxqi:Iteration2forconfinedbeacha4(-pia,n+1ρg-Hiη,n+pia,nρg):Unconfinedbeachwhere 0≤w≤1 denotes the weighting coefficient, where W=(1-w), Hiη is the hydraulic head within the beach in cell i, for the unconfined region of the beach Hiη=hiη+piaρg, hiη is the height of the groundwater in cell i, pia is the air pressure within the unsaturated cell i, ρ is the density of water, g is the gravitational acceleration, Hih is the hydraulic head at the level of the bed based on the surface flow (Hih=zib+hih+pAρg), pA is the atmospheric pressure and qi is the vertical flux expressed in Eq. (20).
The hydraulic conductivity within system (23) is calculated as Ki-1/2=1/(aK+bKcK|ui-1/2p|), where aK and bK are the Forchheimer coefficients, and where the horizontal pore velocity is given by up,i-1/2=Qi-1/2η12(hiη+hi-1η) and where the coefficient cK has a value of 0.9 obtained by model calibration.
In the first iteration the value for the hydraulic conductivity, Ki-1/2, is taken at previous time tn.
In the second iteration the value, Ki-1/2, is updated based on the hydraulic head results obtained from the first iteration.
The seaward boundary of system (23) for the groundwater module is described in the last paragraph of Appendix C.
This section describes the quick procedure of the groundwater module used in calculating the predicted change in groundwater levels within the unconfined region of the beach for the air flow module (see the coupling of flow modules in Fig. 4).
A trial value for the change in groundwater level, dhiη,n∗, is only evaluated for unconfined region of the beach, where the air is entrapped between the wetting front and the groundwater.
Flow through the entire confined region (from the toe of the beach until the most shoreward confined cell, s, Fig. 15) is evaluated using the average horizontal hydraulic gradient across the beach and the Darcian approximation of flow resistance.
The hydraulic gradient is expressed using the hydraulic head at the beach surface at a cross-section M situated in the middle of the fully saturated region, i.e. at xM=12(xtoe+xs+1/2):(25)HMη=zMb+hM+pAρgwhere hM is the water depth above the bed, with the value taken at time level tn+1, pA is the atmospheric pressure and zMb is the level of the bed surface at xM.
The discharge from the confined region into the unconfined region is therefore evaluated as:(26)Qs+1/2η=Khsη+hs+1η2HMη-Hs+1ηxs+1/2-xM
Position of cell s varies with time, as the confined part of the beach becomes longer when the wetting front reaches the groundwater further in the beach (Fig. 15).
The discharge, Qs+1/2η, is the boundary condition for the unconfined groundwater module.
The module itself uses the procedure described in Section 2.4 and Appendix B, but it is now fully explicit, i.e. w=0, and Darcian, i.e.
K=1/aK.
This means that the groundwater level in each unconfined cell can be calculated directly from Eq. (23).
Similar simplified treatment is applied to the left-hand boundary of the groundwater module described in Appendix B.
The discharge through the fully saturated region of the beach between the toe of the beach and the position of the initial shoreline (approximately x=-600 and 0mm) is calculated from Eq.
(26), with HMη based on Eq.
(25) taking for xM, the position midway between the toe of the beach and the initial shoreline.
The resulting relationship is the seaward boundary condition for the groundwater model.
It provides direct coupling between the surface flow and the horizontal groundwater flow.
Table 2 gives a list of symbols.


Generation of familial amyloidotic polyneuropathy-specific induced pluripotent stem cells

Abstract
Familial amyloidotic polyneuropathy (FAP) is a hereditary amyloidosis induced by amyloidogenic transthyretin (ATTR).
Because most transthyretin (TTR) in serum is synthesized by the liver, liver transplantation (LT) is today the only treatment available to halt the progression of FAP, even though LT is associated with several problems.
Despite the urgent need to develop alternatives to LT, the detailed pathogenesis of FAP is still unknown; also, no model fully represents the relevant processes in patients with FAP.
The induction of induced pluripotent stem (iPS) cells has allowed development of pluripotent cells specific for patients and has led to useful models of human diseases.
Because of the need for a tool to elucidate the molecular pathogenesis of FAP, in this study we sought to establish heterozygous ATTR mutant iPS cells, and were successful, by using a Sendai virus vector mixture containing four transcription factors (Oct3/4, Sox2, Klf4, and c-Myc) to reprogram dermal fibroblasts derived from FAP patients.
Moreover, FAP-specific iPS cells had the potential to differentiate into hepatocyte-like cells and indeed expressed ATTR.
FAP-specific iPS cells demonstrated the possibility of serving as a pathological tool that will contribute to understanding the pathogenesis of FAP and development of FAP treatments.
Highlights
•
We established, for the first time, FAP-specific iPS cells by using a Sendai virus.
•
FAP-specific iPS cells have potential to differentiate into hepatocyte-like cells.
•
Hepatocyte-like cells from FAP-specific iPS cells indeed expressed ATTR Val30Met.
•
FAP-specific iPS cells demonstrate the possibility to serve as a pathological tool.

Introduction
Transthyretin (TTR) is a β-sheet-rich protein that is mainly synthesized by the liver (Buxbaum and Reixach, 2009).
TTR normally serves as a plasma transport protein for thyroid hormone and retinol-binding protein with vitamin A (Kanai et al., 1968).
Mutant forms of TTR, however, cause familial amyloidotic polyneuropathy (FAP), which is the most common type of autosomal-dominant hereditary systemic amyloidosis (Saraiva et al., 1983; Ando et al., 2005).
As of today, more than 100 different points of mutation and a deletion in the TTR gene have been reported (Westermark et al., 2002; Ando and Ueda, 2012; Benson and Kincaid, 2007), with the Val30Met mutation being the most common.
Systemic amyloid depositions in FAP cause various symptoms, including cardiac and renal dysfunctions, gastrointestinal disorders, glandular and autonomic dysfunctions, and peripheral neuropathy (Ando and Suhr, 1998; Ando et al., 1993, 1997; Araki, 1984).
Because the liver synthesizes most of the TTR in the serum, liver transplantation (LT) is the only treatment available to halt the progression of FAP (Ando et al., 1995a; Suhr et al., 1995).
Although LT is widely accepted as the only lifesaving treatment option for FAP patients (Ando et al., 1995b), LT involves several problems, such as a shortage of liver donors, the effects of immunosuppressants, and the progression of ocular disorders caused by a continuing production of amyloidogenic TTR (ATTR) by the retina (Ong et al., 1994; Kawaji et al., 2005).
Despite an urgent need to develop alternatives to LT, details about the mechanism of amyloid formation in FAP are still unknown.
Although attempts were made to establish experimental models of FAP, a suitable tool is not yet available (Buxbaum et al., 2003; Pokrzywa et al., 2007; Berg et al., 2009).
Because FAP is an autosomal-dominant inherited disease, TTR secreted into plasma is the heterotetrameric mixture of wild-type TTR and variant TTR.
However, all TTRs used in experiments have been homotetramers, and an artificial Val30Met-overexpressed cell system does not fully represent the relevant processes in patients with FAP (Sousa et al., 2000; Cardoso et al., 2008; Sato et al., 2007).
Therefore, an urgent need exists to establish an experimental model such as heterozygous ATTR mutant cells derived from FAP patients.
Induced pluripotent stem (iPS) cells have an unlimited replicative ability and the potential to differentiate into most cell types in organisms (Takahashi and Yamanaka, 2006; Takahashi et al., 2007; Yu et al., 2007).
The creation of iPS cells has permitted the development of patient-specific pluripotent cells and has led to useful models of human diseases (Saha and Jaenisch, 2009).
Recent studies reported success in generating patient-specific iPS cells for various diseases including neurologic (Dimos et al., 2008; Ebert et al., 2009; Soldner et al., 2009), hematologic (Raya et al., 2009), and metabolic disorders (Maehr et al., 2009).
A report on spinal muscular atrophy-specific iPS cells suggested applications to disease modeling and drug screening by showing the disease-specific changes in cell survival and function (Ebert et al., 2009).
A report on Fanconi anemia-specific iPS cells also indicated a potential value for cell therapy by correcting the genetic defect before iPS cell derivation (Raya et al., 2009).
Thus, although disease-specific iPS cells may help therapeutic research, iPS cells from patients with FAP have not yet been generated.
In this study, we first report the generation of iPS cells from patients with FAP ATTR Val30Met, which we achieved by reprogramming their fibroblasts with a mixture of Sendai virus (SeV) vector, which does not integrate into the host genome (Li et al., 2000) and has a low risk of tumorigenicity, encoding four transcription factors: octamer 3/4 (Oct3/4), sex-determining region Y box 2 (Sox2), Kruppel-like factor 4 (Klf4), and c-Myc (Fusaki et al., 2009; Ban et al., 2011).
FAP-specific iPS cells indeed differentiated into hepatocyte-like cells (Shiraki et al., 2008; Shiraki et al., 2011) and expressed Val30Met ATTR.
FAP-specific iPS cells may thus provide valuable experimental tools to elucidate the molecular pathogenesis of FAP with a potential value for cell therapy applications.
Materials and methods
Reagents
Reagents were purchased and used at the designated concentrations as follows: recombinant human Activin A (HumanZyme, Chicago, IL), 100ng/ml; recombinant human basic fibroblast growth factor (ReproCELL, Yokohama, Japan); recombinant human Bone morphogenetic protein 4 (R&D systems, Minneapolis, MN), 10ng/ml; recombinant human Fibroblast growth factor 10 (PeproTech, London, UK), 10ng/ml; recombinant human hepatocyte growth factor (PeproTech), 10ng/ml; dexamethasone (Dex; Sigma, St.
Louis, MO), 1μM; Y-27632 (Rho-associated kinase inhibitor; Wako Chemical, Osaka, Japan), 10μM; Dulbecco's Modified Eagle Medium (Gibco, Funakoshi, Tokyo, Japan); RPMI-1640 (Invitrogen, Glasgow, UK); B27 supplement (Invitrogen), nonessential amino acids (Gibco); l-glutamine (Gibco); knockout serum replacement (Gibco); polyclonal rabbit anti-human TTR antibody (Dako, Glostrup, Denmark); Mouse anti-Oct3/4 antibody (Santa Cruz Biotechnology, Texas, USA), Rabbit anti-Alphafeto protein antibody (Dako), Goat anti-human Albumin antibody (Bethyl, Texas, USA), Goat anti-Sox17 antibody (R&D systems); and Rabbit anti-HNF-4α antibody (Santa Cruz Biotechnology).
Patients
Skin biopsy samples were obtained from three Japanese female patients with FAP ATTR Val30Met in Kumamoto University Hospital.
All FAP patients in this study had a definitive diagnosis of FAP on the basis of genetic investigations and clinical manifestations of FAP (Table 1).
The research followed the guidelines of the Kumamoto University Ethical Committee.
Generation of iPS cells
Skin fibroblasts from patients with FAP ATTR Val30Met were maintained in Dulbecco's Modified Eagle Medium (Gibco) supplemented with 10% fetal bovine serum.
Induction of human iPS cells was performed as described previously (Fusaki et al., 2009; Ban et al., 2011).
In brief, 1×106 human fibroblasts BJ from neonatal foreskin (American Type Culture Collection, Manassas, VA) or FAP patients' fibroblasts were infected with conventional SeV vectors carrying Oct3/4, Sox2, Klf 4, and a temperature-sensitive vector, which has c-Myc at the HNL position at a multiplicity of infection of 3.
One week after infection, cells were collected and replated on mitomycin C-treated mouse embryonic fibroblast feeder cells.
The next day, medium was changed to a primate embryonic stem cell medium supplemented with 10ng/ml basic fibroblast growth factor.
Embryonic stem cell-like colonies were picked up 28days after infection.
At passage 3, many colonies were negative for the SeV vector; if the vector was present, SeV-negative clones were obtained by incubating cells at nonpermissive temperature (38°C for 3days).
The reprogramming efficiency was calculated as the number of alkaline phosphatase (ALP)-positive embryonic stem cell-like colonies formed per number of infected cells seeded.
Differentiation into hepatocyte-like cells
FAP-specific iPS cells were differentiated into hepatocyte-like cells using feeder free method modified from reported protocols (Shiraki et al., 2008, 2011), with the following modifications: FAP-specific iPS cells were pretreated overnight with 10μM Y-27632 (Wako Chemical) and were then dissociated by using 0.25% trypsin-EDTA and were plated at 100,000 cells per well in 96-well plates that had been previously coated with fibronectin.
The cells were cultured for 5days in DMEM supplemented with 100ng/ml Activin A (HumanZyme), 2% B27 supplement (Invitrogen), nonessential amino acids (Gibco), l-glutamine (Gibco), penicillin and streptomycin, and β-mercaptoethanol.
For differentiation from D5 to D7, RPMI-1640 (Invitrogen) supplemented with 10ng/ml BMP4, 10ng/ml Fgf10 and 2% B27 supplement, nonessential amino acids (Gibco), l-glutamine (Gibco), penicillin and streptomycin, and β-mercaptoethanol.
The medium was then changed to hepatic differentiation medium-DMEM supplemented with 10% knockout serum replacement (Gibco), 10ng/ml hepatocyte growth factor (PeproTech), 1μM Dex (Sigma), nonessential amino acids, l-glutamine, penicillin and streptomycin, and β-mercaptoethanol-for up to 20days.
Medium was replaced every day (Day 0-5) or every 2days (Day 5-20) with fresh differentiation medium supplemented with growth factor.
Human iPS cell lines (201B7), the first iPS cells established from human dermal fibroblasts by Dr. Shinya Yamanaka, and HepG2 cells, a human hepatocellular carcinoma cell line, were used as control (Takahashi et al., 2007).
For albumin and TTR secretion assay, definitive endoderm cells (Day 5) were dissociated with 0.25% trypsin-EDTA (Invitrogen) and then plated at 200,000 cells/well on a synthemax (corning) pre-coated 96-well plate, and culture up to Day 20 using the abovementioned protocol.
ALP staining
ALP staining was performed with ALP substrate (1-Step NBT/BCIP; Pierce, IL, USA) after fixation with 10% neutral buffered formalin solution (Wako Chemical), as previously described (Fusaki et al., 2009; Ban et al., 2011).
Reverse-transcription polymerase chain reaction (RT-PCR) analysis
RNA extraction, cDNA synthesis, and RT-PCR were performed as described previously (Fusaki et al., 2009; Shiraki et al., 2008).
The following primers were used: NANOG homeobox (NANOG): forward 5′-TACCTCAGCCTCCAGCAGAT-3′, reverse 5′-TGCGTCACACCATTGCTATT-3′; telomerase reverse transcriptase (TERT): forward 5′-TGCCCGGACCTCCATCAGAGCCAG-3′, reverse 5′-TCAGTCCAGGATGGTCTTGAAGTCTG-3′; SeV: forward 5′-TGGCTAAGAACATCGGAAGG-3′, reverse 5′-GTTTTGCAACCAAGCACTCA-3′; and β-actin: forward 5′-CAACCGCGAGAAGATGAC-3′, reverse 5′-AGGAAGGCTGGAAGAGTG-3′.
Real-time PCR analysis
The real-time PCR conditions were as follows: denaturation at 95°C for 15s and annealing and extension at 60°C for 60s, for up to 40cycles.
Target messenger RNA (mRNA) levels were expressed as arbitrary units and were determined by using the standard curve method.
The following primers were used: Oct3/4: forward 5′-AGGTGTGGGGGATTCCCCCAT-3′, reverse 5′-GCGATGTGGCTGATCTGCTGC-3′; Sox17: forward 5′-ACTGCAACTATCCTGACGTG-3′, reverse 5′-AGGAAATGGAGGAAGCTGTT-3′; alpha-fetoprotein (AFP): forward 5′-TGCCAACTCAGTGAGGACAA-3′, reverse 5′-TCCAACAGGCCTGAGAAATC-3′; albumin (ALB): forward 5′-GATGTCTTCCTGGGCATGTT-3′, reverse 5′-ACATTTGCTGCCCACTTTTC-3′; TTR: forward 5′-CATTCTTGGCAGGATGGCTTC-3′, reverse 5′-CT CCCAGGTGTC ATCAGCAG-3′; and GAPDH: forward 5′-CGAGATCCCTCCAAAATCAA-3′, reverse 5′-CATGAGTCCTTCCACGATACCAA-3′.
Immunocytochemistry
For whole-mount immunocytochemical analysis, iPS cell cultures were fixed in 4% paraformaldehyde in phosphate-buffered saline (PBS) for 30min, followed by permeabilization with 0.1% Triton-X (Nakalai Tesque) in PBS for 10min at room temperature, rinsed several times with PBS then incubated with diluted antibody in 20% Blocking One (Nakalai Tesque) in PBST (0.1% Tween-20 in PBS) in a humidified chamber overnight at 4°C.
Cells were washed in PBST, and incubated with secondary antibody in 20% Blocking One for 2h at room temperature in the dark.
After washing off the secondary antibody in PBST, cells were counterstained with 6-diamidino-2-phenylindole (DAPI) (Roche Diagnostics, Indianapolis, IN).
The following antibodies were used as first antibodies: rabbit anti-AFP (Dako), Goat anti-human Albumin antibody (Bethyl), anti-Oct3/4 antibody (Santa Cruz Biotechnology), goat anti-Sox17 antibody (R&D systems), rabbit anti-HNF-4α antibody (Santa Cruz Biotechnology), and anti-HN monoclonal antibody IL4.1 (Fusaki et al., 2009).
Secondary antibodies used were Alexa 568-conjugated and Alexa 488-conjugated antibodies (Invitrogen).
To assess the efficiency of differentiation, Sox17-, AFP-, and ALB-positive cells versus total cells (DAPI-positive cells) were quantified using ImageXpress Micro cellular imaging system (Molecular Devices Sunnyvale, CA).
PAS analysis
The cultured cells were fixed in 3.3% formalin for 10min, and intracellular glycogen was stained using a PAS staining solution (Muto Pure Chemicals, Tokyo, Japan), according to the manufacturer's instructions.
Albumin secretion assay
The differentiation medium was changed to fresh medium 48h before the assay.
Albumin secretion was measured by the central clinical laboratory at Kumamoto University, Kumamoto, Japan.
PCR-restriction fragment length polymorphism analysis (PCR-RFLP)
Extraction of RNA from HepG2 cells and the frozen samples of livers from FAP patients A002 and cDNA synthesis were performed as previously described (Fusaki et al., 2009).
RNA was extracted from FAP-specific iPS cells with the RNeasy Micro Kit (Qiagen, Hilden, Germany) according to the manufacturer's protocol, and cDNA synthesis was performed as described elsewhere (Sueyoshi et al., 2012).
PCR primers, which were the same as those used in the real-time PCR reaction, were designed to amplify the Val30Met mutation in exon 2 of the TTR gene.
The PCR conditions were 5min at 98°C; 35cycles at 94°C for 30s, 55°C for 30s, and 72°C for 1min; followed by 7min at 72°C.
The PCR product of 199bp was digested at 37°C for 2h with 5U Nsi1 restriction enzyme (New England Biolabs, Ipswich, MA, USA), which recognized the mutation site.
PCR products were evaluated via a microchip electrophoresis system (Cosmo-I SV1210; Hitachi Electronics Engineering, Tokyo, Japan).
Measurements were obtained according to the manufacturer's manual.
Surface-enhanced laser desorption/ionization time-of-flight mass spectrometry (SELDI-TOF MS)
Serum specimens and culture supernatants were evaluated with the PCS 4000 SELDI-TOF MS instrument (Bio-Rad Laboratories, Hercules, CA, USA) by using the following protocol (Ueda et al., 2009), unless otherwise specified: ion focus mass, 13,800m/z; laser energy, 2000nJ; matrix attenuation, 500m/z; sample rate, 800; shots/pixel, 5; partition, 1 of 4; and acquired mass range from 0 to 100,000m/z.
Baseline smoothing: smoothing before fitting baseline, window 25 points.
Baseline width: automatic.
These baseline settings were the default settings.
External calibration of the instrument was performed by using the All-in-One protein molecular mass standard (Bio-Rad Laboratories).
Conditions: Q10 ProteinChip in 50mmol/l phosphate buffer, pH7.0.
Liquid chromatography-tandem mass spectrometry (LC-MS/MS)
The Amicon Ultra centrifugal filter 10K (Millipore, Billerica, MA, USA) was used to make 7-10-fold concentrated culture supernatants.
The concentration was incubated overnight at 4°C with 5μg of polyclonal rabbit anti-human TTR antibody (Dako).
PureProteome Protein G Magnetic Beads (Millipore) were added to the reaction to capture the immune complexes and were agitated for 2h at 4°C.
After the immune complexes were washed three times with PBST, 25μl of sample buffer (Bio-Rad Laboratories) was added and incubation proceeded for 5min at 95°C.
Sample buffers (25μl), which eluted TTR protein from the beads, were fractionated via sodium dodecyl sulfate-polyacrylamide gel electrophoresis.
Silver staining of gels was performed with ProteoSilverTM Plus Silver Stain Kit (Sigma-Aldrich) according to the manufacturer's protocol.
The bands of TTR were excised from the gel, destained according to the manufacturer's protocol, digested with sequence-grade modified trypsin (Promega, Madison, WI).
The peptide mixtures were dried and redissolved with 40μl of MS-grade water containing 0.1% trifluoroacetic acid and 2% acetonitrile, and were used for nano-flow reversed-phase LC-MS/MS (LTQ Velos Pro; Thermo Fisher Scientific, San Jose, CA).
A capillary reversed-phase LC-MS/MS system composed of an Advance Splitless Nano-Capillary LC dual solvent delivery system (Bruker-Michrom, Auburn, CA), an HTS-xt PAL autosampler (CTC Analytics, Zwingen, Switzerland), and LTQ Velos Pro equipped with an XYZ nanoelectrospray ionization source (AMR, Tokyo, Japan) was used.
The samples were injected into a peptide L-trap column (Chemical Evaluation Research Institute, Tokyo, Japan).
The peptides were separated by using a capillary reversed-phase C18 column (Chemical Evaluation Research Institute) with gradient elution and an ion spray into the mass spectrometer at a spray voltage of 2.3kV.
The peptide and fragment mass tolerances were 2.0Da and 0.8Da, respectively.
To calculate the ratios of wild-type TTR and ATTR to total TTR in culture supernatants, we measured the areas of the peptide mass peaks derived from wild-type TTR and ATTR, respectively.
Peptides (22-34 peptides; GSPAINVAVHVFR) derived from wild-type TTR had a molecular mass of 684m/z.
Peptides (22-34 peptides; GSPAINVAMHVFR) derived from ATTR had a molecular mass of 700m/z.
Western blotting
As described in the LC-MS/MS section, the immune complexes, made from 20-fold concentrated culture supernatants, were washed three times with PBS, 20μl of sample buffer (Bio-Rad Laboratories) was added and incubation proceeded for 5min at 95°C.
Sample buffers (10μl), which eluted TTR protein from the beads, were fractionated via sodium dodecyl sulfate-polyacrylamide gel electrophoresis and transferred to nitrocellulose membranes (GE Healthcare, Buckinghamshire, UK).
Membranes were blocked with 5% nonfat dried milk and PBST and were then incubated overnight at 4°C with antibodies against TTR in 5% bovine serum albumin (Sigma) and PBST.
After the membranes were washed, they were incubated in biotinylated secondary antibodies for 1h and then in horseradish peroxidase-conjugated streptavidin for 1h.
After this process, specific protein bands were detected with an enhanced chemiluminescence system (Amersham Pharmacia Biotech, Buckinghamshire, UK).
Enzyme-Linked ImmunoSorbent Assay (ELISA)
The concentration of TTR protein in the media of differentiated FAP-specific iPS cells was measured by ELISA assay.
The differentiation medium was changed to fresh medium 48h before the assay.
The wells of Nunc-ImmunoTM plate II (Thermo Fisher Scientific) were coated with 7000-fold dilution of TTR sheep anti-human polyclonal antibody (LifeSpan BioSciences, Seattle, USA) in carbonate/bicarbonate buffer and incubated overnight at 4°C.
The coating buffer was removed and the wells were washed three times with 200μl PBST.
250μl blocking buffer and 0.5% gelatin in the coating buffer were added per well and incubated for 1h at room temperature.
The blocking buffer was removed and the wells were washed as described above.
100μl of appropriately diluted samples and serum as the standard were added to each well and incubated for 1h at room temperature.
The samples and standard were removed and the wells were washed as described above.
100μl of 10,000-fold diluted polyclonal rabbit anti-human TTR antibody (Dako) in 0.5% gelatin in the PBST was added to each well and incubated for 1h at room temperature.
The detection antibody was removed and wells were washed as described above.
100μl of 5000-fold diluted polyclonal goat anti-rabbit immunoglobulins/HRP (Dako) in 0.5% gelatin in the PBST was added to each well and incubated for 1h at room temperature.
The secondary antibody was removed and the wells washed as described above.
100μl of TMB (KPL, Gaithersburg, USA) solution was added to each well and 1 and half min later, equal volume of stop solution (1M HCL) was added and read the optical density at 450nm.
Results
Generation of iPS cells from patients with FAP
Dermal fibroblasts obtained from heterozygotic FAP Val30Met patients (Table 1) were cultured and infected with SeV vectors encoding the reprogramming factors Oct3/4, Sox2, Klf4, and c-Myc.
Forms of TTR in the serum of one FAP patient (A002) were analyzed by using SELDI-TOF MS, which detected peaks of approximately 13,761Da for wild-type TTR and 13,792Da for ATTR Val30Met (Fig. 1).
The reprogrammed cells were positive for ALP activity (Fig. 2A).
RT-PCR confirmed the expression of pluripotency markers such as NANOG and TERT in FAP-specific iPS cells.
SeV vectors were diluted during cell growth and removed after the temperature shift treatment (at 38°C for 3days) (Fig. 2B).
Immunostaining with the antibody against SeV protein demonstrated the clones free of viral proteins (Fig. 2C).
The reprogrammed cells exhibited the ES-like morphology and the expression of pluripotency marker Oct3/4 was confirmed by immunostaining (Fig. 2D).
The induction efficiency of FAP-specific iPS cells ranged from 0.054% to 0.41% for each individual (Table 1).
Differentiation of FAP-specific iPS cells into hepatocyte-like cells
For cell differentiation, FAP-specific iPS cells were cultured with serial changes of media as shown in Fig. 3A.
To test whether FAP-specific iPS cells can differentiate into hepatocyte-like cells, we analyzed several markers via real-time PCR analysis on Day 5 (D5), D13 and D20 differentiated FAP-specific iPS cells (Fig. 3B).
A decrease in expression of the pluripotency marker Oct3/4 was accompanied by differentiation of FAP-specific iPS cells.
Expression of the endoderm marker Sox17 was observed on D5 differentiation and decreased gradually after the medium was changed to hepatic differentiation medium on D7 (Fig. 3B).
The hepatic progenitor marker AFP and the mature hepatocyte marker ALB were obviously expressed on D13 and D20.
In addition, immunocytochemical analyses showed Sox17 expression on D5, both HNF-4α and AFP expression on D13, and ALB cytoplasmic staining on D20 (Fig. 3C).
Quantitative imaging analysis revealed that approximately 78±0.6% of cells were Sox17-positive on D5 and approximately 88±1.1% of cells were AFP-positive on D13 and approximately 29±0.9% of cells were ALB-positive on D20 (Fig. 3D).
The ALB secretion in the media of differentiated FAP-specific iPS cells on D20 was approximately 20μg/ml (Fig. 3E).
Moreover, these D20 differentiated FAP-specific iPS cells were also periodic acid-Schiff (PAS)-positive, indicating cytoplasmic glycogen storage (Fig. 3F).
These results clearly indicated that FAP-specific iPS cells had the potential to differentiate into hepatocyte-like cells.
Production of wild-type TTR and ATTR Val30Met by differentiated hepatocyte-like cells
We next sought to determine whether hepatocyte-like cells differentiated from FAP-specific iPS cells would indeed express TTR.
Expression of TTR mRNA was detectable from D13 (Fig. 4A).
To confirm the presence of TTR protein, Western blotting of culture supernatant was performed with an anti-human TTR antibody.
As Fig. 4B shows, expression of TTR protein levels was confirmed in the concentrated D20 culture supernatant.
The approximate concentration of TTR protein in the media of differentiated FAP-specific iPS cells was approximately 2.28μg/ml (Fig. 4C).
In addition, the PCR-RFLP method showed that hepatocyte-like cells differentiated from FAP-specific iPS cells indeed expressed ATTR Val30Met and wild-type TTR mRNA, similar to expression in the liver of the FAP patient from whom FAP-specific iPS cells were generated (Fig. 5A).
Moreover, LC-MS/MS analysis clearly showed that hepatocyte-like cells differentiated from FAP-specific iPS cells indeed expressed both ATTR Val30Met and wild-type TTR at protein level and the ratio of TTR protein to ATTR protein is approximately 1 to 1 in the D20 culture supernatant (Fig. 5B).
Other FAP-specific iPS cell lines also expressed both ATTR Val30Met and wild-type TTR at protein level in the culture supernatant (data not shown).
Discussion
In the present study, we generated iPS cells from patients with FAP ATTR Val30Met by introducing four reprogramming factors (Oct3/4, Sox2, Klf 4, and c-Myc) into dermal fibroblasts via SeV vector infection.
FAP-specific iPS cells had the potential to differentiate into hepatocyte-like cells, a major TTR-producing cell, and indeed expressed ATTR Val30Met and wild-type TTR protein.
FAP-specific iPS cells demonstrated the possibility of serving as a pathological tool, which may contribute to elucidating the molecular pathogenesis of FAP and developing novel therapeutic strategies for FAP.
Attempts to establish the experimental models of FAP (Buxbaum et al., 2003; Pokrzywa et al., 2007; Berg et al., 2009) have not been as successful as researchers would wish.
It has been reported that transgenic mice overexpressing mutated TTR did not display signs of neuropathology (Buxbaum et al., 2003).
It is still controversial whether the pathogenic mechanism of FAP in Drosophila model is in the same manner as human (Pokrzywa et al., 2007; Berg et al., 2009).
In addition, a review of previous studies about the molecular pathogenesis of FAP indicates that most of the studies investigated the pathologic effect of ATTR using non-mutant cells or an artificial high expression system (Sousa et al., 2000; Cardoso et al., 2008; Sato et al., 2007).
Indeed, the neurodegeneration induced by ATTR or endoplasmic reticulum quality control of ATTR was studied with non-mutant mammalian cells transfected with the receptor for advanced glycation end products (Sousa et al., 2000) or wild-type TTR and ATTR (Sato et al., 2007).
Because FAP is an autosomal-dominant hereditary disease and those systems did not utilize heterozygous ATTR mutant cell models, those systems do not fully represent the relevant processes in patients with FAP and are unsuitable for evaluation of therapy.
The ratio of expression of TTR protein to ATTR protein varies among cells.
As with iPS cells designed to be specific for other diseases, iPS cells generated from patients who were heterozygous for FAP are more useful for elucidating the pathogenesis of FAP: TTR and ATTR in heterozygous FAP form heterotetramers, which produce amyloid, and recombinant TTR forms a homotetramer, which is a totally different form of TTR in plasma and tissues.
Generation of various FAP-specific iPS cells may help clarify the mechanism of the phenotypic variations among individuals with the same genotype (Kato-Motozaki et al., 2008) and the cellular and molecular pathogenesis.
In this study, hepatocyte-like cells differentiated from FAP-specific iPS cells indeed secreted TTR and ATTR protein in the ratio of almost 1 to 1 as heterozygous ATTR mutant cells (Fig. 5B).
The amount of TTR protein was approximately 2.28μg/ml (Fig. 4C), which is comparable with the previous result (0.2-2μg/ml) in functional hepatocyte-like cells differentiated from normal iPS cells (Sullivan et al., 2010), and exhibits 50 times higher than the previous result (0.034-0.057μg/ml) in human hepatocytes in primary culture (Wigmore et al., 1997).
Moreover, differentiation of the same FAP-specific iPS cell line into various cells such as neurons and cardiomyocytes may aid understanding of the tissue selectivity of amyloidosis, which differs according to variants (Hammarstrom et al., 2003; Sekijima et al., 2003; Yamashita et al., 2005).
Studies of other FAP-specific iPS cell lines generated from TTR variants other than the Val30Met mutation are currently in progress.
FAP-specific iPS cells may also be used for screening novel drug candidates such as inhibitors of amyloid formation.
Additional investigations are needed to evaluate the phenotype of FAP-specific iPS cells in greater detail and confirm the value of these cells as a novel experimental tool of FAP.
In this study, we took full advantage of the special characteristics of this SeV vector to establish FAP-specific iPS cells.
Because SeV vectors replicate only in the cytoplasm of infected cells and do not integrate into the host genome (Li et al., 2000), no risk of modifying the host genome exists.
Using retrovirus or lentivirus vectors results in integration of viral transgenes into the host genome, which includes a risk of tumorigenicity (Hussein et al., 2011; Gore et al., 2011).
To solve this problem, plasmids (Okita et al., 2008), a Cre/loxP system (Soldner et al., 2009), adenoviruses (Stadtfeld et al., 2008), piggyback (Woltjen et al., 2009), a minicircle vector (Jia et al., 2010), and proteins (Zhou et al., 2009) have been developed.
The risk of integration into the genome still remains, however, for DNA-type vectors (Harui et al., 1999), and those methods also demonstrated low induction efficiency.
Thus, the SeV vector that we used is believed to have a significant advantage compared with available methods because of its safety, efficiency, and convenience.
SeV vectors are slowly diluted and disappear during iPS cell division, and SeV vector-positive cells can be removed by means of an anti-SeV-HN antibody (Fusaki et al., 2009).
Moreover, with the temperature-sensitive SeV vector used in this study, vectors, even if present, could easily be removed after the temperature shift treatment and would not be reactivated in iPS cells (Ban et al., 2011).
FAP-specific iPS cells would thus be safe and may be a source for cell replacement therapy.
Conclusions
We successfully generated, for the first time, FAP-specific iPS cells.
Such FAP-specific iPS cells may serve as valuable experimental tools to clarify the molecular pathogenesis of FAP, with potential value in cell therapy applications.
Acknowledgments
We thank Dr. Konen Obayashi for technical assistance and Mrs.
Hiroko Katsura for her technical support during histopathologic investigations.

High performance inverted bulk heterojunction solar cells by incorporation of dense, thin ZnO layers made using atmospheric atomic layer deposition

Abstract
A thin ZnO (<200nm) film grown by Atmospheric Atomic Layer Deposition (AALD) in a matter of minutes was studied as a hole-blocking layer in poly(3-hexylthiophene-2,5-diyl):[6,6]-phenyl-C61-buyric acid methyl ester (P3HT:PCBM) based inverted solar cells.
These AALD ZnO layers were compact, had a high electron mobility of 3.4+0.1cm2/Vs, had up to 100% transmittance to visible light, and a good wettability for the blend.
Despite the very rapid, open atmosphere growth method, the cell performance was comparable with some of the best inverted bulk heterojunction P3HT:PCBM cells in the literature.
The performance was also maintained after 200 days of storage in air in the dark.
Graphical abstract
Highlights
•
ZnO was deposited in under 5min using Atmospheric Atomic Layer Deposition (AALD).
•
AALD is a fast, ambient method that is compatible with roll-to-roll processing.
•
The AALD ZnO was used as the hole blocking layer in organic solar cells.
•
The devices had performances comparable with the highest reported in literature.
•
The low-temperature deposited AALD ZnO can be used in plastic substrate solar cells.

Introduction
Organic solar cells are a very promising, low cost and scalable alternative to conventional silicon-based solar cells [1-3].
Some of the highest efficiency organic devices are thin-film polymer-fullerene bulk heterojunction junction (BHJ) solar cells [4,5].
In BHJ devices, the p-type polymer and n-type fullerene are intimately mixed to form a nanoscale interpenetrating network, which improves charge separation by providing a large interfacial area, resulting in a significantly increased photocurrent [2,6].
BHJ devices in the so-called 'inverted' structure improve the ease of fabrication because a stable, high work function anode can be used as the top contact to collect holes from the polymer [3].
A transparent conducting cathode is used to collect electrons from the fullerene on the other side of the blend [7].
However, in a BHJ device, the p-type and n-type materials would both be in contact with the cathode and anode, which results in recombination at the electrodes [8].
At the anode, this has been overcome by inserting an electron-blocking layer, such as MoO3 [8,9], V2O5 [8,10], WOx [8] or the polymer PEDOT:PSS [8-10].
At the cathode, TiO2 [9] and ZnO [1-3,7-10] have been used as hole blocking layers due to their low hole conductivity and high electron mobility [11].
These oxides have been deposited using vacuum-based processes, such as conventional atomic layer deposition (ALD) [12,13], and also ambient processes, such as sol-gel [1,2] and electrodeposition [14].
While ambient methods are attractive for the commercialisation of low-cost photovoltaics [1,2], most are batch processes, which limits their throughput.
Recently, ALD11
ALD is the abbreviation for Atomic Layer Deposition.
 growth of oxides has been implemented under atmospheric conditions, which allows the oxides to be deposited rapidly, and which has high potential for implementing in a roll-to-roll process.
This process is termed Atmospheric (or Spatial) Atomic Layer Deposition22
AALD is the abbreviation for Atmospheric Atomic Layer Deposition.
 (AALD) [15-18].
AALD involves separating the precursors in space rather than time.
This can be implemented in many ways [19].
The system we have used involves a gas manifold and the process is schematically shown in Fig. 1 [19,20].
The substrate moves through the metal precursor (M) and oxidant (O) gas streams beneath the AALD manifold, resulting in the oxide growing one monolayer at a time.
The manifold is also in close proximity to the substrate (<100µm) to ensure laminar flow of the gas streams so that the inert gas channels (I) act as 'shields' to prevent oxygen ingress from the atmosphere and also to prevent the precursors from mixing together [19].
The design of the manifold allows the two purge steps between oxidant/metal precursor exposures in conventional ALD to be eliminated, which enables more rapid depositions [16].
In this work, we studied AALD ZnO as the hole blocking layer in the following device architecture: glass/ITO/AALD ZnO/P3HT:PCBM/MoO3/Ag.
P3HT:PCBM was selected because it is well-studied and therefore suitable for investigating the AALD ZnO.
MoO3 was also selected as the electron-blocking layer because we have used it successfully in the past, which allowed us to focus on studying the AALD ZnO, although a process more suitable for roll-to-roll processing, such as slot-die coating of vanadyl-triisopropoxide [10], would ultimately need to be used for large scale, high-throughput fabrication.
In addition to charge selection, the AALD ZnO hole-blocking layer plays an important role in extending device lifetime through UV-filtering [15,21].
The key property requirements of hole-blocking layers are: (i) compactness [1,2], (ii) high stability [1,2], (iii) high electron mobility [1], (iv) transparency to visible light [1,2,22], and (v) good blend wettability [22].
Herein, we show that AALD ZnO fulfils all of these properties to give highly performing inverted BHJ devices.
Experimental
ZnO thin film deposition
The soda glass substrates were of dimension 14mm×14mm×0.7mm and had 180nm of ITO (sheet resistance<10Ω/sq) coated on top (obtained from Praezisions Glas and Optik).
These substrates were ultrasonically cleaned in acetone and isopropanol for 15min, successively.
The AALD ZnO films were deposited onto two ITO substrates at a time, and the substrates held on a platen heated to 150°C.
The platen was oscillated 50µm beneath the AALD manifold (heated to 40°C) at 50mm/s, with almost no delay between each cycle.
Diethylzinc (Sigma-Aldrich) was used as the Zn precursor and H2O as the oxidant.
Nitrogen gas was bubbled through the H2O at 50mL/min, and 25mL/min through the diethylzinc.
The diethylzinc vapour was mixed with the nitrogen gas flowing at 150mL/min through the metal precursor line, which was fed to the AALD manifold.
The H2O vapour in nitrogen gas was also mixed with nitrogen gas flowing at 150mL/min through the oxidant line that was fed to the AALD manifold.
Nitrogen gas was also directly fed to the AALD manifold at 750mL/min to provide the inert gas channels needed to prevent mixing between the diethylzinc and H2O.
The number of oscillations of the platen beneath the AALD manifold used were: 15, 40, 60, 80, and 125.
From cross-sectional SEM, it was found that the film thickness (in nm) was approximately equal to the number of oscillations.
The films deposited were crystalline in the as-deposited state, but post-annealing at 300°C for 1h was undertaken to further improve the crystallinity.
Solar cell fabrication
The AALD ZnO coated substrates were heated at 120°C for 10min and 150μL of the P3HT:PCBM blend was then spin-coated on the films at 600rpm for 6s, followed by 1000rpm for 60s.
Afterwards, the samples were annealed at 150°C for 15min.
The spin-coating equipment used was a Chemat Technology Spin-coater KW-4A.
A strip of Kapton polyimide tape was put onto the bottom of the cell, and a strip etched off the top to expose the back ITO contact (Fig. 2).
Three rectangular 4nm thick MoO3 contacts 0.15cm2 in area were evaporated onto the top of the blend, and 80nm of Ag evaporated on top of that.
Both were evaporated sequentially in a glovebox evaporator.
A drop of silver paste was then put on each contact, on the edge of the Kapton tape, as shown in Fig. 2.
The grey arrows in Fig. 2 indicate where the pins of the solar simulation holder were in contact with the device.
Characterisation
Solar simulations were done using a Newport Oriel class A solar simulator with AM 1.5G filters.
The light intensity was 100mW/cm2.
A mask was used to accurately define an illumination area of 0.15cm2 for each cell.
Cross-sectional SEM was done using a LEO VP-1530 field emission scanning electron microscope.
Energy-dispersive X-ray spectroscopy (EDX) was done using this SEM with an Oxford Instruments detector, cooled with liquid nitrogen.
Dektak profilometry was also used to measure the thickness of AALD ZnO films by etching a strip off the film using 10vol% hydrochloric acid and measuring the step height.
The carrier properties of the thin film were measured with the Van der Pauw method using a Hall effect rig detailed in Refs [18] and [23].
Firstly, the film was etched to a Greek cross with arms 2mm wide and 6mm long to reduce errors due to the finite size of the contacts.
Indium pads were then put on the ends of the arms of the crosses and steel wires attached to the indium pads using Wood's metal.
A Keithley 6220 current source and 2182A Nanovoltmeter were used to measure the sheet resistance.
Hall effect measurements were then done using the same current source and Nanovoltmeter, in addition to a pair of magnets that applied a constant 1T field through the sample.
From the Hall voltage measured, in addition to the measured sheet resistance, applied current and film thickness, the mobility of the film was determined.
The X-ray diffraction patterns of the films were obtained with a Bruker D8 theta/theta system, using CuKα radiation (wavelength of 1.5406Å).
A LynxEye position sensitive detector was used.
The divergence slit was 1o, step size 0.04o and dwell time 2.5s.
Atomic Force Microscopy (AFM) topography images of the films were obtained using a Veeco Dimension 3100 AFM system in TappingModeTM.
Film transmittance was measured using UV-visible spectrophotometry.
The device stability was determined by storing the devices at open-circuit under ambient conditions (~20°C, ~75% relative humidity) in the dark and characterising the device performance after 200 days using the solar simulator with AM 1.5G filters and 100mW/cm2 light intensity.
This was similar to an ISOS-D-1 Shelf test, with the only deviation being that the characterisation was only done after 200 days rather than every week [24].
Results and discussion
Film compactness and device performance
Compact films without pinholes or discontinuities were produced.
This can be seen from the cross-sectional SEM image in Fig. 3a and the top-down topography image in Fig. 3c.
By comparing Fig. 3c with 3d (bare ITO), it can be seen that the AALD ZnO film was also conformal to the ITO substrate, since the topography of the ITO can still be seen in the 80nm thick AALD ZnO film.
The X-ray diffraction patterns in Fig. 3b showed only sharp ZnO peaks in both unannealed and annealed films, indicating high crystallinity and phase purity (to within the limits of the X-ray technique).
Previous X-ray Photoelectron Spectroscopy measurements of ZnO films made using this AALD system have shown no significant traces of contaminants, other than the intentionally introduced dopants [25].
EDX measurements on the films in this work (Fig.
S1 in the supplementary material), have also not shown any detectable contaminants.
The J-V curve of the best performing device is shown in Fig. 3e.
This device was obtained using 125nm thick AALD ZnO (deposited in only 4.5 min) and tested one week after manufacture.
This cell had a power conversion efficiency (PCE or η) of 3.6%, open-circuit voltage (VOC) of 0.602V, short-circuit current density (JSC) of -10.2mA/cm2 and fill factor (FF) of 59%.
These performance values are comparable with the best performing inverted BHJ P3HT:PCBM devices with ZnO hole-blocking layers deposited using conventional vacuum-based ALD, sol-gel and electrodeposition [3,7,8,12,13].
Also, the high VOCs and FFs clearly indicate at a high quality hole-blocking layer.
The device parameters (VOC, JSC, PCE and FF) were found to be fairly constant for AALD ZnO films with a thickness of 40nm or more (Fig. 4a-c).
This indicates that films of 40nm thickness have high compactness, despite the rapidity of the depositions: it only took 1.5min to deposit films of this thickness with excellent hole-blocking properties over a 15cm2 area.
The thinness of an effective AALD ZnO layer and the short time of deposition are significant for industrial scalability, because it means that an effective hole-blocking layer could be obtained rapidly with very little material.
Indeed, AALD is advantageous compared with conventional vacuum-based ALD, sol-gel and electrodeposition.
This is because the AALD oxide can be grown around an order of magnitude faster than standard ALD and does not require a vacuum [19], and also the method is amenable to roll-to-roll processing while sol-gel and electrodeposition are not.
Furthermore, whereas sol-gel often requires ageing of the precursors for a day or more [26], AALD precursors are fast reacting and can be used as-obtained, without requiring any ageing or further treatment for a compact ZnO film to be deposited.
This ZnO film is already compact and crystalline (shown by the XRD pattern for the unannealed film in Fig. 3b), and does not require any further ageing before the blend can be deposited on top.
Also, whereas electrodeposition requires careful optimisation of the electrolyte composition and pH [8], the optimistion of the AALD deposition conditions is relatively facile.
Another positive attribute of using AALD growth for blocking layers is its good potential for coating flexible, polymer substrates.
As mentioned in Section 2.1, the deposition temperature was 150°C and the post-annealing temperature was 300°C.
These both are lower than the typical annealing temperatures of 350-600°C needed to adequately crystallise the sol-gel films [1,17,22,26].
While the 300°C post-anneal is tolerable for growth on polyimides [27], it is not for lower melting polymers, such as polyethylene napthalate (PEN) [28] and polycarbonate (PC) [29].
The post-annealed films are therefore not suitable for coating a wide range of flexible substrates.
From an energy minimisation point of view, it is also advantageous to eliminate any post-annealing steps.
Comparative cells of unannealed and annealed ZnO were made to study the amount of post-annealing needed to optimise the device properties so that they are suitable for use in plastic substrate solar cells.
In addition to post-annealing at 300°C, we have also investigated post-annealing at 150°C.
It was found that without annealing, the average efficiency was 1.7+0.2%.
The efficiency increased with the post-annealing temperature of the 125nm thick AALD ZnO film due to an increase in the fill factor and open-circuit voltage, as can be seen in Table 1 and the J-V curves in Fig. 5.
In order to meet the requirement for flexible device manufacturing, the post-annealing temperature should not exceed the upper working temperature of the plastic substrate.
For PEN, this is 155°C, while those of other common plastic films, such as polyethylene terephthalate (PET) and PC are lower [28,29].
Post-annealing at 150°C is therefore most suitable for plastic substrate solar cells because this improves the device performance compared with no annealing (from 1.7+0.2% to 2.4+0.5%), and yet the processing does not damage the plastic substrates.
Also, since the AALD ZnO films were deposited at 150°C, it is not expected that post-annealing at temperatures lower than 150°C would significantly improve the device performance, especially considering the cost of the increased processing time and increased energy requirement.
Thus, the AALD not only has significant commercial potential because it can deposit compact oxide films rapidly in open atmosphere, but it is also highly compatible with the high throughput roll-to-roll deposition of the oxide on large area plastic substrates.
Stability
Device stability is an extremely important factor for commercialisation [7].
The devices with annealed AALD ZnO and the MoO3 electron blocking layers were kept at open-circuit in the dark under ambient conditions and re-measured after 200 days in a test similar to an ISOS-D-1 Shelf test [24].
It can be seen in Fig. 4 that there was no statistically significant change in the device performance after this period.
The result shows that the AALD ZnO is chemically and structurally stable because if it were not, the blend would have come into contact with the ITO, leading to recombination and a deterioration in device performance [8].
Hole blocking layer electron mobility
The electron mobility of spin-coated ZnO nanoparticles used for hole-blocking layers has been reported to be 0.066cm2/Vs [1,30].
The as-grown AALD ZnO films of this work had two orders of magnitude higher electron mobility of 3.4+0.1cm2/Vs, which is consistent with the high compactness of the films.
This would have allowed the electrons that reached the AALD ZnO layer to be rapidly transported to the cathode.
However, the blend layer dominated the series resistance due to the much lower mobilities of the organic materials (10-5-10-3cm2/Vs) [31,32].
This can be seen from the series resistances of the devices which do not significantly change for different thicknesses or storage time (Fig. 4d).
Transmittance
The ZnO hole-blocking layer should also allow visible light to pass through to the blend, but filter out the UV light to prevent blend degradation and thus improve device longevity [15,22].
Transmittance measurements of annealed 15nm and 125nm thick AALD ZnO films on glass are shown in Fig. 6.
Background measurements on glass only were performed to account for reflection and absorption by the glass.
The onset of absorption occurred at a wavelength of 360nm (band gap of 3.4eV, as expected for ZnO), and the films had high transmittance for visible wavelengths.
For wavelengths between 390nm and 700nm, the annealed 15nm film had an average transmittance of 96%, and the annealed 125nm film 92%.
Both the 15nm and 125nm films had transmittances up to 100%.
Below the onset of absorption, the transmittance of the 125nm film (8% on average) was much lower than that of the 15nm film (65% on average) due to there being more absorbing material, as expected.
These results compare well with the literature, where transmittances are typically 55-100% for visible wavelengths and 20-60% for wavelengths below the onset of absorption [14,22,33], indicating that the smooth, compact ZnO layer admits visible light well to the blend and that the thicker ZnO films are especially effective at limiting UV light transmission.
The high transmittance of visible light by the AALD ZnO films is also consistent with the JSC of the best device being comparable with the highest values from literature.
Film wettability
Good wettability is desirable for the AALD ZnO so that the contact area with the blend and the fraction of photogenerated charge in the PCBM that is collected are both maximised [22].
Good wettability, in addition to compactness, should lead to high fill factors and VOCs because of good contact between the blend and the hole-blocking layer [34].
From Fig. 3a, it can be seen that the blend has very good contact with the AALD ZnO layer to the extent that there was no delamination even after fracturing for cross-sectional SEM.
This led to high VOCs of ~0.6V (Fig. 4a) that were comparable with the highest values in the literature for inverted BHJ P3HT:PCBM devices [3,7,8,12,13].
The fill factors were also high and averaged around ~50% (Fig. 4c).
The fill factor of the best device was also comparable with the highest values in the literature for inverted BHJ P3HT:PCBM devices [3,7,8,12,13].
Conclusions
We have shown that inverted BHJ devices made with rapidly grown ZnO hole blocking layers using atmospheric atomic layer deposition (AALD) give device performances that are comparable with the best inverted P3HT:PCBM BHJ devices reported to date.
Working devices with good, but lower performance were obtained without any post-annealing of the as-grown AALD ZnO.
On the other hand, these unannealed AALD ZnO films are suitable for making flexible, plastic substrate solar cells.
A compromise between device performance and suitability for making plastic substrate solar cells was achieved by post-annealing the ZnO films at 150°C.
The AALD ZnO fulfils the properties required for good hole blocking layers: the films are compact, have a high electron mobility, have up to 100% visible light transmittance, good blend wettability and very good device stability over time.
In addition, the deposition process occurs under atmospheric conditions and has the potential to be implemented in a roll-to-roll process.
AALD ZnO films are therefore highly suited as multifunctional components for inverted BHJ devices.
Acknowledgements
The authors would like to thank the Cambridge Commonwealth Trusts and the Rutherford Foundation of New Zealand, the ERC Advanced Investigator Grant, Novox, ERC-2009-adG 247276, EPSRC of the UK, Girton College (Cambridge), the Marie Curie program (FP7/2007-2013, grant agreement number 219332) and the Comissionat per a Universitats i Recerca (CUR) del DIUE de la Generalitat de Catalunya, Spain.
Supporting information
Supplementary data associated with this article can be found in the online version at 10.1016/j.solmat.2013.04.020.
Supplementary materials
Supplementary Material

A 3.8-V earth-abundant sodium battery electrode

The target material was synthesized by reacting 1.54 g Na2SO4 (Wako, 99%) and 2.73 g FeSO4. The anhydrous FeSO4 precursor was prepared in-house by annealing commercial FeSO4.7H2O (Wako, 99%) under vacuum at 200 degC for 12 h (ref. 35). Na2Fe2(SO4)3 cathode compound was obtained via classical solid-state synthesis by ball milling the precursors for 4 h followed by annealing the mixture at 350 degC for 24 h under steady Ar flow. As SO4-based compounds are prone to dissolvation (in water) and thermal decomposition, we used these sustainable non-aqueous, low-temperature methods. Chemical oxidation was performed to obtain desodiated Na2-xFe2(SO4)3 samples using NO2BF4 (Alfa Aesar, 96%) oxidant dissolved in acetonitrile solvent (Wako, H2O level <5 p.p.m.). The solution was stirred overnight (with steady Ar flow), and the final products were filtered and dried at 60 degC under vacuum.


Controlled synthesis of bismuth sulfide nanorods by hydrothermal method and their photocatalytic activity

Bi(NO3)3*5H2O and thiourea (NH2)2S were used as precursors for Bi and S, respectively. MB has been employed in the photocatalytic evaluation of the prepared photocatalysts. All chemicals in the present work were analytical grade reagents and they were purchased from Sigma-Aldrich.

Bi2S3 nanorods with orthorhombic structure were prepared through the hydrothermal method. In a typical procedure, different weights of thiourea were added to 100 ml distilled water with magnetic stirring, then 4.0 g of Bi(NO3)3 was added to the thiourea solution to obtain Bi2S3 nanorods at different Bi:S ratios. After being stirred for 10 min at 60 degC, the solution was transferred to a Teflon-lined autoclave. The autoclave was transferred into an oven and maintained at various temperatures (100-250 degC) for various reaction times (10-48 h). After cooling the autoclave to room temperature, the collected precipitates were washed with water and ethanol for three times, afterward, dried at 60 degC for 8 h.

Direct evidence for high Na+ mobility and high voltage structural processes in P2-Nax[LiyNizMn1-y-z]O2 (x, y, z <= 1) cathodes from solid-state NMR and DFT calculations
The Na2/3Ni1/3Mn2/3O2 material was synthesized by co-precipitation. Stoichiometric amounts of the precursors, Mn(NO3)*4H2O and Ni(NO3)*6H2O, were dissolved in deionized water. The transition metal nitrate solutions were titrated into a stoichiometric NaOH solution using a peristaltic pump at 10 ml h-1 rate. The solution was stirred slowly to insure homogeneity. The co-precipitated solid M(OH)2 phase was centrifuged and washed three times with deionized water. The co-precipitated material was dried in the oven to remove excess water and was ground with a stoichiometric amount of Na2CO3. The material was precalcined at 500 degC for 5 h and calcined in pellet form at 900 degC for 14 h in a 50 ml porcelain crucible. The synthesis protocol for the P2-Na0.8Li0.12Ni0.22Mn0.66O2 material was described in our previous study.19A Peanut Shell Inspired Scalable Synthesis of Three-Dimensional Carbon Coated Porous Silicon Particles as an Anode for Lithium-Ion Batteries
The porous silicon film was treated with a liquid PAN solution (10 wt% dissolved in DMF) and ball milled for 1 h at 200 r min-1. The recipe contained 40 mg porous silicon film and 600 mg of the above liquid PAN solution. Following this, the mixed slurry was spread onto copper foil with a diameter of 14 mm for a current collector and dried for 3 h in a vacuum drying oven at 80 degC. The as-obtained PAN coated PSP electrodes were then heated in an Ar atmosphere at a rate of 5 degC min-1 to 700 degC and held for 1 h. The carbon-coated PSP loaded on per electrode is ~ 0.85 mg.Synthesis and conductive properties of Ga-doped ZnO nanosheets by the hydrothermal method

All chemicals (analytical grade reagents) were purchased from Tianjin Jiangtian Chemicals Co. Ltd. and were used as received without further purification. The GZO nanoparticles were prepared by the hydrothermal method and the molar ratio of Ga:Zn varied from 0:100, 1:100 to 3:100. In a typical experiment, 7.44 g Zn(NO3)2*6 H2O and proper amount of Ga(NO3)3*9H2O were dissolved in 30 ml de-ionized water and stirring to complete dissolution. Subsequently, 30 ml of sodium hydroxide (4.0 mol L-1) aqueous solution was slowly dripped into the former mixture to obtain precursor. After 30 min of stirring, the precursor was transferred into a 100 ml Teflon autoclave, then the autoclave was heated at 160 degC for 3 h. After the reaction finished, the precipitate was separated and washed with de-ionized water and absolute ethanol several times to remove the ions possibly remaining in the final products. Finally, the resulting products were dried in oven at 75 degC for 15 h.

Graphene oxide assisted solvothermal synthesis of LiMnPO4 naonplates cathode materials for lithium ion batteries

Graphite oxide was prepared by oxidation of graphite powder according to the modified Hummers' method. GO aqueous solution was obtained from centrifugation at 10000 rpm for 30 min following exfoliation of Graphite oxide by ultrasonication for 30 min, and the supernatant was freeze-dried to GO aerogel.

Nanoplates of LiMnPO4 were prepared as follows: Aqueous MnSO4 (5 cm-3, 1 mol dm-3) were added gradually into NMP (15 cm3) containing GO (20 mg) under stirring, and the reaction lasted for 24 h. Before solvothermal process, a turbid liquid of Li3PO4 from LiOH (5 cm3, 3 mol dm-3) and H3PO4 (5 cm3 1 mol dm-3) was added into the above suspension. The suspension was then transferred into a Teflon-lined stainless steel autoclave, and heated at 180 degC for 12 h. After cooling to room temperature, the products (namely S1) were filtered, washed with water and ethanol for several times and finally dried at 80 degC. For comparison, the reaction with 0, 5, 10, 40 and 80 mg GO were carried out under the same process. The product of reaction without GO is named S2.
1. A mixed metal oxidized hydroxide precursor material represented by the chemical formula:          (1-z)(NiaCobMnc(OH)2(a+b+c)·COb'Mnc'(OOH)b'+c'·Mnc"O2c")·          z(NidCoeMnf(OH)2(d+e+f)·Coe'Mnf'(OOH)e'+f'·Mnf"O2f"),  wherein
0< z<0.1;
A=a, B=b+b', C=c+c'+c", A+B+C = 1 and 0<A<1, 0<B<1, 0<C<1;
D=d, E=e+e', F=f+f+f" D+E+F=1 and 0<D<1, 0<E<1, 0<F<1; and
A<D, B>E, C>F, wherein
the precursor material comprises spherical and non-spherical particles having a surface and an interior, and
the particles have a gradient structure wherein a molar ratio of Ni, in comparison to Co and Mn, is in the majority at the surface, and a composition with a metal molar ratio that varies from the surface towards the interior of the particles.2. The precursor material of claim 1, wherein the surface of the particles has a Ni:Co:Mn ratio of about 8:1:1.3. The precursor material of claim 1, wherein the particles are doped with at least one metal ion selected from the group consisting of Mg, Al, Zr, Ti, Ni, Co, and Mn.4. The precursor material of claim 1, wherein the precursor material has an average particle size (D50) in the range from 3-30 microns.5. The precursor material of claim 4, wherein the precursor material has an average particle size (D50) in the range from 7-13 microns.6. The precursor material of claim 1, wherein the precursor material has a tap density in the range from 0.8-2.8 g/cm3.7. The precursor material of claim 6, wherein the precursor material has a tap density in the range from 1.8-2.3 g/cm3.8. The precursor material of claim 1, wherein the precursor material has a surface area in the range from 2-20 m2/g.9. The precursor material of claim 8 , wherein the precursor material has a surface area in the range from 2-8 m2/g.10. The precursor material of claim 1, wherein a sodium level within the precursor material is less than 500 ppm.11. The precursor material of claim 10, wherein a sodium level within the precursor material is less than 300 ppm.12. A lithiated cathode active material for lithium ion batteries prepared with the precursor material of claim 1.13. A method of preparing a mixed metal oxidized hydroxide precursor material comprising the steps of:
co-precipitating a solution comprising a plurality of metal salts, wherein the metals of the metal salts is selected from the group consisting of nickel, cobalt, manganese, and combinations thereof, with an alkaline hydroxide solution and ammonia to form a precipitate;
filtering the precipitate;
washing the precipitate; and
drying the precipitate to form the mixed metal oxidized hydroxide precursor material, wherein the precursor material is represented by a chemical formula of:        (1-z)(NiaCobMnc(OH)2(a+b+c)·COb'Mnc'(OOH)b'+c"·Mnc"O2c")·        z(NidCoeMnf(OH)2(d+e+f)·Coe'Mnf'(OOH)e'+f'·Mnf"O2f"),wherein
0< z<0.1;
A=a, B=b+b', C=c+c'+c", A+B+C = 1 and 0<A<1, 0<B<1, 0<C<1;
D=d, E=e+e', F=f+f+f" D+E+F=1 and 0<D<1, 0<E<1, 0<F<1; and
A<D, B>E, C>F, and
the method is conducted via precipitation in first and second sequential reactors.14. The method of claim 13, wherein at least 90%, but less than 100% of the metals precipitate in the first reactor.15. The method of claim 13, wherein at least one metal ion selected from the group consisting of Mg, Al, Zr, and Ti are added in the first sequential reactor and/or at least one metal ion selected from the group consisting of Mg, Al, Zr, Ti, Ni, Co, and Mn are added in the second sequential reactor to modify the composition of the precipitate.16. The method of claim 13, wherein the alkaline hydroxide maintains the solution at a pH in the range from about 11-13.17. The method of claim 13, wherein the precursor material comprises spherical and non-spherical particles having a surface and an interior.18. The method of claim 17, wherein the particles have a gradient structure wherein a molar ratio of Ni, in comparison to Co and Mn, is in the majority at the surface, and a composition with a metal molar ratio that varies from the surface towards the interior of the particles.19. The method of claim 18, wherein the surface of the particles has a ratio of Ni:Co:Mn of about 8:1:1.20. The method of claim 17, wherein the ammonia is a complexing agent that stabilizes the growth of the particles.21. The method of claim 13, wherein the ammonia:metal molar ratio of the solution is in the range from about 0.1-3.0.22. The method of claim 21, wherein the ammonia:metal molar ratio of the solution is in the range from about 0.5-1.5.23. The method of claim 13, wherein the co-precipitation is conducted at a temperature in the reactors at a temperature in the range from about 50-70 °C.24. The method of claim 13, wherein the precursor material has an average particle size (D50) in the range from 3-30 microns.25. The method of claim 24, wherein the precursor material has an average particle size (D50) in the range from 7-13 microns.26. The method of claim 13, wherein the precursor material has a tap density in the range from 0.8-2.8 g/cm3.27. The method of claim 26, wherein the precursor material has a tap density in the range from 1.8-2.3 g/cm3.28. The method of claim 13, wherein the precursor material has a surface area in the range from 2-20 m2/g.29. The method of claim 28, wherein the precursor material has a surface area in the range from 2-8 m2/g.30. The method of claim 13, wherein a sodium level within the precursor material is less than 500 ppm.31. The method of claim 30, wherein a sodium level within the precursor material is less than 300 ppm.1. A transition metal mixed hydroxide consisting of primary particles and approximately spherical secondary particles formed by aggregation of primary particles, wherein the transition metal mixed hydroxide has an average particle diameter of from 1 μm to 20 μm and contains Mn, Ni, Fe and Co in a molar ratio of a:b:c:d, wherein a is from 0.3 to 0.7, b is from 0.4 to 0.7, c is more than 0 and not more than 0.1, d is from 0 to 0.2, and a+b+c+d=1.2. The transition metal mixed hydroxide according to claim 1, wherein a is larger than b.3. The transition metal mixed hydroxide according to claim 1, wherein d is 0.4. The transition metal mixed hydroxide according to claim 1, wherein the average particle diameter is from 1 μm to 10 μm.5. A lithium mixed metal oxide produced by calcining a mixture of the transition metal mixed hydroxide according to claim 1 and a lithium compound, wherein the lithium mixed metal oxide has an average particle diameter of from 1 μm to 20 μm6. An electrode comprising the lithium mixed metal oxide according to claim 5.7. A non-aqueous electrolyte secondary battery comprising the electrode according to claim 6 as a positive electrode.8. The non-aqueous electrolyte secondary battery according to claim 7 further comprising a separator.9. The non-aqueous electrolyte secondary battery according to claim 8, wherein the separator is composed of a laminated film which has a heat resistant porous layer and a porous film laminated to each other.Relationship between structure and performance of a novel cerium-niobium binary oxide catalyst for selective catalytic reduction of NO with NH3

The cerium-niobium mixed oxides were prepared by co-precipitation method. Desired amount of niobium chloride was dissolved in methanol and cerium nitrate in deionized water, respectively. These two kinds of solutions were mixed together and added into an excess of ammonia solution with continuous stirring. Afterwards, the precipitates were collected by filtration for several times until no residue Cl- (detected by AgNO3 solution), dried at 120 degC overnight and calcined at 500 degC for 5 h under air conditions. The catalysts were crushed and sieved to 40-60 meshes for activity tests. The molar ratios of Ce/Nb were 1/3, 1/1, and 3/1. The single oxides, namely Nb2O5 and CeO2, were prepared by similar precipitation method.


Direct Comparison of Autologous and Allogeneic Transplantation of iPSC-Derived Neural Cells in the Brain of a Nonhuman Primate

Summary
Induced pluripotent stem cells (iPSCs) provide the potential for autologous transplantation using cells derived from a patient's own cells.
However, the immunogenicity of iPSCs or their derivatives has been a matter of controversy, and up to now there has been no direct comparison of autologous and allogeneic transplantation in the brains of humans or nonhuman primates.
Here, using nonhuman primates, we found that the autologous transplantation of iPSC-derived neurons elicited only a minimal immune response in the brain.
In contrast, the allografts caused an acquired immune response with the activation of microglia (IBA-1+/MHC class II+) and the infiltration of leukocytes (CD45+/CD3+).
Consequently, a higher number of dopaminergic neurons survived in the autografts.
Our results suggest that the autologous transplantation of iPSC-derived neural cells is advantageous for minimizing the immune response in the brain compared with allogeneic grafts.
Graphical Abstract
Highlights
•
iPSC-derived autografts cause only a minimal immune response in the primate brain
•
Autografts have advantages over allografts even at an immune-privileged site
•
Dopamine neurons survive even in allografts without immunosuppression
The autologous transplantation of iPSC-derived neurons elicited only a minimal immune response in the brain.
In contrast, the allografts caused an acquired immune response with the activation of microglia (IBA-1+/MHC class II+) and the infiltration of leukocytes (CD45+/CD3+).
Autologous transplantation is advantageous for minimizing the immune response compared with allogeneic grafts.

Introduction
In recent studies, murine induced pluripotent stem cell (iPSC)-derived teratomas in the subcutaneous space induced an immune response in syngeneic mice (Zhao et al., 2011).
In contrast, syngeneic transplantation of skin and bone marrow tissues (Araki et al., 2013) or endothelial, hepatic, and neuronal cells (Guha et al., 2013) derived from iPSCs showed a limited or no immune response, respectively.
These rodent studies investigated the immunogenicity of teratomas, chimeric mouse-derived tissues, or ectopic grafts, but did not convincingly simulate the clinical situation.
Parkinson's disease is one of the most promising targets for cell therapy with pluripotent stem cells, in which differentiated dopaminergic (DA) neurons are transplanted into the putamen of a patient's brain (Lindvall and Björklund, 2011).
In order to assess the immunogenicity of iPSC-derived neural cells in a primate brain, we generated iPSCs from four cynomolgus monkeys and directly compared the autologous and allogeneic transplantation of iPSC-derived neural cells.
Results
iPSCs Derived from Nonhuman Primates Differentiate into DA Neurons
For the first two animals (Nos.
1 and 4), we established iPSCs from fibroblasts derived from the oral mucosa using retroviral vectors (Okita et al., 2011).
For the other two animals (Nos.
6 and 8), we used peripheral blood mononuclear cells (PBMCs) with nonintegrating episomal vectors (Okita et al., 2013).
We selected the best clone from each animal according to the following criteria: a stable embryonic stem cell (ESC)-like morphology of the colonies after passaging, expression of pluripotent markers, few or no integrated transgenes (Figures 1A-1F; Figure S1 available online), and the potential for stable neural differentiation.
A PCR analysis revealed that all of the clones with retroviral vectors showed apparent expression of remaining transgenes (Figures S1C and S1D), whereas the clones with episomal vectors never did (Figure S1F).
To detect the iPSC-derived cells in a brain, we introduced GFP (Figures 1G and 1H).
The selected clones of iPSCs had the potential to generate teratomas in the testes of a severe combined immunodeficiency (SCID) mouse within 12 weeks (Figures 1I-1M).
To efficiently generate DA neurons from monkey iPSCs, we modified previously described protocols (Eiraku et al., 2008; Chambers et al., 2012; Morizane et al., 2011).
Briefly, dissociated iPSCs were incubated in ultralow-attachment 96-well plates in medium containing inhibitors of bone morphogenetic protein (BMP) and Activin/NODAL signaling to initiate neural induction.
To induce differentiation of the cells toward midbrain DA neurons, purmorphamine/FGF8 and FGF2/FGF20 were added sequentially (Figure 1N).
During differentiation, the expression of a pluripotent marker (OCT4) gradually decreased, whereas the levels of neural and DA markers increased (Figures 1O and 1P).
The differentiated neurons expressed markers characteristic of midbrain DA neurons (LMX1A, FOXA2, TH, and PITX3; Figures 1Q and 1R).
Besides DA neurons, other types of neurons, such as serotonergic and GABAergic neurons, as well as proliferating neural progenitors positive for KI67, were also observed as minor populations (Figures 1Q and 1R).
OCT4 was not detected by immunocytochemistry or flow cytometry in the differentiation culture even from the retroviral iPSCs (Figure 1O).
Expression of Major Histocompatibility Complex by Monkey iPSCs In Vitro
Next, we investigated the expression of major histocompatibility complex class I (MHC-I) in iPSC-derived neural cells.
Flow cytometry using antibodies against human leukocyte antigen (HLA)-A, HLA-B, and HLA-C revealed that mature neurons on days 35 and 71 expressed only a low level of MHC-I, and that the expression was enhanced in response to interferon gamma (IFN-γ: 25 ng/ml for 48 hr; Figure 2A).
The expression level of the mRNAs was approximately 1:100 compared with peripheral blood cells in both fibroblast- and blood-cell-derived iPSCs, which was again increased by exposure to IFN-γ (Figure 2B).
These results suggest that donor-derived neurons could express MHC-I when INF-γ was secreted by the host brain under inflammatory conditions.
To ensure that the MHC-Is of the host animal and donor cells were mismatched in the allotransplantation cases, we performed genotyping of the expressed MHCs from the monkeys, which were purpose-bred, second-generation (F2), captive-born animals.
As shown in Figures 2C and 2E, and Table S1, each monkey expressed different MHCs in terms of the A and B alleles.
Based on these results, we chose the most mismatched combination for allotransplantation (Figures 2D and 2E).
Autografts Elicit Only a Minimal Immune Response in the Primate Brain
We injected the iPSC-derived neural cells (day 28) into the original monkey as an autograft, and into the MHC-mismatched monkey as an allograft (Figure 2D).
Each animal received six separate injections (∼8.0 × 105 cells in a 4 μl suspension per tract) in the left striatum, and was observed for 3.5-4 months without immunosuppression.
In the brain, both brain-resident microglia and circulating immune cells work as key players in immunological responses.
Once the microglia are activated, they develop antigen-presenting activity.
PK11195 selectively binds to the translocator protein that is expressed on activated microglia (Shah et al., 1994; Vowinckel et al., 1997).
Therefore, positron emission tomography (PET) studies with [11C]PK11195 have been used to visualize brain inflammation in patients (Debruyne et al., 2003).
In sequential PET studies, we observed increased uptake of [11C]PK11195 in one allograft (animal No.
10) at 3 months (Figures 3A and 3B).
We could not detect any apparent uptake in the other animals or at any other time points (Figure S2).
Intriguingly, the serum level of IFN-γ temporarily increased at 2 months after the transplant in three animals (Figure 3C).
An immunofluorescence study conducted at 3.5-4 months showed that MHC-II+ cells were more frequently found in allografts than in autografts, especially in the monkey with increased uptake of [11C]PK11195 (Figure 3D, No.
10).
The MHC-II staining never overlapped with that of GFP of the donor cells (Figure 3F), whereas it generally overlapped with that of IBA1 (Figure 3G), indicating that MHC-II was expressed by host-derived microglia.
Consistently, the number and density of IBA1+ cells were higher in allografts than in autografts (Figures 3E, 3H, and S4C).
An increase in the expression of MHC might trigger the recruitment of circulating immune cells, including T cells.
An immunofluorescence study revealed that more CD45+ cells (a marker for pan-leukocytes) accumulated in allografts compared with autografts (Figures 3I and 3J).
Most of the CD45+ cells were CD3+ T cells, and 60% of them were CD8+ killer T cells (Figures 3K and 3L).
These findings suggest that an acquired immune response was elicited only in the allografts in the primate brain.
DA Neurons Survive in Both Types of Grafts, but a Higher Number Are Observed in Autografts
In order to evaluate the survival of the grafted cells, we performed MRI scanning and a histological analysis at 3-4 months posttransplantation (Figure 4).
Hematoxylin and eosin (H&E) staining and immunostaining for GFP of the brain slices demonstrated that the grafted cells survived in both auto- and allotransplantation without immunosuppression (Figures 4A-4F).
Furthermore, there were no significant differences in volume between the auto- and allografts (Figure 4K).
Immunostaining for tyrosine hydroxylase (TH), a marker for DA neurons, revealed that a large number of DA neurons (4,428 ± 1,130 per tract, n = 22) survived in the autografts (Figures 4G, 4H, 4L, and 4M).
The surviving DA neurons coexpressed the markers of a mesencephalic phenotype, such as FOXA2, NURR1, and the dopamine transporter (DAT) (Figures 4N-4P).
Even in allografts without immunosuppression, the TH+ neurons survived well (2,247 ± 641 per tract, n = 22), but the number and density were lower than in autografts (Figures 4I, 4J, 4L, and 4M).
We also found a small number of astrocytes (GFP+/GFAP+), as well as mature neurons (GFP+/NEUN+), in vivo (Figure S4B).
Discussion
In this study, we induced DA neurons from monkey iPSCs by directed differentiation in vitro, and demonstrated that autologous transplantation of the iPSC-derived cells elicited only a minimal immune response in the nonhuman primate brain.
Previous reports have suggested that either autologous grafts of iPSC-derived neural cells (Emborg et al., 2013; Maria et al., 2013) or allogeneic grafts of fetal ventral mesencephalic cells (Redmond et al., 2008) can survive in a primate brain without immunosuppression.
None of these studies, however, directly compared the immunogenicity of autologous grafts with that of allogeneic ones.
Our results clearly show differences in both immunogenicity and cell survival between these two types of grafts, and support the idea that immunosuppression is not necessary for autologous transplantation of iPSC-derived neural cells into the brain.
The [11C]PK11195 PET study was useful for real-time visualization of this phenomenon.
Furthermore, this technique can also be applied to patients in the clinical setting, which would help to determine when and if immunosuppressive drugs can be withdrawn.
Although we did not examine acute immune responses or inflammation within 48 hr, it is noteworthy that the responses were observed 2 or 3 months posttransplantation.
In the case of iPSC-based transplantation, there are four possible mechanisms that can cause inflammatory and immune responses: (1) direct allorecognition of mismatched MHC or minor antigens of the donor cells, (2) indirect allorecognition through host-derived antigen-presenting cells, (3) expression of fetal antigens due to immature stem cells or remaining transgenes, and (4) mechanical damage rather than MHC mismatch.
Because of the low expression level of MHC-I by the donor cells, direct allorecognition is unlikely to be the main cause.
However, donor-cell-derived astrocytes (Figure S4B), which can express both MHC-I and MHC-II to recruit T cells in response to IFN-γ (Akesson et al., 2009; Chastain et al., 2011), could be observed in the grafts.
Furthermore, it takes a longer time for astrocytes to differentiate than neurons.
Therefore, although there was no apparent expression of either MHC-I or MHC-II by the grafted GFP+ cells, donor-derived astrocytes may have contributed to the direct reaction in the late stage.
Considering the high expression level of MHC-II by host-derived microglia in the allografts (Figures 3D-3G), indirect allorecognition seems to have played a major role in the present study.
This requires the internalization and processing of the alloantigens, which are then recognized in peptidic form bound to recipient MHC-II molecules, possibly accounting for the late onset of the immune response.
Autografts derived from iPSCs generated by retroviral vectors resulted in the accumulation of larger numbers of IBA1+ and CD45+ cells compared with those generated using episomal vectors, probably due to the residual expression of the transgenes (Figure S3).
This indicates that residual transgenes can be immunogenic, and that it is therefore critical to use integration-free iPSCs.
Mechanical damage caused by needle trauma can also activate host astrocytes and microglia to secrete proinflammatory cytokines, which recruit leukocytes.
Consistently, we found IBA1+ cells along the needle tract in the animals that received control injections (Figures 3E and S4C), but this was limited to a small area and not likely to play a major role.
Another important finding is that, in spite of the immune responses mounted by the host brain, a substantial number of TH+ cells survived in the allografts.
This is consistent with previous clinical reports of human fetal cell transplantation.
Postmortem analyses of the patients revealed robust survival of DA neurons in spite of the fact that numerous immune cells were present around the graft (Kordower et al., 1997).
In two double-blind clinical trials, immunosuppressive drugs were never used (Freed et al., 2001) or were withdrawn after 6 months (Olanow et al., 2003).
In these cases, the cells from multiple fetuses were used without HLA matching, but more than 50,000 TH+ cells had survived after several years.
Our quantitative PCR (qPCR) study in vitro showed that the expression of MHC-I increased in response to IFN-γ, but the expression level was still 1/10 that of untreated monkey peripheral blood cells (Figure 2B).
The in vivo studies revealed that the serum level of IFN-γ increased at 2 months, and CD45+ cells (including CD8+ cells) accumulated in the allografts 3.5-4 months after the transplant.
On the other hand, the levels of INF-γ in the cerebrospinal fluid (CSF) and the levels of tumor necrosis factor α (TNF-α) in both the serum and CSF were below the limit of detection by ELISA (data not shown).
An immunofluorescence study did not reveal any apparent expression of MHC-I by the grafted cells (Figure S4A).
Therefore, it is possible that the immune response in the primate brain was not strong enough to reject all of the donor cells.
These findings closely correlate with the results of previous murine experiments (Hudson et al., 1994; Shinoda et al., 1995).
To apply our findings to a more clinically relevant setting, we investigated the expression of HLA-I during neural differentiation of human ESCs (hESCs) and iPSCs by qPCR (Figure S4D).
The expression level was 1/100 compared with that of human peripheral blood cells in both hESCs and iPSCs, and it was similarly elevated in response to IFN-γ.
It is difficult to precisely compare immunogenicity in monkeys with that in humans, but the low expression level of MHC-I by the donor cells may account for the mild rejection in both monkey and human neural transplantation.
Our results indicate that autologous transplantation is beneficial in terms of the immune response and cell survival.
However, this strategy is associated with higher costs and labor.
An alternative method is allogeneic transplantation using HLA-matched iPSC stocks (Okita et al., 2011; Nakatsuji et al., 2008; Deleidi et al., 2011).
Therefore, as a next step, it is critical to determine whether autografts have advantages over HLA-matched allografts and HLA-mismatched allografts with immunosuppression.
To answer this question, it will be necessary to establish iPSCs from MHC-homozygous monkeys and transplant the iPSC-derived cells into monkeys with the identical MHC haplotype.
The precise influence of HLA mismatch therefore needs to be explored in such future studies.
Experimental Procedures
Nonhuman Primates
Eight purpose-bred male cynomolgus monkeys (Macaca fascicularis) were used for iPSC generation and transplantation.
The animal experiments were performed in accordance with the Guidelines for Animal Experiments of Kyoto University, the Institutional Animal Care and Use Committee of Kobe Institute in RIKEN, and the Guide for the Care and Use of Laboratory Animals of the Institute of Laboratory Animal Resources (Washington, DC, USA).
See also Supplemental Experimental Procedures.
Generation and Neural Differentiation of iPSCs
Fibroblasts from the oral mucosa were transfected retrovirally with five transgenes (OCT3/4, SOX2, KLF4, L-MYC, and LIN28) (Okita et al., 2011).
PBMCs were transfected with a combination of plasmid vectors (OCT3/4, SOX2, KLF4, L-MYC, LIN28, shRNA for TP53, and EBNA1) as described previously (Okita et al., 2013).
The primate iPSCs were maintained on mouse embryonic fibroblast feeders treated with mitomycin-C (Sigma-Aldrich).
They were differentiated into DA neurons through the SFEBq method (Eiraku et al., 2008) with dual SMAD inhibitors (Chambers et al., 2012; Morizane et al., 2011; Figure 1N).
The cells were transplanted on day 28 of differentiation.
For in vitro analysis, the cells were dissociated with Accumax (Innovative Cell Technologies) on day 28 and cultured on eight-well glass chamber slides coated with poly-l-ornithine and laminin for an additional 11 days (for a total of 39 days).
Genotyping of MHC
MHC genotypes were assigned by comparing the sequences with known MHC allele sequences released from the Immuno Polymorphism Database (http://www.ebi.ac.uk/ipd/index.html).
See also Supplemental Experimental Procedures.
Immunostaining and Histological Analyses
MRI and PET Studies
PET scans with [11C]PK11195 were performed with the use of an animal PET scanner (microPET Focus220; Siemens Medical Solutions) to identify the activation of microglia.
High-resolution T1-weighted and T2-weighted images were obtained using a 3T MRI scanner (MAGNETOM Verio; Siemens AG) to identify the injection site of grafts in the postero-dorsal striatum and to evaluate graft survival.
See also Supplemental Experimental Procedures.
Transplantation
Floating aggregates (day 28) were harvested and dissociated into small clumps of 20-30 cells with Accumax.
The cells were suspended in the last culture medium (Figure 1N), which was neurobasal medium with B27 with added ascorbic acid, dibutyryl-cAMP, glial-cell-line-derived neurotrophic factor, and brain-derived neurotrophic factor.
We also added a ROCK inhibitor, Y27632, to increase the survival of the donor cells.
The suspension was prepared at a concentration of 2 × 105 cells/μl, and 4 μl of the suspension was injected through a 22-gauge needle with a Hamilton syringe.
We made six (two coronal × three sagittal) tracts of injection in one side of the putamen.
In total, 4.8 × 106 cells per animal (8.0 × 105 cells/tract × 6 tracts) were injected into one side of the putamen according to the coordinate decided by the MRI image of each monkey.
The same volume of the culture medium was injected to the contralateral side as control.
No immunosuppressant was used.
Under deep anesthesia, the animals were sacrificed and perfused transcardially with 4% paraformaldehyde after 3.5-4 months of observation.
Statistics
The data were expressed as the mean ± SD or mean ± SEM, and differences were tested by commercially available software Prism 6 (GraphPad).
With regard to the histological data for the auto- and allo-tracts, after confirming normal distribution, we proceeded to perform statistical analyses with ratio paired t tests.
Some tracts were omitted from the assessment because of technical problems; p values < 0.05 were considered to be significant.
Acknowledgments
We thank Ms.
E.
Yamasaki and Mr.
K.
Kubota for their technical assistance, Drs.
K.
Tanaka and T.
Sato for the genetic analyses, Dr. Y.
Yamada for his valuable advice on the histological studies, Dr. Y.
Ono for providing antibodies, and Dr. H.
Kawamoto for his critical advice on the manuscript.
This study was supported by grants from the Highway Project for Realization of Regenerative Medicine (Ministry of Education, Culture, Sports, Science and Technology), the Funding Program for World-Leading Innovative R&D on Science and Technology (FIRST Program, Japan Society for the Promotion of Science), and the Shimizu Foundation for Immunology and Neuroscience Grant for 2012.
Supplemental Information
Supplemental Information includes Supplemental Experimental Procedures, four figures, and three tables and can be found with this article online at http://dx.doi.org/10.1016/j.stemcr.2013.08.007.
Supplemental Information
Document S1.
Supplemental Experimental Procedures and Figures S1-S4 and Tables S1-S3

The invention claimed is:
1. A positive active material for a lithium secondary battery containing a lithium transition metal composite oxide having a hexagonal crystal structure in which the transition metal (Me) includes Ni, Co and Mn, wherein in the lithium transition metal composite oxide, a molar ratio of Ni to the transition metal (Me) (Ni/Me) is 0.5 or more and 0.9 or less, a molar ratio of Co to the transition metal (Me) (Co/Me) is 0.1 or more and 0.3 or less, a molar ratio of Mn to the transition metal (Me) (Mn/Me) is 0.03 or more and 0.3 or less, and a value obtained by dividing a half width ratio F(003)/F(104) at a potential of 4.3 V (vs. Li/Li+) by a half width ratio F(003)/F(104) at a potential of 2.0 V (vs. Li/Li+) is 0.9 or more and 1.1 or less.2. The positive active material for a lithium secondary battery according to claim 1, wherein the lithium transition metal composite oxide is represented by the composition formula Li<sub>1+x </sub>(Ni<sub>a</sub>Co<sub>b</sub>Mn<sub>c</sub>)<sub>1−x</sub>O<sub>2 </sub>(−0.1<x<0.1, 0.5≤a≤0.9, 0.1≤b≤0.3, 0.03≤c≤0.3, a+b+c=1).4. A method for producing the positive active material of claim 1 for a lithium secondary battery containing the lithium transition metal composite oxide having the hexaqonal crystal structure in which the transition metal (Me) includes Ni. Co and Mn, wherein when coprecipitating compounds containing Ni, Co and Mn in a solution to prepare a precursor, a solution containing a compound of Ni and Co and a solution containing a Mn compound are simultaneously added dropwise separately, and thereby a precursor of a transition metal composite oxide in which a molar ratio of Ni to the transition metal (Me) is 0.5 or more and 0.9 or less, a molar ratio of Co to the transition metal (Me) is 0.1 or more and 0.3 or less, and a molar ratio of Mn to the transition metal (Me) is 0.03 or more and 0.3 or less, is prepared.5. The method for producing the positive active material for a lithium secondary battery according to claim 4, wherein the precursor has a tap density of 1.4 g/cc or more.1. An anode active material precursor, characterized in, and a composite transition metal oxide composite comprises a transition metal hydroxide,
And a range of electrical conductivity of 0.01-0.1 mS/cm.2. The anode active material precursor according to claim 1, characterized in, determined by the method according to nitrogen adsorption BET specific surface area of 50-200 m2/gof the range.3. The anode active material precursor according to claim 1, characterized in, precursor, of less than 10 nm in a pore volume of the particles is from 1x10-3-5x10-2cm3/g·nm.4. The anode active material precursor according to claim 1, characterized in, composite transition metal hydroxide with each other are mixed transition metal oxide is a solid solution or a composite form.5. The anode active material precursor according to claim 1, characterized in, by the following chemical formula 1 shown:
[Formula 1]
(MOx)A· (M (OH)2)B
Wherein,
Ni is a MaCobM'c,
M'is selected from the alkali metal, alkaline earth metals, Group 13 elements, Group 14, Group 15 elements, Group 16 element, a Group 17 element, a transition metal and one or more of the group consisting of rare-earth element,
1 ≤ x ≤ 1.5,0.6 ≤ a<1.0,0 ≤ b ≤ 0.4,0 ≤ c ≤ 0.4,a + b + c=1,
And the A + B=1, 0.5 ≤ A<1.0, 0<B ≤ 0.5.6. The anode active material precursor according to claim 5, characterized in, M'is selected from the group consisting of Y and the one of Al,Mn,Zr,W,Ti,Mg,Sr,Ba,Ce,Hf,F,P,S,La or more.7. The anode active material precursor according to claim 5, characterized in, of the formula 1, 0.6 to 1.0 and less than A, B are larger than 0 and is not more than 0.4.8. An anode active material, wherein the anode active material precursor as claimed in any one of claim 1-7 use with a lithium precursor is manufactured.10. The anode active material according to claim 9, characterized in, an electrical conductivity of the range of 18-40 mS/cm.11. Claim 1-7 anode active material precursor according to any one of the manufacturing method, comprising the following chemical formula 2 with a transition metal hydroxide represented by the composite temperature of 200-500 °C 0.5-10 hour heat treatment step of:
[Formula 2]
M (OH)2
Wherein,
Ni is a MaCobM'c,
M'is selected from the alkali metal, alkaline earth metals, Group 13 elements, Group 14, Group 15 elements, Group 16 element, a Group 17 element, a transition metal and one or more of the group consisting of rare-earth element,
0.6 ≤ a<1.0,0 ≤ b ≤ 0.4,0 ≤ c ≤ 0.4,a + b + c=1.12. The anode active material precursor for producing method according to claim 11, characterized in, an anode active material precursor represented by the following chemical formula 1:
[Formula 1]
(MOx)A· (M (OH)2)B
Wherein,
Ni is a MaCobM'c,
M'is selected from the alkali metal, alkaline earth metals, Group 13 elements, Group 14, Group 15 elements, Group 16 element, a Group 17 element, a transition metal and one or more of the group consisting of rare-earth element,
1 ≤ x ≤ 1.5,0.6 ≤ a<1.0,0 ≤ b ≤ 0.4,0 ≤ c ≤ 0.4,a + b + c=1,
And the A + B=1, 0.5 ≤ A<1.0, 0<B ≤ 0.5.13. The anode active material precursor for producing method according to claim 12, characterized in, of the formula 1, a ratio of A to B is adjusted as follows:
(I) alter oxygen concentration of the heat treatment;
(Ii) using an oxidizing agent; or
(Iii) the application (i) and a (ii) both.14. The anode active material precursor for producing method according to claim 13, characterized in, KMnO oxidizing agent is selected from the group4, H2O2, Na2O2, FeCl3, CuSO4, CuO,PbO2, MnO2, HNO3, KNO3, K2Cr2O7, CrO3, P2O5, H2SO4, K2S2O8, halogen and C6H5NO2one or more of the group consisting of.A novel hydrogen peroxide sensor based on multiwalled carbon nanotubes/poly(pyrocatechol violet)-modified glassy carbon electrode
MWCNTs (95% purity) were purchased from Chengdu Institute of Organic Chemistry of Academy of Sciences and synthesized by chemical vapor deposition. PCV was obtained from the Jiangsu Chemical Company (China). H2O2 and other chemicals were of analytical grade and were obtained from Shanghai Chemical Reagent (Shanghai, China) used without further purification. Voltammetric experiments were carried out in solutions deaerated by pure nitrogen. The phosphate buffer solution (PBS, 0.1M) was prepared by 0.1M KH2PO4 and 0.1M K2HPO4, and then adjusted the pH with 0.1M H3PO4 and 0.1M KOH solutions. All the solutions were prepared with doubly distilled water, and all experiments were conducted at the room temperature.Effect of chemical treatment on the thermoelectric properties of single walled carbon nanotube networks
The SWCNTs used in this study were synthesized by the CVD method and purchased from Thomas Swan & Co. LTD, Crockhall, Consett, UK (Product Ref: PR0920), in the form of a "wet cake." To prepare SWCNT buckypapers, the SWCNT suspension in 0.5% sodium dodecyl benzene sulfonate (SDBS) was vacuum filtrated using a polycarbonate membrane (pore size 0.2 um, from Millipore). SDBS was removed by rinsing several times with de-ionized water. After drying on a filter in air, the free standing SWCNT films were peeled off from the membrane. The thickness of the obtained buckypapers was approximately 35 um.1. A precursor of a transition metal oxide represented by the following chemical formula 1:
[ chemical formula 1]NiaMnbCo1-(a+b+c+d)ZrcMd[OH(1-x)2-y]A(y/n)
Wherein M is at least one of W and Nb, A is one or more anions other than OH, 0.3. ltoreq. a.ltoreq.0.9, 0.05. ltoreq. b.ltoreq.0.5, 0 < c < 0.05, 0 < d < 0.05, a + b + c + d < 1, 0 < x < 0.5, 0 < y < 0.05, n is the oxidation number of A,
a is selected fromAt least one of: PO (PO)4、CO3、BO3And F.2. The transition metal oxide precursor according to claim 1, wherein when M is W or Nb, the c: d molar ratio is 2:1 to 3: 2.3. The transition metal oxide precursor of claim 1, wherein said a comprises PO4And F.4. The transition metal oxide precursor of claim 1, wherein the transition metal oxide precursor has a tap density of from 1.0g/cc to 2.5 g/cc.5. A composite of lithium and a transition metal oxide, which comprises a product obtained by calcining a precursor of the transition metal oxide according to any one of claims 1 to 4 and a lithium compound.6. The complex of lithium and transition metal oxide of claim 5, wherein the lithium compound is at least one of: lithium hydroxide, lithium carbonate and lithium oxide.7. The complex of lithium and transition metal oxide according to claim 5, wherein the lithium compound is 0.95 to 1.2mol with respect to 1mol of the precursor of the transition metal oxide.8. The composite of lithium and transition metal oxide according to claim 5, wherein the calcination is performed at 600 ℃ to 1000 ℃.9. A positive electrode comprising the complex of lithium and transition metal oxide according to claim 5.10. A secondary battery comprising the positive electrode according to claim 9.Fabrication of Yb3+/Er3+co-doped yttrium-based coordination polymer hierarchical micro/nanostructures: upconversion luminescence properties and thermal conversion to the corresponding oxides

All the reagents were of analytical grade and used without purification. Yb3+/Er3+ co-doped yttrium-based coordination polymer (Y-CP:Yb3+,Er3+ sample 1) hierarchical micro/nanostructures were prepared by simple hydrothermal synthesis without any additives. In a typical process, 0.3 mmol of allantoin was dissolved in 25 ml of distilled water at room temperature. Then, 0.08 mmol of Y(NO3)3*6H2O, 0.015 mmol of Yb(NO3)3*6H2O and 0.005 mmol of Er(NO3)3*6H2O (nY(NO3)3*6H2O:nYb(NO3)3*6H2O:nEr(NO3)3*6H2O = 16:3:1) were added to the solution under vigorous magnetic stirring. Finally, the obtained solution was transferred into a 40 mL Teflon-lined stainless steel autoclave, sealed and maintained at 200 degC for 24 h. After being cooled down to room temperature, the product was isolated by centrifugation and collected by washing with deionized water and ethanol three times. The final product was obtained by drying under vacuum at 60 degC for 24 h. The Yb3+/Er3+ co-doped Y2O3 (Y2O3:Yb3+,Er3+) hierarchical micro/nanostructures were prepared by calcining sample 1 at 800 degC for 4 h under atmospheric pressure with a heating rate of 1 degC min-1.


1. A positive electrode active material precursor for a non-aqueous electrolyte secondary battery, the positive electrode active material precursor comprising:
a nickel composite hydroxide particle, wherein
a cross section of the nickel composite hydroxide particle includes a void,
a ratio of an area of the void to the cross section of the nickel composite hydroxide particle is less than or equal to 5.0%,
a circular region having a radius of 1.78 µm is set at a position where a ratio of an area of the void to the circular region is maximum, on the cross section of the nickel composite hydroxide particle, and
the ratio of the area of the void to the circular region is less than or equal to 20%.2. A method of manufacturing a positive electrode active material precursor for a non-aqueous electrolyte secondary battery, the method comprising:
a crystallization process performed in an atmosphere in which an oxygen concentration is less than or equal to 20 volume %, the crystallization process including crystallizing the positive electrode active material precursor including a nickel composite hydroxide particle, in a reaction solution including at least a mixed aqueous solution of a metal salt including nickel salt, an ammonium ion supplier, and an alkaline material, in which an ammonium ion concentration is greater than or equal to 5.0 g/L and a pH value is greater than or equal to 11.0 based on a liquid temperature of 50 °C.The escape of heavy atoms from the ionosphere of HD209458b.
I.
A photochemical-dynamical model of the thermosphere

Highlights
► We developed a new model to explain the FUV transit observations of HD209458b.
► We find a qualitative agreement between the model and the observed atmosphere.
► We constrain the mass loss rate and escape mechanism based on stellar heating.
► We present density profiles for the detected heavy atoms and ions.
► Diffusive separation of heavy species is prevented by the escape of H and protons.
Abstract
The detections of atomic hydrogen, heavy atoms and ions surrounding the extrasolar giant planet (EGP) HD209458b constrain the composition, temperature and density profiles in its upper atmosphere.
Thus the observations provide guidance for models that have so far predicted a range of possible conditions.
We present the first hydrodynamic escape model for the upper atmosphere that includes all of the detected species in order to explain their presence at high altitudes, and to further constrain the temperature and velocity profiles.
This model calculates the stellar heating rates based on recent estimates of photoelectron heating efficiencies, and includes the photochemistry of heavy atoms and ions in addition to hydrogen and helium.
The composition at the lower boundary of the escape model is constrained by a full photochemical model of the lower atmosphere.
We confirm that molecules dissociate near the 1μbar level, and find that complex molecular chemistry does not need to be included above this level.
We also confirm that diffusive separation of the detected species does not occur because the heavy atoms and ions collide frequently with the rapidly escaping H and H+.
This means that the abundance of the heavy atoms and ions in the thermosphere simply depends on the elemental abundances and ionization rates.
We show that, as expected, H and O remain mostly neutral up to at least 3Rp, whereas both C and Si are mostly ionized at significantly lower altitudes.
We also explore the temperature and velocity profiles, and find that the outflow speed and the temperature gradients depend strongly on the assumed heating efficiencies.
Our models predict an upper limit of 8000K for the mean (pressure averaged) temperature below 3Rp, with a typical value of 7000K based on the average solar XUV flux at 0.047AU.
We use these temperature limits and the observations to evaluate the role of stellar energy in heating the upper atmosphere.

Introduction
The detection of hot atomic hydrogen in the upper atmosphere of HD209458b (Vidal-Madjar et al., 2003, 2004) has inspired numerous attempts to model physical and chemical processes in highly irradiated atmospheres, including rapid escape as one of the most challenging aspects.
Subsequent detection of heavy atoms and ions (Vidal-Madjar et al., 2004; Linsky et al., 2010) point out the need for more complex models that include the chemistry associated with these species as well as the collision coupling between them and the major species.
Indeed, close-in extrasolar planets offer a natural laboratory to constrain the theory of rapid escape, including hydrodynamic escape.
This is important because aspects of the theory are controversial, and yet rapid escape is believed to have played a role in shaping the early evolution of the atmospheres in the Solar System (e.g., Zahnle and Kasting, 1986; Hunten et al., 1987).
Escape may also be a crucial factor in determining atmospheric conditions and habitability of super-Earths and Earth-like planets around M dwarfs (e.g., Tarter et al., 2007) that may be amenable to spectroscopic studies in the near future (e.g., Charbonneau et al., 2009).
The basic ideas about the nature of the upper atmospheres around close-in EGPs were laid out almost as soon as the first planet, 51 Peg b (Mayor and Queloz, 1995), was detected.
For instance, Coustenis et al. (1998) argued that heating by the stellar EUV radiation and interaction with the stellar wind leads to high temperatures of several thousand Kelvins in the upper atmosphere and exosphere of close-in EGPs.
They also suggested that the upper atmosphere is primarily composed of atoms and ions, and that hydrodynamic escape might be able to drag species heavier than H and He into the exosphere.
At the same time, Schneider et al. (1998) argued that material escaping from the atmospheres of close-in EGPs would form a potentially observable comet-like tail.
When Vidal-Madjar et al. (2003, 2004) detected the transits of HD209458b in the stellar FUV emission lines, they also argued that the planet is followed by comet-like tail of escaping hydrogen, and that hydrodynamic escape is required to drag oxygen and carbon atoms to the thermosphere.
The model of Yelle (2004, 2006) was the first attempt to model the aeronomy and escape processes in detail and most of the assumptions in that work have been adopted by subsequent investigators.
It solved the vertical equations of continuity, momentum, and energy for an escaping atmosphere, including photochemistry in the ionosphere and transfer of stellar XUV radiation.
Based on a composition of hydrogen and helium, the results demonstrated that H2 dissociates in the thermosphere, which at high altitudes is dominated by H and H+.
The model also showed that stellar heating leads to temperatures of ∼10,000K in the upper atmosphere, and predicted an energy-limited mass loss rate of 4.7×107kgs-1 (Yelle, 2006).
Yelle (2004) argued that conditions beyond ∼3Rp were too complex and uncertain to be modeled reliably and therefore chose an upper boundary at 3Rp, rather than at infinity, as adopted in early solar wind models.
This led to a requirement for boundary conditions for the fluid equations at a finite radius.
Yelle (2004) required consistency between fluid and kinetic simulations, based on the well established fact that kinetic and fluid approaches provide consistent results for the escape flux (e.g., Lemaire and Scherer, 1973).
This led to subsonic velocities of a few kms-1 in his model - although the presence of a sonic point at a higher altitude was not ruled out.
Many other models for the upper atmospheres of close-in EGPs have been published (e.g., Lammer et al., 2003; Lecavelier des Etangs et al., 2004; Jaritz et al., 2005; Tian et al., 2005; Erkaev et al., 2007; Garcia Munoz, 2007; Schneiter et al., 2007; Penz et al., 2008; Holström et al., 2008; Murray-Clay et al., 2009; Stone and Proga, 2009; Guo, 2011; Trammell et al., 2011).
These include one-dimensional, two-dimensional, and three-dimensional models that make different assumptions regarding heating efficiency, the effect of stellar tides, photochemistry, and the escape mechanism.
Despite significant differences in the temperature and velocity profiles, almost all of the existing models agree that close-in EGPs such as HD209458b are surrounded by an extended, hot thermosphere that is undergoing some form of escape.
Most of the models to date concentrate on the distribution of H and H+ in the upper atmosphere.
Garcia Munoz (2007) developed the only model to address the presence of O and C+ in the thermosphere (Vidal-Madjar et al., 2004; Linsky et al., 2010).
This model is otherwise similar to Yelle (2004), but it includes the photochemistry of heavy ions, atoms, molecules, and molecular ions.
It also extends to higher altitudes, and includes the effect of substellar tidal forces and stellar wind, albeit in an approximate manner.
Koskinen et al. (2007a,b) developed a three-dimensional model for the thermospheres of EGPs at wide orbits.
They pointed out that the atmospheres of close-in EGPs do not escape hydrodynamically unless they receive enough stellar XUV energy to dissociate molecules in the EUV heating layer below the exobase.
Although their results are limited to the specific case of H2, they can be generalized as follows.
The most important molecules H2 (through the formation of H3+), CO, H2O, and CH4 act as strong infrared coolants in the thermosphere.
High temperatures and rapid escape are only possible once these molecules are dissociated.
Koskinen et al. (2007b) showed that H2 dissociates in the thermosphere of a Jupiter-type planet orbiting a Sun-like star within 0.2AU.
Once H2 dissociates, it is reasonable to assume that other molecules dissociate too.
At this point the pressure scale height is enhanced by a factor of ∼10 when H becomes the dominant species in the thermosphere and temperatures reach 10,000K.
It should be noted that a composition of H and H+ with high temperatures does not guarantee that the atmosphere escapes hydrodynamically.
For instance, Koskinen et al. (2009) showed that hydrodynamic escape is extremely unlikely to occur on a planet such as HD17156b because of its high mass and eccentric orbit.
These types of results have implications on statistical studies that characterize the escape of planetary atmospheres by relying on the so-called energy-limited escape (e.g., Watson et al., 1981; Lecavelier des Etangs, 2007; Sanz-Forcada et al., 2010).
These studies often include an efficiency factor in the mass loss rate that is based on the heating efficiency of the upper atmosphere (e.g., Lammer et al., 2009).
Unless the atmosphere is escaping rapidly, the heating efficiency could be considerably larger than the fraction of energy that actually powers escape through adiabatic cooling.
Under diffusion-limited escape or in the Jeans regime the energy-limited escape rate is just an upper limit and the true escape rate can be lower.
Ideally, the uncertainties in the models can be limited by detailed observations of the escaping species.
At present, multiple observations are only available for HD209458b, and they reveal the presence of H, O, C+, and Si2+ at high altitudes in the thermosphere (Vidal-Madjar et al., 2003, 2004; Linsky et al., 2010).
Visible and infrared observations have also revealed the presence of Na, H2O, CH4, and CO2 in the lower atmosphere (Charbonneau et al., 2002; Knutson et al., 2008; Swain et al., 2009).
Taken together, these observations are beginning to reveal the composition and thermal structure in the atmosphere of HD209458b.
The purpose of the current paper is to characterize the density profiles of all of the detected species in the thermosphere, and to explain the presence of the heavy atoms and ions at high altitudes in the upper atmosphere.
The results can be used to infer some basic properties of the atmosphere.
To this end, we introduce a one-dimensional escape model for the upper atmosphere of HD209458b that includes the photochemistry of heavy atoms and ions.
As pointed out above, previous models agree broadly on the qualitative nature of the thermosphere but the temperature, density, and velocity profiles predicted by them differ significantly (see Section 3.1).
Some authors have argued that the density of H in the thermosphere is not sufficient to explain the observed transit depths (see Koskinen et al. (2010a) for a review), thus lending support to alternative interpretations of the observations such as the presence of energetic neutral atoms (Holström et al., 2008) or a comet-like tail of hydrogen shaped by radiation pressure (Vidal-Madjar et al., 2003).
Accurate modeling of the thermosphere is required to enable better judgment between different explanations of the observations.
The differences between previous models arise from different assumptions regarding heating rates and boundary conditions.
In addition to modeling the density profiles of the detected heavy species, we have improved these aspects of the calculations in our work.
For instance, the lower boundary conditions are constrained by results from a detailed photochemical model of the lower atmosphere (Lavvas et al., in preparation).
With regard to the upper boundary conditions, we demonstrate that for HD209458b the extrapolated 'outflow' boundary conditions (e.g., Tian et al., 2005) are consistent with recent results from kinetic theory (Volkov et al., 2011a,b) as long as the upper boundary is at a sufficiently high altitude - although uncertainties regarding the interaction of the atmosphere with the stellar wind may limit the validity of both boundary conditions.
We highlight the effect of heating efficiency and stellar flux on the density and temperature profiles, and constrain the likely heating rates by using photoelectron heating efficiencies based on the results of Cecchi-Pestellini et al. (2009) and our own estimates (Section 3.1).
As a result we provide a robust qualitative description of the density profiles, and constrain the mean temperature and velocity profile in the thermosphere.
A second paper by Koskinen et al. (2012) (Paper II) compares our results directly with the observations.
Methods
Hydrodynamic model
We use a one-dimensional escape model for HD209458b (Rp=1.32RJ, Mp=0.69MJ, a=0.047AU) that is similar to the models of Yelle (2004) and Garcia Munoz (2007).
Because such models are extensively discussed in the literature, we include only a brief overview of the model here, with the emphasis on how it differs from previous work.
The model solves the one-dimensional equations of motion for an escaping atmosphere composed of several neutral and ionized species:(1)∂ρs∂t+1r2∂∂r(r2ρsv)+1r2∂∂r(r2Fs)=∑tRst(2)∂(ρv)∂t+1r2∂∂r(r2ρv2)=-ρg-∂p∂r+fμ(3)∂(ρE)∂t+1r2∂∂r(r2ρEv)=ρQR-p1r2∂∂r(r2v)+1r2∂∂rr2κ∂T∂r+Φμwhere ρs is the density of species s, v is the vertical velocity, Fs is the diffusive flux of species s, Rst is the net chemical source term for species s, fμ is a force term arising from viscous acceleration, E=cvT is the specific internal energy of the gas, QR is the specific net radiative heating rate, κ is the coefficient of heat conduction, and Φμ is the viscous dissipation functional (e.g., O'Neill and Chorlton, 1989).
The total density and pressure are given by ρ=∑sρs and p=∑snskT, respectively, where electrons contribute to the total pressure.
We assumed equal temperatures for the neutral species, ions and electrons, and calculated the electron density at each altitude from the requirement of charge neutrality.
The model solves separate continuity equations for each species, but treats the atmosphere otherwise as a single fluid.
The differences in the velocities of the individual species are taken into account by including the diffusive flux Fs.
We calculated the fluxes by solving simultaneous equations for multiple species based on the diffusion equation given by Chapman and Cowling (1970) Eq.
(18.2,6), p.344.
We also included a force term due to the ambipolar electric field given by eE=-(1/ne)dpe/dr, where the subscript e refers to electrons, that can be important in highly ionized flows.
The collision terms account for neutral-neutral, resonant and non-resonant ion-neutral, and Coulomb collisions.
This method is in principle similar to those of Yelle (2004) and Garcia Munoz (2007).
We verify that the single temperature and diffusion approximations are valid for HD209458b based on our results in Section 3.2.2.
The model includes heat conduction and terms due to viscosity in both the momentum and energy equations.
Thus the equations are consistent with the level of approximation in the Navier-Stokes (NS) equations.
The NS equations themselves are a simplification of the 13-moment solution to the Boltzmann equation (e.g., Gombosi, 1994) that is valid when the Knudsen number Kn=Λ/L≪1, where Λ is the mean free path and L is the typical length scale for significant changes in density or temperature.
Broadly speaking, the equations are valid below the exobase, and terms due to heat conduction and viscosity gain significance as Kn→1.
We note that the exobase on HD209458b is typically located at a very high altitude (see Section 3.1), and viscosity and heat conduction are not particularly important.
We included species such as H, H+, He, He+, C, C+, O, O+, N, N+, Si, Si+, Si2+, and electrons in the hydrodynamic model.
We also generated simulations that included Mg, Mg+, Na, Na+, K, K+, S, and S+, but the presence of these species did not affect the density profiles of H, O, C+, or Si2+ significantly.
The model includes photoionization, thermal ionization, and charge exchange between atoms and ions.
The reaction rate coefficients for these processes are listed in Table 1.
Multiply charged ions were included only if the ionization potential of their parent ion was sufficiently low compared to the thermal energy and radiation field in the upper atmosphere.
We note that our model also includes impact ionization by thermal electrons.
In general, this can be important for species with low ionization potential such as alkali metals (e.g., Batygin and Stevenson, 2010), although we find photoionization to be more significant in the thermosphere (see Section 3.2).
In order to simulate photochemistry in a numerically robust fashion, we coupled the dynamical model with the ASAD chemistry integrator developed at the University of Cambridge (Carver et al., 1997).
In most cases we used the IMPACT integration scheme that is provided by ASAD.
We did not include any molecules in the present simulations, and thus placed the lower boundary of the hydrodynamic model at p0=1μbar (see Section 2.1.1).
Molecular chemistry is not significant in the thermosphere, where our results agree qualitatively with Garcia Munoz (2007) despite simpler chemistry (see Section 3.2).
This is an important result because it implies that complex molecular photochemistry does not need to be included in the models for the thermosphere.
However, the chemistry of molecular ions may be important on HD209458b below the 0.1μbar level and it needs to be studied in greater detail.
The upper atmosphere is heated by stellar XUV radiation.
We simulated heating and photoionization self-consistently by using the model density profiles and the UV spectrum of the average Sun.
The spectrum covers wavelengths between 0.1 and 3000Å.
The XUV spectrum between 0.1 and 1050Å was generated by the SOLAR2000 model (Tobiska et al., 2000).
It includes strong emission lines separately and weaker lines binned by 50Å.
The Lyman α line was included with a wavelength spacing of 0.5Å from Lemaire et al. (2005) and the rest of the spectrum was taken from Woods and Rottman (2002).
We assumed that most of the Lyman α radiation absorbed by H is resonantly scattered and does not contribute significantly to the heating of the atmosphere.
This is because the lifetime of the 2p state of H is only 1.6ns, compared with the typical collision timescale of ∼1s near the temperature peak in the thermosphere of HD209458b.
References for photoabsorption cross sections of the different species are included in Table 1.
In general, we divided the incident stellar flux by a factor of 4 to account for uniform redistribution of energy around the planet.
This is expected to be approximately valid in the lower thermosphere based on the three-dimensional simulations of Koskinen et al. (2010b).
In the extended upper thermosphere, on the other hand, radiation passes through to the night side and leaves only a small region free of direct heating (e.g., see Fig. 2 of Koskinen et al. (2007b)).
The current model also includes heating due to photoabsorption by C, O, N, and metals.
This is relatively insignificant - although it leads to some additional heating in the lower thermosphere by FUV radiation.
Heating of the thermosphere is mostly driven by photoionization and the generation of photoelectrons, although direct excitation of atoms and molecules may also play a role.
Photoelectrons excite, ionize, and dissociate atoms and molecules until they lose enough energy and become thermalized i.e., share their energy with thermal electrons in Coulomb collisions.
Thermal electrons share their energy with ions and eventually, the neutral atmosphere.
In highly ionized atmospheres such as on HD209458b the photoelectron heating efficiency can be close to 100% (Cecchi-Pestellini et al., 2009), depending on the energy of the photoelectrons.
We used scaled heating efficiencies that depend on photoelectron energy to estimate the net heating efficiency in the atmosphere (Section 3.1).
Generally, we refer to two different definitions of heating efficiency in Section 3.1 in order to highlight the effect of heating efficiency on the temperature and velocity profiles.
The net heating efficiency ηnet is defined simply as the fraction of the absorbed stellar energy that heats the atmosphere.
Photoelectron heating efficiency, on the other hand, applies to photoelectrons with energy Ep=hν-Is, where Is is the ionization potential of species s and hν is the energy of the ionizing photon.
The photoelectron heating efficiency is the fraction of Ep that heats the thermosphere, and it is generally higher than ηnet because it does not account for recombination.
The net heating efficiency is often used to calculate mass loss rates for extrasolar planets (e.g., Lammer et al., 2009).
Therefore it is important not to confuse these two definitions of heating efficiency.
We included radiative cooling by recombination under the assumption that the thermosphere is optically thin to the emitted photons.
This implies that the ionization potential energy Is never contributes to heating at any levels.
We also considered the influence of Lyman α cooling by excited H, although this cooling rate is uncertain and likely to be low for HD209458b.
We discuss the effect of different cooling rates further in Section 3.1.
Lower boundary conditions
As stated above, we placed the lower boundary of the hydrodynamic model at p0=1μbar and did not include H2 or other molecules in the model.
This decision was motivated by photochemical calculations for HD209458b (Lavvas et al., in preparation) that we used to constrain the lower boundary condition.
The photochemical model was originally developed for the atmosphere of Titan (Lavvas et al., 2008a,b) but it was recently expanded to simulate EGP atmospheres.
It calculates the chemical composition from the deep troposphere (1000bar) up to the thermosphere above the 0.1nbar level by solving the coupled continuity equations for all species based on a database of ∼1500 reaction rate coefficients and 103 photolysis processes.
Forward and reverse rates are included for each reaction with the ratio of the rate coefficients defined by thermochemical data.
Thus, the results are consistent with thermochemical equilibrium at deep atmospheric levels but differences develop at higher altitudes due to photolysis, diffusion, and eddy mixing.
At the lower boundary the chemical abundances of the main species (H, C, N, and O) are set to their thermodynamic equilibrium values and, depending on the vertical temperature profile and their abundances, species are allowed to condense.
Fig. 1 shows the mixing ratios of H2, H, H2O, O, CH4, CO, CO2, and C as a function of altitude from the photochemical model.
In general, the results are similar to those of Moses et al. (2011).
The H2/H transition is located near 1μbar.
At lower pressures, the mixing ratio of H2 decreases rapidly with altitude and falls below 0.1 above the 0.1μbar level.
In agreement with Garcia Munoz (2007), the dissociation of H2 is caused by dissociation of H2O, which leads to the production of OH radicals that attack the H2 molecules.
We note that the exact location of the H2/H transition depends on the temperature profile and, depending on the thermal structure, it could occur even below the 1μbar level.
The major oxygen-bearing molecules, CO and H2O, have roughly equal abundances from 10 to 10-5bar.
This is in line with thermochemical equilibrium at the temperatures and pressures relevant to HD209458b (Lodders and Fegley, 2002).
H2O and CO are effectively dissociated at pressures lower than 0.3 and 0.1μbar, respectively.
We note that molecular abundances could be significant at 0.1-1μbar, and technically the results of the hydrodynamic calculations are only valid above the 0.1μbar level.
The presence of molecules such as H2, H2O, and CO can lead to enhanced UV heating as well as efficient radiative cooling by H3+, H2O and CO in the 0.1-1μbar region.
The photochemical model also includes the chemistry of silicon.
Due to condensation into forsterite and enstatite (e.g., Visscher et al., 2010), the abundance of Si in the observable atmospheres of giant planets was thought to be negligible and thus the photochemistry of silicon in planetary atmospheres has not been studied before.
The photochemical calculations indicate that, in agreement with thermochemical calculations (Visscher et al., 2010), SiO is the dominant silicon-bearing gas.
SiO dissociates in the thermosphere at a similar pressure level as H2O and CO.
We note that the detection of Si2+ in the thermosphere implies that silicon does not condense in the atmosphere of HD209458b (Paper II).
In addition to composition, lower boundary conditions are required for temperature and velocity.
We specified T0 and p0 at the lower boundary, and used them to calculate ρ0 from the ideal gas law.
The steady state continuity equation ρ0v0r2=Fc, where Fc is the flux constant, was used to calculate the velocity v0 at the lower boundary during each time step.
The flux constant is solved self-consistently by the model, and it depends largely on the terms in the energy equation.
In general we assumed that T0≈1300K, which is close to the effective temperature of the planet.
We note that this temperature is largely unconstrained.
Radiative transfer models for close-in EGPs (e.g., Showman et al., 2009 and references therein) do not account for heating by stellar UV radiation or possible opacity sources created by photochemistry (e.g., Zhanle et al., 2009).
Therefore these models may not accurately predict the temperature at the base of the thermosphere.
Sing et al. (2008a,b) used observations of the Na D line profile to constrain the temperature profile in the upper atmosphere.
They suggested that Na condenses into clouds near the 3mbar level, and thus predicted a deep minimum in temperature in this region that is required for condensation.
The detection of Si2+ implies that condensation of Na in the lower atmosphere is unlikely (Paper II), and thus this result is unreliable.
Relying on the same observations, Vidal-Madjar et al. (2011a,b) predicted that the temperature increases steeply from 1300K to 3500K near the 1μbar level.
However, their method to retrieve the temperature relies on the density scale height of Na to express the optical depth along the line of sight (LOS).
This is not consistent with the argument that Na is depleted above the 3mbar level.
If such a depletion takes place, the density scale height of Na is not the same as the scale height of the atmosphere and it cannot be used to retrieve temperatures.
Upper boundary conditions
Previous models of the thermosphere disagree on the details of the density and temperature profiles (e.g., Yelle, 2004; Tian et al., 2005; Garcia Munoz, 2007; Murray-Clay et al., 2009).
This is partly due to different boundary conditions, although assumptions regarding the heating rates and composition are probably more important (see Section 3.1).
Unfortunately, the planetary wind equations can have an infinite number of both subsonic and supersonic solutions.
In time-dependent models, the upper boundary conditions in particular can determine if the solution is subsonic or supersonic, and they can alter the temperature and velocity profiles (e.g., Garcia Munoz, 2007).
The choice of proper boundary conditions is therefore important.
Volkov et al. (2011a,b) studied the escape of neutral atmospheres under different circumstances by using the kinetic Monte Carlo (DSMC) method.
Because the fluid equations are a simplification of the kinetic equations at low values of Kn, the hydrodynamic model should ideally be consistent with the DSMC results both above and below the exobase.
Volkov et al. (2011a,b) found that the nature of the solutions depends on the thermal escape parameter X0=GMpm/kT0r0 and the Knudsen number Kn0 at the lower boundary r0 of a region where diabatic heating is negligible.
They argued that hydrodynamic escape is possible when X0<2-3 (see also Opik, 1963; Hunten, 1973).
When X0≳3, on the other hand, the sonic point is at such a high altitude that the solution is practically subsonic and with X0≳6 the escape rate is similar to the thermal Jeans escape rate.
The results of the DSMC calculations can be incorporated into hydrodynamic models with appropriate upper boundary conditions.
Volkov et al. (2011a,b) suggest that the modified Jeans escape rate, which is based on the drifting Maxwellian velocity distribution function, is a good approximation of the DSMC results in fluid models, consistent with Yelle (2004).
In order to contrast the modified Jeans upper boundary conditions (hereafter, the modified Jeans conditions) with other possibilities, we used them and the extrapolated upper boundary conditions (hereafter, the 'outflow' conditions) adopted by Tian et al. (2005) and Garcia Munoz (2007) in our simulations.
In general, we placed the upper boundary at 16Rp.
The impact of the boundary conditions is discussed in Section 3.1.5.
We formulated the outflow conditions simply by extrapolating values for density, temperature and velocity with a constant slope from below.
For the modified Jeans conditions, we calculated the effusion velocity vs at the upper boundary separately for each species by using Eq.
(9) from Volkov et al. (2011b).
Using this equation violates the conservation of electric charge at the upper boundary because the small mass of the electrons causes their velocity to be much larger than the velocity of the protons.
In reality charge separation is prevented by the generation of an ambipolar electric field that ensures that the vertical current is zero at the upper boundary.
This electric field causes the ions to escape faster while it slows the electrons down.
Effectively this lowers the escape velocity vesc=2GM/r of the ions and increases the escape velocity of the electrons.
In order to incorporate the ambipolar electric field in the modified Jeans conditions we expressed the Jeans parameters for ions and electrons as:(4)Xi=GMmikTr-ϕeqikT(5)Xe=GMmekTr+ϕe|qe|kTwhere ϕe is the ambipolar electric potential, qi,e is the electric charge and subscripts i and e stand for ions and electrons, respectively.
We used these Jeans parameters to calculate the effusion velocities for the electrons and ions, and then solved iteratively for ϕe by using the condition of zero current i.e., ne|qe|ve=∑iniqivi.
This approach is consistent with kinetic models for the solar and polar winds (Lemaire and Scherer, 1971a,b).
Having obtained the correct effusion velocities for the charged and neutral species, we evaluated the mass weighted outflow velocity from:(6)v=1ρ∑sρsvsand used this velocity as an upper boundary condition.
The values of temperature and density that are required for this calculation were extrapolated from below.
As the model approaches steady state, the solution approaches a value of v at the upper boundary that is consistent with the modified Jeans velocity.
Numerical methods
We solved the equations of motion on a grid of 400-550 levels with increasing altitude spacing.
The radius rn from the center of the planet at level n is thus given by a geometric series (e.g., Garcia Munoz, 2007):(7)rn=r1+∑i=1n-1fciδz0where r1=1.08Rp, δz0=10km, and fc=1.014.
We solved the equations of motions in two parts, separating advection (Eulerian terms) from the other (Lagrangian) terms.
The Lagrangian solution is performed first, and all variables are updated before advection.
We used the flux conservative van Leer scheme (e.g., van Leer, 1979) for advection, and the semi-implicit Crank-Nicholson scheme (e.g., Jacobson, 1999) to solve for viscosity and conduction in the momentum and energy equations, respectively.
We employed a time step of 1s in all of our calculations.
Despite the sophisticated numerical apparatus, the model is still occasionally unstable, particularly in the early stages of new simulations.
The primary source of the instabilities are pressure fluctuations (sound waves) that are not balanced by gravity.
We used a two-step Shapiro filter (Shapiro, 1970) periodically to remove numerical instabilities.
We assumed that a steady state has been reached once the flux constant Fc is constant with altitude and the flux of energy is approximately conserved.
Results
Temperature and velocity profiles
In this section we constrain the range of mean temperatures and velocities based on stellar heating in the thermosphere of HD209458b and similar close-in EGPs.
We discuss the general dependency of the temperature and velocity profiles on the net heating efficiency and stellar flux, and relate this discussion to new temperature and velocity profiles for HD209458b that are based on realistic photoelectron heating efficiencies calculated specifically for close-in EGPs.
This discussion is necessary because the temperature and velocity profiles from previous models of the upper atmosphere differ significantly, and the differences affect the density profiles of the observed species (see Section 3.2).
As an example, Fig. 2 shows the temperature profiles based on several earlier models.
In addition to boundary conditions, the discrepancies evident in this figure arise from different assumptions about the heating rates.
General dependency
We note that the thermal structure in the upper atmospheres of the giant planets in the Solar System is not very well understood despite modeling and observations that are far more extensive than those available for extrasolar planets (e.g., Yelle and Miller, 2004).
It is therefore useful to test the reaction of the model to different heating rates and profiles.
We used our model to calculate temperature and velocity profiles based on different heating efficiencies and stellar fluxes.
These profiles are shown in Fig. 3.
First, we used the average solar spectrum (Section 2.1) and varied the net heating efficiency ηnet from 0.1 to 1.
Second, we multiplied the solar spectrum by factors of 2×, 10×, and 100×, and used ηnet=0.5.
The range of enhanced fluxes covers solar maximum conditions and stars that are more active than the Sun.
In each case we assumed that ηnet is constant and does not vary with altitude.
We used planetary parameters of HD209458b and set the upper boundary to 16Rp with outflow boundary conditions, and the lower boundary to the 1μbar level with a temperature of 1300K.
A net heating efficiency of 50% is similar to the heating efficiency in the jovian thermosphere (Waite et al., 1983), and we may consider this as a representative case of a typical gas giant (hereafter, the H50 model).
The maximum temperature in the H50 model is 11,500K and the temperature peak is located near 1.5Rp (p=0.3nbar).
As ηnet varies from 0.1 to 1, the peak shifts from 1.4Rp (0.5nbar) to 1.9Rp (0.1nbar) and the maximum temperature varies from 10,000K to 13,200K.
It is interesting to note that the temperature profile depends strongly on the heating efficiency but the location of the peak and the maximum temperature depend only weakly on ηnet.
This is because the vertical velocity increases with heating efficiency, leading to more efficient cooling by faster expansion that controls the peak temperature while enhanced advection and high altitude heating flatten the temperature gradient above the peak.
As a result, the temperature profile is almost isothermal when ηnet=1.
It is also interesting that the temperature profiles in the models that are based on ηnet=0.5 and the solar flux enhanced by factors of 2-100 differ from models with the average solar flux and a higher heating efficiency.
For instance, one might naively assume that a model with ηnet=0.5 and 2× the average solar flux would be similar to a model with the average solar flux and ηeta=1.
Surprisingly, this is not the case - despite the fact that the mass loss rates from these models are identical.
This is because of the way radiation penetrates into the atmosphere - doubling the incoming flux is not the same as doubling the heating rate at each altitude for the same flux.
As the stellar flux increases further, the temperature peak shifts first to higher altitudes, and then to lower altitudes so that for the 100× case the peak is located again near 1.5Rp.
Despite the hugely increased stellar flux, the peak temperature is only 18,300K for the 100× case.
This is again because the enhanced adiabatic and advective cooling driven by faster expansion control the temperature even in the absence of efficient radiative cooling mechanisms.
Koskinen et al. (2010a) suggested that the mean temperature of the thermosphere below 3Rp can be constrained by observations, and used their empirical model to fit temperatures to the H Lyman α transit data (Vidal-Madjar et al., 2003; Ben-Jaffel, 2007, 2008).
A quantity that can be compared with their results is the pressure averaged temperature of the hydrodynamic model, which is given by:(8)Tp¯=∫p1p2T(p)d(lnp)ln(p2/p1)For ηnet ranging from 0.1 to 1, the pressure averaged temperature below 3Rp varies from 6200K to 7800K for the average solar flux.
In the H50 model the pressure averaged temperature is 7000K.
We note that T¯p is a fairly stable feature of our solutions - in contrast to the details of the temperature profile and velocities it is relatively insensitive to different assumptions about the boundary conditions and heating efficiencies.
Obviously, T¯p depends on the stellar flux, although it only increases to 9800K in the 100× case.
The effect of changing the heating efficiency on the velocity profile is quite dramatic.
As ηnet ranges from 0.1 to 1 (with the average solar flux), the velocity at the upper boundary increases from 2.6kms-1 to 25kms-1.
However, the velocity does not increase linearly with stellar flux or without a bound - in the 100× case the velocity at the upper boundary is only 30kms-1.
An interesting qualitative feature of the solutions is that the sonic point moves to a lower altitude with increasing heating efficiency or stellar flux.
With ηnet=0.1 the isothermal sonic point is located above the upper boundary whereas with ηnet=1 it is located at 4Rp.
This behavior is related to the temperature gradient and it is discussed further in Section 3.1.3.
Basically the sonic point, when it exists, moves further from the planet as the high altitude heating rate decreases.
It is now clear that assumptions regarding the heating efficiency and radiative transfer have a large impact on the temperature and velocity profiles and the results from the previous models reflect this fact (see Fig. 2).
The differences between models have implications on the interpretation of observations.
For instance, Vidal-Madjar et al. (2003) and Linsky et al. (2010) suggested that the UV transit observations probe the velocity structure of the escaping gas.
Obviously, the nature of this velocity structure depends on the properties of the upper atmosphere.
On the other hand, Ben-Jaffel and Hosseini (2010) argued that the observations point to a presence of hot energetic atoms and ions within the Roche lobe of the planet.
We believe that it is important to properly quantify the role of stellar heating in creating the hot, escaping material before other options are pursued.
This means that detailed thermal structure calculations that rely on a proper description of photoelectron heating efficiencies are required.
Below we discuss a new approach to modeling the temperature profile in the thermosphere of HD209458b and its impact on the velocity and density profiles.
Energy balance and temperatures on HD209458b
In the previous section we discussed models where the net heating efficiency ηnet was fixed at a constant value at all altitudes.
In this section we discuss more realistic models of HD209458b that rely on new approximations of photoelectron heating efficiency and derive an estimate of ηnet based on these models.
Here we also include radiative cooling from recombination and, in one case, H Lyman α emissions by excited H (Murray-Clay et al., 2009).
Our aim was to calculate the most likely range of temperatures in the thermosphere of HD209458b based on average solar fluxes.
Fig. 4 shows the temperature and velocity profiles at 1-5Rp based on different approximations (see Table 2 for the input parameters).
Model C1 assumes a constant photoelectron heating efficiency of 93% at all altitudes and photoelectron energies.
This heating efficiency is appropriate for photoelectrons created by 50eV photons at an electron mixing ratio of xe=0.1 (Cecchi-Pestellini et al., 2009).
Model C2 is otherwise similar to C1 but the heating efficiency varies with photoelectron energy and altitude (see below).
Models C3 and C4 are also based on C1, but C3 includes the substellar tidal forces in the equations of motion (e.g., Garcia Munoz, 2007) and C4 includes Lyman α cooling.
All of these models are based on the outflow boundary conditions for temperature, velocity, and density.
Cecchi-Pestellini et al. (2009) also estimated the heating efficiencies for photoelectrons released by photons of energy E≳50eV at different values of the electron mixing ratio xe.
We used their heating efficiencies for xe=0.1 in the C2 model.
They parameterized their results in terms of the vertical column density NH of H.
We fitted the heating efficiency as a function of NH for 50eV photons with a regular transmission function, and modified this function accordingly for different cutoff altitudes and heating efficiencies of photons with different energies (see Figs.
3 and 4 of Cecchi-Pestellini et al. (2009)).
We note that xe≈0.1 near the temperature peak of our models and thus the results are appropriate for our purposes.
However, they are only applicable to photons with E≳50eV.
We used simple scaling to estimate the heating efficiencies for low energy photons with E<50eV.
As NH increases, the heating efficiency for 50eV photons saturates at 93%.
We assumed that the saturation heating efficiency for low energy photons is also 93%.
In reality, this heating efficiency may be closer to 100% but the difference is small.
In order to estimate the altitude dependence of the heating efficiency, we note that the rate of energy deposition by Coulomb collisions between photoelectrons of energy Ep and thermal electrons with a temperature T can be estimated from:(9)-dFEdr=L(Ep,e)Φpene(eVcm-3s-1)where FE is the flux of energy, Φpe is the photoelectron flux (cm-2s-1), ne is the density of thermal electrons (cm-3) andL(Ep,e)=3.37×10-12ne0.03Ep0.94Ep-EeEp-0.53Ee2.36(eVcm2)
with Ee=8.618×10-5Te is the stopping power (Swartz et al., 1971).1
Due to a historical precedent, the units here are in cgs.
1 Assuming that all of the energy is deposited by electrons that are thermalized within a path element dr, we can estimate the e-folding length scale for thermalization of photoelectrons with different energies as follows:(10)Λpe≈EpneLWe calculated Λpe for different photoelectrons based on the C1 model, and compared the result with the vertical length scale H of the atmosphere.
The latter is either the scale height or Rp, depending on which is shorter.
When Λpe/H≳0.005-0.01 we assumed that the heating efficiency decreases with altitude according to the transmission function for 50eV photons.
The limiting value was chosen to obtain a rough agreement with the results of Cecchi-Pestellini et al. (2009) for 50eV photons, and it implies that the heating efficiency approaches zero when Λpe/H≳0.1.
We parameterized the result in terms of the column density of H based on the density profiles of the C1 model, and connected our results for low energy photoelectrons smoothly with the results of Cecchi-Pestellini et al. (2009) for photons with E≳50eV.
We then generated the C2 model from the C1 model with the new heating efficiencies.
Fig. 5 shows the resulting heating efficiencies for 20, 30, 48, and 100eV photons.
Fig. 6 shows the volume heating rate due to EUV photons of different energies as a function of pressure based on the C2 model.
The maximum temperature of 12,000K is reached near 1.5Rp (p=0.6nbar).
This region is heated mainly by EUV photons with wavelengths between 200 and 900Å (E=14-62eV).
The saturation heating efficiency of 93% for these photons is higher than the corresponding heating efficiency in the jovian thermosphere (Waite et al., 1983).
This is because of strong ionization that leads to frequent Coulomb collisions between photoelectrons and thermal electrons.
Radiation with wavelengths shorter than 300Å (E>40eV) or longer than 912Å (13.6eV) penetrates past the temperature peak to the lower atmosphere.
The heating efficiency for photons with E>25eV approaches zero at high altitudes where heating is mostly due to low energy EUV photons.
The net heating efficiency for the C2 model is ηnet=0.44 (Table 2), which is close to the H50 model.
The location of the peak and maximum temperature in the C2 model agree with the H50 model, but the temperature at higher altitudes in the C2 model decreases much more rapidly with altitude than in the H50 model.
Fig. 7 shows the terms in the energy equation based on the C2 model.
In line with previous studies, stellar heating is mainly balanced by adiabatic cooling.
Advection cools the atmosphere at low altitudes below the temperature peak, whereas at higher altitudes it acts as a heating mechanism.
In fact, above 2Rp the adiabatic cooling rate is higher than the stellar heating rate because thermal energy is transported to high altitudes by advection from the temperature peak.
The radiative cooling term that is centered near 1.3Rp arises from recombination following thermal ionization.
Recombination following photoionization is included implicitly in the model and the rate is not included in the output.
Conduction is not significant at any altitude in the model.
We note that the rates displayed in Fig. 7 balance to high accuracy, thus implying that the simulation has reached steady state.
The differences between the C1 and C2 models are subtle.
The peak temperatures are similar, and the temperature profiles generally coincide below 3Rp.
Above this radius the temperature in the C2 model decreases more rapidly with altitude than in the C1 model and subsequently the sonic point moves to higher altitudes above the model domain.
The results indicate that the assumption of a constant photoelectron heating efficiency is appropriate below 3Rp whereas at higher altitudes it changes the nature of the solution.
This should not be confused with the assumption of a constant ηnet, which leads to a different temperature profile when compared with either C1 and C2 (see Fig. 3).
In general, the maximum and mean temperatures in models C1-C4 are relatively similar.
Thus we conclude that the mean temperature in the thermosphere of HD209458b is approximately 7000K and the maximum temperature is 10,000-12,000K.
The substellar tide is included in the C3 model.
We included it mainly to compare our results with previous models (Garcia Munoz, 2007; Penz et al., 2008; Murray-Clay et al., 2009).
The substellar tide is not a particularly good representation of the stellar tide in a globally averaged sense.
In reality, including tides in the models is much more complicated than simply considering the substellar tide (e.g., Trammell et al., 2011).
Compared to the C1 model, the maximum temperature in the C3 model is cooler by ∼1000K and at high altitudes the C3 model is cooler by 1000-2000K.
The velocity is faster and hence adiabatic cooling is also more efficient.
The substellar tide drives supersonic escape (see also, Penz et al., 2008) and the sonic point in the C3 model is at a much lower altitude than in the C1 model (see Section 3.1.3).
However, it is not clear how the sonic point behaves as a function of latitude and longitude.
Given that the tide is also likely to induce horizontal flows, it cannot be included accurately in 1D models.
Murray-Clay et al. (2009) argued that radiative cooling due to the emission of Lyman α photons by excited H is important on close-in EGPs.
The photons are emitted when the 2p level of H, which is populated by collisions with thermal electrons and other species, decays radiatively.
We included this cooling mechanism in the C4 model by using the rate coefficient from Glover and Jappsen (2007) that includes a temperature-dependent correction to the rate coefficient given by Black (1981).
We also included an additional correction factor of 0.1 based on detailed level population and radiative transfer calculations by Menager et al. (2011).
The effect of Lyman α cooling is largest near the temperature peak where the C4 model is 1500K cooler than the C1 model, but generally the difference is not large.
We note that the H Lyman α cooling rate here cannot be generalized as such to other EGPs because the level populations and opacities depend on the thermal structure and composition of the atmosphere.
Critical points
As we have pointed out, the location of the sonic point depends on the energy equation through the temperature profile.
Here we show that the use of the isothermal approximation in estimating the location of the sonic point can lead to significant errors unless the atmosphere really is isothermal.
The inviscid continuity and momentum equations can be combined to give an expression for the critical point ξc of a steady-state solution (Parker, 1965):(11)-ddξc2ξ2=-1ξ2dc2dξ+2c2ξ3=W2ξ4where ξ=r/r0,c=kT/m is the isothermal speed of sound, W=GMp/r0, and m is the mean atomic weight.
It is often assumed that the vertical velocity at the critical point is given by v2=c2(ξc) so that the critical point coincides with the isothermal sonic point (Parker, 1958).
However, Parker (1965) suggested that subsonic solutions are also possible if the density at the base of the flow exceeds a critical value determined from the energy equation.
In fact, he argued that conduction at the base of the corona may not be sufficient to drive a supersonic solar wind.
This led him to suggest that supersonic expansion is possible only if there is significant heating of the corona over large distances above the base.
For an isothermal atmosphere with a temperature T0, Eq.
(11) reduces to the famous result for the altitude of the sonic point (Parker, 1958):(12)ξc=W22c02where W2/c02 is the thermal escape parameter X0 at the lower boundary r0.
The isothermal sonic point in the C1 model is located at 7.2Rp where c(ξc)=7.2kms-1.
The volume averaged temperature of the C1 model below this point is approximately 7100K.
Assuming that r0=Rp, T0=7100K, and m=mH, X0∼16 and Eq.
(12) yields ξc∼8.
In this case the analytic result agrees fairly well with the hydrodynamic model if one accounts for partial ionization of the atmosphere by assuming that the mean atomic weight2
The mean atomic weight can be less than 1 because electrons contribute to the number density but not significantly to the mass density.
2 is m=0.9mH.
On the other hand, the isothermal sonic point in the C2 model is at 15.4Rp where c(ξc)=4.1kms-1.
This is because the temperature gradient of the model is steeper than the corresponding gradient in the C1 model.
The volume averaged mean temperature below 15Rp in the C2 model is 3900K.
With this temperature and m=mH, Eq.
(12) predicts a sonic point at 14.6Rp.
However, at 15Rp the atmosphere is mostly ionized and m=0.6mH.
With this value, the sonic point from Eq.
(12) would be located at 8.8Rp.
These examples show that there are significant caveats to using Eq.
(12) to estimate the altitude of the sonic point on close-in EGPs without accurate knowledge of the temperature and density profiles.
A variety of outcomes are possible and it is difficult to develop a consistent criteria for choosing values of T and m that would produce satisfactory results.
Another problem is that the atmosphere is not isothermal.
In fact, the temperature gradient above the heating peak in models C1-C4 (Table 2) is relatively steep, and in some cases it approaches the static adiabatic gradient (T∝r-1) as defined by Chamberlain (1961).
Assuming that the temperature profile can be fitted with c2=c02/ξβ above the heating peak, the estimated values of β for the C1 and C2 models are 0.7 and 0.9, respectively.
We note that the velocity in the C1 model exceeds the isentropic speed of sound (cγ=γkT/m, where γ=5/3) at 9.8Rp, where cγ=8.7kms-1.
This altitude is significantly higher than the altitude of the isothermal sonic point.
The velocity in the C2 model does not exceed the isentropic speed of sound below the upper boundary of 16Rp.
Thus the temperature profile has a significant impact on the nature of the solution and the escape mechanism.
This means that estimating the altitude of the sonic point without observations and detailed models for guidance is almost certain to produce misleading results.
Past models for the upper atmosphere of HD209458b have predicted a variety of altitudes for the sonic point.
On the other hand, Yelle (2004) pointed out that stellar heating in the thermosphere is mostly balanced by adiabatic cooling and our calculations confirm this.
Parker (1965) argued that the critical point stretches to infinity when β→1 i.e., as the temperature gradient is close to adiabatic.
Based on this, we should perhaps expect that the sonic point on close-in EGPs is located at a fairly high altitude.
This is confirmed by our hydrodynamic simulations.
In all of our models except for one, the sonic point is located significantly above 5Rp.
The exception is the C3 model, which includes the substellar tide.
The isentropic sonic point in this model is located at 3.9Rp, where cγ=8.2kms-1.
This is because the substellar tide leads to a lower effective value of the potential W.
However, the tidal potential depends on latitude and longitude, and the substellar results cannot be generalized globally.
Mass loss rates
Here we evaluate the mass loss rates based on our models.
We define the mass loss rate simply as:(13)Ṁ=4πr2ρv
We note that the solar spectrum that we used in this study contains the total flux of 4×10-3Wm-2 at wavelengths shorter than 912Å (the ionization limit of H) when normalized to 1AU.
This value is close to the average solar flux of 3.9×10-3Wm-2 at the same wavelengths (e.g, Ribas et al., 2005).
In order to simulate a global average, we divided the flux by a factor of 4 in the model.
This means that the incident flux on HD209458b at 0.047AU with wavelengths shorter than 912Å in our model is 0.45Wm-2.
The net heating efficiencies given in Table 2 are based on this value.
Considering first the models with constant ηnet ranging from 0.1 to 1 (see Section 3.1.1), the mass loss rate varies almost linearly with ηnet from 107kgs-1 and 108kgs-1 while the pressure averaged temperature below 3Rp changes only by 1500K.
This is because in a hydrodynamic model such as ours the net energy has nowhere else to go but adiabatic expansion and cooling, and thus escape is energy-limited.
The bulk of the energy is absorbed below 3Rp, and the mass loss rate is largely set by radiative transfer in this region.
The mass loss rate for HD209458b predicted by the C2 model is 4.1×107kgs-1 (ηnet=0.44).
The C3 model has the highest mass loss rate, although this rate is only higher by a factor of 1.13 than the mass loss rate in the C1 model.
Thus we predict a mass loss rate of 4-6×107kgs-1 from HD209458b based on the average solar flux at 0.047AU.
Garcia Munoz (2007) demonstrated that the mass loss rate is insensitive to the upper boundary conditions even when they have a large impact on the temperature and velocity profiles, particularly at high altitudes.
Indeed, complex hydrodynamic models are not required to calculate mass loss rates under energy-limited escape as long as reasonable estimates of the net heating efficiency are available.
It is also important to note that the current estimates of mass loss rates based on the observations (e.g., Vidal-Madjar et al., 2003) are not direct measurements.
Instead, they are all based on different models.
However, models that predict the same mass loss rate can predict different transit depths and models predicting different mass loss rates can match the observations equally well.
Thus the models should not be judged on how well they agree with published mass loss rates but rather on how well they agree with the observed density profiles or transit depths.
Hydrodynamic models with realistic heating rates are required to match the observations, and the mass loss rate then follows.
The globally averaged mass loss rate of about 4-6×107kgs-1 from HD209458b agrees well with similar estimates calculated by Yelle (2004, 2006) and Garcia Munoz (2007), respectively, but it is significantly larger than the value calculated by Murray-Clay et al. (2009).
These authors report a mass loss rate of 3.3×107kgs-1 based on the substellar atmosphere.
When multiplied by 1/4 this corresponds to a global average rate of about 8.3×106kgs-1.
However, the substellar mass loss rate is also enhanced by tides, so a comparable global average taking this into account would be even less than 8.3×106kgs-1, which is already roughly a factor of 6 smaller than our calculations.
The Murray-Clay et al. (2009) models differ in many respects from the models described here including the treatment of boundary conditions and radiative cooling, the numerical approach, and the adoption of a gray approximation for stellar energy deposition.
In order to explore the reason for the disagreement in escape rates, we have modified our model to implement the gray assumption by using the approach described in Murray-Clay et al. (2009) (see Section 3.2).
Specifically, we adopted an incident flux3
By chance the incident flux is equal to the mean solar flux divided by 4 that we used as a 'globally averaged' value.
Here, however, it is taken to be the substellar value.
3 of 0.45Wm-2 and a mean photon energy of 20eV.
The mass loss rate based on the substellar atmosphere for this model is 2.8×107kgs-1, in good agreement with the Murray-Clay et al. (2009) results.
Thus, the difference between the Murray-Clay et al. (2009) models and the others is due to the gray assumption, and the fact that they estimated the incident flux on HD209458b based on the solar flux integrated between 13eV and 40eV.
This energy range contains only about 25% of the total solar flux at energies higher than 13.6eV.
Although not discussed by Murray-Clay et al. (2009), the restricted energy range is likely an attempt to account for the fact that the absorption cross section decreases with energy implying that photons of sufficiently high energy will be absorbed too deep in the atmosphere to affect escape or the thermal structure, or composition of the thermosphere.
Whether this is true, however, depends on the composition and temperature of the atmosphere.
The gray assumption also fails to include the fact that the net heating efficiency increases with higher photon energy.
These difficulties highlight the basic problem with a gray model, that the results can only be accepted with confidence if verified by a more sophisticated calculation or direct observations.
Constraints from kinetic theory
Hydrodynamic models should be consistent with kinetic theory of rarefied media even if the modeled region is below the exobase.
This is because the atmosphere is escaping to space, and the density decreases with altitude, falling below the fluid regime at some altitude above the exobase.
Therefore the conditions in the exosphere affect the flow solutions even below the exobase.
Inappropriate use of the hydrodynamic equations can lead one to overestimate the flow velocity and mass loss rate, and these errors also affect the temperature and density profiles.
Thus it is important to demonstrate that the hydrodynamic solutions agree with constraints from kinetic theory (e.g., Volkov et al., 2011a,b).
As an example, we calculated Kn0 and X0 (see Section 2.1.2) based on the C1 and C2 models.
The Knudsen number Kn depends on the mean collision frequency, and it is much smaller than unity at all altitudes below 16Rp.
Thus the exobase is located above the model domain (see also Murray-Clay et al., 2009).
Calculating values for X0 is complicated by the broad stellar heating profile.
We consider the region where stellar heating is negligible to be where the flux of energy(14)E∞=FccpT+12v2-GMpr-κr2∂T∂ris approximately constant.
This criteria is consistent with the equations of motion, and it means that r0 that should be used to calculate X0 is above the upper boundary of our model because significant stellar heating persists at all altitudes.
Thus we evaluated values of X near the upper boundary for guidance.
We also calculated the values with both the mass of the proton (XH) and the mean atomic weight (X).
In the C1 model, XH decreases with altitude, and above 11.4Rp it has values of less than 3.
The mean atomic weight near the upper boundary is ∼0.6amu, and thus the general value of X<2 above 11.1Rp.
The sonic point in the C1 model is below 11Rp, and it is in a region where stellar heating is significant.
In the C2 model, both X and XH are greater than 3 at all altitudes below 16Rp.
In fact, X increases with altitude above 9Rp because the temperature gradient parameter exceeds unity.
Thus the values X in the C1 and C2 models are consistent with the difference in altitude between the sonic points in these models (see Section 3.1.3).
Indeed, our results show, in line with Parker's original ideas about the solar wind, that supersonic escape is possible if there is significant heating of the atmosphere over large distances above the temperature peak.
Such heating flattens the temperature gradient and brings the sonic point closer to the planet.
We note that there are some caveats to applying the simple criteria based on Kn0 and X0 to close-in extrasolar planets.
The upper atmospheres of these planets are strongly ionized, and the DSMC simulations of Volkov et al. (2011a,b) apply only to neutral atmospheres.
Partly due to ionization, the collision frequencies in the thermospheres of close-in planets are also high.
Further, the atmospheres are affected by a broad stellar heating profile in altitude whereas the DSMC calculations do not include any diabatic heating.
However, the results of Volkov et al. (2011a,b) also indicate that consistency with kinetic theory can be enforced approximately by applying the modified Jeans conditions to the hydrodynamic model at some altitude close to the exobase.
This result is likely to be more general, and it applies to ionized atmospheres as long as ambipolar diffusion is taken into account (see Section 2.1.2).
We compared the temperature and velocity profiles from the C1 and C2 models with results from similar models C5 and C6 that use the modified Jeans conditions.
Note again that our version of the modified Jeans conditions includes the polarization electrostatic field that is required in strongly ionized media.
Fig. 8 shows the temperature and velocity profiles from the models.
There is no difference between the C5 model and the C1 model as long as the upper boundary of the C5 model is at a sufficiently high altitude.
In this case we extended it to 36Rp.
When the upper boundary is placed at lower altitudes, the flow decelerates and the temperature increases near the upper boundary.
A comparison between the C2 and C6 models provides an example of the difference that can arise when the modified Jeans boundary conditions are used significantly below the exobase.
A better agreement is achieved if the upper boundary of the C6 model is placed at a slightly higher altitude.
In summary, we have shown that the C1 and C2 models are both consistent with kinetic theory.
We note that extending the models to 16Rp or higher is not necessarily justified because it ignores the complications arising from the possible influence of the stellar tide, the stellar wind, and interactions of the flow with the magnetic field of the planet or the star.
We placed the upper boundary at a relatively high altitude to make sure that the boundary is well above the region of interest, but generally we do not consider our results to be accurate above 3-5Rp.
Instead, our results provide robust lower boundary conditions for multidimensional models of the escaping material outside the Roche lobe of the planet.
Such models often cannot include detailed photochemical or thermal structure calculations.
The results from the more complex models can then be used to constrain the upper boundary conditions in 1D models.
This type of an iteration is a complex undertaking, and it will be pursued in future work.
Density profiles
In this section we provide a qualitative understanding of the density profiles and transition altitudes that affect the interpretation of the observations.
Based on the gas giants in the Solar System it might be expected that heavy species undergo diffusive separation in the thermosphere.
If this were the case, the transit depths in the O I, C II, and Si III lines (Vidal-Madjar et al., 2004; Linsky et al., 2010) should not be significantly higher than the transit depth at visible wavelengths.
It is therefore important to explain why diffusive separation does not take place in the thermosphere of HD209458b, and to clarify why H and O remain mostly neutral while C and Si are mostly ionized.
Also, doubly ionized species such as Si2+ are not common in planetary ionospheres, and their presence needs to be explained.
In order to do this, we modeled the ionization and photochemistry of the relevant species, and prove that diffusive separation does not take place.
In order to illustrate the results, Fig. 9 shows the density profiles of H, H+, He, He+, O, O+, C, C+, Si, Si+, and Si2+ from the C2 model.
The location of the H/H+ transition obviously depends on photochemistry, but it also depends on the dynamics of escape.
With a fixed pressure at the lower boundary, a faster velocity leads to a transition at a higher altitude.
Thus the transition occurs near 3.1Rp in the C2 model whereas in the C1 and C3 models it occurs at 3.8Rp and 5Rp, respectively.
These results disagree with Yelle (2004) and Murray-Clay et al. (2009) who predicted a lower transition altitude, but they agree qualitatively with the solar composition model of Garcia Munoz (2007).
They also agree with the empirical constraints derived by Koskinen et al. (2010a) from the observations.
Once again, the differences between the earlier models and our work arise from different boundary conditions, and assumptions regarding heating rates and photochemistry.
We demonstrate this by reproducing the results of Murray-Clay et al. (2009) with our model.
In order to do so, we set the lower boundary to 30nbar with a temperature of 1000K, and included the substellar tide in the equations of motion.
We only included H, H+, and electrons in the model, and used the recombination rate coefficient and Lyman α cooling rate from Murray-Clay et al. (2009).
We also calculated the heating and ionization rates with the gray approximation by assuming a single photon energy of 20eV for the stellar flux of 0.45Wm-2 at the orbital position of HD209458b.
Fig. 10 shows the density profiles of H and H+ based on this model (hereafter, the MC09 model).
The H/H+ transition in the MC09 model occurs near 1.4Rp.
If we replace the gray approximation with the full solar spectrum in this model, the H/H+ transition moves higher to 2-3Rp.
This is because photons with different energies penetrate to different depths in the atmosphere, extending the heating profile in altitude around the heating peak.
This is why the temperature at the 30nbar level in the C2 model is 3800K and not 1000K.
In order to test the effect of higher temperatures in the lower thermosphere, we extended the MC09 model to p0=1μbar (with T0=1300K) and again used the full solar spectrum for heating and ionization.
With these conditions, the H/H+ transition moves up to 3.4Rp, in agreement with the C2 model.
We conclude that the unrealistic boundary conditions and the gray approximation adopted by Murray-Clay et al. (2009) and Guo (2011) lead to an underestimated overall density of H and an overestimated ion fraction.
Thus their density profiles yield a H Lyman α transit depth of the order of 2-3% i.e., not significantly higher than the visible transit depth.
We note that Yelle (2004) also predicted a relatively low altitude of 1.7Rp for the H/H+ transition - despite the fact that his model does not rely on the gray approximation and the lower boundary is in the deep atmosphere.
The reason for the low altitude of the H/H+ transition in this case is the neglect of heavy elements.
In the absence of heavy elements, H3+ forms near the base of the model and subsequent infrared cooling balances the EUV heating rates.
This prevents the dissociation of H2 below the 10nbar level.
In reality, reactions with OH and thermal decomposition dissociate H2 near the 1μbar level (see Section 2.1.1) and cooling by H3+ is negligible at all altitudes.
It should be noted that even if H2 does not initially dissociate, H3+ can be removed from the lower thermosphere in reactions with carbon and oxygen species (e.g., Garcia Munoz, 2007) unless these species undergo diffusive separation.
The subsequent lack of radiative cooling will then dissociate H2 again near the 1μbar level.
In our models, charge exchange with oxygen (reactions R14 and R15 in Table 1) dominates the photochemistry of H below 3Rp and charge exchange with silicon (R25, R26) is also important below 1.4Rp.
These reactions are secondary in a sense that they require the ions to be produced by some other mechanism.
In fact, H+ is mainly produced by photoionization (P1), although thermal ionization (R3) is also important near the temperature peak.
The production rates are mainly balanced by loss to radiative recombination (R1).
The net chemical loss timescale for H is longer than the timescale for advection above 1.7Rp.
This allows advection from below to replenish H at higher altitudes.
The density profiles of O and O+ are strongly coupled to H and H+ by charge exchange (see also Garcia Munoz, 2007).
As a result, the O/O+ transition occurs generally near the H/H+ transition.
For instance, in the C2 model it is located near 3.4Rp.
The same is not true of carbon.
The C/C+ transition occurs at a much lower altitude than the H/H+ and O/O+ transitions.
For instance, in the C2 model it is located near 1.2Rp.
C+ is mainly produced by photoionization (P4), although thermal ionization (R8) and charge exchange with He+ (R13) are also important near the temperature peak.
The production is balanced by loss to radiative recombination (R10).
The chemical loss timescale for C is shorter than the timescale for advection below 1.8Rp.
Thus advection is unable to move the C/C+ transition to altitudes higher than 1.2Rp.
Silicon is almost fully ionized near the lower boundary of the model.
Much of the Si+ below 4Rp is produced by charge exchange of Si with H+, He+, and C+ (R22, R23, R24).
The low ionization potential of Si (8.2eV) means that Si+ can also be produced by thermal ionization (R18), and photoionization (P6) by stellar FUV radiation and X-rays that propagate past the EUV heating peak.
Above 4Rp, Si+ is mostly produced by photoionization.
Linsky et al. (2010) suggested that the balance of Si+ and Si2+ depends on charge exchange with H+ and H, respectively, and our results confirm this.
However, the location of the Si+/Si2+ transition also depends on the dynamics.
For instance, in the C2 model it occurs near 5.8Rp while in the C1 model it occurs near 8.5Rp.
Thus slow outflow and high temperatures favor Si2+ as the dominant silicon species as long as the flux constant is high enough to overcome diffusive separation (Paper II).
We have now explained the presence of the atoms and ions that have been detected in the thermosphere of HD209458b.
Due to advection and charge exchange, H and O are predominantly neutral up to about 3Rp and give rise to the observed transit depths in the H Lyman α and O I lines.
Carbon, on the other hand, is ionized at a low altitude and thus C+ is also detectable in the upper atmosphere.
Si+ is the dominant silicon species below 5Rp, but charge exchange with H ensures that there is also a significant abundance of Si2+ in the atmosphere.
We note that these conclusions are only valid if the heavier species are carried along to high altitudes by the escaping hydrogen.
We show that this is the case below in Section 3.2.2.
The EUV ionization peak (EIP) layer
Koskinen et al. (2010b) explored the properties of the ionospheres of EGPs at different orbital distances from a Sun-like host star by using a hydrostatic general circulation model (GCM) that also includes realistic heating rates, photochemistry, and transport of constituents.
They predicted that the EIP layer on HD209458b is centered at 1.35Rp where the electron density is ne=6.4×1013m-3 and xe∼3×10-2.
In the C2 model, the EIP layer is centered at 1.3Rp (p=2nbar) where ne=4.4×1013m-3 and xe=3.7×10-2.
The vertical outflow velocity at 1.3Rp is 90ms-1.
Thus the results of Koskinen et al. (2010b) were not significantly affected by the simplifying assumptions of the GCM.
This means that hydrostatic GCMs can be extended to relatively low pressures as long as the escape rates are incorporated as boundary conditions.
We also calculated the plasma frequency based on the electron densities in the C2 model.
This constrains the propagation of possible radio emissions from the ionosphere.
The ordinary plasma frequency ωp/2π exceeds 12MHz at all altitudes below 5Rp and reaches a maximum of almost 64MHz in the EIP layer.
This presents a limitation on the detection of radio emissions from the ionospheres of close-in EGPs.
Any emissions that originate from the ionosphere at 1-5Rp and have frequencies lower than 10-70MHz can be screened out by the ionosphere itself.
We note that a detection of radio emissions from an EGP has not yet been achieved (e.g., Bastian et al., 2000; Lazio and Farrell, 2007; Lecavelier des Etangs et al., 2011; Grieß meier et al., 2011).
Such a detection would be an important constraint on the magnetic field strength and the ionization state of the source region (e.g., Grieß meier et al., 2007).
Models of the ionosphere are required to predict radio emissions from the possible targets.
The escape of heavy atoms and ions
In this section we verify a posteriori that the velocity and temperature differences between different species in the thermosphere of HD209458b are small.
This demonstrates that the single fluid approximation of the momentum and energy equations is valid, and that diffusive separation of the heavy species does not take place.
Our model includes velocity differences between different species by including all of the relevant collisions between them through the inclusion of diffusive fluxes in the continuity equations.
However, we have explicitly assumed that Tn=Ti=Te, and this assumption in particular needs to be verified.
Also, the diffusion approach to the continuity equation is only valid if the velocity differences between the species are reasonably small.
We calculated the collision frequencies based on the C2 model, and found that collisions with neutral H dominate the transport of heavy neutral atoms such as O below 3.5Rp.
At altitudes higher than this, collisions with H+ are more frequent.
In Paper II we demonstrate that a mass loss rate of 6×106kgs-1 is required to prevent diffusive separation of O (the heaviest neutral species detected so far) in the thermosphere.
The mass loss rate in our models is Ṁ>107kgs-1 and thus O is dragged along to high altitudes by H.
On the other hand, collisions with H+ dominate the transport of heavy ions such as Si+ as long as the ratio [H+]/[H]≳10-4 (Paper II).
This explains why Coulomb collisions in our models are more frequent than heavy ion-H collisions at almost all altitudes apart from the immediate vicinity of the lower boundary.
These collisions are much more efficient in preventing diffusive separation than collisions with neutral H.
Fig. 11 compares the timescale for diffusion τD for O and Si+ with the timescale for advection τv based on the C2 model.
In both cases, τD≫τv and thus diffusion is not significant.
This implies that there are no significant velocity differences between heavy atoms and hydrogen.
We note that Coulomb collisions of doubly ionized species with H+ are more frequent than collisions between a singly ionized species and H+.
Thus diffusion is even less significant for a species like Si2+.
We verified these results from our simulations by switching diffusion off in the model and rerunning the C2 model.
As a result the density of the heavy atoms and ions increased slightly at high altitudes, but the differences are not significant - the results were nearly identical to the density profiles of the original simulation.
We note that the atmosphere can also be mixed by vertical motions associated with circulation that are sometimes parameterized in one-dimensional models by the eddy diffusion coefficient Kzz (e.g., Moses et al., 2011).
This mechanism is efficient in bringing the heavy elements to the lower thermosphere but it is unlikely to mix the atmosphere up to 3Rp and beyond.
Also, there is no generally accepted method of estimating the degree of global mixing based on circulation models, and most circulation models for EGPs do not adequately treat the relevant energy deposition and forcing mechanisms in the upper atmosphere.
Thus there is considerable uncertainty over the values of Kzz and rapid escape is a much more likely explanation for the lack of diffusive separation on HD209458b.
In fact, the calculations of Koskinen et al. (2007b) show that the temperature in the thermosphere of planets such as HD209458b is high enough to practically guarantee an effective escape rate.
The only way to prevent this is to provide enough radiative cooling to offset most of the XUV flux, but there are no radiative cooling mechanisms efficient enough to achieve this in a thermosphere composed of atoms and ions.
As we noted above, the temperatures of the electrons, ions, and neutrals are roughly equal in the thermosphere of HD209458b.
In order to show this, we assumed that photoelectrons share their energy with thermal electrons, which then share this energy further with ions and neutrals.
We also assumed that the collisions frequencies between the species are higher than the timescale for advection.
If the velocity differences between the species are negligible, the steady state 5-moment energy equations for thermal electrons and ions (Schunk and Nagy, 2000) can be used to obtain the following approximations4
Note that conduction and viscosity are not important in the thermosphere of HD209458b.
4:(15)Te-Ti≈13mimeqRkneνei(16)Ti-Tn≈13mi+mnmiqRkniνinwhere qR is the volume heating rate, and νei and νin are the electron-ion and ion-neutral momentum transfer collision frequencies, respectively.
We calculated the temperature differences for H, H+, and electrons based on the C2 model.
The difference between the electron and ion temperatures decreases with altitude and is mostly less than 2K.
The difference between the ion and neutral temperatures, on the other hand, increases with altitude.
The ion temperature is approximately 10K higher than the neutral temperature near 5Rp and the difference reaches 150K at 16Rp.
In both cases, the temperature differences are negligible compared to the temperature of the thermosphere.
Further, the timescale for advection in the C2 model is always significantly longer than the relevant collision timescales.
Thus we have shown that Te≈Ti≈Tn and that Eqs.
(15) and (16) are approximately valid.
Discussion and conclusions
We have constructed a new model for the upper atmosphere of HD209458b in order to explain the detections of H, O, C+, and Si2+ at high altitudes around the planet (Vidal-Madjar et al., 2003, 2004; Linsky et al., 2010).
There are many different interpretations of the observed transits in the H Lyman α line (Vidal-Madjar et al., 2003; Ben-Jaffel, 2007, 2008; Holström et al., 2008; Koskinen et al., 2010a), and these interpretations rely on results from models of the upper atmosphere that are based on many uncertain assumptions (see Section 3.1.1 and Koskinen et al. (2010a) for a review).
Also, the detection of heavy atoms and ions in the thermosphere is not without controversy, and the detection of Si2+ is particularly intriguing.
Thus these observations present several interesting challenges to modelers.
The observed transit depths are large, and substantial abundances of the relevant species are required to explain the observations.
However, on every planet in the Solar System heavier species are removed from the thermosphere by molecular diffusion and doubly ionized species are not commonly observed.
Also, the observations imply that H and O remain mostly neutral in the thermosphere while C and Si are mostly ionized at a relatively low altitude.
Hydrodynamic models coupled with chemistry and thermal structure calculations are required to explain the detection of these species in the upper atmosphere and the differences between their density profiles.
Ours is the first such model that benefits from repeated detections of both neutral atoms and ions to constrain the composition and temperature.
Koskinen et al. (2010a) demonstrated that the H Lyman α transit observations (Ben-Jaffel, 2007, 2008) can be explained with absorption by H in the thermosphere if the base of the hot layer of H is near 1μbar, the mean temperature within the layer is about 8250K, and the atmosphere is mostly ionized above 3Rp.
These parameters are based on fitting the data with a simple empirical model of the upper atmosphere.
The density and temperature profiles from our new hydrodynamic model agree qualitatively with these constraints, demonstrating that the basic assumptions of Koskinen et al. (2010a) are reasonable.
This confirms once again that a comet-like tail (Vidal-Madjar et al., 2003) or energetic neutral atoms (Holström et al., 2008) are not necessarily required to explain the H Lyman α observations.
In line with recent results by Moses et al. (2011) and the empirical constraints mentioned above, we used a photochemical model of the lower atmosphere to show that H2 dissociates near the 1μbar level.
Above this level, the lack of efficient radiative cooling and strong stellar EUV heating lead to high temperatures.
We constrained the range of possible mean (pressure averaged) temperatures based on the average solar flux by using the hydrodynamic model to calculate temperatures with different heating efficiencies.
For net heating efficiencies between 0.1 and 1, the mean temperature below 3Rp varies from 6000K to 8000K.
This means that 8000K is a relatively strict upper limit on the mean temperature if the XUV flux of HD209458 is similar to the corresponding flux of the Sun.
A mean temperature of 8250K estimated from the observations implies the presence of an additional non-radiative heat source, or that the XUV flux from HD209458 is higher than the average solar flux.
Given that our best estimate of the net heating efficiency is 0.44 (see Section 3.1.2), the XUV flux of H209458 would have to be 5-10 times higher than the average solar flux to cause a mean temperature of 8250K (see Section 3.1.1).
If the mean XUV flux of HD209458 is generally higher than the solar flux and the observations took place during stellar maximum, such an enhancement is not impossible.
This would also lead to higher outflow velocity and mass loss rate.
However, the uncertainty in the observed transit depths is also large (Ben-Jaffel, 2008, 2010), and it can accommodate a range of temperatures.
Therefore our reference model C2 with a mean temperature of 7200K also agrees qualitatively with the empirical constraints.
In this respect, it is interesting to note that with 100× solar flux, the mean temperature is still only 9800K.
Temperatures significantly higher than 8000K (e.g., Ben-Jaffel and Hosseini, 2010) therefore imply a strong non-radiative heat source.
In contrast to the mean temperature, the velocity and details of the temperature profile depend strongly on the heating efficiency and stellar flux (see Section 3.1.1).
They are also sensitive to the upper and lower boundary conditions.
This explains the large range of temperature and density profiles predicted by earlier models that arise from different boundary conditions and assumptions about the stellar flux, radiative transfer, and heating efficiencies.
The differences highlight the need for accurate thermal structure calculations that are constrained by the available observations.
These calculations are important because the density profiles of the detected species depend on the temperature and velocity profiles, and inappropriate assumptions made by the models can bias the interpretation of the observations.
In the absence of stellar gravity, the location of the sonic point and the outflow speed also depend on the heating efficiency.
As the heating efficiency increases from 0.1 (in models with the average solar flux), the high altitude temperature increases and the sonic point moves to lower altitudes, reaching down to 4Rp with a net heating efficiency of 1.
We found that supersonic solutions are possible as long as there is significant heating over a large altitude range above the temperature peak.
This conclusion is supported both by the hydrodynamic model and new constraints from kinetic theory (Volkov et al., 2011a,b).
However, the isentropic sonic point of the C2 model is located above the model domain.
In principle, this is an interesting result but it should be treated with caution.
We used parameterized heating efficiencies for low energy photons, and the location of the sonic point is very sensitive to the temperature profile.
Also, the stellar tide can enhance the escape rates at the substellar and antistellar points.
We did not include this effect because it may produce horizontal flows that cannot be modeled in 1D.
As long as the upper boundary is at a sufficiently high altitude, we found that the results based on the outflow boundary conditions and modified Jeans conditions are identical (see Section 3.1.5).
This shows that our simulations are roughly consistent with kinetic theory.
An agreement between these two types of boundary conditions on HD209458b is an interesting theoretical result.
It shows that the boundary conditions for hydrodynamic escape are appropriate in this case.
However, an upper boundary at 16Rp or higher is not necessarily justified for other reasons because we did not consider the effect of the possible planetary magnetic field, interaction of the atmosphere with the stellar wind, or horizontal transport (e.g., Stone and Proga, 2009; Trammell et al., 2011).
We chose an upper boundary at a high altitude in order to preserve the integrity of the solution in our region of interest below 5Rp.
The purpose of this work is to model energy deposition and photochemistry in this region.
These aspects are often simplified in more complex models to a degree that it may be difficult to separate the effect of multiple dimensions and other complications from differences arising simply because of different assumptions about heating efficiencies and chemistry.
Also, the uncertainty in the observations does not necessarily justify the introduction of more free parameters to the problem until the basic properties of the thermosphere are better understood.
However, technically we do not consider our solutions to be accurate far above 3-5Rp.
Instead, our results provide robust lower boundary conditions for more complex multidimensional models that characterize the atmosphere outside the Roche lobe of the planet.
Results from such models can then be used to constrain the upper boundary conditions of the 1D models further.
In order to model the density profiles of the detected species in the ionosphere, we assumed solar abundances of the heavy elements (Lodders, 2003), although this assumption can be adjusted as required to explain the observations (Paper II).
As we already stated we found that H2, H2O, and CO dissociate above the 1μbar level, releasing H, O, and C to the thermosphere (see also Moses et al., 2011).
We note that the detection of Si2+ in the upper atmosphere implies that silicon does not condense into clouds of forsterite and enstatite in the lower atmosphere as argued by e.g., Visscher et al. (2010).
The dominant Si species is then SiO, which dissociates at a similar pressure level as the other molecules.
In fact, practically all molecules dissociate below 0.1μbar.
This leads to an important simplification in hydrodynamic models of the thermosphere.
The complex chemistry of molecular ions does not need to be included as long as the lower boundary is above the dissociation level.
We found that the H/H+ transition occurs near 3Rp or, depending on the velocity profile, at even higher altitudes.
The O/O+ transition is coupled to the H/H+ transition through charge exchange reactions.
Thus both H and O are mostly neutral up to the boundary of the Roche lobe at 3-5Rp.
In contrast, C is ionized near the 1μbar level and C+ is the dominant carbon species in the thermosphere.
Si is also ionized near the 1μbar level, and the balance between Si+ and Si2+ is determined by charge exchange with H+ and H, respectively.
Si+ is the dominant silicon ion below 5Rp but the abundance of Si2+ is also significant.
We found that neutral heavy atoms are dragged to the thermosphere by the escaping H, while heavy ions are transported efficiently by the escaping H+.
Thus the advection timescale is much shorter than the diffusion timescale of the detected species, and diffusive separation does not take place in the thermosphere.
We also verified that the neutral, ion, and electron temperatures are roughly equal.
Taken together, these results imply that the thermospheres of close-in EGPs can differ fundamentally from the gas giant planets in the Solar System.
For instance, the thermosphere of HD209458b is composed mainly of atoms and atomic ions, and diffusive separation of the common heavy species is prevented by the escape of H and H+.
It is important to note, however, that results such as these cannot be freely generalized to other extrasolar planets.
As in the Solar System, each planet should be studied separately.
For instance, the dissociation of molecules depends on the temperature profile that is shaped by the composition through radiative cooling and stellar heating.
The mass loss rate and escape velocity, that determine whether diffusive separation takes place or not, depends on the escape mechanism that again depends on the temperature and composition of the upper atmosphere.
The results from different models can only be verified by observations that are therefore required for multiple planets if we are to characterize escape in different systems and under different conditions.
Acknowledgments
We are grateful to A.
Volkov for reading the manuscript and providing useful feedback.
We thank H.
Menager, M. Barthelemy, J.-M.
Grießmeier, N.
Lewis, D.
S.
Snowden, and C.
Cecchi-Pestellini for useful discussions and correspondence.
We also acknowledge the "Modeling atmospheric escape" workshop at the University of Virginia and the International Space Science Institute (ISSI) workshop organized by the team "Characterizing stellar and exoplanetary environments" for interesting discussions and an opportunity to present our work.
The calculations for this paper relied on the High Performance Astrophysics Simulator (HiPAS) at the University of Arizona, and the University College London Legion High Performance Computing Facility, which is part of the DiRAC Facility jointly funded by STFC and the Large Facilities Capital Fund of BIS.
SOLAR2000 Professional Grade V2.28 irradiances are provided by Space Environment Technologies.

1. A method of making a lithium-rich positive electrode material precursor, comprising:
(1) mixing manganese salt and M salt to obtain a first mixed material, wherein M is a metal element;
(2) carrying out vacuum drying on the first mixed material at the temperature of 60-90 ℃ for 6-12 h, and then carrying out fine grinding treatment to obtain a second mixed material;
(3) calcining the second mixed material to obtain a lithium-rich anode material precursor, wherein the calcining treatment is completed at 300-900 ℃ for 0.5-48 h, and the heating rate in the calcining treatment is 0.1-20 ℃/min; the calcination treatment is carried out in an oxidizing atmosphere, and the concentration of oxygen in the oxidizing atmosphere is 0.01-100 v%;
the lithium-rich cathode material precursor has the composition shown in the formula (I),
Mx(Mn2+aMn3+bMn4+c)1-xOn(I)
in the formula (I), x is more than or equal to 0.01 and less than or equal to 0.5, a is more than or equal to 10 and less than or equal to 40 percent, b is more than or equal to 20 and less than or equal to 50 percent, c is more than or equal to 14 and less than or equal to 40 percent, and n is the number of oxygen atoms required by the valence of other elements.2. The method of claim 1, wherein M is selected from at least one of magnesium, strontium, aluminum, iron, cobalt, nickel, copper, zinc, scandium, titanium, vanadium, chromium, niobium, yttrium, ruthenium, and rhodium.3. The method of claim 1, wherein the manganese salt and the M salt are each independently a chloride, carbonate, acetate, nitrate, or sulfate.4. The method of claim 3, wherein the M salt is a cobalt salt and a nickel salt; in the step (1), the manganese salt, the cobalt salt and the nickel salt are mixed according to a molar ratio of (30-90) to (1-30).5. A lithium-rich cathode material precursor, characterized in that the lithium-rich cathode material precursor is prepared by the method of any one of claims 1 to 4.6. A method of making a lithium-rich positive electrode material, comprising:
(1) preparing a lithium-rich cathode material precursor according to the method of any one of claims 1 to 4;
(2) and mixing the lithium-rich positive electrode material precursor with a lithium source and roasting to obtain the lithium-rich positive electrode material.7. A lithium-rich positive electrode material prepared by the method of claim 6.
What is claimed is:
1. A positive active material for a lithium secondary battery, the positive active material comprising:
a core part and a shell part that both comprise a nickel-based composite oxide represented by Formula 1 below:
<in-line-formulae>Li<sub>a</sub>[Ni<sub>x</sub>Co<sub>y</sub>Mn<sub>z</sub>]O<sub>2 </sub>  <Formula 1></in-line-formulae>
where 0.8≦a≦1.2, 0.05≦x≦0.9, 0.1≦y≦0.8, 0.1≦z≦0.8, and x+y+z=1,
wherein a content of nickel in the core part is larger than that in the shell part, and the core part comprises needle-like particles.2. The positive active material of claim 1, wherein the core part or a core part precursor has open pores.3. The positive active material of claim 1, wherein the core part or a core part precursor has a curved surface structure.4. The positive active material of claim 1, wherein the shell part is formed such that the shell part permeates into open pores of the core part.5. The positive active material of claim 1, wherein an amount of the core part is in a range of about 40 to about 90 parts by weight based on 100 parts by weight of the positive active material, and an amount of the shell part is in a range of about 10 to about 60 parts by weight based on 100 parts by weight of the positive active material.6. The positive active material of claim 1, wherein the core part comprises LiNi0.6CO0.2Mn0.2O2, and the shell part comprises LiNi0.33Co0.33Mn0.33O2.7. The positive active material of claim 1, wherein a content of nickel in the core part is in a range of about 50 to about 90 mole %, and a content of nickel in the shell part is in a range of about 5 to about 49 mole %.8. The positive active material of claim 1, wherein a diameter of the core part is in a range of about 1 to about 10 μm, and a diameter of the shell part is in a range of about 5 to about 10 μm.9. A method of preparing a positive active material for a lithium secondary battery, the method comprising:
a first process for mixing a first precursor solution comprising a nickel salt, a cobalt salt, and a manganese salt and a first base to prepare a first mixture and inducing a reaction in the first mixture to obtain a precipitate;
a second process for adding to the precipitate a second precursor solution comprising a nickel salt, a cobalt salt, and a manganese salt and a second base to obtain a second mixture and inducing a reaction in the second mixture to obtain a composite metal hydroxide; and
mixing the composite metal hydroxide with a lithium salt and heat treating the mixed composite metal hydroxide to prepare the positive active material according to claim 1,
wherein a content of nickel in the first precursor solution is adjusted to be larger than that in the second precursor solution.10. The method of claim 9, wherein a reaction time of the second mixture in the second process is adjusted to be longer than that of the first mixture in the first process.11. The method of claim 9, wherein the precipitate of the first process has a porous structure with open pores.12. The method of claim 9, wherein a reaction time of the first mixture in the first process is in a range of about 5 to about 7 hours, and a reaction time of the second mixture in the second process is in a range of about 8 to about 10 hours13. The method of claim 9, wherein an amount of the nickel salt in the first process is in a range of about 1 to about 1.2 moles based on 1 mole of the cobalt salt.14. The method of claim 9, wherein a pH of the first mixture in the first process is adjusted to be in a range of about 10 to about 11, and a pH of the second mixture in the second process is adjusted to be in a range of about 11.5 to about 12.0.15. A lithium secondary battery comprising the positive active material according to claim 1.16. A positive active material precursor for a lithium secondary battery, the positive active material precursor comprising:
a nickel-based composite hydroxide represented by Formula 2 below and needle-like particles:
<in-line-formulae>Ni<sub>x</sub>Co<sub>y</sub>Mn<sub>z</sub>OH   <Formula 2></in-line-formulae>
wherein 0.05≦x≦0.9, 0.1≦y≦0.8, 0.1≦z≦0.8, and x+y+z=1.17. The positive active material precursor of claim 16, wherein the positive active material precursor has open pores and a curved surface structure.1. The lithium ion battery for manufacturing can be used as an active anode material of lithium metal (M) oxide powder of the carbonate precursor compound, M comprises 20 to 90 µM % Ni, 10 to 70 µM % Mn and a 10 to 40 µM % Co, precursor further comprises sodium and sulfur impurities, wherein the molar ratio of sodium to sulfur (Na/S) was 0.4<Na/S<2.2. Carbonate precursor compound according to claim 1, having the general formula MCO3, wherein M=NixMnyCozAv, Aas a dopant, wherein 0.20 ≤ x ≤ 0.90,0.10 ≤ y ≤ 0.67, and a 0.10 ≤ z ≤ 0.40, v ≤ 0.05, and the x + y + z + v=1.3. Carbonate precursor compound according to claim 2, wherein A is a Mg,Al,Ti,Zr,Ca,Ce,Cr,Nb,Sn,Zn B according to any one or more of the well.4. The lithium ion battery for manufacturing can be used as an active anode material of lithium metal (M) oxide powder of the carbonate precursor compound, having the general formula MCO<sub>3</sub>, wherein M=Ni<sub>x</sub> Mn<sub>y</sub> Co<sub>z</sub> A<sub>v</sub>, Aas a dopant, wherein 0.30,0.55 ≤ y ≤ 0.80 0.10 ≤ x<, and the 0<z ≤ 0.30, v ≤ 0.05, and the x + y + z + v=1, further comprising sodium and sulfur impurities precursor, wherein the molar ratio of sodium to sulfur (Na/S) was 0.4<Na/S<2.5. Carbonate precursor compound according to claim 1, wherein % by weight sodium expressed by (Naweight) and sulfur (Sweight) sum of the contents (2 * Naweight) +Sweightmore than 0.4% and less than 1.6% by weight by weight.6. Carbonate precursor compound according to claim 5, wherein 0.7% by weight sodium content of between 0.1 and a between, 0.9% by weight sulfur content of between 0.2 and the sum between.7. For a rechargeable battery positive electrode material of a lithium metal oxide powder, having the general formula Li<sub>1+a</sub> M<sub>1-a</sub> O<sub>2</sub>, wherein M=Ni<sub>x</sub> Mn<sub>y</sub> Co<sub>z</sub> A<sub>v</sub>, Aas a dopant, wherein-0.05 ≤ a ≤ 0.25,0.20 ≤ x ≤ 0.90,0.10 ≤ y ≤ 0.67, and a 0.10 ≤ z ≤ 0.40, v ≤ 0.05, and the x + y + z + v=1, powder having a particle size distribution 10μm ≤ D50 ≤ 20 µm, 0.9 ≤ BET ≤ 5 has a specific surface, with BET m<sup>2</sup> /grepresented, sodium and sulfur powder also includes impurities, wherein % by weight sodium expressed by (Na<sub>weight</sub>) and sulfur (S<sub>weight</sub>) sum of the contents (2 * Na<sub>weight</sub>) +S<sub>weight</sub> more than 0.4% and less than 1.6% by weight of the weight, and wherein the molar ratio of sodium to sulfur (Na/S) was 0.4<Na/S<2.8. Lithium metal oxide powder according to claim 7, which includes a secondary LiNaSO4phase.9. Lithium metal oxide powder according to claim 7, wherein the secondary LiNaSO40.5% by weight of at least the relative weights of phase, such as by the powder XRD pattern of of the ritterwell (Rietveld) analyzing the measured.10. Lithium metal oxide powder according to claim 7, wherein:
0.4<Na/S<1, and the powder further comprises Na<sub>2</sub> SO<sub>4</sub> ;or
1<Na/S<2, and the powder further comprises Li<sub>2</sub> SO<sub>4</sub>.11. Lithium metal oxide powder according to claim 7, wherein A is a Mg,Al,Ti,Zr,Ca,Ce,Cr,Nb,Sn,Zn B according to any one or more well.12. For a rechargeable battery positive electrode material of lithium metal oxide powder, having the general formula Li<sub>1+a</sub> M<sub>1-a</sub> O<sub>2</sub>, wherein M=Ni<sub>x</sub> Mn<sub>y</sub> Co<sub>z</sub> A<sub>v</sub>, Aas a dopant, wherein<0.30,0.55 ≤ y ≤ 0.80 0.10 ≤ a ≤ 0.25,0.10 ≤ x, and the 0<z ≤ 0.30, v ≤ 0.05, and the x + y + z + v=1, powder having a particle size distribution 10μm ≤ D50 ≤ 20 µm, 0.9 ≤ BET ≤ 5 has a specific surface, with BET m<sup>2</sup> /grepresented, also include sodium and sulfur impurities powder, wherein sodium % by weight represented by (Na<sub>weight</sub>) and sulfur (S<sub>weight</sub>) sum of the contents (2 * Na<sub>weight</sub>) +S<sub>weight</sub> exceeds 0.4 wt % and less than 1.6% by weight, and wherein the molar ratio of sodium to sulfur (Na/S) was 0.4<Na/S<2.13. A method for preparing a carbonate precursor compounds of the method according to claim 2, comprising the steps of:
-Ni ions is provided which comprises, Mn ions, Co ions and A source of the feed solution, wherein Ni ions, Mn ions, Co ions and water soluble sulfate ion is present in the compound A,
-Na ion of the ionic solution comprising carbonate solution and,
-M'is provided which comprises a slurry containing seed crystals of ions, wherein M'=Nix'Mny'Coz'A'n', A'as a dopant, and the 0 ≤ x' ≤ 1,0 ≤ y'≤ 1,0 ≤ z' ≤ 1,0 ≤ n'≤ 1 and the x' + y '+ z' + n '=1,
-mixing in a reactor feed solution, ionic solution with the slurry, thereby obtaining a reactive liquid mixture,
-carbonate precipitation in the reaction mixture of liquids to the seed crystal, so as to obtain a reaction liquid mixture with a carbonate precursor, and
-liquid mixture is separated from the reaction of the carbonate precursor.14. Method according to claim 13, wherein M'ions present in the water-insoluble compounds, water-insoluble compound is M'CO3, M'(OH)2, M'as claimed in any one of oxides and M'OOH.15. Method according to claim 13, wherein the metal content in seed slurry to a molar ratio of metal content in the feed solution (M'seed/Mfeed) between 0.001 and a 0.1 between, and wherein the carbonate precursor by the median particle size of M'seed/Mfeeddetermined by the ratio.16. Method according to claim 13, wherein A and the A ' to B according to any one or more Mg,Al,Ti,Zr,Ca,Ce,Cr,Nb,Sn,Zn as well.17. Method according to claim 13, wherein the reactor in the NH3concentration is less than 5.0 g/L.18. Method according to claim 13, wherein M=M '.19. Method according to claim 13, further comprising hydroxide solution where the ionic solution in the solution and the bicarbonate either or both, and the OH/CO3than, or OH/HCO3than, or less than 1/10 the ratio of the two.20. Method according to claim 13, wherein the seeds have a D50 mean particle size of 3 μm between 0.1 and an.MnO2 nanoflakes/hierarchical porous carbon nanocomposites for high-performance supercapacitor electrodes
The overall synthetic procedure of the MnO2 nanoflakes/HPCs composites was outlined in Fig. 1. Hollow mesoporous silica spheres were used as template and phenolic resin was employed as carbon source. The uncalcined hollow mesoporous silica spheres were synthesized according to a previous report [26]. Briefly, 0.1 g of the hollow mesoporous silica was impregnated with 0.5 g of 20 wt% ethanol solution of phenolic resin and stirred for 24 h to form a uniform mixture. Then the mixture was evaporated at room temperature for 5 h, followed by heating in an oven at 100 degC for 24 h. The obtained yellow powder was carbonized at 800 degC for 4 h in argon atmosphere. Finally, the resulting samples were washed with deionized water and ethanol and dried at 60 degC in a vacuum oven for 24 h to obtain SiO2/C composite.

After that, 0.1 g of SiO2/C composite and 0.1 g of P123 (Aldrich, EO20PO70EO20) were ultrasonically dispersed in 30 mL deionized water in a 100 mL flask. 1 mL of 0.05 M KMnO4 aqueous solution was added and after stirring for 10 min in an ice bath, then 10.17 M Mn(NO3)2 solution with different volume (42, 200 and 576 uL, respectively) were mixed into the above suspension, and correspondingly, 0.05 M KMnO4 solution with various volume (3, 14 and 40 mL) was added dropwise. The precipitates were collected by filtration and rinsed with deionized water and ethanol for several times. The silica template was removed by 2 M NaOH solution at 40 degC for 24 h, and then the samples were washed with deionized water and ethanol, followed by drying at 60 degC for 12 h. The content of MnO2 in the three MnO2/HPCs composites was estimated to be 40%, 75% and 90% by TG test, respectively. And the composites were then denoted as MnO2/HPCs-40, MnO2/HPCs-75, MnO2/HPCs-90, respectively.1. A heterogeneous metal support material, which comprises a host material and a particulate dopant material, the body material is comprised of the secondary particles comprise primary particles are aggregated into,
Characterized in, particulate dopant material is uniformly distributed in a host material of the secondary particles.3. A multi-phase metal support material according to claim 2, wherein the multi-phase of a metallic support material has the general formula: (the dopant material would)a( host material)b,
Wherein a weight fraction and the b, 0 a 0.4, preferably 0.001 a 0.4, more preferably 0.001 a 0.02, and wherein b=1-a.4. As claim 1-3 material according to any one of a heterogeneous metal support, wherein the dopant material is MgO, Cr2O3,ZrO2,A12O3and the TiO2of the one or more, and the form of nanoparticles.5. A multi-phase metal support material according to claim 3, wherein,
Dopant material is TiO2,
The host material is a NixMnyCozhydroxide, an oxyhydroxide and oxides of one or a mixture thereof, wherein x, y, z are atomic fractions, 0 ≤ x ≤ 1, 0 ≤ y ≤ 1, 0 ≤ z ≤ 1, and the x + y + z=1.6. As claim 1-5 material according to any one of a heterogeneous metal support, wherein the secondary particles are spherical shaped multi-phase metal support material.7. As claim 1-6 material according to any one of a heterogeneous metal support, wherein the dopant material is MgF2and the CaF2one, or to another water-insoluble metal halide, and the dopant material is in the form of nanoparticles.8. As claim 1-7 heterogeneous metal support material according to any one, wherein the dopant material range in size from 5-200 nm, preferably 10-50 nm.9. A particulate dopant material is uniformly distributed in a host material, thereby obtaining a multi-phase composite material of the metal support method, from a host material of the secondary particles comprise primary particles integrated, the method comprising the steps of:
-providing 1st fluid, which comprises a host material of the precursor solution;
-2nd providing a fluid, comprising a precipitating agent;
-fluid provides 3rd, which comprises a complexing agent;
-fluid in the 1st, 3rd and the 2nd fluid in the fluid in the one or more, or 4th in the suspending fluid in a fluid dopant consisting of granular material, providing an amount of dopant material insoluble particulate; and
-fluid of the 1st, 3rd and 4th 2nd fluid and a fluid mixture fluid that may be present, thereby precipitating dopant material and the body.10. Method according to claim 9, wherein the precursor solution is an aqueous solution of metal salt, and the dopant material of the suspension is a suspension of water and a suspension stabilizer.11. Claim 9 or 10 A method, in which the particulate dopant material comprises a stabilized nanoparticles, preferably nanoparticles stabilized to a stabilized metal or metal oxide nano-particles, and the precursor is a metal nitrate, a chloride, sulfate powder is one of a halide and mixtures or combinations thereof.12. As claim 9-11 A method according to any one, wherein the dopant material is MgO, Cr2O3,ZrO2,Al2O3and the TiO2of the one or more, and has a 5-200 nm range of sizes.13. Claim 1-8 The use according to any one of the materials, wherein the material with a lithium source is manufactured by sintering the cathode material for a secondary battery.14. Use according to claim 13, wherein the dopant material is MgO, Cr2O3,ZrO2,Al2O3and the TiO2one of the, cathode material is a lithium transition metal oxide.15. Use according to claim 14, wherein the dopant material is Al2O3, cathode material is LiNiO2.An integral contravariant formulation of the fully non-linear Boussinesq equations

Abstract
In this paper we propose an integral form of the fully non-linear Boussinesq equations in contravariant formulation, in which Christoffel symbols are avoided, in order to simulate wave transformation phenomena, wave breaking and nearshore currents in computational domains representing the complex morphology of real coastal regions.
Following the approach proposed by Chen (2006), the motion equations retain the term related to the approximation to the second order of the vertical vorticity.
A new Upwind Weighted Essentially Non-Oscillatory scheme for the solution of the fully non-linear Boussinesq equations on generalised curvilinear coordinate systems is proposed.
The equations are rearranged in order to solve them by a high resolution hybrid finite volume-finite difference scheme.
The conservative part of the above-mentioned equations, consisting of the convective terms and the terms related to the free surface elevation, is discretised by a high-order shock-capturing finite volume scheme in which an exact Riemann solver is involved; dispersive terms and the term related to the approximation to the second order of the vertical vorticity are discretised by a cell-centred finite difference scheme.
The shock-capturing method makes it possible to intrinsically model the wave breaking, therefore no additional terms are needed to take into account the breaking related energy dissipation in the surf zone.
The model is verified against several benchmark tests, and the results are compared with experimental, theoretical and alternative numerical solutions.
Highlights
•
We propose a contravariant formulation of the fully non-linear Boussinesq equations.
•
The presented contravariant formulation is free of Christoffel symbols.
•
The equations retain the approximation to the second order of the vertical vorticity.
•
We present a new Upwind Weighted Essentially Non-Oscillatory scheme.
•
The shock-capturing scheme allows an explicit simulation of the wave breaking.

Introduction
The modelling of surface wave transformation phenomena in coastal regions and breaking waves are of fundamental importance for the simulation of the surf zone hydrodynamics.
The two-dimensional Boussinesq equations make it possible to represent most of the above-mentioned phenomena.
These equations arise from the depth integration of the Euler equations once the depth dependence of the variables is known.
The standard Boussinesq equations are based on the assumption of linear distribution over the depth of the vertical component of the velocity and encompass frequency dispersion by taking into account the effects of the vertical acceleration on the pressure vertical distribution.
The most common forms of the standard Boussinesq equations include the lowest order of both frequency dispersion and non-linearities and are able to adequately represent wave phenomena only in a range of values of the water depth, h0, to deep water wave length, L0, ratio up to 0.2.
Madsen and Sørensen (1992) proposed an extended form of the Boussinesq equations, expressed in function of the depth averaged velocity, which allowed the representation of wave phenomena even in regions where the h0/L0 ratio is close to 0.5.
By introducing an additional third order term, which can be thought as a slight modification of the Padé approximant of the full dispersive relation, in the depth integrated momentum equation, Madsen and Sørensen (1992) improved the dispersion properties of the Standard Boussinesq equations.
Nwogu (1993) derived an alternative form of the extended Boussinesq equations in which the dependent variable was the velocity at an arbitrary distance from the still water level.
Nwogu (1993) improved the linear dispersion properties of the standard Boussinesq equations retaining terms up to O(ε) and O(μ2) in variable depth power expansion, where ε=a0/h0 and μ=h0/L0 are non-dimensional parameters related to the order of magnitude of non-linearities and frequency dispersion and a0 is the deep water wave amplitude.
Nwogu (1993) included an additional frequency dispersion term in the continuity equation and improved, compared to the Standard Boussinesq equations, the representation of the dispersion phenomena in intermediate and deep water.
Wei et al. (1995) followed the Nwogu (1993) approach using the velocity at a certain depth as a dependent variable.
They derived a fully non-linear extension of the Boussinesq equations by retaining terms up to O(μ2) and O(εμ2) in variable depth power expansion and consequently improved the accuracy of the model just seaward of the surf zone where the wave height to water depth ratio is essentially equal to 1.
Chen et al. (1999) used a model based on the fully non-linear extended Boussinesq equations proposed by Wei et al. (1995) in order to simulate wave phenomena and breaking-induced nearshore circulation.
They modelled the wave breaking and energy dissipation in the surf zone by introducing an eddy viscosity term into the governing momentum equation.
Velocity and free surface elevation fields obtained by numerical integration of the above-mentioned Boussinesq equations were averaged in time and gave the possibility to predict longshore and rip currents and to simulate the coupled interaction of surface waves and currents.
Chen et al. (2003) improved the above-mentioned model by introducing an additional term in the momentum equation in order to ensure the property of vertical vorticity conservation and to improve the simulation of surface waves and longshore currents.
Chen et al. (2003) started from the three-dimensional Euler equations in which the convective terms were explicitated directly in terms of vector product between velocity and vorticity.
The introduction in the above-mentioned Euler equations of the vertical distribution of the velocity and pressure, obtained by retaining terms up to O(εμ2) and O(μ2) in depth power expansion, and of the vertical component of the vorticity gave a new form of the Boussinesq type momentum equation for the partially rotational motion.
The vertical vorticity conservation is correct to the second order consistently with the order of approximation for the wave motion.
Erduran et al. (2005) proposed a method for the solution of the extended Boussinesq equations, in the form proposed by Madsen and Sørensen (1992), based on a hybrid scheme consisting of finite volume and finite difference procedures.
The governing equations were rearranged to obtain a conservative part that can be treated by a finite volume method.
The remaining terms in the depth averaged momentum equation were considered as source and sink terms and discretised by a finite difference method.
The presence of the dispersive terms in the Boussinesq equations balances the amplitude dispersion phenomenon and makes the wave fronts stable.
For this reason the Boussinesq equations are not able to automatically and intrinsecally represent shallow water wave breaking.
Many authors introduce breaking dissipation schemes in the Boussinesq equations which have to be applied in the surf zone.
Zelt (1991), Karambas and Koutitas (1992) and Kennedy et al. (2000) provide a wave breaking model in the form of an eddy viscosity term added to the momentum equation.
This term takes into account the energy dissipation due to the wave breaking induced turbulence.
Different methods are based on the surface roller concept introduced by Svendsen (1984): the underlying idea is that the surface roller can be considered as a volume of water carried by the wave.
In the wave breaking model proposed by Schäffer et al. (1993) and Madsen et al. (1997), the roller propagates with the wave celerity and the velocity below the roller is obtained from the irrotational theory.
This leads to additional terms in the momentum equation and consequently in an increase of momentum flux which simulates wave breaking.
By the consideration that in the surf zones, the wave breaking induced energy dissipation is substantially related to the breaking generated vorticity, Svendsen et al. (1996), Veeramony and Svendsen (2000) and Musumeci et al. (2005) solved the vorticity transport equation in order to simulate the vorticity vertical distribution.
In the above-mentioned models the dissipation term due to the wave breaking is related to the presence of the vorticity.
As the weak solutions of the integral form of the motion equations (numerically solved by a shock-capturing scheme) are able to directly simulate wave breaking, the explicit introduction in the equations of terms representing the breaking wave dissipation is not necessary.
Shock-capturing schemes permit an explicit simulation of the wave breaking phenomenon, thus these schemes do not require any empirical calibration.
Hybrid methods have been recently used by many authors in order to take advantage of the shock capturing methods in simulating breaking waves.
Tonelli and Petti (2009) extended the hybrid scheme proposed by Erduran et al. (2005) to the two-dimensional Madsen and Sørensen (1992) equations seaward of the surf zone and implemented a shock-capturing method for the solution of the non-linear shallow water equations in the surf zone: the Riemann problem was solved by an approximated HLL solver and a MUSCL-TVD technique was used for fourth order reconstructions.
Roeber et al. (2010) solved Nwogu's (1993) one-dimensional equations by a hybrid scheme including a shock capturing method based on a Godunov-type procedure; they introduced an eddy viscosity term into the governing equation in order to control the instabilities produced by the sharp gradients of transport quantities.
Shi et al. (2012) modified the fully non-linear Boussinesq equations proposed by Chen (2006) by introducing a moving reference level, presented for the first time by Kennedy et al. (2001), in order to optimise the non-linear behaviour of the resulting model equations.
The governing equations were written in conservative form with an approximated Riemann solver and a MUSCL technique being used.
Shi et al. (2012) followed the Tonelli and Petti (2009) approach in modelling wave breaking: the Boussinesq equations switched into the non-linear shallow water equations where the Froude number exceeds a certain threshold.
In this paper a new integral form of the fully non-linear Boussinesq equations in contravariant formulation is proposed in order to simulate wave evolution, wave breaking and breaking-induced nearshore currents in computational domains representing the complex morphology of real coastal regions.
Following the approach proposed by Chen (2006), motion equations retain the term related to the second order vertical vorticity.
Breaking wave propagation in the surf zone is simulated with a high-order shock-capturing scheme: an exact Riemann solver and a WENO reconstruction technique are used.
The computation of flows over domains representing the complex morphology of real cases can be carried out using two different strategies.
The first of these strategies is based on the use of unstructured grids (Hu and Shu (1999), Petti and Bosa (2007), Mandal and Rao (2011), Sørensen et al. (2004), Arminjon and St-Cyr (2003), Gallerano and Napoli (1999), Gallerano et al. (2005), Titarev and Drikakis (2011), Cioffi et al. (2005)).
The second of these strategies is based on the use of computational cells that result from the intersection of curvilinear boundary conforming coordinate lines.
By adopting the latter strategy the motion equations can be expressed in contravariant formulation (Luo and Bewley (2004), Gallerano and Cannata (2011a), Rossmanith et al. (2004), Wesseling et al. (1999), Zijlema et al. (1995), Segal et al. (1992)).
In numerical solutions of motion equations in contravariant formulation, a contradiction related to the presence of Christoffel symbols appears.
It is well known that non-conservative forms of the convective terms present in the conservation laws do not permit numerical methods for the solution of the above-mentioned laws to converge to weak solutions.
Consequently the integration of the conservation laws in the solutions of which shocks are present, requires convective terms to be expressed in conservative form.
The contravariant formulation of motion equations involves covariant derivatives that give rise to Christoffel symbols.
These terms are extra source terms.
They come in with the variability of base vectors and do not permit the definition of convective terms in a conservative form.
Furthermore, computational errors can be produced by numerical discretisation of the Christoffel symbols on highly distorted grids and the numerical accuracy can be reduced.
Consequently the contravariant fully non-linear Boussinesq equations, numerically integrated on generalised boundary conforming curvilinear grids, must be free of Christoffel symbols.
In this work a new Upwind Weighted Essentially Non-Oscillatory scheme is proposed for the solution of the fully non-linear Boussinesq equations.
A new integral form for this set of equations expressed directly in contravariant formulation is presented.
In order to avoid Christoffel symbols, the contravariant form of the motion equations is integrated on an arbitrary surface and is resolved in the direction identified by a constant parallel vector field.
In this way we present an integral form of the contravariant fully non-linear Boussinesq equations in which Christoffel symbols are avoided.
The equations are solved by a hybrid finite volume-finite difference scheme.
Convective terms and terms related to the free surface elevation gradient are discretised by a high order finite volume upwind WENO scheme; dispersive terms and the term related to the second order vertical vorticity are discretised by a finite-difference scheme.
The upwind WENO scheme needs a flux calculation at the cell interfaces.
These fluxes are calculated by means of the solution of a Riemann problem.
An Exact Riemann Solver is used in this work.
No additional dissipative term to improve the modelling of breaking related energy decay and breaking induced nearshore circulation is used in this paper.
The paper is organised as follows: the conservative form of the Cartesian fully non-linear Boussinesq equations is presented in Section 2; the integral form of the contravariant fully non-linear Boussinesq equations is presented in Section 3; Section 4 shows the numerical scheme used to solve the equations; in Section 5 accuracy tests and applications to problems of wave breaking and nearshore currents are presented and conclusions are made in Section 6.
Conservative form of the Cartesian fully non-linear Boussinesq equations
Let H=h+η be the total local water depth, where h is the local still water depth and η is the local surface displacement.
Using a Taylor expansion of the velocity about an arbitrary distance from the still water surface, σ, and assuming zero horizontal vorticity, as proposed by Nwogu (1993), Wei et al. (1995), Chen et al. (2003) and Chen (2006), the vertical distribution of the horizontal velocity can be written as:(1)u→=u→α+u→2zwhere u→α is the horizontal velocity at an arbitrary distance from the still water level, z=σ and u→2z=σ-z∇∇⋅hu→α+σ2/2-z2/2∇∇⋅u→α consists of the second order terms in depth power expansion of the velocity vector in which ∇ is the two-dimensional differential operator defined as ∇=(∂/∂x,∂/∂y) in a Cartesian reference system.
The following vectors can be defined: r→=Hu→α and s→=Hu¯→2, in which u¯→2 is the depth averaged value of u→2z.
The explicit expression of s→ is reported in Appendix A.
The fully non-linear Boussinesq equations expressed in a conservative form in a two-dimensional Cartesian system as a function of the dependent variables H and r→ are:(2)∂H∂t+∇⋅r→=-∇⋅s→(3)∂r→∂t+∇⋅r→⊗r→H+GH∇η=-r→H∇⋅s→-HV→+T→+W→-R→where ⊗ is the tensor product between vectors, G is the constant of gravity, V→ and T→ are the dispersive terms obtained by retaining terms up to O(μ2) and O(εμ2) in depth power expansions of the horizontal velocity according to Wei et al. (1995a), W→ is the term related to the approximation to the second order of the vertical component of the vorticity according to Chen(2006) and R→ is the bottom resistance term.
The expressions for V→, T→ and W→ are given in the Appendix A.
Finite volume methods are effective tools for the numerical integration of depth integrated motion equations in nearshore regions and allow an adequate representation of the breaking waves.
Their application to the Boussinesq equations is not straightforward because of the presence of high order derivatives in dispersive terms on the right-hand side of Eq.
(3).
It is possible to rearrange the Eq.
(3) in order to obtain a momentum equation that can be numerically solved by a hybrid finite volume-finite difference scheme, as suggested by Erduran et al. (2005), Tonelli and Petti (2012) and Shi et al. (2012).
In order to obtain a system of motion equations in which a conservative part to be solved by a finite volume shock-capturing scheme is present, let us:a)
Decompose the term HV→ on the right-hand side of Eq.
(3) according to Wei and Kirby (1995):(4)HV→=∂∂tHV→'-∂H∂tV→'+HV→''where the expressions for V→' and V→'' are given in the Appendix A;
b)
Rewrite the free surface elevation term on the left-hand side of Eq.(3) as:(5)GH∇η=GH∇H-H∇h=G∇H22-H∇h
c)
Introduce the auxiliary variable r→* defined as:(6)r→*=r→+HV→'
Expressions (4), (5) and (6) permit us to rewrite the Eq.
(3) in the following form:(7)∂r→*∂t+∇⋅r→⊗r→H+G∇H22=GH∇h-R→-r→H∇⋅s→+∂H∂tV→'-HV→''-HT→-HW→
The dependent variables of the new system of Eqs.
(2) and (7) are H and r→*.
The conservative part at the left-hand side of Eqs.
(2) and (7), consisting of the dispersive terms and the terms related to the free surface elevation, can be numerically solved by a shock-capturing finite volume scheme whereas the terms at the right-hand side of the above-mentioned equations can be discretised by a cell-centred finite difference scheme.
Integral form of the contravariant fully non-linear Boussinesq equations
Let us rewrite Eqs.
(2) and (7) in contravariant formulation in a two-dimensional system of generalised curvilinear coordinates.
We consider a transformation xl=xl(ξ1,ξ2) from the Cartesian coordinates x→ to the curvilinear coordinates ξ→ (note that hereinafter the superscript indicates the generic component and not the powers).
Let g→l=∂x→/∂ξl be the covariant base vectors and g→l=∂ξl/∂x→ the contravariant base vectors.
The metric tensor and its inverse are defined, respectively, by glm=g→l⋅g→m and glm=g→l⋅g→ml,m=1,2.
The Jacobian of the transformation is g=detglm.
The transformation relationships between the components of the generic vector b→ in the Cartesian coordinate system and its contravariant and covariant components, bl and bl, in the curvilinear coordinate system are given by:(8)bl=g→l⋅b→,b→=blg→l,bl=g→l⋅b→b→=blg→l
In the following equations, a comma with an index in a subscript stands for covariant differentiation.
The covariant derivative is defined as b,ml=∂bl/∂ξm+Γmklbk, where Γmkl is the Christoffel symbol (Aris, 1989) given by:(9)Γmkl=g→l⋅∂g→k/∂ξm
Let r*l be the l-th contravariant component of the vector r→* defined by the expression (6).
r*l is given by:(10)r*l=rl+HV'l
The differential contravariant form of the continuity Eq.
(2) and momentum Eq.
(7) can be expressed as:(11)∂H∂t+rm,m=-sm,m(12)∂r*l∂t+rlrmH+GglmH22,m=GHglmh,m-Rl-rlHsm,m+∂H∂tV'l-HV''l-HTl-HWl
In Eq.
(11) the second term on the left-hand side is the flux term.
In Eq.
(12) the second term on the left-hand side is the flux term, the first term on the right-hand side is the source term related to the bottom slope, the second term on the right-hand side, Rl is the bottom resistance term approximated by a quadratic law as in Tonelli and Petti (2012).
Expressions for terms sl, V'l, V″l, Tl and Wl are given in the Appendix B.
The integration on the generic surface element of area ΔA of Eqs.
(11) and (12) results in the following expressions:(13)∬ΔA∂H∂tdA+∬ΔArl,ldA+∬ΔAsl,ldA=0(14)∬ΔA∂r*l∂tdA+∬ΔArlrmH+GglmH22,mdA=∬ΔAGHglmh,mdA-∬ΔARldA-∬ΔArlHsm,mdA+∬ΔA∂H∂tV'ldA-∬ΔAHV''ldA-∬ΔAHTldA-∬ΔAHWldAin which the covariant derivative in the second integral at the left-hand side of Eq.
(14) does not allow the avoidance of the appearance of the Christoffel symbols.
Our main goal is to formalise an integral expression of the contravariant fully non-linear Boussinesq equations in which the Christoffel symbols are absent.
From a general point of view, in order to express the momentum equation in an integral form, the rate of change of the momentum in a material volume and the total net force must be projected along a physical direction.
A given curvilinear coordinate line changes its direction in space unlike what happens in a Cartesian reference system.
Consequently, the volume integral of the projection of the motion equations along a curvilinear coordinate line has no physical meaning, since it does not represent the volume integral of the projection of the above-mentioned equations along a physical direction (Aris, 1989).
Let us consider a constant parallel vector field and equate the rate of change of the momentum of a material volume to the total net force in this direction.
We choose, as a parallel vector field, the one which is normal to the coordinate line on which the ξl component is constant at point P0∈ΔA whose coordinates are ξ01 and ξ02.
In this work, following the approach proposed in Gallerano and Cannata (2011b), we use the contravariant base vector defined at point P0 which is normal to the coordinate line on which ξl is constant in order to identify the parallel vector field.
Let λk(ξ1,ξ2) be the covariant component of g→lξ01ξ02, given by:(15)λkξ1ξ2=g→lξ01ξ02⋅g→kξ1ξ2
We indicate g˜→l=g→lξ01ξ02 and g→k=g→kξ1ξ2.
We integrate over an arbitrary surface element of area ΔA and resolve in the direction λk the rate of change of the depth integrated momentum (per unit mass) and the depth integrated resultant force (per unit mass).
Consequently we get:(16)∬ΔA∂r*k∂tλkdA+∬ΔArkrmH+GgkmH22,mλkdA=∬ΔAGHgkmh,mλkdA-∬ΔARkλkdA-∬ΔArkHsm,mλkdA+∬ΔA∂H∂tV'kλkdA-∬ΔAHV''kλkdA-∬ΔAHTkλkdA-∬ΔAHWkλkdA
Since the vector field is constant and parallel, λk,m=0.
Using Green's theorem for the second integral at the left-hand side of Eq.
(13) and for the second integral at the left-hand side of Eq.
(16), using Eq.
(15) and recalling that g→k⋅g→m=δkm, we get:(17)∬ΔA∂H∂tdA+∫LrmnmdL=-∬ΔAsl,ldA∬ΔAg˜→l⋅g→k∂r*k∂tdA+∫Lg˜→l⋅g→krkrmH+Gg˜→l⋅g→mH22nmdL=∬ΔAg˜→l⋅g→kGHgkmh,mdA-∬ΔAg˜→l⋅g→kRkdA-∬ΔAg˜→l⋅g→krkHsm,mdA+∬ΔAg˜→l⋅g→k∂H∂tV'kdA(18)-∬ΔAg˜→l⋅g→kHV''kdA-∬ΔAg˜→l⋅g→kHTkdA-∬ΔAg˜→l⋅g→kHWkdAwhere L is the contour line of ΔA and nm is the m-th component of the covariant outward normal.
Eqs. (17) and (18) represent the integral expressions of the fully non-linear Boussinesq equations in contravariant formulation in which Christoffel symbols are absent.
These equations are accurate to O(μ2) and O(εμ2) in dispersive terms and retain the conservation of potential vorticity up to O(μ2), in accordance with the formulation proposed by Chen (2006).
Let us introduce a restrictive condition on the surface element of area ΔA: in the following this surface element must be considered as a surface element which is bounded by four curves lying on the coordinate lines.
Since dA=gdξ1dξ2 and by indicating with H¯˜ the averaged value of H over the surface element of area ΔA, given by H¯˜=1/ΔA∬ΔAHgdξ1dξ2, Eq.
(17) can be rewritten as:(19)∂H¯˜∂t=-1ΔA∑μ=12∫Δξμ+grμdξυ-∫Δξμ-grμdξυ-1ΔA∬ΔA∂skg∂ξkdξ1dξ2where Δξμ+ and Δξμ- indicate the segments of the contour line on which ξμ is constant and which are respectively located at the larger and smaller value of ξμ.
In Eq.
(19) the indexes μ and ν are cyclic.
Let us define r¯˜*l as:(20)r¯˜*l=1ΔA∬ΔAg˜→l⋅g→kr*kgdξ1dξ2
By dividing Eq.
(18) by ΔA and by using Eq.
(20), Eq.
(18) becomes:(21)∂r¯˜*l∂t=1ΔA-∑μ=12∫Δξμ+g˜→l⋅g→krkrμH+g˜→l⋅g→μGH22gdξυ-∫Δξμ-g˜→l⋅g→krkrμH+g˜→l⋅g→μGH22gdξυ+∬ΔAg˜→l⋅g→kGη-η¯˜gmkh,mgdξ1dξ2+Gη¯˜∑μ=12∫Δξμ+g˜→l⋅g→μhgdξυ-∫Δξμ-g˜→l⋅g→μhgdξυ+G2∑μ=12∫Δξμ+g˜→l⋅g→μh2gdξυ-∫Δξμ-g˜→l⋅g→μh2gdξυ-∬ΔAg˜→l⋅g→kRkgdξ1dξ2-∬ΔAg˜→l⋅g→krkHsm,mgdξ1dξ2+∬ΔAg˜→l⋅g→k∂H∂tV'kgdξ1dξ2-∬ΔAg˜→l⋅g→kHV''kgdξ1dξ2-∬ΔAg˜→l⋅g→kHTkgdξ1dξ2-∬ΔAg˜→l⋅g→kHWkgdξ1dξ2where η is the free surface elevation and η¯ is the averaged value of η over the surface element ΔA.
The left-hand side of Eq.
(21) represents the surface average of the time derivative of the l-th contravariant component of the r→* vector expressed as a function of the contravariant base vector g→l defined in ξ01, ξ02.
The second, third and fourth terms on the right-hand side of Eq.
(21) result from the splitting of the term related to the bottom slope, according to Xing and Shu (2006).
It must be emphasised that Eqs.
(19) and (21) are free of Christoffel symbols.
The numerical scheme
The numerical integration of Eqs.
(19) and (21) is carried out by a high order upwind WENO scheme.
The computational domain discretisation is based on a grid defined by the coordinate lines ξ1 and ξ2 and by the points of coordinates ξ1=iΔξ1 and ξ2=jΔξ2, which represent the centres of the calculation cells Ii;j=(ξi-1/21,ξi+1/21)×(ξi-1/22,ξi+1/22).
tn is the time level of the known variables, while tn+1=tn+Δt is the time level of the unknown variables.
Let us indicate with L(r1,r2) and with LB(S1,S2) respectively the first and the second term on the right-hand side of Eq.
(19).
Let us indicate with D(H,r1,r2) the sum of the convective and free surface elevation terms (which is split according to Xing and Shu (2006) in order to ensure a well-balanced scheme) on the right-hand side of Eq.
(21) and with D(H,r1,r2) the bottom friction term, the sum of dispersive terms and the term related to the approximation to the second order of the vertical vorticity on the right-hand side of this equation.
By integrating Eqs.
(19) and (21) over [tn,tn+1] we get:(22)H≃i;jn+1=H≃i;jn-1ΔA∫tntn+1Lr1r2+LBs1s2dt(23)r≃*i;jln+1=r≃*i;jln-1ΔA∫tntn+1DHr1r2+DBHr1r2s1s2dt
Eqs. (22) and (23) represent the advancing from time level tn to time level tn+1, of the variables H¯i;j and r≃*i;jl.
The state of the system is known at the centre of the calculation cell and it is defined by the cell-averaged values H¯i;j and r≃*i;jl.
In this paper, time integration of Eqs.
(22) and (23) is carried out by means of a third order accurate Strong Stability Preserving Runge-Kutta method (SSPRK) reported in Spiteri and Ruuth (2002).
The SSPRK method can be written in compact form as follows:(24)H≃i;j0=H≃i;jn;r¯˜*i;jl0=r¯˜*i;jln(25)H≃i;jp=∑q=0p-1ΩpqHi;jq+ΔtφpqLr1qr2q+LBs1ps2p(26)r≃*i;jlp=∑q=0p-1Ωpqr*i;jlp+ΔtφpqDHqr1qr2q+DBHpr1pr2ps1ps2p(27)H≃i;jn+1=H≃i;j3;r¯˜*i;jln+1=r¯˜*i;jl3where p=1; 2; 3.
See Spiteri and Ruuth (2002) for Ωpq and φpq values.
The computation of L(r1,r2), D(H,r1,r2), LB(s1,s2) and DB(H,r1,r2,s1,s2) terms needs the numerical approximation of the spatial integrals on the right-hand side of Eqs.
(19) and (21).
According to the method proposed by Erduran et al. (2005) and used among the others by Tonelli and Petti (2012) and Shi et al. (2012), this numerical approximation is carried out by means of a hybrid finite volume-finite difference scheme.
By applying this method, once the values of the auxiliary variable r¯˜*l are known, the values of the original variables r¯˜l at each stage of the Runge-Kutta method are computed by solving the following equation:(28)r≃*l=rl≃+H¯˜V¯˜′lin which V¯′l includes first and second derivative of r¯˜l/H¯˜ with respect to ξ1 and ξ2 and cross derivatives (see Appendix B).
The numerical approximation of the derivatives in the V¯˜'l term is carried out by a second order central difference scheme.
The velocity at the elevation σ, averaged over the generic computational cell Ii;j and indicated with r¯˜l/H¯˜i;j can be found by solving a system of equations with tridiagonal matrix formed by Eq.
(28) in which all cross-derivatives are moved to the right-hand side of the equation.
Once the values of r¯˜l/H¯˜i;j are known, the LB(s1,s2) and DB(H,r1,r2,s1,s2) terms on the right-hand side of Eqs.
(25) and (26) are discretised using a second order central difference scheme at the cell centroids, as in Wei and Kirby (1995) and Shi et al. (2012).
Since the LB and DB terms need to be updated using H,r1,r2,s1,s2 at the corresponding time step, an iteration is needed to achieve convergence, as suggested by Shi et al. (2012).
Convective terms and terms related to the free surface elevation that define the L(r1,r2) and D(H,r1,r2) terms on the right-hand side of Eqs.
(25) and (26) are computed by a high-order finite volume WENO scheme.
According to the procedure proposed by Gallerano et al. (2012) this numerical scheme is based on the following sequence:1.
Starting from cell averaged values, the point values of the unknown variables at the centre of the contour segments which define the calculation cells are computed by means of WENO reconstructions.
Two WENO reconstructions defined on two adjacent cells are used to get two point values of the unknown variables at the centre of the contour segment which is common with the two adjacent cells.
2.
The point values of the unknown variables at the centre of the contour segments are advanced in time by means of the so-called exact solution of a local Riemann problem, with initial data given by the pair of point-values computed by two WENO reconstructions defined on the two adjacent cells.
In accordance with the procedure proposed by Rossmanith et al. (2004), all necessary Riemann problems are solved in a locally valid orthonormal basis.
This orthonormalisation allows one to solve Cartesian Riemann problems that are devoid of geometric terms.
3.
The spatial integrals that define the L(r1,r2) and D(H,r1,r2) terms are numerically approximated by means of a high order quadrature rule, starting from point values of the dependent variables computed at the previous step.
Results
In this section the proposed scheme for the solution of the fully non-linear Boussinesq equations in contravariant form, expressed by Eqs.
(19) and (21), is tested against a set of benchmark test cases.
In all the tests, simulations are carried out by switching from Boussinesq equations to non-linear shallow water equations when the wave height to water depth ratio is equal to 0.8, according to Tonelli and Petti (2012) and Shi et al. (2012).
1D: Breaking on a sloping beach
The results from the model described in the previous Sections are first compared to a set of experimental data with regular waves performed by Hansen and Svendsen (1979).
The experiments were conducted in a 40m long wave flume with a horizontal bottom at a depth of 0.36m and a planar slope of 1:34.26 where waves shoal and break (see Fig. 1).
This test permits us to verify the ability of the model to represent phenomena of shoaling, breaking and after breaking wave energy decay.
The results obtained with the model presented in this paper are also compared with the results obtained by applying the numerical integration procedure described in Section 4 to the one-dimensional weakly non-linear Boussinesq equations proposed by Madsen and Sørensen (1992) and to those proposed by Nwogu (1993).
Table 1 reports the period and wave heights at the start of the slope for the test cases simulated.
The spatial discretisation is Δx=0.02m and the time step is Δt=0.005s.
Figs. 2 and 3 show the comparison between computed and measured time-averaged wave heights as the waves propagate up the slope respectively for test cases 1 and 2: the computed values are obtained by using Madsen's equations, Nwogu's equations and the proposed fully non-linear Boussinesq model.
It can be seen that wave heights at the beginning of the slope are well represented with all the equations used, but significant differences are evident as the waves approach the breaking point.
Madsen's equations significantly underpredict the peak wave height at breaking: this is consistent with the fact that these equations are based on the lowest order weakly-non-linear theory.
Nwogu's equations slightly underpredict wave height at the breaking point for test case 1 (see Fig. 2) while they overpredict peak wave height for test case 2 (see Fig. 3).
From Figs.
2 and 3 it can be noted that, for both test cases, the wave heights computed with the proposed fully non-linear model are in a better agreement with the experimental data in the shoaling region, at the breaking point and in the surf zone where wave energy decays.
The comparison between the computed wave heights obtained using the Nwogu's equations and those obtained using the proposed fully non linear Boussinesq model shows the importance of retaining terms up to O(εμ2) in variable depth power expansion.
Figs. 4 and 5 show the comparison between measured and computed values of the mean water level respectively for test cases 1 and 2: the computed values are obtained by using Madsen's equations, Nwogu's equations and the proposed fully non-linear Boussinesq model.
From Figs.
4 and 5 it can be seen that the prediction of the wave induced set-down and setup, obtained with all the above-mentioned equations, is in good agreement with the experimental data.
The capability of the model to adequately represent the flow characteristics in the surf zone will be further investigated with a two-dimensional test case in Section 5.4.
Spectral evolution over a submerged bar
The ability of the proposed model to simulate the wave decomposition and spectrum evolution over a submerged bar is verified: numerical results are compared against experimental results from two test cases presented by Beji and Battjes (1994) and two test cases presented by Beji and Battjes (1994) whose characteristics are listed in Table 2.
The above-mentioned authors performed an experimental study on the propagation of unidirectional regular and irregular waves over a submerged bar whose geometry is sketched is Fig. 6.
Beji and Battjes (1993, 1994) recorded the free surface elevation at different measurement stations whose location is sketched in Fig. 6.
The validation of the model proposed in this work is carried out by directly comparing the spectra presented by Beji and Battjes (1993, 1994) with the spectra obtained from the time series of the simulated free surface elevation in correspondence with the same measurement stations shown in Fig. 6.
The simulation of each of the irregular waves is performed starting from the generation, in the input section, of time series of the free surface elevation defined by JONSWAP type spectra with frequency peak fp and peak wave height Hp equal to those of the waves experimentally generated by Beji and Battjes (1993, 1994) and listed in Table 2.
The waves are internally generated at the abscissa x=0m (Fig. 6) following the procedure proposed by Wei et al. (1999).
This procedure requires the addition of a source function F(t) to the continuity equation.
With regard to the generation of irregular waves, the time series of the aforementioned source function is obtained as a superposition of N elementary harmonics each characterised by a frequency ωi, an amplitude Di and a random value of the phase ξi that follows a uniform probability distribution in the interval [0, 2π].
The expression of this source function is(29)Ft=∑i=1NDicosωit-ξiin which the i-th amplitude Di is computed starting from JONSWAP type spectra with the values of the peak frequency fp and peak wave height Hp equal to those of the waves performed by Beji and Battjes (1993, 1994) and listed in Table 2.
Consistent with Beji and Battjes (1993, 1994), for each of the random wave test cases (tests B, C and D in Table 2) five different simulations of the spatial and temporal evolution of the free surface elevation are carried out by varying the set of random values ξi for the phases of the elementary harmonics that compose the source function F(t).
In order to produce the power spectra, the statistical analysis of the time series of the simulated surface elevation, at points corresponding to the different measurement stations, is carried out according to the procedure described in Beji and Battjes (1993) and reported below.
Each of the aforementioned time series of the free surface elevation, containing 9000 data points, is divided into two parts each containing 4096 data points.
In this process the data points at the end of each series are discarded.
Each set of 4096 data points is divided into M data segments each of which is 10% cosine tapered.
Using a standard FFT package, from each of the aforesaid M data segments a power spectrum is computed.
Each of these spectra is rescaled to take account of the tapering (Bendat and Piersol, 1971).
Consistently with Beji and Battjes (1993, 1994), the spectra shown in this section are the result of an average in the frequency domain carried out on the spectra of all the data segments of the five different simulations.
In particular, for the test case B of Table 2 the number M of segments is equal to 4, each data segment is 40.98s long and the corresponding frequency resolution is equal to 0.024Hz.
The resulting spectral estimates have 80 degrees of freedom and a statistical error equal to 15.8% consistently with the spectral estimates shown in Beji and Battjes (1994).
For the test cases C and D of Table 2 the number M of data segments is equal to 16, each data segment is 25.6s long and the corresponding frequency resolution is equal to 0.039Hz.
The resulting spectra have 320 degrees of freedom and a statistical error equal to 7.9% consistently with the spectral estimates shown in Beji and Battjes (1993).
The numerical simulations are carried out with a spatial discretization step of 0.05m and a time step of 0.02s.
Fig. 7 shows the comparison between the computed and measured values of the free surface elevation for the regular wave shown in Beji and Battjes (1994) (test A of Table 2).
At station 2, the wave train is still almost sinusoidal and the agreement between numerical and experimental data is very good.
Also in the next stations the agreement between the numerical and experimental results is very good.
In particular, consistent with what had already been highlighted by Beji and Battjes (1994) in the successive stations the waves are progressively steepened by interaction with the bar and lose vertical symmetry to assume a typical sawtooth shape; furthermore the secondary wave mode gains energy from the main wave mode.
Fig. 7 shows the ability of the proposed model to simulate wave transformation due to the wave-bar interaction in terms of both phases and amplitudes.
In Fig. 8 is shown the spatial evolution of the spectrum of the non-breaking irregular wave (test B of Table 2).
In this figure the spectra from the simulated time series of surface elevation are compared against the spectra reported by Beji and Battjes (1994) for the same test case.
Fig. 8 shows a good agreement between the spectra obtained from the numerical simulations and those from the experiments, attesting the ability of the proposed model to correctly simulate the spectral evolution of irregular waves propagating over a submerged bar: both the bounded harmonics amplification phenomenon occurring during the shoaling process (stations 2 and 3 in Fig. 8) and the spectral decomposition occurring in the deepening region behind the bar, where free second and higher harmonics propagated in relatively deep water (stations 5, 6 and 7 in Fig. 8).
In Fig. 9 the spectral evolution of the non-breaking wave from Beji and Battjes (1993) (test C of Table 2) is compared with the spectra obtained from the numerical simulations carried out with the proposed model.
Consistent with Beji and Battjes (1993), all spectra shown are normalised so that the total area under each spectrum is equal to 1.
A good agreement can be seen between the results obtained from the numerical simulations and those from the experimental measurements of Beji and Battjes (1993) at all measurement stations.
From Fig. 9 it can be seen that the numerical model correctly represents the fact that the wave decomposition and the redistribution of the total energy among the fundamental harmonic and other harmonics mainly occur mainly occur, due to de-shoaling, in the region behind the bar where water depth increases once again.
In Fig. 10 is shown the comparison between the spatial evolution of the spectra obtained from the numerical simulations and those presented by Beji and Battjes (1993) for a breaking wave (test D of Table 2).
Once again, the agreement between the spectra obtained from the numerical simulations and those obtained from the experimental measurements is good.
Moreover, comparing Fig. 9 with Fig. 10, it can be seen that, in agreement with the experimental data, the spatial evolution of the spectra for the breaking wave does not show significant differences with the spatial evolution of the spectra for the non-breaking wave, as already noted by Beji and Battjes (1993).
Undertow prediction under breaking waves
In this section the ability of the proposed shock-capturing scheme to simulate vertical distribution of the horizontal velocity components under breaking waves is verified.
In the surf zone, vertical profiles of horizontal velocity components averaged over a wave period predicted by most of the Boussinesq models involving wave breaking dissipation models and by non-linear shallow water equations solved by shock-capturing schemes are characterised by uniform values below the mean trough level and by a direction opposite to that of the horizontal velocity above the mean trough level.
Lynett (2006) proved that these undertows below the mean trough level were underestimated by the above-mentioned Boussinesq models.
Lynett (2006) proposed a method to improve the undertow prediction obtained by the Boussinesq models which involve wave breaking dissipation models.
In particular, in the surf zone Lynett (2006) added a specific exponential profile a posteriori to the instantaneous vertical profile of the horizontal velocity components obtained by a Boussinesq model.
In the surf zone, vertical profiles of the horizontal velocity components averaged over a wave period obtained by the shock-capturing scheme proposed in this paper present distributions similar to those predicted by the Boussinesq models which involve breaking dissipation models and, in the same way as these, underestimate the undertow below the mean trough level, as it is shown in this section.
In this paper the improvement of the undertow predictions is obtained by using the methodology proposed by Lynett (2006).
In particular, the modified instantaneous vertical profile of the horizontal velocities u→L is given by(30)u→L=u→+u→Bwhere u→ is the instantaneous horizontal velocity vector obtained by the proposed model and u→B is the exponential vertical profile proposed in Lynett (2006).
The modified vertical profiles of the horizontal velocities averaged over a wave period and obtained with the proposed model are compared to the experimental data by Ting and Kirby (1996).
These experiments were performed in a 40m long wave flume in which a planar beach with a 1:35 slope was set.
The comparison between numerical and experimental results is made for the case of spilling breaking wave with a period of 2s presented by the above authors.
In Fig. 11 is shown the comparison between the measured and the computed mean crest level, the mean water level and the mean trough level.
The agreement between the experimental and numerical values is very good.
In the aforementioned procedure the only calibration parameter k to be specified is given as proportional to the inverse of the total water depth H.
In Lynett (2006) the value of this parameter is set to 5/H.
Lynett (2006) states that the value of this parameter can be different for different Boussinesq models and it has to be calibrated on a case-by-case basis.
The k value assumed in the simulations whose results are shown in Fig. 12 is equal to 3.8/H.
Fig. 12 shows the comparison between numerically simulated vertical profiles of the horizontal velocity components averaged over a wave period and the experimental values: both the modified and unmodified profiles are shown.
From Fig. 12 it can be seen that undertow prediction below the mean trough level carried out by solving the non-linear shallow water equations by means of the shock-capturing scheme proposed in this paper are underestimated with respect to the experimental results.
From the same figure it can be noted that the modified vertical profiles averaged over a wave period show an undertow prediction in good agreement with the experimental data.
Symmetric channel constriction
In this Section, the shock-capturing properties of the scheme when the equations are already switched to the non-linear shallow water equations are tested on highly distorted grids by means of the simulation of a super-critical flow in a channel with wall constrictions.
The result of the numerical simulation on the highly distorted grid is shown in Fig. 13.
In this figure the contour plot of the water depth with 16 uniformed spaced contour lines, from 1.05m to 1.8m, is shown.
It can be seen that the shocks are nicely resolved.
By comparing the numerical results with the analytical solution for this test case, computed by adopting the procedure proposed by Ippen (1951), L1 norms of the error in total water depth H and in modulus of the depth-averaged velocity vector u→ are computed and shown in Table 3.
It can be seen that the L1 norms of the error computed on highly distorted grids is less than 7% greater to the L1 norms of the error computed on the regular grid, despite the presence of discontinuities in the solution.
So the capability of the proposed scheme to simulate wave motion inside the surf zone, where shock waves are present, is not compromised by the presence of the highly distorted grid.
Convergence test: wave evolution in a rectangular basin
According to Shi et al. (2001) and Shi et al. (2003), the wave evolution in a closed basin is computed to test the numerical convergence of the scheme with both space and time discretisation.
The basin size is 20×20m and the water depth is constant at 0.5m.
The initial condition is a superelevation of the water surface above the still water level given by a motionless Gaussian hump placed at the centre of the basin:(31)η0=h0exp-12ϑ2x-xc2+y-yc2where h0=0.2m is the initial maximum elevation of the hump, ϑ=1.12 and (xc,yc) are the coordinates of the centre of the basin.
Spatial profiles of surface elevation at t=0,2,4,6,8 and 10s are shown in Fig. 14 for illustrative purposes.
In order to test convergence with space discretisation a sequence of different grid spacing is considered.
As in Shi et al. (2001) the sequence is given by Δx/i, with ∆x=0.4m and i=1,2,…,8.
The time step remains constant at Δt=0.01s for the different grid sizes.
The convergence rate with grid refinement is evaluated by the RMS differences of simulated surface elevations between i and i+1 at t=5s and at the same computational points.
The convergence rate is evaluated as in Shi et al. (2001) with the following formula:(32)Rij=logRMSi/RMSjlogΔxi/Δxj
Convergence rates with grid refinement are shown in Fig. 15.
The logarithmic RMS differences linearly decrease with the grid size, so the convergence rates can be considered fairly constant.
The average Ri,j obtained is 3.59 that is compatible with the fifth-order and second-order used in the scheme.
In order to test convergence with time discretisation a sequence of time steps is considered.
As in Shi et al. (2001) the sequence is given by ∆t/i with ∆t=0.2m and i=1,2,…,10.
Spatial discretisation is constant at ∆x=0.2m for all the tests.
Fig. 16 shows convergence rates with time refinement.
The average Rij obtained is 2.33.
This value obtained with the proposed third-order Runge-Kutta scheme is similar to that obtained by Shi et al.(2003) with a fourth-order Adams-Bashforth-Moulton predictor-corrector scheme.
It has to be noted that unlike the latter scheme, the proposed Runge-Kutta scheme allows an adaptive time-stepping as suggested by Shi et al. (2012).
Rip current test
The ability of the proposed model to reproduce wave propagation from deep water up to the shore, including the surf zone, and breaking induced nearshore circulation, is tested against a set of experimental data performed by Hamm (1992).
These experiments were conducted in a 30×30m wave tank.
The geometry of the bottom consisted of a horizontal region of water depth 0.5m followed by a planar slope of 1:30 with a rip channel excavated along the centre line (see Fig. 17).
In this work the test case performed by Hamm (1992) related to the evolution in the above-mentioned tank of a monochromatic wave with a period T=1.25s and height H0=0.07m is numerically reproduced.
The tank presents an axis of symmetry perpendicular to the wave generator, consequently only half of the wave tank is numerically simulated to save computational time.
Waves are internally generated at x=0m and a wide absorbing layer is set behind the generation zone, according to the procedure proposed by Wei et al. (1999).
A wet and dry technique is used to catch moving shoreline: if the water-depth at the centre of the computational cell is greater than a given threshold value the fluxes at the cell interfaces are computed using the wet bed solution of the Riemann problem, otherwise the fluxes at the cell interfaces are computed using the dry bed solution of the Riemann problem (Toro, 2001).
Simulations are carried out with the same constant friction factor, fw=0.03, suggested by Sørensen et al. (1998) and Tonelli and Petti (2012).
Reflective boundary conditions are used at lateral walls.
The simulations are run for 520s with a time step Δt=0.005s.
The simulations are carried out on three different computational grids: a Cartesian grid with Δx=0.05m and Δy=0.075m, a highly distorted grid obtained by deforming the above-mentioned Cartesian grid following the procedure proposed by Visbal and Gaitonde (2002) (see Fig. 18) and a curvilinear boundary conforming grid (see Fig. 19).
Let us indicate with FNBE the scheme obtained by applying the numerical integration procedure presented in Section 4 to the contravariant motion Eqs.
(19) and (21) that are accurate up to O(μ2) and O(εμ2) and include the term related to the approximation to the second order of the vertical vorticity.
Let us indicate with NBE the scheme obtained by applying the same numerical integration procedure to the contravariant motion equations obtained by neglecting in Eqs.
(19) and (21) the O(εμ2) contribution to the dispersive terms and the O(μ2) contribution to the vertical vorticity.
The neglected terms are given in Appendix B and consist of: a) all the terms that multiply η on the right-hand side of Eq.
(B.1); b) the third term on the right-hand side of Eq.
(B.3) and c) terms defined by Eqs.
(B.4), (B.5) and (B.6).
The equations resulting from neglecting the above-mentioned terms are accurate only up to O(μ2), do not include the term related to the approximation to the second order of the vertical vorticity and are consistent with the Boussinesq equations proposed by Nwogu (1993) in a Cartesian reference system.
Fig. 20 shows a snapshot of the wave field at t=190s, when the breaking induced circulations are fully developed, carried out by applying the FNBE scheme on the highly distorted grid illustrated in Fig. 18.
It must be emphasised that the Christoffel symbols, expressed by Eq.
(9), that appear in the contravariant formulation of the Boussinesq Eqs.
(13) and (14) are extra source terms that come in with the variability of the contravariant base vectors.
The numerical discretization of these terms introduces computational errors related to mesh non-uniformities that can affect the numerical solution.
The integral contravariant formulation of the fully non-linear Boussinesq equations, expressed by Eqs.
(19) and (21), is free of Christoffel symbols.
From Fig. 20 it is possible to highlight that spurious oscillations do not appear in the numerical solution even in domain regions characterised by a highly distorted computational grid.
Consequently it can be deduced that numerical solutions resulting from the integration of the contravariant equations in which Christoffel Symbols are absent are not affected by numerical errors that come in with the distortion of the grid cells.
In Fig. 21 the instantaneous surface elevation over the bathymetry obtained by applying the FNBE scheme on the curvilinear grid illustrated in Fig. 19 is shown.
The presence of the channel causes the occurrence of a rip current.
This current flowing offshore-ward interacts with the incoming waves.
As shown in Fig. 21, the effects of this interaction are an increment in wave height and a local bend in the wave fronts at the channel location.
In order to verify the need to retain terms up to O(μ2) and O(εμ2) in variable depth power expansion and the term related to the approximation to the second order of the vertical vorticity in the Boussinesq equations, the numerical results obtained with the FNBE scheme are compared with the results obtained with the NBE scheme.
Wave heights computed with the FNBE and NBE schemes on the three above-mentioned computational grids are compared with the wave heights measured by Hamm (1992) along two cross-sections: one inside the rip channel (see section A-A in Fig. 17) and one in the part of the domain characterised by linearly varying bottom elevation (see section B-B in Fig 17).
Figs. 22 and 23 show the results related respectively to these two sections and carried out by the FNBE and NBE scheme on the Cartesian grid.
It can be noted that using the NBE scheme the peak wave height at breaking is overestimated, especially for the rip channel section where the rip current opposing the incoming waves makes them steeper.
Numerical results carried out by the FNBE scheme show a better agreement with the experimental data.
In Figs.
24 and 25 the results, for sections A-A and B-B respectively (Fig. 17), obtained by applying the FNBE and NBE schemes on the distorted grid illustrated in Fig. 18 are compared with the experimental data: wave height profiles for the distorted grid are similar to those shown in Figs.
22 and 23 obtained by applying the FNBE and NBE schemes on the Cartesian grid.
From Figs.
24 and 25 it can once again be deduced that the wave transformation in the shoaling and breaking region is not altered by the highly distorted computational grid.
In Figs.
26 and 27 the wave heights respectively for the two cross-sections computed with the FNBE and NBE schemes applied on a curvilinear boundary conforming grid, illustrated in Fig. 19, are compared with the experimental data.
Wave heights computed on the curvilinear grid are similar to those already shown for the application of the FNBE and NBE scheme on the Cartesian grid (see Figs.
22 and 23) and on the highly distorted grid (see Figs.
24 and 25).
Consequently it has been shown that the retaining in the motion equations of the O(εμ2) contribution to the dispersive terms and the O(μ2) contribution to the vertical vorticity, gives rise to numerical results in a better agreement with the experimental data with respect to those obtained by neglecting them.
Fig. 28 shows the time-averaged velocity field and the mean water elevation carried out by applying the FNBE scheme on the curvilinear grid illustrated in Fig. 19.
In this figure it can be seen that the presence of the rip channel makes the waves break at different positions in the cross-shore direction.
The waves propagating on the part of the domain characterised by the linearly varying bottom elevation (section B-B in Fig. 17) and water depths lower than those present at the rip channel (section A-A in Fig. 17) start to break earlier than the waves propagating on the rip channel.
Different values of breaking induced wave setup cause gradients in mean water level elevation driving alongshore currents that turn offshore-ward producing the rip current at the channel location.
This figure shows the occurrence of a clockwise vortex, with the centre approximately at coordinates x=22m and y=14m.
Furthermore, it is possible to identify the occurrence of small vortices, located around the abscissa x=19m and between the ordinates y=5m and y=9m, that are not present in the velocity field obtained with the NBE.
In Fig. 29 a comparison between the time-averaged velocity field obtained with the FNBE and NBE schemes is shown.
From this figure it can be seen that the clockwise vortex, with the centre approximatively at coordinates x=22m and y=14m, resulting from the application of the FNBE scheme is larger than the one resulting from the application of the NBE scheme.
Furthermore, the NBE scheme produces longshore currents characterised by higher velocity values than those produced by the FNBE scheme: the maximum values for the alongshore velocities are respectively of 0.28m/s and 0.25m/s.
The presence in the motion equations of the term related to approximation to the second order of the vertical vorticity provides results that best represent the articulated and complex nature of the breaking induced nearshore circulations.
Conclusions
In this paper an integral form of the fully non-linear Boussinesq equations in contravariant formulation has been proposed in which Christoffel symbols are avoided in order to simulate wave transformation phenomena, wave breaking and nearshore currents in computational domains representing the complex morphology of real coastal regions.
The motion equations include the term related to the approximation to the second order of the vertical vorticity.
A new Upwind Weighted Essentially Non-Oscillatory scheme for the solution of the fully non-linear Boussinesq equations on generalised curvilinear coordinate systems has been proposed.
A high order shock capturing method in which an exact Riemann solver is involved has been used to intrinsically model the wave breaking.
It has been demonstrated that the proposed numerical scheme applied to the contravariant fully non-linear Boussinesq equations in which Chrystoffel symbols are absent has good non-oscillatory properties and good shock-capturing properties on highly distorted grids.
It has moreover been demonstrated that the proposed model permits us to represent, with good agreement with the experimental data, the wave evolution from deep waters up to the shoreline and the breaking induced nearshore currents on computational domains representing the complexity of real coastlines.
Furthermore, retaining in the motion equations the high order dispersive terms and the term related to the approximation to the second order of the vertical vorticity gives rise to simulations whose results are in a better agreement with the experimental data with respect to those obtained by neglecting them.
Cartesian expressions of the terms: s→, V→, V→', V→'' and W→
(A.1)s→=Hσ22-16h2-hη+η2∇∇⋅r→H+σ+12h-η∇∇⋅hr→H(A.2)V→=12σ2∇∇⋅∂∂tr→H+σ∇∇⋅h∂∂tr→H-∇12η2∇⋅∂∂tr→H+η∇⋅h∂∂tr→H(A.3)V→'=12σ2∇∇⋅r→H+σ∇∇⋅hr→H-∇12η2∇⋅r→H+η∇⋅hr→H(A.4)V→''=∇∂∂tη22∇⋅r→H+∇∂η∂t∇⋅hr→H(A.5)T→=∇σ-ηr→H⋅∇∇⋅hr→H+12σ2-η2r→H⋅∇∇⋅r→H+12∇∇⋅hr→H+η∇⋅r→H2(A.6)W→=i^Ω-syH-Θ-ryH+j^ΩsxH-ΘrxHin which: Ω=∂∂xryH-∂∂yrxH and Θ=∂∂xsxH-∂∂ysyH
Contravariant expression of the terms: sl, Vl, V'l, V''l, Tl and Wl
(B.1)sl=h+ησ22-16h2-hη+η2glmrkH,k,m+σ+12h-ηglmhrkH,k,m(B.2)Vl=σ22glm∂∂trkH,k,m+σglmh∂∂trkH,k,m-glm12η2∂∂trkH,k+ηh∂∂trkH,k,m(B.3)V'l=12σ2glmrkH,k,m+σglmhrkH,k,m-glm12η2rkH,k+ηhrkH,k,m(B.4)V''l=glm∂∂tη22rkH,k,m+glm∂η∂thrkH,k,m(B.5)Tl=glmσ-ηriHhrkH,k,i+12σ2-η2riHrkH,k,i,m+12glmhrkH,k+ηrkH,k2,m(B.6)Wl=εmigipr,mpHεjlsjH+εmigips,mpHεjlrjHin which: εmi=1gifmiisanevenpermutationof(1,2)-1gifmiisanoddpermutationof(1,2)0ifthetwoindicesareequal

1. A production method for producing transition metal composite hydroxide particles by a crystallization reaction to be a precursor for a cathode active material for a non-aqueous electrolyte rechargeable battery, comprising:
a nucleation process for performing nucleation by controlling an aqueous solution for nucleation that includes a metal compound that includes at least a transition metal and an ammonium ion donor so that the pH value at a standard liquid temperature of 25° C. becomes 12.0 to 14.0; and
a particle growth process for causing nuclei to grow by controlling an aqueous solution for particle growth that includes the nuclei that were obtained in the nucleation process so that the pH value is less than in the nucleation process and is 10.5 to 12.0;
the reaction atmosphere in the nucleation process and at the beginning of the particle growth process being a non-oxidizing atmosphere in which an oxygen concentration is 5% by volume or less; and
in the particle growth process, atmosphere control by which the reaction atmosphere is switched from the non-oxidizing atmosphere to an oxidizing atmosphere in which the oxygen concentration is greater than 5% by volume at timing from the start of the particle growth process within a range of 5% to 35% of the overall particle growth process time, and is then switched from the oxidizing atmosphere to a non-oxidizing atmosphere in which the oxygen concentration is 5% by volume or less so that the crystallization time in the oxidizing atmosphere in the particle growth process is 3% to 20% of the overall particle growth process time being performed at least one time.4. (canceled)5. The production method for producing transition metal composite hydroxide particles according to claim 1, wherein the transition metal composite hydroxide particles are transition metal composite hydroxide particles that are expressed by the general expression (A): NixMnyCozMt(OH)2+a, where x+y+z+t=1, 0.3≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, 0≤a≤0.5, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W.6. The production method for producing transition metal composite hydroxide particles according to claim 5, further comprising a coating process for coating the transition metal composite hydroxide particles with a compound that includes the additional element M after the particle growth process.7. Transition metal composite hydroxide particles that are the precursor for cathode active material for a non-aqueous electrolyte rechargeable battery, comprising secondary particles that are formed by an aggregation of plate-shaped primary particles and fine primary particles that are smaller than the plate-shaped primary particles;
the secondary particles having a center section that is formed by an aggregation of the plate-shaped primary particles, and one layered structure of a low-density section that is formed by an aggregation of the fine primary particles and a high-density section that is formed by an aggregation of the plate-shaped primary particles on the outside of the center section;
the average value of the ratio of the center section outer diameter with respect to the particle size of the secondary particles being 30% to 80%, and the average value of the high-density section radial direction thickness with respect to the particle size of the secondary particles being 5% to 25%; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90-d10)/average particle size] that indicates the extent of the particle size distribution of 0.65 or less.8. Transition metal composite hydroxide particles that are the precursor for cathode active material for a non-aqueous electrolyte rechargeable battery, comprising secondary particles that are formed by an aggregation of plate-shaped primary particles and fine primary particles that are smaller than the plate-shaped primary particles;
the secondary particles having a center section that is formed by an aggregation of plate-shaped primary particles, and two or more layered structure of a low-density section that is formed by an aggregation of the fine primary particles and a high-density section that is formed by an aggregation of the plate-shaped primary particles on the outside of the center section; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90-d10)/average particle size] that indicates the extent of the particle size distribution of 0.65 or less.9. The transition metal composite hydroxide particles according to claim 8, wherein the average value of the ratio of the center section outer diameter with respect to the particle size of the secondary particles is 20% to 70%, and the average value of the high-density section radial direction thickness per layer with respect to the particle size of the secondary particles is 5% to 25%.10. The transition metal composite hydroxide particles according to claim 7, wherein the transition metal composite hydroxide particles are transition metal composite hydroxide particles that are expressed by the general expression (A): NixMnyCozMtt≤(OH)2+a, where, x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, 0≤a≤0.5, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W).11. The transition metal composite hydroxide particles according to claim 10, wherein the additional element is uniformly distributed inside the secondary particles and/or coated on the surface of the secondary particles.12. A production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery comprising:
a mixing process for forming a lithium mixture by mixing the transition metal composite hydroxide particles according to claim 7 with a lithium compound; and
a calcination process for performing calcination of the lithium mixture formed in the mixing process at a temperature of 650° C. to 980° C. in an oxidizing atmosphere.13. The production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 12, wherein in the mixing process the lithium mixture is adjusted so that the ratio of the sum of the number of atoms of metals other than lithium included in the lithium mixture, and the number of atoms of lithium is 1:0.95 to 1.5.14. The production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 13, further comprising a heat treatment process for heat treating the transition metal composite hydroxide particles at 105° C. to 750° C. before the mixing process.15. The production method for producing cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 12, wherein the cathode active material comprises layered hexagonal crystal lithium nickel manganese composite oxide particles that are expressed by the general expression (B): Li1+uNixMnyCozMtO2, where −0.05≤u≤0.50, x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W.16. Cathode active material for a non-aqueous electrolyte rechargeable battery comprising secondary particles that are formed by an aggregation of plural primary particles,
the cathode active material comprising layered hexagonal crystal lithium nickel manganese composite oxide particles that are expressed by the general expression (B): Li1+uNixMnyCozMtO2, where −0.05≤u≤0.50, x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W,
the secondary particles having a center section having solid or hollow structure, and at least a hollow section where there are no primary particles and an outer-shell section that is electrically connected to the center section on the outside of the center section;
the average value of the ratio of the center section outer diameter with respect to the particle size of the secondary particles being 30% to 80%, and the average value of the ratio of the outer-shell section radial direction thickness being 5% to 25%; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90-d10)/average particle size] that indicates the extent of the particle size distribution of 0.7 or less.17. Cathode active material for a non-aqueous electrolyte rechargeable battery comprising secondary particles that are formed by an aggregation of plural primary particles,
the cathode active material comprising layered hexagonal crystal lithium nickel manganese composite oxide particles that are expressed by the general expression (B): Li1+uNixMnyCozMtO2, where, −0.05≤u≤0.50, x+y+z+t=1, 0.3≤x≤0.95, 0.05≤y≤0.55, 0≤z≤0.4, 0≤t≤0.1, and M is one or more additional element that is selected from among Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta and W,
the secondary particles comprising a center section, and a hollow section where there are no primary particles, an outer-shell section that is electrically connected to the center section, and at least one inner-shell section between the hollow section and the outer-shell section on the outside of the center section; and
the secondary particles having an average particle size of 1 μm to 15 μm, and an index [(d90-d10)/average particle size] that indicates the extent of the particle size distribution of 0.7 or less.18. The cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 17, wherein the average value of the ratio of the center section outer diameter with respect to the particle size of the secondary particles is 20% to 70%, and the average value of the ratio of the total thickness in the radial direction of the inner-shell section and the outer-shell section with respect to the particles size of the secondary particles is 10% to 35%.19. The cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 16, wherein the specific surface area is 0.7 m2/g to 3.0 m2/g.20. (canceled)21. A non-aqueous electrolyte rechargeable battery comprising a cathode, an anode, a separator and a non-aqueous electrolyte, wherein the cathode active material for a non-aqueous electrolyte rechargeable battery according to claim 16 is used as cathode material for the cathode.Structural studies of magnesium nitride fluorides by powder neutron diffraction

Abstract
Samples of ternary nitride fluorides, Mg3NF3 and Mg2NF have been prepared by solid state reaction of Mg3N2 and MgF2 at 1323-1423K and investigated by powder X-ray and powder neutron diffraction techniques.
Mg3NF3 is cubic (space group: Pm3m) and has a structure related to rock-salt MgO, but with one cation site vacant.
Mg2NF is tetragonal (space group: I41/amd) and has an anti-LiFeO2 related structure.
Both compounds are essentially ionic and form structures in which nitride and fluoride anions are crystallographically ordered.
The nitride fluorides show temperature independent paramagnetic behaviour between 5 and 300K.
Graphical abstract
Definitive structures of the ternary magnesium nitride fluorides Mg3NF3 and the lower temperature polymorph of Mg2NF have been determined from powder neutron diffraction data.
The nitride halides are essentially ionic and exhibit weak temperature independent paramagnetic behaviour.
Highlights
► Definitive structures of Mg3NF3 and Mg2NF were determined by neutron diffraction.
► Nitride and fluoride anions are crystallographically ordered in both structures.
► Both compounds exhibit weak, temperature independent paramagnetic behaviour.
► The compounds are essentially ionic with ionicity increasing with F- content.

Introduction
Metal nitride fluorides are a relatively unexplored class of inorganic materials [1,2].
Andersson referred to these as "pseudo-oxides" as they have compositions derived from oxides effectively by the substitution of O2- ions by N3- and F- ions [3].
The first examples of these were magnesium nitride fluorides prepared by Andersson [4,5] and three phases were reported which had structures related to the rocksalt structure of MgO.
Mg3NF3 is cubic with N3- and F- crystallographically ordered and one quarter of the Mg2+ sites are empty in an ordered fashion.
L-Mg2NF, a low temperature polymorph, is tetragonal with ordered anions and contains magnesium in square-pyramidal co-ordination.
The high temperature polymorph, H-Mg2NF, is observed after treating L-Mg2NF at high temperature and pressure (1100-1350°C; 25-30kbar).
It is cubic and isostructural with MgO with the N3- and F- anions disordered.
Magnesium oxide, MgO, and related compounds have found considerable applications as refractory ceramics [6], catalyst supports [7-9] and recently as additives in the firing of biomass fuels [10].
Following the discovery of the magnesium nitride fluorides, investigations by Ehrlich et al. produced further alkaline earth metal compounds, Ca2NF, Sr2NF and Ba2NF, which were all reported from powder diffraction evidence to be isostructural with the respective group 2 rocksalt-type oxides [11].
Galy et al. also reported Ca2NF as a rocksalt structure from powder measurements [12].
More recent single crystal X-ray diffraction experiments have, however, identified alternative structures for some of these compounds.
Nicklow et al. reported a Ca2NF phase isostructural with L-Mg2NF [13].
Under slightly different conditions the same group reported a Ca2NF phase with a structure in which the simple rock salt cubic cell parameter is doubled due to the anion ordering of N3- and F- [14].
This doubled, ordered rock salt-type structure is also observed for Sr2NF [15], but as one progresses further down group 2, single crystal studies reveal that the equivalent barium compound, Ba2NF, forms with a simple NaCl structure with disordered anions, in agreement with the earlier work of Ehrlich et al. [16].
Recently Ba2NF was reported to form also with a layered anti-α-NaFeO2 structure from powder neutron diffraction measurements [17].
This structure consists of a cubic close packed arrangement of Ba2+ cations, with the N3- and F- anions ordered over the octahedral interstices to produce alternating layers comprising either edge-sharing NBa6 or FBa6 octahedra-a structure type more usually observed for alkaline earth metal nitride chlorides, bromides and iodides [18].
Rare earth nitride fluorides were also discovered in the early 1970s.
Tanguy et al. initially reported compounds in the solid solution LaNxF3-3x with a stability range x=0.34-0.54 [19].
Cerium forms an analogous solid solution CeNxF3-3x otherwise differing from La only in the solubility range (x=0.33-0.50) [20].
By contrast, Gadolinium forms only the line phase, Gd3NF6 [21] and more recently Pr3NF6 and Ce3NF6 were prepared and studied by neutron diffraction [22].
The structures of these compounds are related to fluorite, although all show an anion excess and for Ln=Ce and Pr, neutron diffraction has indicated the existence of fluorine interstitials.
Variation in stoichiometry, structure and anion distribution within ternary nitride halide systems is likely to have profound effects on electronic properties, but reports on this are limited.
DFT calculations performed by Fang et al. on compounds in the Mg-N-F ternary system show a decreasing band gap from insulating MgF2 (calculated direct band gap, Eg, of 6.8eV) through Mg3NF3 (Eg=3.6eV) and L-Mg2NF (Eg=2.1eV) to semiconducting Mg3N2 with a calculated direct band gap of 1.6eV (2.8eV experimentally) [23].
In this work we report the solid state synthesis of the nitride fluoride Mg3NF3 and the low temperature polymorph of the dimagnesium nitride fluoride, L-Mg2NF and provide detailed models of their crystal structures from powder neutron diffraction experiments.
Powder neutron diffraction has allowed us to establish the distribution of nitride and fluoride in these "pseudo-oxides" without ambiguity and to draw conclusions as to the type of bonding in these compounds.
We also report the magnetic behaviour of the nitride fluorides.
Experimental details
Starting materials
Magnesium nitride, Mg3N2 was prepared by reaction of the pure metal (Alfa 99.9%) with nitrogen using liquid sodium (Riedel-Haën, >99%) as a solvent.
All manipulations were carried out in an inert atmosphere.
In an argon-filled glovebox a piece of magnesium (ca.
15g) was cut from a larger ingot and the covering oxide layer physically removed with a file.
The clean magnesium metal was submerged in molten sodium contained within a stainless steel crucible.
The crucible was then sealed inside a stainless steel reaction vessel fitted with a cold finger.
The vessel was heated for 72h at 1023K under 1bar of nitrogen pressure.
Upon cooling, the vessel was placed under a vacuum of 10-4Torr and heated for 24h at 823K to remove the sodium.
Once cooled to room temperature, the vessel was loaded into a glovebox and opened in a nitrogen atmosphere.
Mg3N2 was obtained as a yellow-green powdered solid.
Phase purity was confirmed using powder X-ray diffraction by comparison to ICDD PDF database (card number 35-778; Supplementary Fig. S1).
Nitride fluoride synthesis
The ternary magnesium nitride fluorides were prepared by the high temperature solid state reaction of stoichiometric amounts of synthesised Mg3N2 with anhydrous MgF2 (Aldrich, 99.9%).
All manipulations were carried out under inert nitrogen or argon atmospheres.
The appropriate 1:3 and 1:1 stoichiometric molar ratios were used in the synthesis of Mg3NF3 (1) and Mg2NF (2) respectively, as shown below:(1)Mg3N2+3MgF2→2Mg3NF3(2)Mg3N2+MgF2→2Mg2NF
In an N2-filled recirculating glovebox, mixtures of the starting materials were thoroughly mixed and ground together using an agate pestle and mortar, after which the samples were pressed into pellets using a hand press inside the glove box.
The pellets were then transferred to an evacuable glovebox with a purified argon atmosphere and wrapped in molybdenum foil liners, which serve to prevent unwanted side reactions between the pellet and the stainless steel crucible.
The foil-wrapped pellets were sealed inside the stainless steel crucibles using an arc welder within an argon-filled glove box.
The sealed crucibles were transferred to a high temperature tube furnace and heated for 5 days at 1323-1423K under flowing argon to prevent aerial oxidation.
The reaction vessels were slowly cooled at 20Kh-1 to room temperature following heating and opened in a nitrogen filled glovebox using pipe cutters.
Characterisation by powder X-ray diffraction (PXD)
All products were initially characterised by PXD.
Data were collected using a Philips X-pert diffractometer operating with CuKα radiation.
Initial scans were carried out from 5≤2θ/°≤80 with a step size of 0.02° and a scan time of 50min.
Due to the air-sensitive nature of the products, a dedicated air-tight aluminium sample holder with Mylar windows was employed [24].
Characterisation by powder neutron diffraction (PND)
Time of flight (ToF) PND data were collected using the medium resolution, high intensity POLARIS diffractometer at the ISIS facility, Rutherford Appleton Laboratory.
For each sample, ca.
2g of material was sealed in a vanadium can, made airtight using an indium wire gasket.
All data were collected at 298K with collection times of 1-2h.
Diffraction data collected using the low angle (〈2θ〉=35°; d-spacing range ∼0.5-8.1Å), backscattering (〈2θ〉=145°; d-spacing range ∼0.5-3.2Å) and 〈2θ〉=90° detector bank (d-spacing range ∼0.5-4.2Å) were used for structure refinement.
Structure refinement
Rietveld refinement using both the PXD and ToF PND data was performed using the General Structure Analysis System (GSAS) through the windows based EXPGUI interface [26,27].
The overall refinement strategy was largely identical when using either PXD or PND data and employed selected background and peak shape functions as appropriate for the different instruments.
For the PXD refinements, the initial starting models used for 1 and 2 were those proposed by Andersson for Mg3NF3 and L-Mg2NF, respectively [3].
The background coefficients (GSAS Function 7-a linear interpolation function), zeropoint and scale factor were varied initially, followed by refinement of the unit cell parameters.
Modelling of the peak shapes was carried out using Function 2, a Simpson's rule integration of the pseudo-Voigt function.
The atomic parameters, peak profile parameters and isotropic thermal vibration parameters were subsequently refined.
Impurity phases were added once refinement of the main phase was almost complete.
The anisotropic thermal vibration parameters of the main phase were one of the last variables to be refined, whilst in the final refinement cycles all parameters were simultaneously varied until convergence was achieved.
For the POLARIS PND refinements, the appropriate starting models derived from the PXD experiments above were refined against data from the three detector banks.
Initially data from the backscattering bank were used for each refinement, with the low angle and 90° histograms added later in the refinement.
Initially background coefficients (GSAS Function 6-a combination of power series to account for background contributions at low and high Q) and the scale factor were refined.
Unit cell parameters and peak shape were varied subsequently.
(Modelling of the peak shapes was carried out using ToF peak shape function 3, a convolution of back-to-back exponentials with a pseudo-Voigt function.) Absorption coefficients, atomic positions, and isotropic thermal vibration parameters were refined once stability and convergence was achieved.
Impurity phases were added once refinement of the main phase was almost complete in each case.
The anisotropic thermal vibration parameters of the main phase were refined once all other parameters were stable, whilst in the final refinement cycles all parameters were simultaneously varied.
Magnetic measurements
Variable temperature magnetic susceptibility measurements were performed for both 1 and 2 (ca.
10-25mg for each sample) using a Quantum Design MPMS-XL 5T SQUID magnetometer.
All samples were loaded in a nitrogen-filled, recirculating glovebox.
Data were collected between 5 and 300K under fields of 1000Oe with points at 1K intervals from 5 to 30K and 5K intervals between 30 and 300K.
Data were corrected for core diamagnetism and the diamagnetic contribution of the sample holders (gelatine capsules).
Results and discussion
Mg3NF3 (1)
An off-white powder, Mg3NF3 (1) was successfully synthesised at 1323K.
Initial powder diffraction measurements produced a pattern that matched the ICDD database entry (PDF Card number 25-0517).
Indexing of the peaks gave a cubic unit cell with a=4.2153(4)Å, which is in good agreement with the literature value of 4.216Å [3].
Longer PXD scans revealed the presence of a small impurity of the MgF2 reagent, which was included as a secondary phase in the final cycles of the refinement and refined to a final phase fraction of 1.1(2)wt%.
Given the similarity in form factor for isoelectronic N3- and F-, obtaining a definitive structure for 1 in which partial site disorder could be categorically discarded from PXD data alone was not possible.
PND data were collected to address this problem as there is sufficient contrast in the coherent neutron scattering lengths to allow a convincing solution to be obtained (N: b=9.36fm, F: b=5.65fm) [28].
Starting from the refined PXD model, the refinement progressed smoothly and converged to produce a good fit to the data.
Attempts to refine occupancies of the anion sites to investigate possible N/F disorder reduced the quality of the fit, increased R-factors and caused some parameters to become unstable, indicating an ordered model in which N3- occupies only the 1b (1/2, 1/2, 1/2) site and F- the 3d (1/2, 0, 0) site to be correct.
The details of the both PXD and PND refinements can be found in Table 1.
Important interatomic distances can be found in Table 2.
A fitted profile plot for the PND data for the backscattering bank (〈2θ〉=145°) is shown in Fig. 1.
(Additional refinement fits and a table of anisotropic thermal displacement parameters can be found in the Supplementary information.)
The structure of 1 (Fig. 2) is related to that of MgO, with N3- and F- replacing O2- in an ordered fashion in the cubic unit cell.
Perhaps the key difference between 1 and MgO lies with the cation-anion ratio and hence defect structure.
In MgO there is a 1:1 ratio of cations to anions with Mg located on the 4a site in the ideal rock salt structure (space group Fm-3m) whereas in Mg3NF3 there is a 3:4 ratio of cations to anions.
In the resulting primitive cell of the nitride fluoride (space group Pm-3m), Mg is situated on the 3c (0, 1/2, 1/2) site with the (0, 0, 0) site of the rock salt structure vacant.
Magnesium is octahedrally co-ordinated to two nitride anions and four fluoride anions.
The Mg-N (Mg-F) bond distance is 2.1070 (1)Å, which is in excellent agreement with the literature value of 2.108 (1)Å [3].
This is also very close to the Mg-O bond length of 2.1061 (1)Å in the rock salt structure of the oxide [29].
The distance also falls within the range of Mg-N bonds seen in the anti-bixbyite Mg3N2 (2.084 (1)-2.179 (1)Å) [30] although is slightly longer than the Mg-F bond length in the binary fluoride (1.984 (2)-1.994 (1)Å) [31].
The Mg-Mg distance is 2.980 (1)Å which is longer than in Mg3N2 (2.716 (1)Å) but shorter than that found in MgF2 (3.045 (3)Å) and Mg metal itself (3.1903 (1)-3.2025 (1)Å) [32].
Bond valence calculations using the PND structure as a basis were performed using the VALENCE software [33].
Applying the parameters of Brese and O'Keeffe (Rij(Mg-N)=1.85, Rij(Mg-F)=1.58, b=0.37) [34] gives sums of 1.96 for Mg, 3.00 for N and 0.97 for F.
The results would thus imply that the bonding in M3NF3 is very close to purely ionic and that the Mg-N bond in the nitride fluoride is among the most ionic metal-nitrogen bonds in the solid state.
Similarly the Mg…Mg interaction cannot be viewed as so significant in Mg3NF3 as it appears to be in Mg3N2 [24].
Certainly DFT calculations would also suggest that with F 2p, N 2p and unoccupied 3s states well separated the nitride fluoride can be viewed as ionic [17].
Mg2NF (2)
The synthesis of a pale cream powder of 2 was achieved at 1423K.
Initial experiments had shown that a higher temperature was required for formation of 2 otherwise a small impurity of 1 was always observed.
Powder diffraction measurements showed a match to the pattern for the previously reported low temperature phase of Mg2NF (ICDD Card number 25-0516).
The reflections could be indexed to a tetragonal unit cell with a=4.1816(4)Å and c=10.0322(6)Å which compares well with the literature values of a=4.186Å and c=10.042Å [3].
As for 1, initial Rietveld refinement of 2 was performed against PXD data.
The refinement progressed smoothly and gave a good fit to the data using the L-Mg2NF structure as a starting model.
PND data were collected for 2 and a refinement taking the PXD model as the starting point progressed smoothly and converged to produce a good fit to the data.
As with 1, possible disorder was investigated by attempting to vary the anion occupancies but doing so always resulted in increases to the residuals and/or physically unrealistic parameters.
Parameters from both the PXD and PND refinements of 2 can be found in Table 3.
Important interatomic distances are displayed in Table 4.
The profile plot for the refinement against PND data for the backscattering bank (〈2θ〉=145°) is shown in Fig. 3.
(Further PXD and PND refinement results and a table of anisotropic thermal displacement parameters can be found in the Supplementary information.)
The structure adopted by 2 is anti-LiFeO2 type (Fig. 4), where F- occupies the equivalent Fe3+ position, N3- occupies the equivalent Li+ position and Mg2+ is located on the equivalent anion (O2-) site.
The structure can be viewed as an intermediate structure between that of rock salt and zinc blende.
The magnesium cation in 2 is co-ordinated to three nitride and two fluoride anions in a square pyramidal co-ordination.
An additional, longer Mg-F interaction completes a distorted, axially extended octahedron (Fig. 5).
The Mg-N/F bond distances in the pyramid base are 2.1253 (1)Å with the apical Mg-N bond at 2.1393 (1)Å.
As for 1 these fall within typical Mg-N bond length ranges seen in Mg3N2 (2.084 (1)-2.179 (1)Å), and are slightly longer than Mg-F bonds in MgF2 (1.984 (2)-1.994 (1)Å) [24,25].
The additional Mg-F distance lies at 2.8730 (1)Å.
The Mg-Mg distance is 2.7430 (1)Å, which is closer to that in Mg3N2 (2.716 (1)Å) than in MgF2 (3.045 (3)Å) or Mg metal (3.1903 (1)-3.2025 (1)Å).
Bond valence calculations performed using the structure refined from PND data and the same rij and b parameters used for Mg3NF3 above [28], gave sums of 1.90 for Mg, 2.85 for N and 0.92 for F.
These values are slightly lower than expected and lower than those found in Mg3NF3, above, but nevertheless suggest a strong ionic component to bonding in Mg2NF.
"Underbonding" is not uncommon in nitridic compounds and given, for example, the low values reported for Mg and N in Mg3N2, it is likely that Mg…Mg nonbonding interactions could play a role in Mg2NF as they do in the binary magnesium nitride [24].
It is not totally surprising, therefore, that the members of the Mg-N-F system become more ionic as one increases the fluoride content from Mg3N2 through the nitride fluorides Mg2NF and Mg3NF3 to MgF2.
Bond valence figures can also demonstrate how weak the additional axial Mg-F interaction is.
In MgF2 the bond valence of a single Mg-F bond is 0.336.
In 1 the bond valence of Mg-F is 0.241.
The square pyramid Mg-F bond in 2 gives a bond valence of 0.230 but the long range Mg-F bond of 2 has a bond valence of only 0.03.
Therefore it is tempting to dismiss this connection as a relatively weak interaction rather than classify it as an Mg-F bond.
Analysis of the anisotropic thermal displacement parameters shows that the ellipsoid for the fluorine position is elongated in the c-axis, suggestive of some measure of (local) anion disorder or possibly indicative of fluoride ion mobility.
This feature is also seen in the isostructural Ca2NF [11], where the authors carried out refinements with partial oxygen substitution on the F site in an attempt to alleviate the large displacement parameters.
In the case of Ca2NF this model was rejected as it worsened the fit.
Attempts to include partial oxygen substitution in our model for 2 also decreased fit quality, and when freely refined, fluorine occupancy always refined back to unity.
Magnetic measurements
SQUID magnetometry measurements revealed that both samples 1 and 2 exhibit weak temperature independent paramagnetic behaviour (typically χM∼5×10-4emumol-1) between 5 and 300K.
This behaviour is commensurate with other alkaline earth nitride halides and suggests either weakly paramagnetic materials or intrinsically diamagnetic materials with very small levels of alkaline earth metal impurities (below the detection limit of PXD and PND) [15,16].
(A plot of χM vs. T for 1 and 2 is available as Supplementary information.)
Summary
Two ternary nitride fluorides of magnesium, Mg3NF3 and Mg2NF, have been synthesised via the solid state reaction of appropriate ratios of Mg3N2 and MgF2.
Powder neutron diffraction studies have confirmed structures in which the fluoride and nitride anions are completely ordered in both cases.
Bond valence calculations reveal that both these nitride fluorides contain among the most ionic metal-nitrogen bonds in the solid state.
It is worthy of note that the original work of Andersson provided extremely accurate models for both structures given the stage of development of powder refinement methods at the time of publication [3].
Magnetic measurements demonstrate that both nitride fluorides exhibit very weak paramagnetism with susceptibilities essentially independent of temperature.
Acknowledgments
Neutron beamtime at ISIS was provided by the UK Science and Technology Facilities Council (STFC).
DHG thanks the University of Nottingham for funding a studentship for M.A.B., and WestCHEM for funding R.W.H. as a WestCHEM Fellow.
Supplementary Information
Supplementary data associated with this article can be found in the online version at doi:10.1016/j.jssc.2011.11.008.
Supplementary materials
Supplementary material

Structural studies of magnesium nitride fluorides by powder neutron diffraction

Abstract
Samples of ternary nitride fluorides, Mg3NF3 and Mg2NF have been prepared by solid state reaction of Mg3N2 and MgF2 at 1323-1423K and investigated by powder X-ray and powder neutron diffraction techniques.
Mg3NF3 is cubic (space group: Pm3m) and has a structure related to rock-salt MgO, but with one cation site vacant.
Mg2NF is tetragonal (space group: I41/amd) and has an anti-LiFeO2 related structure.
Both compounds are essentially ionic and form structures in which nitride and fluoride anions are crystallographically ordered.
The nitride fluorides show temperature independent paramagnetic behaviour between 5 and 300K.
Graphical abstract
Definitive structures of the ternary magnesium nitride fluorides Mg3NF3 and the lower temperature polymorph of Mg2NF have been determined from powder neutron diffraction data.
The nitride halides are essentially ionic and exhibit weak temperature independent paramagnetic behaviour.
Highlights
► Definitive structures of Mg3NF3 and Mg2NF were determined by neutron diffraction.
► Nitride and fluoride anions are crystallographically ordered in both structures.
► Both compounds exhibit weak, temperature independent paramagnetic behaviour.
► The compounds are essentially ionic with ionicity increasing with F- content.

Introduction
Metal nitride fluorides are a relatively unexplored class of inorganic materials [1,2].
Andersson referred to these as "pseudo-oxides" as they have compositions derived from oxides effectively by the substitution of O2- ions by N3- and F- ions [3].
The first examples of these were magnesium nitride fluorides prepared by Andersson [4,5] and three phases were reported which had structures related to the rocksalt structure of MgO.
Mg3NF3 is cubic with N3- and F- crystallographically ordered and one quarter of the Mg2+ sites are empty in an ordered fashion.
L-Mg2NF, a low temperature polymorph, is tetragonal with ordered anions and contains magnesium in square-pyramidal co-ordination.
The high temperature polymorph, H-Mg2NF, is observed after treating L-Mg2NF at high temperature and pressure (1100-1350°C; 25-30kbar).
It is cubic and isostructural with MgO with the N3- and F- anions disordered.
Magnesium oxide, MgO, and related compounds have found considerable applications as refractory ceramics [6], catalyst supports [7-9] and recently as additives in the firing of biomass fuels [10].
Following the discovery of the magnesium nitride fluorides, investigations by Ehrlich et al. produced further alkaline earth metal compounds, Ca2NF, Sr2NF and Ba2NF, which were all reported from powder diffraction evidence to be isostructural with the respective group 2 rocksalt-type oxides [11].
Galy et al. also reported Ca2NF as a rocksalt structure from powder measurements [12].
More recent single crystal X-ray diffraction experiments have, however, identified alternative structures for some of these compounds.
Nicklow et al. reported a Ca2NF phase isostructural with L-Mg2NF [13].
Under slightly different conditions the same group reported a Ca2NF phase with a structure in which the simple rock salt cubic cell parameter is doubled due to the anion ordering of N3- and F- [14].
This doubled, ordered rock salt-type structure is also observed for Sr2NF [15], but as one progresses further down group 2, single crystal studies reveal that the equivalent barium compound, Ba2NF, forms with a simple NaCl structure with disordered anions, in agreement with the earlier work of Ehrlich et al. [16].
Recently Ba2NF was reported to form also with a layered anti-α-NaFeO2 structure from powder neutron diffraction measurements [17].
This structure consists of a cubic close packed arrangement of Ba2+ cations, with the N3- and F- anions ordered over the octahedral interstices to produce alternating layers comprising either edge-sharing NBa6 or FBa6 octahedra-a structure type more usually observed for alkaline earth metal nitride chlorides, bromides and iodides [18].
Rare earth nitride fluorides were also discovered in the early 1970s.
Tanguy et al. initially reported compounds in the solid solution LaNxF3-3x with a stability range x=0.34-0.54 [19].
Cerium forms an analogous solid solution CeNxF3-3x otherwise differing from La only in the solubility range (x=0.33-0.50) [20].
By contrast, Gadolinium forms only the line phase, Gd3NF6 [21] and more recently Pr3NF6 and Ce3NF6 were prepared and studied by neutron diffraction [22].
The structures of these compounds are related to fluorite, although all show an anion excess and for Ln=Ce and Pr, neutron diffraction has indicated the existence of fluorine interstitials.
Variation in stoichiometry, structure and anion distribution within ternary nitride halide systems is likely to have profound effects on electronic properties, but reports on this are limited.
DFT calculations performed by Fang et al. on compounds in the Mg-N-F ternary system show a decreasing band gap from insulating MgF2 (calculated direct band gap, Eg, of 6.8eV) through Mg3NF3 (Eg=3.6eV) and L-Mg2NF (Eg=2.1eV) to semiconducting Mg3N2 with a calculated direct band gap of 1.6eV (2.8eV experimentally) [23].
In this work we report the solid state synthesis of the nitride fluoride Mg3NF3 and the low temperature polymorph of the dimagnesium nitride fluoride, L-Mg2NF and provide detailed models of their crystal structures from powder neutron diffraction experiments.
Powder neutron diffraction has allowed us to establish the distribution of nitride and fluoride in these "pseudo-oxides" without ambiguity and to draw conclusions as to the type of bonding in these compounds.
We also report the magnetic behaviour of the nitride fluorides.
Experimental details
Starting materials
Magnesium nitride, Mg3N2 was prepared by reaction of the pure metal (Alfa 99.9%) with nitrogen using liquid sodium (Riedel-Haën, >99%) as a solvent.
All manipulations were carried out in an inert atmosphere.
In an argon-filled glovebox a piece of magnesium (ca.
15g) was cut from a larger ingot and the covering oxide layer physically removed with a file.
The clean magnesium metal was submerged in molten sodium contained within a stainless steel crucible.
The crucible was then sealed inside a stainless steel reaction vessel fitted with a cold finger.
The vessel was heated for 72h at 1023K under 1bar of nitrogen pressure.
Upon cooling, the vessel was placed under a vacuum of 10-4Torr and heated for 24h at 823K to remove the sodium.
Once cooled to room temperature, the vessel was loaded into a glovebox and opened in a nitrogen atmosphere.
Mg3N2 was obtained as a yellow-green powdered solid.
Phase purity was confirmed using powder X-ray diffraction by comparison to ICDD PDF database (card number 35-778; Supplementary Fig. S1).
Nitride fluoride synthesis
The ternary magnesium nitride fluorides were prepared by the high temperature solid state reaction of stoichiometric amounts of synthesised Mg3N2 with anhydrous MgF2 (Aldrich, 99.9%).
All manipulations were carried out under inert nitrogen or argon atmospheres.
The appropriate 1:3 and 1:1 stoichiometric molar ratios were used in the synthesis of Mg3NF3 (1) and Mg2NF (2) respectively, as shown below:(1)Mg3N2+3MgF2→2Mg3NF3(2)Mg3N2+MgF2→2Mg2NF
In an N2-filled recirculating glovebox, mixtures of the starting materials were thoroughly mixed and ground together using an agate pestle and mortar, after which the samples were pressed into pellets using a hand press inside the glove box.
The pellets were then transferred to an evacuable glovebox with a purified argon atmosphere and wrapped in molybdenum foil liners, which serve to prevent unwanted side reactions between the pellet and the stainless steel crucible.
The foil-wrapped pellets were sealed inside the stainless steel crucibles using an arc welder within an argon-filled glove box.
The sealed crucibles were transferred to a high temperature tube furnace and heated for 5 days at 1323-1423K under flowing argon to prevent aerial oxidation.
The reaction vessels were slowly cooled at 20Kh-1 to room temperature following heating and opened in a nitrogen filled glovebox using pipe cutters.
Characterisation by powder X-ray diffraction (PXD)
Characterisation by powder neutron diffraction (PND)
Time of flight (ToF) PND data were collected using the medium resolution, high intensity POLARIS diffractometer at the ISIS facility, Rutherford Appleton Laboratory.
For each sample, ca.
2g of material was sealed in a vanadium can, made airtight using an indium wire gasket.
All data were collected at 298K with collection times of 1-2h.
Diffraction data collected using the low angle (〈2θ〉=35°; d-spacing range ∼0.5-8.1Å), backscattering (〈2θ〉=145°; d-spacing range ∼0.5-3.2Å) and 〈2θ〉=90° detector bank (d-spacing range ∼0.5-4.2Å) were used for structure refinement.
Structure refinement
Rietveld refinement using both the PXD and ToF PND data was performed using the General Structure Analysis System (GSAS) through the windows based EXPGUI interface [26,27].
The overall refinement strategy was largely identical when using either PXD or PND data and employed selected background and peak shape functions as appropriate for the different instruments.
For the PXD refinements, the initial starting models used for 1 and 2 were those proposed by Andersson for Mg3NF3 and L-Mg2NF, respectively [3].
The background coefficients (GSAS Function 7-a linear interpolation function), zeropoint and scale factor were varied initially, followed by refinement of the unit cell parameters.
Modelling of the peak shapes was carried out using Function 2, a Simpson's rule integration of the pseudo-Voigt function.
The atomic parameters, peak profile parameters and isotropic thermal vibration parameters were subsequently refined.
Impurity phases were added once refinement of the main phase was almost complete.
The anisotropic thermal vibration parameters of the main phase were one of the last variables to be refined, whilst in the final refinement cycles all parameters were simultaneously varied until convergence was achieved.
For the POLARIS PND refinements, the appropriate starting models derived from the PXD experiments above were refined against data from the three detector banks.
Initially data from the backscattering bank were used for each refinement, with the low angle and 90° histograms added later in the refinement.
Initially background coefficients (GSAS Function 6-a combination of power series to account for background contributions at low and high Q) and the scale factor were refined.
Unit cell parameters and peak shape were varied subsequently.
(Modelling of the peak shapes was carried out using ToF peak shape function 3, a convolution of back-to-back exponentials with a pseudo-Voigt function.) Absorption coefficients, atomic positions, and isotropic thermal vibration parameters were refined once stability and convergence was achieved.
Impurity phases were added once refinement of the main phase was almost complete in each case.
The anisotropic thermal vibration parameters of the main phase were refined once all other parameters were stable, whilst in the final refinement cycles all parameters were simultaneously varied.
Magnetic measurements
Variable temperature magnetic susceptibility measurements were performed for both 1 and 2 (ca.
10-25mg for each sample) using a Quantum Design MPMS-XL 5T SQUID magnetometer.
All samples were loaded in a nitrogen-filled, recirculating glovebox.
Data were collected between 5 and 300K under fields of 1000Oe with points at 1K intervals from 5 to 30K and 5K intervals between 30 and 300K.
Data were corrected for core diamagnetism and the diamagnetic contribution of the sample holders (gelatine capsules).
Results and discussion
Mg3NF3 (1)
An off-white powder, Mg3NF3 (1) was successfully synthesised at 1323K.
Initial powder diffraction measurements produced a pattern that matched the ICDD database entry (PDF Card number 25-0517).
Indexing of the peaks gave a cubic unit cell with a=4.2153(4)Å, which is in good agreement with the literature value of 4.216Å [3].
Longer PXD scans revealed the presence of a small impurity of the MgF2 reagent, which was included as a secondary phase in the final cycles of the refinement and refined to a final phase fraction of 1.1(2)wt%.
Given the similarity in form factor for isoelectronic N3- and F-, obtaining a definitive structure for 1 in which partial site disorder could be categorically discarded from PXD data alone was not possible.
PND data were collected to address this problem as there is sufficient contrast in the coherent neutron scattering lengths to allow a convincing solution to be obtained (N: b=9.36fm, F: b=5.65fm) [28].
Starting from the refined PXD model, the refinement progressed smoothly and converged to produce a good fit to the data.
Attempts to refine occupancies of the anion sites to investigate possible N/F disorder reduced the quality of the fit, increased R-factors and caused some parameters to become unstable, indicating an ordered model in which N3- occupies only the 1b (1/2, 1/2, 1/2) site and F- the 3d (1/2, 0, 0) site to be correct.
The details of the both PXD and PND refinements can be found in Table 1.
Important interatomic distances can be found in Table 2.
A fitted profile plot for the PND data for the backscattering bank (〈2θ〉=145°) is shown in Fig. 1.
(Additional refinement fits and a table of anisotropic thermal displacement parameters can be found in the Supplementary information.)
The structure of 1 (Fig. 2) is related to that of MgO, with N3- and F- replacing O2- in an ordered fashion in the cubic unit cell.
Perhaps the key difference between 1 and MgO lies with the cation-anion ratio and hence defect structure.
In MgO there is a 1:1 ratio of cations to anions with Mg located on the 4a site in the ideal rock salt structure (space group Fm-3m) whereas in Mg3NF3 there is a 3:4 ratio of cations to anions.
In the resulting primitive cell of the nitride fluoride (space group Pm-3m), Mg is situated on the 3c (0, 1/2, 1/2) site with the (0, 0, 0) site of the rock salt structure vacant.
Magnesium is octahedrally co-ordinated to two nitride anions and four fluoride anions.
The Mg-N (Mg-F) bond distance is 2.1070 (1)Å, which is in excellent agreement with the literature value of 2.108 (1)Å [3].
This is also very close to the Mg-O bond length of 2.1061 (1)Å in the rock salt structure of the oxide [29].
The distance also falls within the range of Mg-N bonds seen in the anti-bixbyite Mg3N2 (2.084 (1)-2.179 (1)Å) [30] although is slightly longer than the Mg-F bond length in the binary fluoride (1.984 (2)-1.994 (1)Å) [31].
The Mg-Mg distance is 2.980 (1)Å which is longer than in Mg3N2 (2.716 (1)Å) but shorter than that found in MgF2 (3.045 (3)Å) and Mg metal itself (3.1903 (1)-3.2025 (1)Å) [32].
Bond valence calculations using the PND structure as a basis were performed using the VALENCE software [33].
Applying the parameters of Brese and O'Keeffe (Rij(Mg-N)=1.85, Rij(Mg-F)=1.58, b=0.37) [34] gives sums of 1.96 for Mg, 3.00 for N and 0.97 for F.
The results would thus imply that the bonding in M3NF3 is very close to purely ionic and that the Mg-N bond in the nitride fluoride is among the most ionic metal-nitrogen bonds in the solid state.
Similarly the Mg…Mg interaction cannot be viewed as so significant in Mg3NF3 as it appears to be in Mg3N2 [24].
Certainly DFT calculations would also suggest that with F 2p, N 2p and unoccupied 3s states well separated the nitride fluoride can be viewed as ionic [17].
Mg2NF (2)
The synthesis of a pale cream powder of 2 was achieved at 1423K.
Initial experiments had shown that a higher temperature was required for formation of 2 otherwise a small impurity of 1 was always observed.
Powder diffraction measurements showed a match to the pattern for the previously reported low temperature phase of Mg2NF (ICDD Card number 25-0516).
The reflections could be indexed to a tetragonal unit cell with a=4.1816(4)Å and c=10.0322(6)Å which compares well with the literature values of a=4.186Å and c=10.042Å [3].
As for 1, initial Rietveld refinement of 2 was performed against PXD data.
The refinement progressed smoothly and gave a good fit to the data using the L-Mg2NF structure as a starting model.
PND data were collected for 2 and a refinement taking the PXD model as the starting point progressed smoothly and converged to produce a good fit to the data.
As with 1, possible disorder was investigated by attempting to vary the anion occupancies but doing so always resulted in increases to the residuals and/or physically unrealistic parameters.
Parameters from both the PXD and PND refinements of 2 can be found in Table 3.
Important interatomic distances are displayed in Table 4.
The profile plot for the refinement against PND data for the backscattering bank (〈2θ〉=145°) is shown in Fig. 3.
(Further PXD and PND refinement results and a table of anisotropic thermal displacement parameters can be found in the Supplementary information.)
The structure adopted by 2 is anti-LiFeO2 type (Fig. 4), where F- occupies the equivalent Fe3+ position, N3- occupies the equivalent Li+ position and Mg2+ is located on the equivalent anion (O2-) site.
The structure can be viewed as an intermediate structure between that of rock salt and zinc blende.
The magnesium cation in 2 is co-ordinated to three nitride and two fluoride anions in a square pyramidal co-ordination.
An additional, longer Mg-F interaction completes a distorted, axially extended octahedron (Fig. 5).
The Mg-N/F bond distances in the pyramid base are 2.1253 (1)Å with the apical Mg-N bond at 2.1393 (1)Å.
As for 1 these fall within typical Mg-N bond length ranges seen in Mg3N2 (2.084 (1)-2.179 (1)Å), and are slightly longer than Mg-F bonds in MgF2 (1.984 (2)-1.994 (1)Å) [24,25].
The additional Mg-F distance lies at 2.8730 (1)Å.
The Mg-Mg distance is 2.7430 (1)Å, which is closer to that in Mg3N2 (2.716 (1)Å) than in MgF2 (3.045 (3)Å) or Mg metal (3.1903 (1)-3.2025 (1)Å).
Bond valence calculations performed using the structure refined from PND data and the same rij and b parameters used for Mg3NF3 above [28], gave sums of 1.90 for Mg, 2.85 for N and 0.92 for F.
These values are slightly lower than expected and lower than those found in Mg3NF3, above, but nevertheless suggest a strong ionic component to bonding in Mg2NF.
"Underbonding" is not uncommon in nitridic compounds and given, for example, the low values reported for Mg and N in Mg3N2, it is likely that Mg…Mg nonbonding interactions could play a role in Mg2NF as they do in the binary magnesium nitride [24].
It is not totally surprising, therefore, that the members of the Mg-N-F system become more ionic as one increases the fluoride content from Mg3N2 through the nitride fluorides Mg2NF and Mg3NF3 to MgF2.
Bond valence figures can also demonstrate how weak the additional axial Mg-F interaction is.
In MgF2 the bond valence of a single Mg-F bond is 0.336.
In 1 the bond valence of Mg-F is 0.241.
The square pyramid Mg-F bond in 2 gives a bond valence of 0.230 but the long range Mg-F bond of 2 has a bond valence of only 0.03.
Therefore it is tempting to dismiss this connection as a relatively weak interaction rather than classify it as an Mg-F bond.
Analysis of the anisotropic thermal displacement parameters shows that the ellipsoid for the fluorine position is elongated in the c-axis, suggestive of some measure of (local) anion disorder or possibly indicative of fluoride ion mobility.
This feature is also seen in the isostructural Ca2NF [11], where the authors carried out refinements with partial oxygen substitution on the F site in an attempt to alleviate the large displacement parameters.
In the case of Ca2NF this model was rejected as it worsened the fit.
Attempts to include partial oxygen substitution in our model for 2 also decreased fit quality, and when freely refined, fluorine occupancy always refined back to unity.
Magnetic measurements
SQUID magnetometry measurements revealed that both samples 1 and 2 exhibit weak temperature independent paramagnetic behaviour (typically χM∼5×10-4emumol-1) between 5 and 300K.
This behaviour is commensurate with other alkaline earth nitride halides and suggests either weakly paramagnetic materials or intrinsically diamagnetic materials with very small levels of alkaline earth metal impurities (below the detection limit of PXD and PND) [15,16].
(A plot of χM vs. T for 1 and 2 is available as Supplementary information.)
Summary
Two ternary nitride fluorides of magnesium, Mg3NF3 and Mg2NF, have been synthesised via the solid state reaction of appropriate ratios of Mg3N2 and MgF2.
Powder neutron diffraction studies have confirmed structures in which the fluoride and nitride anions are completely ordered in both cases.
Bond valence calculations reveal that both these nitride fluorides contain among the most ionic metal-nitrogen bonds in the solid state.
It is worthy of note that the original work of Andersson provided extremely accurate models for both structures given the stage of development of powder refinement methods at the time of publication [3].
Magnetic measurements demonstrate that both nitride fluorides exhibit very weak paramagnetism with susceptibilities essentially independent of temperature.
Acknowledgments
Neutron beamtime at ISIS was provided by the UK Science and Technology Facilities Council (STFC).
DHG thanks the University of Nottingham for funding a studentship for M.A.B., and WestCHEM for funding R.W.H. as a WestCHEM Fellow.
Supplementary Information
Supplementary data associated with this article can be found in the online version at doi:10.1016/j.jssc.2011.11.008.
Supplementary materials
Supplementary material

Marine 187Os/188Os isotope stratigraphy reveals the interaction of volcanism and ocean circulation during Oceanic Anoxic Event 2

Abstract
High-resolution osmium (Os) isotope stratigraphy across the Cenomanian-Turonian Boundary Interval from 6 sections for four transcontinental settings has produced a record of seawater chemistry that demonstrates regional variability as a function of terrestrial and hydrothermal inputs, revealing the impact of palaeoenvironmental processes.
In every section the 187Os/188Os profiles show a comparable trend; radiogenic values in the lead up to Oceanic Anoxic Event 2 (OAE 2); an abrupt unradiogenic trend at the onset of OAE 2; an unradiogenic interval during the first part of OAE 2; and a return to radiogenic values towards the end of the event, above the Cenomanian-Turonian boundary.
The unradiogenic trend in 187Os/188Os is synchronous in all sections.
Previous work suggests that activity of the Caribbean LIP (Large Igneous Province) was the source of unradiogenic Os across the OAE 2 and possibly an instigator of anoxia in the oceans.
Here we assess this hypothesis and consider the influence of activity from other LIPs; such as the High Arctic LIP.
A brief shift to high radiogenic 187Os/188Os values occurred in the Western Interior Seaway before the onset of OAE 2.
We evaluate this trend and suggest that a combination of factors collectively played critical roles in the initiation of OAE 2; differential input of nutrients from continental and volcanogenic sources, coupled with efficient palaeocirculation of the global ocean and epeiric seas, enhanced productivity due to higher nutrient availability, which permitted penecontemporaneous transport of continental and LIP-derived nutrients to trans-equatorial basins.
Highlights
•
Use Osi isotope stratigraphy to infer driving mechanism(s) of Oceanic Anoxic Event 2.
•
Osi profiles from 6 sites illustrate the regional variability of seawater chemistry.
•
δ13Corg and Osi records combined generate an integrated timescale model based on the GSSP.
•
Anoxia is a function of feedback based on climate change and volcanism.
•
Quantitatively constrain the duration of volcanism from Caribbean LIP.

Introduction
The Cenomanian-Turonian boundary (CTB) OAE 2 records an extensive period of global anoxia, represented worldwide by sections containing organic-rich marine sedimentary rocks.
Strata marking the onset of OAE 2 are globally correlated by a 2 to 4‰ positive excursion in the carbon stable isotope composition of organic matter (δ13Corg) and marine carbonates (δ13Ccarb), which are interpreted to reflect the onset of massive organic carbon burial and widespread oxygen deficiency in the oceans (Jenkyns, 1980; Schlanger et al., 1987).
The OAE 2 has been studied using numerous proxies (e.g. carbon, strontium, osmium, calcium, neodymium, lithium, TEX86 and phosphorus; Arthur et al., 1987; McArthur et al., 2004; Forster et al., 2007; Mort et al., 2007; MacLeod et al., 2008; Turgeon and Creaser, 2008; Voigt et al., 2008; Blättler et al., 2011; Pogge von Strandmann et al., 2013; Zheng et al., 2013) to determine the driving mechanisms for organic carbon burial and anoxia.
Among the processes thought to play a role are: enhanced volcanism and CO2 output; increased land and sea surface temperatures; an accelerated hydrological cycle, sea level rise and increased rates of ocean circulation; and changes in nutrient supply and productivity.
These have all been supported by different proxy studies (e.g. Jenkyns, 1980; Arthur et al., 1987; Arthur and Sageman, 1994; Mort et al., 2007; Turgeon and Creaser, 2008; Martin et al., 2012).
In this study, we present a high-resolution initial osmium isotope (187Os/188Os; Osi) stratigraphy of the upper Cenomanian to lower Turonian from 4 transcontinental sections, and the Osi data from two previously analyzed representative sections of the proto-North Atlantic and Tethyan margin (Fig. 1; ODP Site 1260 and Furlo; Turgeon and Creaser, 2008) with additional analysis to enhance resolution.
These data are predominantly controlled by the mass balance of two end-member Os isotope components: weathered continental crust (∼1.4) and mantle inputs (0.13) attributed to enhanced submarine volcanism (Peucker-Ehrenbrink and Ravizza, 2000).
This, coupled with the short residence time of Os in seawater (⩽10 kyr; Oxburgh, 2001), makes 187Os/188Os composition an excellent monitor of palaeoceanographic changes in the geological record (Peucker-Ehrenbrink and Ravizza, 2000; Cohen, 2004), particularly across the CTB where there is evidence for accelerated weathering, as well as evidence of submarine volcanic activity (Snow et al., 2005; Frijia and Parente, 2008; Turgeon and Creaser, 2008; Pogge von Strandmann et al., 2013).
Analysis of osmium isotope trends recorded from different sites provides information about changes in these inputs to the marine realm, as well as the interconnectivity of oceanic water masses with epeiric seas.
The Osi data reported from the previous study show similar profiles, and suggest that for at least ∼700 kyr of the late Cenomanian-early Turonian the ocean basins were relatively well connected.
The Osi data also show that a major pulse of volcanism interpreted to be associated with activity from Large Igneous Provinces (LIPs), i.e., Caribbean and High Arctic (Fig. 1; Snow et al., 2005; Tegner et al., 2011) occurred at or just prior to the onset of OAE 2.
However, the High Arctic LIP is largely understudied due to lack of exposure.
Since constraints on timing and duration of activity from LIP volcanism are ambiguous (Tegner et al., 2011; Zheng et al., 2013), we associate the Osi data presented here with activity from the better temporally constrained Caribbean LIP (Turgeon and Creaser, 2008).
The Osi stratigraphic profiles across the CTB in this study are from (Fig. 1): the Portland #1 core, which is representative of the Global Stratotype Section and Point (GSSP) near Pueblo, Colorado (Western Interior Seaway (WIS); Kennedy et al., 2000); the Wunstorf core in Germany, a representative section in the NW European pelagic shelf sea (Voigt et al., 2008); the Vocontian Basin, south east France (Western Tethys; Grosheny et al., 2006; Jarvis et al., 2011); and DSDP Site 530 (proto-South Atlantic; Forster et al., 2008).
The Portland #1 core has the most refined temporal control for the studied interval based on detailed biostratigraphy, new radioisotopic dating, astrochronology, and chemostratigraphy (Sageman et al., 2006; Meyers et al., 2012a; Ma et al., submitted), and provides a critical framework for global correlation.
The Wunstorf core has a similarly good age control based on bio-, chemo-, and cyclostratigraphy and can be correlated to the Portland #1 core succession (Voigt et al., 2008).
Based on the GSSP time scale, and confirmed by additional estimates from different OAE 2 sites, the duration of the C isotope excursion that characterizes the OAE 2 is between 500-600 kyr (Meyers et al., 2012b).
In this study we investigate the Osi stratigraphy of multiple sections over an interval of ∼1.8 Myr from the late Cenomanian to the early Turonian and demonstrate that Osi values show some differences prior to OAE 2 depending on geographic location and depositional setting.
These variations are interpreted to reflect differential water mass exchange between epeiric settings and the open ocean modulated by sea-level change, as well as changes in terrigenous weathering rates due to enhanced global warming, which may have also affected nutrient fluxes and primary production levels.
These results suggest that epeiric seas, like the WIS or the European shelf sea, may have played an important role in the driving mechanism for OAE 2.
Additionally, we show that in comparison to the pre-OAE 2 interval, the syn-OAE 2 Osi values from Site 1260 and Furlo combined with Portland, Wunstorf, the Vocontian Basin and Site 530 are remarkably similar.
Coupled with the new geochronology from the WIS (Meyers et al., 2012a) a refined timing for the onset and duration of LIPs and its temporal association with OAE 2 is developed.
Furthermore, our interpretation of the Osi profile concurs with the hypothesis of increased ocean circulation based on analysis of neodymium (Nd) isotopes (Martin et al., 2012; Zheng et al., 2013).
OAE 2 section geology
Portland #1 Core, Colorado, USA
The studied interval was sampled from the USGS Portland #1 core (32°22.6′N, 105°01.3′W; Dean and Arthur, 1998; Meyers et al., 2001; Fig. 1).
This core was taken about 40 km west of the site near Pueblo, CO that was ratified as the GSSP for the CTB (Kennedy et al., 2005), and its stratigraphy has been correlated, essentially bed for bed, to the GSSP section (Sageman et al., 2006).
The Pueblo region was ratified as the GSSP site because the boundary interval contains abundant biostratigraphic index taxa, several options for geochronologic calibration, shows no obvious signs of condensation or significant disconformity, and has various stratigraphic markers that can be correlated over tens of thousands of square km (Hattin, 1971; Elder et al., 1994; Kennedy et al., 2005).
Within the Portland core, the Cenomanian-Turonian Boundary Interval (CTBI) was studied in a 17.7 m-thick section of the Bridge Creek Limestone (∼12 m) and Hartland Shale (∼12.6 m) Members of the Greenhorn Formation (Cobban and Scott, 1972).
These units include organic-rich calcareous shales and rhythmically interbedded couplets of shale and fossiliferous biomicritic limestone.
The stratigraphy is also characterized by four bentonite units of 1 to 20 cm that have been regionally correlated (Elder, 1988).
Recent sanidine 40Ar/39Ar and zircon 206Pb/238U geochronology integrated with astrochronology constrain the CTB at 93.90±0.15 Ma (Meyers et al., 2012a).
The CTBI contains a variety of fossil taxa useful for biostratigraphy (e.g., Gale et al., 1993; Kennedy et al., 2000, 2005; Keller and Pardo, 2004; Keller et al., 2004; Cobban et al., 2006) some of which have intercontinental distributions; however, their transcontinental synchronicity is limited.
The dominant foraminifera species spanning the CTBI are Rotalipora cushmani, Whiteinella archaeocretacea and Helvetoglobotruncana helvetica (Eicher and Worstell, 1970).
The FO (first occurrence) of the ammonite Watinoceras devonense (Fig. 2; Kennedy et al., 2000) marks the basal Turonian, recorded at the base of bed 86 of the Bridge Creek Limestone (Meyers et al., 2001; bed numbers are based on Cobban and Scott, 1972).
The FO of W. devonense coincides with the FO of Mytiloides puebloensis (Kennedy et al., 2000), which can be traced through both Tethyan and Boreal regions (Kennedy et al., 2005).
The onset of OAE 2 is identified by an abrupt 2-3‰ VPDB δ13Corg positive shift from values of ∼-27‰ in the upper Hartland Shale, 4.3 m below the CTB (Fig. 2; Supplementary Material, Table 1a; Sageman et al., 2006).
The positive excursion is characteristic of the isotopic response during OAE 2 and, although many localities record increased organic carbon deposition at this level (e.g., Tsikos et al., 2004), sites within the WIS do not.
Here the onset is characterized by organic-poor interbedded limestones and shales that are generally bioturbated.
Shale interbeds in the upper half of the OAE 2 interval, however, do become enriched in TOC in the WIS.
The end of OAE 2 is expressed by a gradual fall in δ13Corg back to ∼-27‰ (Sageman et al., 2006).
A high-resolution time scale for the study interval has been developed in recent years based on integration of new radioisotopic dates and astrochronological methods (Meyers et al. 2001, 2012a; Sageman et al., 2006; Ma et al., submitted).
The astrochronological techniques yield a more accurate interpolation of time for the intervals between dated tuff horizons because they include evolutive assessment of changes in linear sedimentation rate (not corrected for compaction).
Both radioisotopic and astrochronologic methods indicate a duration for OAE 2 of ∼600 kyr measured from the δ13Corg onset.
Wunstorf, NW Germany
The Wunstorf section was sampled from drill core from 52°24.187′N, 09°29.398′E and represents the European type section for the CTBI (Fig. 1; Voigt et al., 2008).
The CTBI succession (Hesseltal Formation) at Wunstorf was deposited in the distal Lower Saxony Basin, which was part of the western European shelf sea (Wilmsen, 2003).
The 26.5 m-thick Hesseltal Formation comprises cyclically interbedded couplets of organic-carbon rich shales, marls and limestones interpreted to represent nine short eccentricity cycles based on spectral analytical results (Voigt et al., 2008).
Accordingly, OAE 2, as defined by the δ13Ccarb curve, includes 4.3 short eccentricity cycles or 21.2 precession cycles, respectively, indicating a duration of 430-445 kyr (Voigt et al., 2008).
The biostratigraphy of the Hesseltal Formation is established by zonation with inoceramids, ammonites, acme occurrences of macrofossils and planktonic foraminifera (Ernst et al., 1984; Lehmann, 1999; Voigt et al., 2008).
The ammonite and inoceramid zonation can be compared to that of the GSSP in detail.
Although no macrofossils are recorded directly from the Wunstorf core, a series of index taxa can be placed based on a bed-by-bed correlation between the Lower Saxony Basin and the Munsterland Cretaceous Basin (Voigt et al., 2007, 2008).
The FO of the ammonite Metoicoceras geslinianum is equivalent to the FO of Sciponoceras gracile at the GSSP (Gale et al., 2005, 2008), which corresponds to the base of the Hesseltal Formation at Wunstorf (Lehmann, 1999).
The FO of W. devonense, the index taxon for the CTB (Fig. 2; Kennedy et al., 2005), is located in the Wunstorf core at 37.5±1 m (Lehmann, 1999; Voigt et al., 2008).
Previously, the stratigraphic extent of OAE 2 was constrained by δ13Ccarb (Voigt et al., 2008).
Here we present δ13Corg for the Wunstorf section, which shows frequent oscillations from -25 to -27‰ VPDB prior to OAE 2 (Fig. 2; Supplementary Material, Table 1b).
A facies change depicts the onset throughout the European shelf (Voigt et al., 2007).
This change records an initial positive excursion in the δ13Corg, consistent with the δ13Ccarb, followed by a second more distinct increase in the δ13Corg.
At Wunstorf, δ13Corg only clearly records the second increase; however OAE 2 initiation corresponds to the first increase.
The duration of OAE 2 at Wunstorf was estimated to be ∼435 kyr based on spectral analysis of lithological cyclicity (Voigt et al., 2008), which differs from the astrochronological and radioisotopic derived duration at the GSSP (∼600 kyr; applied in this study).
Voigt et al. (2008) discussed several options for this discrepancy as the possible lack of strata, different definitions of onset and termination of OAE 2 in the Portland #1 and Wunstorf cores, and incorrect orbital frequency assignment to the dominant cycle length.
The new organic δ13Corg curve of this study (Fig. 2) shows five distinct cycles close to the short eccentricity filter of Voigt et al. (2008).
Such a reinterpretation would reduce the temporal discrepancy and is consistent with the recently documented stronger obliquity control during OAE 2 (Meyers et al., 2012).
Further spectral analytical research is needed to fully address this question.
Vocontian Basin (Pont d'Issole and Vergons), SE France
The Vocontian Basin was part of the western gulf in the European Alpine region of the NW Tethys Ocean ∼30°N (Jarvis et al., 2011; Fig. 1).
High rates of subsidence throughout the mid-Cretaceous provided accommodation space for thick rhythmically bedded bioturbated limestone-marl successions, where the variable facies are indicative of a fluctuating hemipelagic depositional environment of moderate depth.
Different depositional and structural processes dependent on their location in the basin have affected CTB sections within the Vocontian Basin; e.g.
the Vergons section is affected by syn-sedimentary slumping in the uppermost Cenomanian, but otherwise exposes a continuous Upper Albian-Lower Turonian succession, while the thinner Pont d'Issole section is complete through the CTBI.
A ∼20 m thick package of black organic-rich calcareous shales, termed the "Niveau Thomel" (Takashima et al., 2009; Jarvis et al., 2011), characterize the CTBI.
Detailed biostratigraphy has been obtained for the 24 m Pont d'Issole section (Grosheny et al., 2006; Jarvis et al., 2011).
The distribution of index taxa R. cushmani and H. helvetica, coupled with complete δ13Corg and δ13Ccarb records (Fig. 2; Jarvis et al., 2011), permits bed-scale correlation with the GSSP near Pueblo.
Above the onset of OAE 2, samples were taken from Pont d'Issole, whereas below the onset some of the samples (n=4) came from Vergons (Supplementary Material, Table 2d), which is correlated with Pont d'Issole based on litho-, bio-, and stable-isotope stratigraphy and is undisturbed by faulting in the pre-OAE 2 interval.
The OAE 2 in the Pont d'Issole section includes a distinct facies change to finely laminated black shales (total organic carbon, TOC 0.3-3.5 wt.%) that occurs about a metre below the distinctive positive δ13Corg excursion (3‰) that marks the base of OAE 2 (Fig. 2; Supplementary Material, Table 1d; Jarvis et al., 2011).
High-frequency fluctuations in the δ13Corg record, up to 1‰ in magnitude, occur throughout OAE 2, associated with the alternation of lithological units.
The termination of OAE 2 is recorded by a gradual return to ∼-26‰.
DSDP Site 530, Hole 530A, South Atlantic
Palaeotectonic reconstruction situates Site 530 at 37°S, 38°W (Forster et al., 2008; Fig. 1).
Site 530 is located on the abyssal floor of the Angola Basin, 4645 metres below sea level (mbsl) and approximately 150 km west of the base of the continental slope of SW Africa with a 3-4 degree incline.
Drilling penetrated to a final depth of 1121 metres below sea floor (mbsf) after encountering durable basalt at 1103 mbsf (Forster et al., 2008).
The δ13Corg excursion marking OAE 2 occurs within a 49 m section of the CTBI.
Low sample resolution due to poor core recovery, and thus limited nannofossil data, only provide an approximate stratigraphic identification of the CTB.
ODP Site 1260, Hole 1260B, Demerara Rise and Furlo, Italy
In an effort to augment the understanding of seawater chemistry prior to OAE 2 provided by Turgeon and Creaser (2008), additional samples (n=12 [ODP] and n=6 [Furlo]; Fig. 1) were analyzed and the resolution of the Osi profiles was increased.
The facies at Site 1260 include a mixture of terrigenous detritus and carbonates, with high organic contents up to ∼23 wt.%.
The δ13Corg positive excursion reaches a maximum enrichment of -22.1‰ VPDB and the entire excursion is 1.2 m thick (Fig. 2; Supplementary Material, Table 1c; Forster et al., 2007).
In the Furlo section the CTBI lies within the Scaglia Bianca Formation, which includes abundant biosiliceous limestone.
The Livello Bonarelli is a 1 m thick condensed interval of millimetre-laminated black shale and brown radiolarian sand that represents the sedimentary expression of part of OAE 2 (Arthur and Premoli Silva, 1982).
Up to 20 m beneath the Bonarelli level there are numerous centimetre scale organic-rich shale layers (Jenkyns et al., 2007).
The δ13Corg record has a narrow variation in background values prior to OAE 2, ∼-25.9 to -26.5‰.
The characteristic positive excursion in δ13Corg is a 4‰ shift, -27.2 to -23.1‰, occurring within <0.5 m (Fig. 2; Supplementary Material, Table 1e).
Methods
Analytical protocol
In this study we have applied δ13Corg and Re-Os methodologies to determine the geochemical signatures of OAE 2 related strata.
We have used published analytical protocols (e.g., Selby and Creaser, 2003; Jarvis et al., 2011), which are described in detail in the Supplementary Material together with our sampling protocol from core and outcrop.
OAE 2 correlation
To date, the CTBI has been correlated 'globally' using biostratigraphy and carbon isotope chemostratigraphy.
Typically, characteristic peaks and troughs in the δ13C record are combined with key bioevents to establish correlation.
The six sections presented here (Fig. 2) are correlated according to this method using points 'A', 'B' and 'C' of the δ13Corg curve that are similar to those first defined by Pratt et al. (1985) in the Western Interior and used later by Tsikos et al. (2004).
For this correlation method, 'A' represents the last value of relatively depleted δ13Corg before the first major shift to positive values (typically -24 to -22‰).
This shift marks the base of δ13Corg excursion defined as OAE 2 (reference respective of location).
'B' defines a trough of depleted values following the initial positive excursion that occurs prior to the second positive shift (Pratt et al., 1985).
'C' is the last relatively enriched δ13Corg value before the trend back toward pre-excursion values, or the end of the so-called "plateau" (Tsikos et al., 2004).
In order to establish a common chronostratigraphic framework for comparing Osi data from distant localities, the chemostratigraphic method described above, confirmed by available biostratigraphic data, is used to extend the Pueblo GSSP timescale from the Portland #1 core (Meyers et al., 2012a) to the other sites.
The Portland core record has the highest resolution CTB timescale based on integration of new radioisotope dates (Ar-Ar and U-Pb) and astrochronology (Meyers et al., 2012a), and new work (Ma et al., submitted) has extended this timescale further down section into the Cenomanian.
As a result, our new Osi data and Osi results from a previous study (Turgeon and Creaser, 2008), can be plotted relative to individual timescales created for each section by exporting temporal information from the Portland #1 core (Fig. 3).
Timescale development is based on the following steps (see Table 1):i.
The new geochronology for the CTBI (Meyers et al., 2012a) employs a short eccentricity band pass to more accurately interpolate the age datum levels between dated tuff horizons.
Based on this method, the stage boundary is constrained to 93.90±0.15 Ma.
ii.
The ages of the 'A', 'B' and 'C' markers defined by the δ13Corg record of the Portland core are also precisely determined using this approach (Fig. 2; Table 1).
iii.
Nominal ages for the 'A', 'B', and 'C' markers are exported to the 'A', 'B' and 'C' datum levels of the δ13Corg curve in the other sections (Fig. 2), allowing calculation of local linear sedimentation rate values between the datum levels (Table 1).
A variable sedimentation rate is more realistic over such time frames, i.e., ∼100 kyr.
In some sections there is a distinct decrease in rate in the B-C interval, which likely reflects condensation related to global sea-level highstand.
Thus, the linear sedimentation rate calculated for A-B is applied to develop a timescale below the 'B' datum, and a linear sedimentation rate for B-C is used for the sections above the 'B' datum (Table 1).
iv.
Each timescale is developed using the onset of 'A' as the temporal datum set to 0 kyr (Fig. 3).
This creates a coherent global framework using the onset of δ13Corg excursion as the key datum level.
Although our methodology increases resolution and reduces uncertainty in the time scales for each section, it cannot eliminate uncertainty (e.g., constant sedimentation rates are still assumed for time scale segments).
For the purpose of comparing δ13Corg and Osi records between different localities, however, we believe the chronostratigraphic framework is sufficient to recognize differences in the timing of key events.
Initial 187Os/188Os (Osi)
The Osi values in this study were determined from Re-Os data and the 187Re decay constant (1.666e-11a-1; Smoliar et al., 1996; Supplementary Material, Table 2a-f) using the CTB age of 93.90 Ma that was determined from astrochronologic interpolation between volcanic ash ages (based on both 40Ar/39Ar and 206Pb/238U determinations; Gradstein et al., 2012; Meyers et al., 2012a).
Analytical uncertainty for individual calculated Osi is ⩽0.01.
The reproducibility of calculated Osi, based on 12 analyses of the USGS rock reference material SDO-1 (Devonian Ohio Shale), was ∼0.04 (2 SD; Supplementary Material, Table 3).
This uncertainty was used to account for the maximum uncertainty in the sample set for the calculated Osi.
Calculated Osi ratios assume closed system behaviour after deposition with respect to both rhenium and osmium.
Furthermore, the 187Os/188Os ratios reflect the isotope composition of the local seawater and are unaffected by mineral detritus.
Results
Re-Os abundance
Across the onset of OAE 2 there is a dramatic shift to very high values in Os isotope concentration.
At Portland Os concentration increases by ∼1000 ppt within ∼10 cm; at Wunstorf an increase of ∼1000 ppt within ∼30 cm; Site 1260 increases by ∼1000 ppt in <60 cm; in the Vocontian Basin there is an increase of ∼3500 ppt within 50 cm.
In both Furlo and Site 530 there are very considerable changes in the Os concentration; >10 000 ppt within 10 cm and 40 cm, respectively.
Conversely, Re abundance is relatively constant at each section, therefore the dramatic difference between the Re and Os abundance produce a similar profile in 187Re/188Os to the Osi profile, with an abrupt decrease in the 187Re/188Os directly associated with the abrupt increase in Os.
187Os/188Os isotope stratigraphy
The Osi profiles for all six sections show a similar trend; highly radiogenic values that suddenly become unradiogenic, before gradually returning to radiogenic values (Figs.
2, 3; all Osi data presented in full in Supplementary Material Table 2).
At Portland the Osi values show some distinct fluctuations prior to the onset of OAE 2 (point 'A' on the δ13Corg curve).
The Os trend from ∼1.0 to 0.9, briefly return to ∼1.0, and then drop abruptly to ∼0.7 at ∼-237 kyr (below 'A').
The trend toward unradiogenic values then reverses back toward the radiogenic end member up until the major shift to unradiogenic Osi at 'A'.
In the Site 1260 record, a trend from ∼0.6 to 1.0 in the lowest samples is followed by a shift in the opposite direction, toward the unradiogenic end-member, but the values are variable and some spikes to >1 (radiogenic) persist.
From -157 kyr there is a consistent trend toward unradiogenic Osi reaching a minimum value of ∼0.2 at the 'A' datum.
At Wunstorf the rock units prior to 'A' are bereft of Re and Os.
The Vocontian Basin records shifts to radiogenic Osi values (>0.9), before a gradual decrease to ∼0.76 followed by a brief increase to ∼0.82.
A few metres below the positive excursion a major shift to <0.3 occurs.
The Osi values at Furlo remain stable at ∼0.55 then shift suddenly to ∼0.65.
Above this horizon there are no samples until 'A' when the Osi is unradiogenic <0.3.
Site 530 has Osi values of ∼0.70 before showing a 0.2 decrease.
The trend reverses to ∼0.7, then the major unradiogenic shift to <0.2 at 'A'.
Importantly, the Osi record in the Portland core (Fig. 3) is significantly different between ∼-230 kyr and ∼-50 kyr relative to the other 4 sites (no data for Wunstorf).
From 'A' through to the lower Turonian, the Osi profiles and values are very similar across Portland, Furlo, Site 530 and Wunstorf, progressively trending from unradiogenic (∼0.2) to radiogenic values (∼0.6 to 0.7; Fig. 3) within ∼350 kyr.
The Osi values from point 'A' remain unradiogenic for ∼200-250 kyr before becoming progressively more radiogenic (Fig. 3).
The majority of the Osi data from the Vocontian Basin, from slightly before the onset of the positive δ13Corg excursion through the initial ∼200 kyr are unradiogenic at ∼0.2, with some fluctuation to ∼0.4.
In contrast to other sites that show a progressive return to radiogenic Osi values, the Vocontian Basin remains at values of ∼0.4 for an additional 200 kyr and then becomes radiogenic (0.94) very rapidly (within ∼80 kyr; Fig. 3).
This abrupt change could indicate a minor hiatus during the latter part of OAE 2.
The Osi values at Site 530 remain unradiogenic (0.12-0.25) for ∼145 kyr, returning to radiogenic values after ∼270 kyr.
However, due to poor core recovery there is a ∼125 kyr gap in the Osi record (Fig. 3).
Discussion
Heterogeneous seawater 187Os/188Os prior to OAE 2
Overall the Osi profiles from each section show similar variability in Osi values and in the 187Re/188Os composition before and during OAE 2.
Combined with previous Os isotope stratigraphy (Turgeon and Creaser, 2008) and detailed litho-, bio-, and chemostratigraphy, the sections are interpreted to be reliable records of the CTBI.
The Osi values for all sites in the WIS, western Tethys and proto-North Atlantic from -800 kyr to -210 kyr are radiogenic, and range from ∼0.5 to ∼1.0, illustrating that the seawater 187Os/188Os ratio during this time was not homogeneous, but was controlled by the 187Os/188Os composition of the fluxes entering the individual basins (Figs.
1, 3).
The radiogenic heterogeneity and high Osi values at Portland are attributed to the influence of weathered crustal components from the Sevier Orogenic Belt and the Canadian Shield, the major sources of weathered material to the basin.
Recent seawater Os isotope studies during glacial episodes in the last 200 kyr demonstrate how regional variation is correlated to the heterogeneous flux of material into proximal basins (Paquay and Ravizza, 2012).
This hypothesis is supported by the observed radiogenic Osi values for >500 kyr prior to 'A' at Portland and elsewhere (Fig. 3).
We therefore infer that water masses were reasonably well connected until ∼-210 kyr, but the 187Os/188Os composition of the seawater in the individual basins was strongly influenced by regional factors (Figs.
1, 3).
In addition, the heterogeneity of the 187Os/188Os data may provide information on vertical mixing as a function of depth and circulation; the variations may indicate that seawater was not always well mixed.
Implications of basin connectivity
Between ∼-300 and -200 kyr, Osi values at Portland in the WIS reverse toward more radiogenic values.
A similar pattern is observed at Site 1260, Vocontian Basin, Furlo and Site 530, although in each of these sites the radiogenic Osi inflection is brief (only a single data point) before the decline in Osi values (Fig. 3).
There are two possible mechanisms that could contribute to produce an Osi signal of this type within a shallow epeiric seaway: increased input of weathered material and restriction of the connection to the open ocean, which would allow a radiogenic (weathering input) signal to dominate (e.g., Portland and the Vocontian Basin).
In contrast, it is assumed that deep water sites preserve a signal more consistently representative of the open ocean (e.g., Site 1260 and 530).
The shallow epeiric setting at Portland would certainly have become restricted from the global ocean during sea-level lowstands, however, the degree of sea level fall necessary to produce restriction is difficult to know.
There is evidence of a small hiatus and a bone bed within the uppermost Hartland Shale, and two seaward stepping parasequences in the Dakota Formation of SW Utah correlate basinward to a level just below this hiatus (Elder et al., 1994), suggesting that a minor relative sea-level fall may have occurred (Gale et al., 2008).
Subsequently, the lowermost beds of the Bridge Creek Limestone contain a diverse marine fauna with many Tethyan taxa (Kauffman, 1984), and there is strong evidence for transgression during the deposition of the basal limestone bed (Arthur and Sageman, 2005).
Thus, the onset may have been immediately preceded by a relative fall in sea level that could have briefly reduced or shut down exchange of water masses with the global ocean, followed by a rapid sea-level rise.
Basin restriction may also provide an explanation for the delayed return to pre-OAE 2 Osi values in the Vocontian Basin.
Osi values return to ∼0.3 at 'B' comparable to other sections (Fig. 3).
Yet between 'B' and 'C' the Osi values fluctuate around ∼0.4 for an additional ∼200 kyr relative to other sites, which suggests that mixing with the rest of the proto-North Atlantic was temporarily limited.
Implications of enhanced weathering
To explain the radiogenic pre-OAE 2 Osi values, a continuous radiogenic continental input into the ocean is required (Peucker-Ehrenbrink and Ravizza, 2000).
Hence, the other mechanism resulting in radiogenic Osi values is a significant increase in the flux of weathered material to a basin.
Interpreted increases in temperature before 'A' indicate a period of significant warming (Clarke and Jenkyns, 1999; Forster et al., 2007; Jenkyns et al., 2004; Barclay et al., 2010), an intensification of the hydrological cycle, and more extensive flooding in continental interiors, which led to the build-up of terrestrially derived nutrients and organic-rich sediments in shallow basin water masses immediately prior to 'A'.
The radiogenic Osi prior to 'A' reflects sequestration of hydrogenous Os derived from the continent as a result of high weathering rates.
If, in fact, the WIS did become briefly restricted, the influence of local weathering inputs and changes in mixing between basins would be amplified in the seawater chemistry.
Additionally, there is evidence that increased input of weathered material influenced the Os chemistry of the shallow bathyal Site 1260 and the abyssal Site 530 before 'A'.
Continental turbiditic sediments deposited on the continental slope at Site 1260 produce an oscillating Osi profile before the onset.
At Site 530, comparatively less radiogenic Osi values prior to 'A' suggest that juvenile turbidites were sourced from juvenile detritus from the Walvis Ridge.
The high rates of weathering produced waters enriched in micro-nutrients that led to an increase in productivity coincident with OAE 2, which is supported by bulk rock enrichments of Si, P, Ba, Cu, Mo, Ni and Zn in black shales at Demerara Rise ODP sites (Jimenez Berrosoco et al., 2008).
In addition, enhanced weathering is inferred from Sr isotope trends, which despite possessing a longer residence time (1-4 Ma) have been interpreted to reflect global warming prior to, and during OAE 2 (Frijia and Parente, 2008).
Caribbean large igneous province and OAE 2
In contrast to the elevated radiogenic Osi values just before 'A' at Portland, the Osi values of Site 1260, Vocontian Basin, Furlo and Site 530 show a progressive trend to unradiogenic Osi values (0.75 to 0.55) over ∼155 kyr (Fig. 3) suggesting that hydrothermal input dominated Os chemistry in the open oceans.
Within the WIS the stratigraphic evidence for sea-level rise is coincident with an abrupt shift of radiogenic Osi values to very unradiogenic values at Portland ∼50 kyr prior to 'A'.
Therefore the trend to almost homogeneous unradiogenic Osi recorded in all sites at 'A' requires a sustained source of unradiogenic Os input to the ocean.
Basaltic igneous provinces release unradiogenic Os, close to chondritic values (∼0.13; Cohen and Coe, 2002).
There are two potential sources of volcanism: the Caribbean LIP and the High Arctic LIP.
The eruption history from the High Arctic remains poorly constrained (Tegner et al., 2011) and trends interpreted at this stage are relatively ambiguous (Zheng et al., 2013).
Consequently, the abrupt unradiogenic trend is interpreted to reflect an episode of submarine mafic volcanism from the Caribbean LIP (Fig. 3), sufficient to influence the global Os isotope budget (Turgeon and Creaser, 2008).
The high-resolution of the Osi data presented here make an important contribution to the discussion of Caribbean LIP onset and cessation.
Evidence supports the hypothesis that an influx of unradiogenic Os in the marine Os record is a direct consequence of volcanism (Ravizza and Peucker-Ehrenbrink, 2003).
From ∼-50 kyr all sites show a synchronous abrupt trend towards unradiogenic Osi values (Fig. 3).
Based on the trend to unradiogenic Osi values at Site 1260, Vocontian Basin, Furlo and Site 530 we suggest that the initiation of volcanism was at least ∼200 kyr prior to 'A' (∼94.58 Ma; Fig. 3, CLIP i), with the major pulse of submarine volcanism happening at ∼-30 kyr (94.41 Ma; Fig. 3, CLIP ii), where all locations possess near mantle-like Osi values.
The timing of Caribbean LIP ii is supported by the rapid change in Os concentration (Section 4.2; Supplementary Material, Table 2) in all sections with the exception of Wunstorf where there is no record (Fig. 3).
The sudden and high increase in Os concentrations occurs within 1 metre of deposition, which equates to <20 kyr at Furlo and Site 530, and <10 kyr at Portland, Site 1260 and Vocontian Basin.
The increase in concentration is directly synchronous with the abrupt decrease to very low seawater Osi values and is contemporaneous with 'A' within <20 kyr.
The trend recorded in the new sections studied here is consistent with the pattern observed in the previous work by Turgeon and Creaser (2008), where there was a clear and large increase in Os concentration at the onset of OAE 2.
As discussed, high weathering rates across the CTB released large amounts of organic-rich material to the oceans, which sequester hydrogenous Os (Peucker-Ehrenbrink and Ravizza, 2000).
The trend therefore implies that within <20 kyr the amount of unradiogenic dissolved Os to seawater significantly reduced the influence of radiogenic Os (Cohen and Coe 2002, 2007; Ravizza and Peucker-Ehrenbrink, 2003).
Therefore, the observed regional variations in the data support the short residence time of Os in seawater and confirm the capability of Os to detect short-term forcing mechanisms, such as activity from LIPs.
The interaction of both volcanism and enhanced global weathering on Osi means that quantifying the magnitude and isolating the extent of the two signals is problematic, since the extent of weathering on seawater chemistry is masked by the inputs from the Caribbean LIP to the global ocean.
We can only estimate the Os contribution to seawater chemistry using a mixing model and assumed abundances.
If we assume that the average seawater 187Os/188Os prior to the LIP onset was ∼0.8, and use an average Os abundance in seawater of 10 ppq (based on the present-day average; Peucker-Ehrenbrink and Ravizza, 2000), a basalt 187Os/188Os of 0.13 (Meisel et al., 2001) and an average Os abundance, we can evaluate the approximate Os contribution from the Caribbean LIP to the global ocean using a progressive mixing model (Faure, 1986, Eqs.
(9.2) and (9.10)).
We note that there are no published Os data for the Caribbean LIP.
However, basalts can have variable Os abundances (1 to 600 ppt; Martin, 1991; Crocket and Paul, 2008); typical values range from 1 to 30 ppt (e.g., Shirey and Walker, 1998; Allegre et al., 1999; Dale et al., 2008).
Using an Os abundance for a basalt of 30 ppt would require 75% Os contribution from the LIP to yield the least radiogenic Osi observed at all locations.
Considerably less Os input from the LIP (25%) is needed if the LIP basalts possess higher Os abundances (100 ppt) and if the Os contribution to seawater also occurred through the addition of gas known to be enriched 20 times that of the basalt (e.g., Yudovskya et al., 2008).
If we assume the emplacement and weathering of the LIP are direct indicators of volcanic activity (Cohen and Coe 2002, 2007), we can estimate the duration of volcanism at the Caribbean LIP based on the marine 187Os/188Os record.
During the emplacement of the LIP we assume that growth of the plateau does not continue to affect the Os isotope composition (Robinson et al., 2009), since the Osi values are homogeneous (∼0.2; Fig. 3).
The subsequent trend to radiogenic Osi values ∼200 kyr after 'A' potentially represents the cessation of volcanism.
If we consider that the predominant 187Os/188Os of the ocean prior to the Caribbean LIP was 0.8, the influence of Os abundance and isotopic composition from the Caribbean LIP was less than 5% once the seawater 187Os/188Os had reached ∼0.50, which occurred ∼450 kyr after the onset (until ∼94.13 Ma; Fig. 3).
Hiatuses identified during the CTBI
At Portland the ∼17 kyr hiatus above 'B' was previously identified by Meyers and Sageman (2004), and the hiatus just before 'A', though quantitatively unconstrained, is equally minor based on site comparison (Elder et al., 1994; Ma et al., submitted).
This study has identified one hiatus in the higher part of the OAE 2 at Site 1260.
At Site 1260 Erbacher et al. (2005) suggested that ∼150 kyr is missing from Site 1258, yet present at Site 1260.
However, distinct lithological breaks in the core images at 425.19 m and the δ13Corg record indicate that the hiatus may also be present in the latter section.
A 150 kyr hiatus is inferred here from the Os isotope profile (Fig. 3).
Palaeocirculation across OAE 2
A model of quasi-estuarine circulation that was proposed for the WIS, which includes surface outflows causing deeper Atlantic/Tethyan water masses to be advected into the basin (Slingerland et al., 1996), is also suggested as a means to import Caribbean LIP influenced proto-Pacific waters into the proto-Atlantic and Tethys (Trabucho-Alexandre et al., 2010).
The similar shape of the Osi profiles (from ∼-50 kyr until ∼200 kyr into OAE 2) suggest that unradiogenic Os-bearing water was rapidly transported from the proto-Pacific into and across the proto-North Atlantic/Tethys, and into the WIS (Fig. 4).
This model is consistent with the hypothesis that palaeocirculation was not sluggish, as also indicated by climate models (Trabucho-Alexandre et al., 2010) and data from Nd isotopes.
The latter suggest a dynamic deep/bottom-water circulation (MacLeod et al., 2008; Martin et al., 2012) that is interpreted to reflect the relationship between bottom-water sources, climate, ocean anoxia, and circulation (Martin et al., 2012).
Conclusions
Submarine volcanism alone cannot be the sole driving mechanism for OAEs, especially OAE 2.
Osi data from 6 transatlantic and epeiric sections demonstrate that OAE 2 resulted from a combination of interacting factors.
An influx of nutrients from the continents preconditioned the oceans and helped to trigger OAE 2 through increased productivity and, similarly to Jones and Jenkyns (2001), we infer that rising sea level may have been the tipping point for the development of widespread anoxia.
The Osi profile at Portland suggests that the restriction of the epeiric WIS during the pre-OAE 2 interval amplified the affects of high weathering rates as abundant organic-rich sediments sequestered radiogenic Os derived from the ancient continental crust.
The close similarity of Osi profiles from ∼50 kyr prior to the OAE 2 and throughout the syn-OAE 2 interval indicates that transgression progressed to a point where a homogeneous global seawater signal was delivered to multiple proto-transatlantic basins by active ocean circulation.
Furthermore, the synchronicity of the unradiogenic Osi pattern suggests that the magnitude of Caribbean LIP volcanism was sufficient to simultaneously influence the seawater chemistry of each basin; the abundance of organic-rich sediments added to the water column as a result of enhanced continental weathering permitted sequestration of hydrogenous unradiogenic Os from the contemporaneous Caribbean LIP.
The temporal coincidence provides empirical evidence for the duration of the Caribbean LIP of ∼450 kyr.
Acknowledgments
We would like to thank C.
Dale, A.
Finlay, J. Trabucho-Alexandre, and Joanne Peterkin for laboratory assistance and discussion.
Thanks to H.C.
Jenkyns for providing additional samples from Furlo for Osi analysis.
The project was possible thanks to a NERC small grant to D.S. and D.R.G.
(NE/G009678/1), NERC standard grants to D.R.G. and D.S.
(NE/H021868/1) and I.J.
(NE/H020756/1), BP funding to D.S., and a grant for fieldwork awarded to A.D.C.
Du Vivier by the Geologists' Association.
We are grateful for the constructive reviews from H.
Jenkyns, G.
Ravizza and G.
Henderson that helped improve this manuscript.
Supplementary material
Supplementary material related to this article can be found online at http://dx.doi.org/10.1016/j.epsl.2013.12.024.
Supplementary material
The following is the Supplementary material related to this article.MMC 1
The document contains additional information on sampling and analytical protocol to support the manuscript by Du Vivier et al.
MMC 2
Table 1.
The workbook contains 6 tables of δ13Corg data for each section analyzed in the manuscript: Portland, Wunstorf, Site 1260, Vocontian Basin, Furlo, and Site 530.
MMC 3
Table 2.
The workbook contains 6 tables of Re-Os geochemical data for samples for the CTB, for each section analyzed in the manuscript: Portland, Wunstorf, Site 1260, Vocontian Basin, Furlo, and Site 530.
MMC 4
Table 3.
The workbook contains Re-Os geochemical data for USGS Std, SDO-1.

Marine 187Os/188Os isotope stratigraphy reveals the interaction of volcanism and ocean circulation during Oceanic Anoxic Event 2

Abstract
High-resolution osmium (Os) isotope stratigraphy across the Cenomanian-Turonian Boundary Interval from 6 sections for four transcontinental settings has produced a record of seawater chemistry that demonstrates regional variability as a function of terrestrial and hydrothermal inputs, revealing the impact of palaeoenvironmental processes.
In every section the 187Os/188Os profiles show a comparable trend; radiogenic values in the lead up to Oceanic Anoxic Event 2 (OAE 2); an abrupt unradiogenic trend at the onset of OAE 2; an unradiogenic interval during the first part of OAE 2; and a return to radiogenic values towards the end of the event, above the Cenomanian-Turonian boundary.
The unradiogenic trend in 187Os/188Os is synchronous in all sections.
Previous work suggests that activity of the Caribbean LIP (Large Igneous Province) was the source of unradiogenic Os across the OAE 2 and possibly an instigator of anoxia in the oceans.
Here we assess this hypothesis and consider the influence of activity from other LIPs; such as the High Arctic LIP.
A brief shift to high radiogenic 187Os/188Os values occurred in the Western Interior Seaway before the onset of OAE 2.
We evaluate this trend and suggest that a combination of factors collectively played critical roles in the initiation of OAE 2; differential input of nutrients from continental and volcanogenic sources, coupled with efficient palaeocirculation of the global ocean and epeiric seas, enhanced productivity due to higher nutrient availability, which permitted penecontemporaneous transport of continental and LIP-derived nutrients to trans-equatorial basins.
Highlights
•
Use Osi isotope stratigraphy to infer driving mechanism(s) of Oceanic Anoxic Event 2.
•
Osi profiles from 6 sites illustrate the regional variability of seawater chemistry.
•
δ13Corg and Osi records combined generate an integrated timescale model based on the GSSP.
•
Anoxia is a function of feedback based on climate change and volcanism.
•
Quantitatively constrain the duration of volcanism from Caribbean LIP.

Introduction
The Cenomanian-Turonian boundary (CTB) OAE 2 records an extensive period of global anoxia, represented worldwide by sections containing organic-rich marine sedimentary rocks.
Strata marking the onset of OAE 2 are globally correlated by a 2 to 4‰ positive excursion in the carbon stable isotope composition of organic matter (δ13Corg) and marine carbonates (δ13Ccarb), which are interpreted to reflect the onset of massive organic carbon burial and widespread oxygen deficiency in the oceans (Jenkyns, 1980; Schlanger et al., 1987).
The OAE 2 has been studied using numerous proxies (e.g. carbon, strontium, osmium, calcium, neodymium, lithium, TEX86 and phosphorus; Arthur et al., 1987; McArthur et al., 2004; Forster et al., 2007; Mort et al., 2007; MacLeod et al., 2008; Turgeon and Creaser, 2008; Voigt et al., 2008; Blättler et al., 2011; Pogge von Strandmann et al., 2013; Zheng et al., 2013) to determine the driving mechanisms for organic carbon burial and anoxia.
Among the processes thought to play a role are: enhanced volcanism and CO2 output; increased land and sea surface temperatures; an accelerated hydrological cycle, sea level rise and increased rates of ocean circulation; and changes in nutrient supply and productivity.
These have all been supported by different proxy studies (e.g. Jenkyns, 1980; Arthur et al., 1987; Arthur and Sageman, 1994; Mort et al., 2007; Turgeon and Creaser, 2008; Martin et al., 2012).
In this study, we present a high-resolution initial osmium isotope (187Os/188Os; Osi) stratigraphy of the upper Cenomanian to lower Turonian from 4 transcontinental sections, and the Osi data from two previously analyzed representative sections of the proto-North Atlantic and Tethyan margin (Fig. 1; ODP Site 1260 and Furlo; Turgeon and Creaser, 2008) with additional analysis to enhance resolution.
These data are predominantly controlled by the mass balance of two end-member Os isotope components: weathered continental crust (∼1.4) and mantle inputs (0.13) attributed to enhanced submarine volcanism (Peucker-Ehrenbrink and Ravizza, 2000).
This, coupled with the short residence time of Os in seawater (⩽10 kyr; Oxburgh, 2001), makes 187Os/188Os composition an excellent monitor of palaeoceanographic changes in the geological record (Peucker-Ehrenbrink and Ravizza, 2000; Cohen, 2004), particularly across the CTB where there is evidence for accelerated weathering, as well as evidence of submarine volcanic activity (Snow et al., 2005; Frijia and Parente, 2008; Turgeon and Creaser, 2008; Pogge von Strandmann et al., 2013).
Analysis of osmium isotope trends recorded from different sites provides information about changes in these inputs to the marine realm, as well as the interconnectivity of oceanic water masses with epeiric seas.
The Osi data reported from the previous study show similar profiles, and suggest that for at least ∼700 kyr of the late Cenomanian-early Turonian the ocean basins were relatively well connected.
The Osi data also show that a major pulse of volcanism interpreted to be associated with activity from Large Igneous Provinces (LIPs), i.e., Caribbean and High Arctic (Fig. 1; Snow et al., 2005; Tegner et al., 2011) occurred at or just prior to the onset of OAE 2.
However, the High Arctic LIP is largely understudied due to lack of exposure.
Since constraints on timing and duration of activity from LIP volcanism are ambiguous (Tegner et al., 2011; Zheng et al., 2013), we associate the Osi data presented here with activity from the better temporally constrained Caribbean LIP (Turgeon and Creaser, 2008).
The Osi stratigraphic profiles across the CTB in this study are from (Fig. 1): the Portland #1 core, which is representative of the Global Stratotype Section and Point (GSSP) near Pueblo, Colorado (Western Interior Seaway (WIS); Kennedy et al., 2000); the Wunstorf core in Germany, a representative section in the NW European pelagic shelf sea (Voigt et al., 2008); the Vocontian Basin, south east France (Western Tethys; Grosheny et al., 2006; Jarvis et al., 2011); and DSDP Site 530 (proto-South Atlantic; Forster et al., 2008).
The Portland #1 core has the most refined temporal control for the studied interval based on detailed biostratigraphy, new radioisotopic dating, astrochronology, and chemostratigraphy (Sageman et al., 2006; Meyers et al., 2012a; Ma et al., submitted), and provides a critical framework for global correlation.
The Wunstorf core has a similarly good age control based on bio-, chemo-, and cyclostratigraphy and can be correlated to the Portland #1 core succession (Voigt et al., 2008).
Based on the GSSP time scale, and confirmed by additional estimates from different OAE 2 sites, the duration of the C isotope excursion that characterizes the OAE 2 is between 500-600 kyr (Meyers et al., 2012b).
In this study we investigate the Osi stratigraphy of multiple sections over an interval of ∼1.8 Myr from the late Cenomanian to the early Turonian and demonstrate that Osi values show some differences prior to OAE 2 depending on geographic location and depositional setting.
These variations are interpreted to reflect differential water mass exchange between epeiric settings and the open ocean modulated by sea-level change, as well as changes in terrigenous weathering rates due to enhanced global warming, which may have also affected nutrient fluxes and primary production levels.
These results suggest that epeiric seas, like the WIS or the European shelf sea, may have played an important role in the driving mechanism for OAE 2.
Additionally, we show that in comparison to the pre-OAE 2 interval, the syn-OAE 2 Osi values from Site 1260 and Furlo combined with Portland, Wunstorf, the Vocontian Basin and Site 530 are remarkably similar.
Coupled with the new geochronology from the WIS (Meyers et al., 2012a) a refined timing for the onset and duration of LIPs and its temporal association with OAE 2 is developed.
Furthermore, our interpretation of the Osi profile concurs with the hypothesis of increased ocean circulation based on analysis of neodymium (Nd) isotopes (Martin et al., 2012; Zheng et al., 2013).
OAE 2 section geology
Portland #1 Core, Colorado, USA
The studied interval was sampled from the USGS Portland #1 core (32°22.6′N, 105°01.3′W; Dean and Arthur, 1998; Meyers et al., 2001; Fig. 1).
This core was taken about 40 km west of the site near Pueblo, CO that was ratified as the GSSP for the CTB (Kennedy et al., 2005), and its stratigraphy has been correlated, essentially bed for bed, to the GSSP section (Sageman et al., 2006).
The Pueblo region was ratified as the GSSP site because the boundary interval contains abundant biostratigraphic index taxa, several options for geochronologic calibration, shows no obvious signs of condensation or significant disconformity, and has various stratigraphic markers that can be correlated over tens of thousands of square km (Hattin, 1971; Elder et al., 1994; Kennedy et al., 2005).
Within the Portland core, the Cenomanian-Turonian Boundary Interval (CTBI) was studied in a 17.7 m-thick section of the Bridge Creek Limestone (∼12 m) and Hartland Shale (∼12.6 m) Members of the Greenhorn Formation (Cobban and Scott, 1972).
These units include organic-rich calcareous shales and rhythmically interbedded couplets of shale and fossiliferous biomicritic limestone.
The stratigraphy is also characterized by four bentonite units of 1 to 20 cm that have been regionally correlated (Elder, 1988).
Recent sanidine 40Ar/39Ar and zircon 206Pb/238U geochronology integrated with astrochronology constrain the CTB at 93.90±0.15 Ma (Meyers et al., 2012a).
The CTBI contains a variety of fossil taxa useful for biostratigraphy (e.g., Gale et al., 1993; Kennedy et al., 2000, 2005; Keller and Pardo, 2004; Keller et al., 2004; Cobban et al., 2006) some of which have intercontinental distributions; however, their transcontinental synchronicity is limited.
The dominant foraminifera species spanning the CTBI are Rotalipora cushmani, Whiteinella archaeocretacea and Helvetoglobotruncana helvetica (Eicher and Worstell, 1970).
The FO (first occurrence) of the ammonite Watinoceras devonense (Fig. 2; Kennedy et al., 2000) marks the basal Turonian, recorded at the base of bed 86 of the Bridge Creek Limestone (Meyers et al., 2001; bed numbers are based on Cobban and Scott, 1972).
The FO of W. devonense coincides with the FO of Mytiloides puebloensis (Kennedy et al., 2000), which can be traced through both Tethyan and Boreal regions (Kennedy et al., 2005).
The onset of OAE 2 is identified by an abrupt 2-3‰ VPDB δ13Corg positive shift from values of ∼-27‰ in the upper Hartland Shale, 4.3 m below the CTB (Fig. 2; Supplementary Material, Table 1a; Sageman et al., 2006).
The positive excursion is characteristic of the isotopic response during OAE 2 and, although many localities record increased organic carbon deposition at this level (e.g., Tsikos et al., 2004), sites within the WIS do not.
Here the onset is characterized by organic-poor interbedded limestones and shales that are generally bioturbated.
Shale interbeds in the upper half of the OAE 2 interval, however, do become enriched in TOC in the WIS.
The end of OAE 2 is expressed by a gradual fall in δ13Corg back to ∼-27‰ (Sageman et al., 2006).
A high-resolution time scale for the study interval has been developed in recent years based on integration of new radioisotopic dates and astrochronological methods (Meyers et al. 2001, 2012a; Sageman et al., 2006; Ma et al., submitted).
The astrochronological techniques yield a more accurate interpolation of time for the intervals between dated tuff horizons because they include evolutive assessment of changes in linear sedimentation rate (not corrected for compaction).
Both radioisotopic and astrochronologic methods indicate a duration for OAE 2 of ∼600 kyr measured from the δ13Corg onset.
Wunstorf, NW Germany
The Wunstorf section was sampled from drill core from 52°24.187′N, 09°29.398′E and represents the European type section for the CTBI (Fig. 1; Voigt et al., 2008).
The CTBI succession (Hesseltal Formation) at Wunstorf was deposited in the distal Lower Saxony Basin, which was part of the western European shelf sea (Wilmsen, 2003).
The 26.5 m-thick Hesseltal Formation comprises cyclically interbedded couplets of organic-carbon rich shales, marls and limestones interpreted to represent nine short eccentricity cycles based on spectral analytical results (Voigt et al., 2008).
Accordingly, OAE 2, as defined by the δ13Ccarb curve, includes 4.3 short eccentricity cycles or 21.2 precession cycles, respectively, indicating a duration of 430-445 kyr (Voigt et al., 2008).
The biostratigraphy of the Hesseltal Formation is established by zonation with inoceramids, ammonites, acme occurrences of macrofossils and planktonic foraminifera (Ernst et al., 1984; Lehmann, 1999; Voigt et al., 2008).
The ammonite and inoceramid zonation can be compared to that of the GSSP in detail.
Although no macrofossils are recorded directly from the Wunstorf core, a series of index taxa can be placed based on a bed-by-bed correlation between the Lower Saxony Basin and the Munsterland Cretaceous Basin (Voigt et al., 2007, 2008).
The FO of the ammonite Metoicoceras geslinianum is equivalent to the FO of Sciponoceras gracile at the GSSP (Gale et al., 2005, 2008), which corresponds to the base of the Hesseltal Formation at Wunstorf (Lehmann, 1999).
The FO of W. devonense, the index taxon for the CTB (Fig. 2; Kennedy et al., 2005), is located in the Wunstorf core at 37.5±1 m (Lehmann, 1999; Voigt et al., 2008).
Previously, the stratigraphic extent of OAE 2 was constrained by δ13Ccarb (Voigt et al., 2008).
Here we present δ13Corg for the Wunstorf section, which shows frequent oscillations from -25 to -27‰ VPDB prior to OAE 2 (Fig. 2; Supplementary Material, Table 1b).
A facies change depicts the onset throughout the European shelf (Voigt et al., 2007).
This change records an initial positive excursion in the δ13Corg, consistent with the δ13Ccarb, followed by a second more distinct increase in the δ13Corg.
At Wunstorf, δ13Corg only clearly records the second increase; however OAE 2 initiation corresponds to the first increase.
The duration of OAE 2 at Wunstorf was estimated to be ∼435 kyr based on spectral analysis of lithological cyclicity (Voigt et al., 2008), which differs from the astrochronological and radioisotopic derived duration at the GSSP (∼600 kyr; applied in this study).
Voigt et al. (2008) discussed several options for this discrepancy as the possible lack of strata, different definitions of onset and termination of OAE 2 in the Portland #1 and Wunstorf cores, and incorrect orbital frequency assignment to the dominant cycle length.
The new organic δ13Corg curve of this study (Fig. 2) shows five distinct cycles close to the short eccentricity filter of Voigt et al. (2008).
Such a reinterpretation would reduce the temporal discrepancy and is consistent with the recently documented stronger obliquity control during OAE 2 (Meyers et al., 2012).
Further spectral analytical research is needed to fully address this question.
Vocontian Basin (Pont d'Issole and Vergons), SE France
The Vocontian Basin was part of the western gulf in the European Alpine region of the NW Tethys Ocean ∼30°N (Jarvis et al., 2011; Fig. 1).
High rates of subsidence throughout the mid-Cretaceous provided accommodation space for thick rhythmically bedded bioturbated limestone-marl successions, where the variable facies are indicative of a fluctuating hemipelagic depositional environment of moderate depth.
Different depositional and structural processes dependent on their location in the basin have affected CTB sections within the Vocontian Basin; e.g.
the Vergons section is affected by syn-sedimentary slumping in the uppermost Cenomanian, but otherwise exposes a continuous Upper Albian-Lower Turonian succession, while the thinner Pont d'Issole section is complete through the CTBI.
A ∼20 m thick package of black organic-rich calcareous shales, termed the "Niveau Thomel" (Takashima et al., 2009; Jarvis et al., 2011), characterize the CTBI.
Detailed biostratigraphy has been obtained for the 24 m Pont d'Issole section (Grosheny et al., 2006; Jarvis et al., 2011).
The distribution of index taxa R. cushmani and H. helvetica, coupled with complete δ13Corg and δ13Ccarb records (Fig. 2; Jarvis et al., 2011), permits bed-scale correlation with the GSSP near Pueblo.
Above the onset of OAE 2, samples were taken from Pont d'Issole, whereas below the onset some of the samples (n=4) came from Vergons (Supplementary Material, Table 2d), which is correlated with Pont d'Issole based on litho-, bio-, and stable-isotope stratigraphy and is undisturbed by faulting in the pre-OAE 2 interval.
The OAE 2 in the Pont d'Issole section includes a distinct facies change to finely laminated black shales (total organic carbon, TOC 0.3-3.5 wt.%) that occurs about a metre below the distinctive positive δ13Corg excursion (3‰) that marks the base of OAE 2 (Fig. 2; Supplementary Material, Table 1d; Jarvis et al., 2011).
High-frequency fluctuations in the δ13Corg record, up to 1‰ in magnitude, occur throughout OAE 2, associated with the alternation of lithological units.
The termination of OAE 2 is recorded by a gradual return to ∼-26‰.
DSDP Site 530, Hole 530A, South Atlantic
Palaeotectonic reconstruction situates Site 530 at 37°S, 38°W (Forster et al., 2008; Fig. 1).
Site 530 is located on the abyssal floor of the Angola Basin, 4645 metres below sea level (mbsl) and approximately 150 km west of the base of the continental slope of SW Africa with a 3-4 degree incline.
Drilling penetrated to a final depth of 1121 metres below sea floor (mbsf) after encountering durable basalt at 1103 mbsf (Forster et al., 2008).
The δ13Corg excursion marking OAE 2 occurs within a 49 m section of the CTBI.
Low sample resolution due to poor core recovery, and thus limited nannofossil data, only provide an approximate stratigraphic identification of the CTB.
ODP Site 1260, Hole 1260B, Demerara Rise and Furlo, Italy
In an effort to augment the understanding of seawater chemistry prior to OAE 2 provided by Turgeon and Creaser (2008), additional samples (n=12 [ODP] and n=6 [Furlo]; Fig. 1) were analyzed and the resolution of the Osi profiles was increased.
The facies at Site 1260 include a mixture of terrigenous detritus and carbonates, with high organic contents up to ∼23 wt.%.
The δ13Corg positive excursion reaches a maximum enrichment of -22.1‰ VPDB and the entire excursion is 1.2 m thick (Fig. 2; Supplementary Material, Table 1c; Forster et al., 2007).
In the Furlo section the CTBI lies within the Scaglia Bianca Formation, which includes abundant biosiliceous limestone.
The Livello Bonarelli is a 1 m thick condensed interval of millimetre-laminated black shale and brown radiolarian sand that represents the sedimentary expression of part of OAE 2 (Arthur and Premoli Silva, 1982).
Up to 20 m beneath the Bonarelli level there are numerous centimetre scale organic-rich shale layers (Jenkyns et al., 2007).
The δ13Corg record has a narrow variation in background values prior to OAE 2, ∼-25.9 to -26.5‰.
The characteristic positive excursion in δ13Corg is a 4‰ shift, -27.2 to -23.1‰, occurring within <0.5 m (Fig. 2; Supplementary Material, Table 1e).
Methods
Analytical protocol
In this study we have applied δ13Corg and Re-Os methodologies to determine the geochemical signatures of OAE 2 related strata.
We have used published analytical protocols (e.g., Selby and Creaser, 2003; Jarvis et al., 2011), which are described in detail in the Supplementary Material together with our sampling protocol from core and outcrop.
OAE 2 correlation
To date, the CTBI has been correlated 'globally' using biostratigraphy and carbon isotope chemostratigraphy.
Typically, characteristic peaks and troughs in the δ13C record are combined with key bioevents to establish correlation.
The six sections presented here (Fig. 2) are correlated according to this method using points 'A', 'B' and 'C' of the δ13Corg curve that are similar to those first defined by Pratt et al. (1985) in the Western Interior and used later by Tsikos et al. (2004).
For this correlation method, 'A' represents the last value of relatively depleted δ13Corg before the first major shift to positive values (typically -24 to -22‰).
This shift marks the base of δ13Corg excursion defined as OAE 2 (reference respective of location).
'B' defines a trough of depleted values following the initial positive excursion that occurs prior to the second positive shift (Pratt et al., 1985).
'C' is the last relatively enriched δ13Corg value before the trend back toward pre-excursion values, or the end of the so-called "plateau" (Tsikos et al., 2004).
In order to establish a common chronostratigraphic framework for comparing Osi data from distant localities, the chemostratigraphic method described above, confirmed by available biostratigraphic data, is used to extend the Pueblo GSSP timescale from the Portland #1 core (Meyers et al., 2012a) to the other sites.
The Portland core record has the highest resolution CTB timescale based on integration of new radioisotope dates (Ar-Ar and U-Pb) and astrochronology (Meyers et al., 2012a), and new work (Ma et al., submitted) has extended this timescale further down section into the Cenomanian.
As a result, our new Osi data and Osi results from a previous study (Turgeon and Creaser, 2008), can be plotted relative to individual timescales created for each section by exporting temporal information from the Portland #1 core (Fig. 3).
Timescale development is based on the following steps (see Table 1):i.
The new geochronology for the CTBI (Meyers et al., 2012a) employs a short eccentricity band pass to more accurately interpolate the age datum levels between dated tuff horizons.
Based on this method, the stage boundary is constrained to 93.90±0.15 Ma.
ii.
The ages of the 'A', 'B' and 'C' markers defined by the δ13Corg record of the Portland core are also precisely determined using this approach (Fig. 2; Table 1).
iii.
Nominal ages for the 'A', 'B', and 'C' markers are exported to the 'A', 'B' and 'C' datum levels of the δ13Corg curve in the other sections (Fig. 2), allowing calculation of local linear sedimentation rate values between the datum levels (Table 1).
A variable sedimentation rate is more realistic over such time frames, i.e., ∼100 kyr.
In some sections there is a distinct decrease in rate in the B-C interval, which likely reflects condensation related to global sea-level highstand.
Thus, the linear sedimentation rate calculated for A-B is applied to develop a timescale below the 'B' datum, and a linear sedimentation rate for B-C is used for the sections above the 'B' datum (Table 1).
iv.
Each timescale is developed using the onset of 'A' as the temporal datum set to 0 kyr (Fig. 3).
This creates a coherent global framework using the onset of δ13Corg excursion as the key datum level.
Although our methodology increases resolution and reduces uncertainty in the time scales for each section, it cannot eliminate uncertainty (e.g., constant sedimentation rates are still assumed for time scale segments).
For the purpose of comparing δ13Corg and Osi records between different localities, however, we believe the chronostratigraphic framework is sufficient to recognize differences in the timing of key events.
Initial 187Os/188Os (Osi)
The Osi values in this study were determined from Re-Os data and the 187Re decay constant (1.666e-11a-1; Smoliar et al., 1996; Supplementary Material, Table 2a-f) using the CTB age of 93.90 Ma that was determined from astrochronologic interpolation between volcanic ash ages (based on both 40Ar/39Ar and 206Pb/238U determinations; Gradstein et al., 2012; Meyers et al., 2012a).
Analytical uncertainty for individual calculated Osi is ⩽0.01.
The reproducibility of calculated Osi, based on 12 analyses of the USGS rock reference material SDO-1 (Devonian Ohio Shale), was ∼0.04 (2 SD; Supplementary Material, Table 3).
This uncertainty was used to account for the maximum uncertainty in the sample set for the calculated Osi.
Calculated Osi ratios assume closed system behaviour after deposition with respect to both rhenium and osmium.
Furthermore, the 187Os/188Os ratios reflect the isotope composition of the local seawater and are unaffected by mineral detritus.
Results
Re-Os abundance
Across the onset of OAE 2 there is a dramatic shift to very high values in Os isotope concentration.
At Portland Os concentration increases by ∼1000 ppt within ∼10 cm; at Wunstorf an increase of ∼1000 ppt within ∼30 cm; Site 1260 increases by ∼1000 ppt in <60 cm; in the Vocontian Basin there is an increase of ∼3500 ppt within 50 cm.
In both Furlo and Site 530 there are very considerable changes in the Os concentration; >10 000 ppt within 10 cm and 40 cm, respectively.
Conversely, Re abundance is relatively constant at each section, therefore the dramatic difference between the Re and Os abundance produce a similar profile in 187Re/188Os to the Osi profile, with an abrupt decrease in the 187Re/188Os directly associated with the abrupt increase in Os.
187Os/188Os isotope stratigraphy
The Osi profiles for all six sections show a similar trend; highly radiogenic values that suddenly become unradiogenic, before gradually returning to radiogenic values (Figs.
2, 3; all Osi data presented in full in Supplementary Material Table 2).
At Portland the Osi values show some distinct fluctuations prior to the onset of OAE 2 (point 'A' on the δ13Corg curve).
The Os trend from ∼1.0 to 0.9, briefly return to ∼1.0, and then drop abruptly to ∼0.7 at ∼-237 kyr (below 'A').
The trend toward unradiogenic values then reverses back toward the radiogenic end member up until the major shift to unradiogenic Osi at 'A'.
In the Site 1260 record, a trend from ∼0.6 to 1.0 in the lowest samples is followed by a shift in the opposite direction, toward the unradiogenic end-member, but the values are variable and some spikes to >1 (radiogenic) persist.
From -157 kyr there is a consistent trend toward unradiogenic Osi reaching a minimum value of ∼0.2 at the 'A' datum.
At Wunstorf the rock units prior to 'A' are bereft of Re and Os.
The Vocontian Basin records shifts to radiogenic Osi values (>0.9), before a gradual decrease to ∼0.76 followed by a brief increase to ∼0.82.
A few metres below the positive excursion a major shift to <0.3 occurs.
The Osi values at Furlo remain stable at ∼0.55 then shift suddenly to ∼0.65.
Above this horizon there are no samples until 'A' when the Osi is unradiogenic <0.3.
Site 530 has Osi values of ∼0.70 before showing a 0.2 decrease.
The trend reverses to ∼0.7, then the major unradiogenic shift to <0.2 at 'A'.
Importantly, the Osi record in the Portland core (Fig. 3) is significantly different between ∼-230 kyr and ∼-50 kyr relative to the other 4 sites (no data for Wunstorf).
From 'A' through to the lower Turonian, the Osi profiles and values are very similar across Portland, Furlo, Site 530 and Wunstorf, progressively trending from unradiogenic (∼0.2) to radiogenic values (∼0.6 to 0.7; Fig. 3) within ∼350 kyr.
The Osi values from point 'A' remain unradiogenic for ∼200-250 kyr before becoming progressively more radiogenic (Fig. 3).
The majority of the Osi data from the Vocontian Basin, from slightly before the onset of the positive δ13Corg excursion through the initial ∼200 kyr are unradiogenic at ∼0.2, with some fluctuation to ∼0.4.
In contrast to other sites that show a progressive return to radiogenic Osi values, the Vocontian Basin remains at values of ∼0.4 for an additional 200 kyr and then becomes radiogenic (0.94) very rapidly (within ∼80 kyr; Fig. 3).
This abrupt change could indicate a minor hiatus during the latter part of OAE 2.
The Osi values at Site 530 remain unradiogenic (0.12-0.25) for ∼145 kyr, returning to radiogenic values after ∼270 kyr.
However, due to poor core recovery there is a ∼125 kyr gap in the Osi record (Fig. 3).
Discussion
Heterogeneous seawater 187Os/188Os prior to OAE 2
Overall the Osi profiles from each section show similar variability in Osi values and in the 187Re/188Os composition before and during OAE 2.
Combined with previous Os isotope stratigraphy (Turgeon and Creaser, 2008) and detailed litho-, bio-, and chemostratigraphy, the sections are interpreted to be reliable records of the CTBI.
The Osi values for all sites in the WIS, western Tethys and proto-North Atlantic from -800 kyr to -210 kyr are radiogenic, and range from ∼0.5 to ∼1.0, illustrating that the seawater 187Os/188Os ratio during this time was not homogeneous, but was controlled by the 187Os/188Os composition of the fluxes entering the individual basins (Figs.
1, 3).
The radiogenic heterogeneity and high Osi values at Portland are attributed to the influence of weathered crustal components from the Sevier Orogenic Belt and the Canadian Shield, the major sources of weathered material to the basin.
Recent seawater Os isotope studies during glacial episodes in the last 200 kyr demonstrate how regional variation is correlated to the heterogeneous flux of material into proximal basins (Paquay and Ravizza, 2012).
This hypothesis is supported by the observed radiogenic Osi values for >500 kyr prior to 'A' at Portland and elsewhere (Fig. 3).
We therefore infer that water masses were reasonably well connected until ∼-210 kyr, but the 187Os/188Os composition of the seawater in the individual basins was strongly influenced by regional factors (Figs.
1, 3).
In addition, the heterogeneity of the 187Os/188Os data may provide information on vertical mixing as a function of depth and circulation; the variations may indicate that seawater was not always well mixed.
Implications of basin connectivity
Between ∼-300 and -200 kyr, Osi values at Portland in the WIS reverse toward more radiogenic values.
A similar pattern is observed at Site 1260, Vocontian Basin, Furlo and Site 530, although in each of these sites the radiogenic Osi inflection is brief (only a single data point) before the decline in Osi values (Fig. 3).
There are two possible mechanisms that could contribute to produce an Osi signal of this type within a shallow epeiric seaway: increased input of weathered material and restriction of the connection to the open ocean, which would allow a radiogenic (weathering input) signal to dominate (e.g., Portland and the Vocontian Basin).
In contrast, it is assumed that deep water sites preserve a signal more consistently representative of the open ocean (e.g., Site 1260 and 530).
The shallow epeiric setting at Portland would certainly have become restricted from the global ocean during sea-level lowstands, however, the degree of sea level fall necessary to produce restriction is difficult to know.
There is evidence of a small hiatus and a bone bed within the uppermost Hartland Shale, and two seaward stepping parasequences in the Dakota Formation of SW Utah correlate basinward to a level just below this hiatus (Elder et al., 1994), suggesting that a minor relative sea-level fall may have occurred (Gale et al., 2008).
Subsequently, the lowermost beds of the Bridge Creek Limestone contain a diverse marine fauna with many Tethyan taxa (Kauffman, 1984), and there is strong evidence for transgression during the deposition of the basal limestone bed (Arthur and Sageman, 2005).
Thus, the onset may have been immediately preceded by a relative fall in sea level that could have briefly reduced or shut down exchange of water masses with the global ocean, followed by a rapid sea-level rise.
Basin restriction may also provide an explanation for the delayed return to pre-OAE 2 Osi values in the Vocontian Basin.
Osi values return to ∼0.3 at 'B' comparable to other sections (Fig. 3).
Yet between 'B' and 'C' the Osi values fluctuate around ∼0.4 for an additional ∼200 kyr relative to other sites, which suggests that mixing with the rest of the proto-North Atlantic was temporarily limited.
Implications of enhanced weathering
To explain the radiogenic pre-OAE 2 Osi values, a continuous radiogenic continental input into the ocean is required (Peucker-Ehrenbrink and Ravizza, 2000).
Hence, the other mechanism resulting in radiogenic Osi values is a significant increase in the flux of weathered material to a basin.
Interpreted increases in temperature before 'A' indicate a period of significant warming (Clarke and Jenkyns, 1999; Forster et al., 2007; Jenkyns et al., 2004; Barclay et al., 2010), an intensification of the hydrological cycle, and more extensive flooding in continental interiors, which led to the build-up of terrestrially derived nutrients and organic-rich sediments in shallow basin water masses immediately prior to 'A'.
The radiogenic Osi prior to 'A' reflects sequestration of hydrogenous Os derived from the continent as a result of high weathering rates.
If, in fact, the WIS did become briefly restricted, the influence of local weathering inputs and changes in mixing between basins would be amplified in the seawater chemistry.
Additionally, there is evidence that increased input of weathered material influenced the Os chemistry of the shallow bathyal Site 1260 and the abyssal Site 530 before 'A'.
Continental turbiditic sediments deposited on the continental slope at Site 1260 produce an oscillating Osi profile before the onset.
At Site 530, comparatively less radiogenic Osi values prior to 'A' suggest that juvenile turbidites were sourced from juvenile detritus from the Walvis Ridge.
The high rates of weathering produced waters enriched in micro-nutrients that led to an increase in productivity coincident with OAE 2, which is supported by bulk rock enrichments of Si, P, Ba, Cu, Mo, Ni and Zn in black shales at Demerara Rise ODP sites (Jimenez Berrosoco et al., 2008).
In addition, enhanced weathering is inferred from Sr isotope trends, which despite possessing a longer residence time (1-4 Ma) have been interpreted to reflect global warming prior to, and during OAE 2 (Frijia and Parente, 2008).
Caribbean large igneous province and OAE 2
In contrast to the elevated radiogenic Osi values just before 'A' at Portland, the Osi values of Site 1260, Vocontian Basin, Furlo and Site 530 show a progressive trend to unradiogenic Osi values (0.75 to 0.55) over ∼155 kyr (Fig. 3) suggesting that hydrothermal input dominated Os chemistry in the open oceans.
Within the WIS the stratigraphic evidence for sea-level rise is coincident with an abrupt shift of radiogenic Osi values to very unradiogenic values at Portland ∼50 kyr prior to 'A'.
Therefore the trend to almost homogeneous unradiogenic Osi recorded in all sites at 'A' requires a sustained source of unradiogenic Os input to the ocean.
Basaltic igneous provinces release unradiogenic Os, close to chondritic values (∼0.13; Cohen and Coe, 2002).
There are two potential sources of volcanism: the Caribbean LIP and the High Arctic LIP.
The eruption history from the High Arctic remains poorly constrained (Tegner et al., 2011) and trends interpreted at this stage are relatively ambiguous (Zheng et al., 2013).
Consequently, the abrupt unradiogenic trend is interpreted to reflect an episode of submarine mafic volcanism from the Caribbean LIP (Fig. 3), sufficient to influence the global Os isotope budget (Turgeon and Creaser, 2008).
The high-resolution of the Osi data presented here make an important contribution to the discussion of Caribbean LIP onset and cessation.
Evidence supports the hypothesis that an influx of unradiogenic Os in the marine Os record is a direct consequence of volcanism (Ravizza and Peucker-Ehrenbrink, 2003).
From ∼-50 kyr all sites show a synchronous abrupt trend towards unradiogenic Osi values (Fig. 3).
Based on the trend to unradiogenic Osi values at Site 1260, Vocontian Basin, Furlo and Site 530 we suggest that the initiation of volcanism was at least ∼200 kyr prior to 'A' (∼94.58 Ma; Fig. 3, CLIP i), with the major pulse of submarine volcanism happening at ∼-30 kyr (94.41 Ma; Fig. 3, CLIP ii), where all locations possess near mantle-like Osi values.
The timing of Caribbean LIP ii is supported by the rapid change in Os concentration (Section 4.2; Supplementary Material, Table 2) in all sections with the exception of Wunstorf where there is no record (Fig. 3).
The sudden and high increase in Os concentrations occurs within 1 metre of deposition, which equates to <20 kyr at Furlo and Site 530, and <10 kyr at Portland, Site 1260 and Vocontian Basin.
The increase in concentration is directly synchronous with the abrupt decrease to very low seawater Osi values and is contemporaneous with 'A' within <20 kyr.
The trend recorded in the new sections studied here is consistent with the pattern observed in the previous work by Turgeon and Creaser (2008), where there was a clear and large increase in Os concentration at the onset of OAE 2.
As discussed, high weathering rates across the CTB released large amounts of organic-rich material to the oceans, which sequester hydrogenous Os (Peucker-Ehrenbrink and Ravizza, 2000).
The trend therefore implies that within <20 kyr the amount of unradiogenic dissolved Os to seawater significantly reduced the influence of radiogenic Os (Cohen and Coe 2002, 2007; Ravizza and Peucker-Ehrenbrink, 2003).
Therefore, the observed regional variations in the data support the short residence time of Os in seawater and confirm the capability of Os to detect short-term forcing mechanisms, such as activity from LIPs.
If we assume the emplacement and weathering of the LIP are direct indicators of volcanic activity (Cohen and Coe 2002, 2007), we can estimate the duration of volcanism at the Caribbean LIP based on the marine 187Os/188Os record.
During the emplacement of the LIP we assume that growth of the plateau does not continue to affect the Os isotope composition (Robinson et al., 2009), since the Osi values are homogeneous (∼0.2; Fig. 3).
The subsequent trend to radiogenic Osi values ∼200 kyr after 'A' potentially represents the cessation of volcanism.
If we consider that the predominant 187Os/188Os of the ocean prior to the Caribbean LIP was 0.8, the influence of Os abundance and isotopic composition from the Caribbean LIP was less than 5% once the seawater 187Os/188Os had reached ∼0.50, which occurred ∼450 kyr after the onset (until ∼94.13 Ma; Fig. 3).
Hiatuses identified during the CTBI
At Portland the ∼17 kyr hiatus above 'B' was previously identified by Meyers and Sageman (2004), and the hiatus just before 'A', though quantitatively unconstrained, is equally minor based on site comparison (Elder et al., 1994; Ma et al., submitted).
This study has identified one hiatus in the higher part of the OAE 2 at Site 1260.
At Site 1260 Erbacher et al. (2005) suggested that ∼150 kyr is missing from Site 1258, yet present at Site 1260.
However, distinct lithological breaks in the core images at 425.19 m and the δ13Corg record indicate that the hiatus may also be present in the latter section.
A 150 kyr hiatus is inferred here from the Os isotope profile (Fig. 3).
Palaeocirculation across OAE 2
A model of quasi-estuarine circulation that was proposed for the WIS, which includes surface outflows causing deeper Atlantic/Tethyan water masses to be advected into the basin (Slingerland et al., 1996), is also suggested as a means to import Caribbean LIP influenced proto-Pacific waters into the proto-Atlantic and Tethys (Trabucho-Alexandre et al., 2010).
The similar shape of the Osi profiles (from ∼-50 kyr until ∼200 kyr into OAE 2) suggest that unradiogenic Os-bearing water was rapidly transported from the proto-Pacific into and across the proto-North Atlantic/Tethys, and into the WIS (Fig. 4).
This model is consistent with the hypothesis that palaeocirculation was not sluggish, as also indicated by climate models (Trabucho-Alexandre et al., 2010) and data from Nd isotopes.
The latter suggest a dynamic deep/bottom-water circulation (MacLeod et al., 2008; Martin et al., 2012) that is interpreted to reflect the relationship between bottom-water sources, climate, ocean anoxia, and circulation (Martin et al., 2012).
Conclusions
Submarine volcanism alone cannot be the sole driving mechanism for OAEs, especially OAE 2.
Osi data from 6 transatlantic and epeiric sections demonstrate that OAE 2 resulted from a combination of interacting factors.
An influx of nutrients from the continents preconditioned the oceans and helped to trigger OAE 2 through increased productivity and, similarly to Jones and Jenkyns (2001), we infer that rising sea level may have been the tipping point for the development of widespread anoxia.
The Osi profile at Portland suggests that the restriction of the epeiric WIS during the pre-OAE 2 interval amplified the affects of high weathering rates as abundant organic-rich sediments sequestered radiogenic Os derived from the ancient continental crust.
The close similarity of Osi profiles from ∼50 kyr prior to the OAE 2 and throughout the syn-OAE 2 interval indicates that transgression progressed to a point where a homogeneous global seawater signal was delivered to multiple proto-transatlantic basins by active ocean circulation.
Furthermore, the synchronicity of the unradiogenic Osi pattern suggests that the magnitude of Caribbean LIP volcanism was sufficient to simultaneously influence the seawater chemistry of each basin; the abundance of organic-rich sediments added to the water column as a result of enhanced continental weathering permitted sequestration of hydrogenous unradiogenic Os from the contemporaneous Caribbean LIP.
The temporal coincidence provides empirical evidence for the duration of the Caribbean LIP of ∼450 kyr.
Acknowledgments
We would like to thank C.
Dale, A.
Finlay, J. Trabucho-Alexandre, and Joanne Peterkin for laboratory assistance and discussion.
Thanks to H.C.
Jenkyns for providing additional samples from Furlo for Osi analysis.
The project was possible thanks to a NERC small grant to D.S. and D.R.G.
(NE/G009678/1), NERC standard grants to D.R.G. and D.S.
(NE/H021868/1) and I.J.
(NE/H020756/1), BP funding to D.S., and a grant for fieldwork awarded to A.D.C.
Du Vivier by the Geologists' Association.
We are grateful for the constructive reviews from H.
Jenkyns, G.
Ravizza and G.
Henderson that helped improve this manuscript.
Supplementary material
Supplementary material related to this article can be found online at http://dx.doi.org/10.1016/j.epsl.2013.12.024.
Supplementary material
The following is the Supplementary material related to this article.MMC 1
The document contains additional information on sampling and analytical protocol to support the manuscript by Du Vivier et al.
MMC 2
Table 1.
The workbook contains 6 tables of δ13Corg data for each section analyzed in the manuscript: Portland, Wunstorf, Site 1260, Vocontian Basin, Furlo, and Site 530.
MMC 3
Table 2.
The workbook contains 6 tables of Re-Os geochemical data for samples for the CTB, for each section analyzed in the manuscript: Portland, Wunstorf, Site 1260, Vocontian Basin, Furlo, and Site 530.
MMC 4
Table 3.
The workbook contains Re-Os geochemical data for USGS Std, SDO-1.


Electrochemical lithium storage kinetics of self-organized nanochannel niobium oxide electrodes

Oriented nanochannel Nb2O5 films were prepared by electrochemical anodization of Nb foils (99.8% purity, Alfa Aesar). The anodization process was performed in a 10 wt.% K2HPO4 containing glycerol solution with a Pt counter electrode. The Nb foils were biased at 10 and 20 V for 5-25 min at 180 degC to produce nanoporous films with thicknesses ranging from 7 to 18 μm. After electrochemical anodization, the as-prepared films were cleaned with distilled water several times. The Nb2O5 films on Nb substrates were heated to 440 degC in air at a heating rate of 1 degC min-1. After annealing at 440 degC for 20 min, the furnace was cooled to room temperature automatically. The crystal structure of the obtained samples was identified by X-ray diffraction (XRD, X'Pert PRO Multi-Purpose X-ray diffractometer with Cu Kα radiation, operating at 60 kV and 55 mA). The film thickness and morphology of the prepared samples were examined by field-emission scanning electron microscopy (FE-SEM, JEOL 7500) and field-emission transmission electron microscopy (FE-TEM, JEM-2100F, operating at 200 kV).
Opening the gateways for diatoms primes Earth for Antarctic glaciation

Abstract
The abrupt onset of Antarctic glaciation during the Eocene-Oligocene Transition (∼33.7Ma, Oi1) is linked to declining atmospheric pCO2 levels, yet the mechanisms that forced pCO2 decline remain elusive.
Biogenic silicon cycling is inextricably linked to both long and short term carbon cycling through the diatoms, siliceous walled autotrophs which today account for up to 40% of primary production.
It is hypothesised that during the Late Eocene a sharp rise in diatom abundance could have contributed to pCO2 drawdown and global cooling by increasing the proportion of organic carbon buried in marine sediment.
Diatom and sponge silicon isotope ratios (δ30Si) are here combined for the first time to reconstruct the late Eocene-early Oligocene ocean silicon cycle and provide new insight into the role of diatom productivity in Antarctic glaciation.
At ODP site 1090 in the Southern Ocean, a 0.6‰ rise in diatom δ30Si through the late Eocene documents increasing diatom silicic acid utilisation with high, near modern values attained by the earliest Oligocene.
A concomitant 1.5‰ decline in sponge δ30Si at ODP site 689 on the Maud Rise tracks an approximate doubling of intermediate depth silicic acid concentration in the high southern latitudes.
Intermediate depth silicic acid concentration peaked at ∼31.5Ma, coincident with the final establishment of a deepwater pathway through the Tasman Gateway and Drake Passage.
These results suggest that upwelling intensification related to the spin-up of a circum-Antarctic current may have driven late Eocene diatom proliferation.
Organic carbon burial associated with higher diatom abundance and export provides a mechanism that can account for pCO2 drawdown not only at, but also prior to, Antarctic glaciation as required by a pCO2 'threshold' mechanism for ice sheet growth.
Graphical Abstract
Highlights
•
Diatom and sponge δ30Si used to reconstruct the Eocene-Oligocene silicon cycle.
•
0.6‰ late Eocene rise in diatom δ30Si (increasing silicic acid utilisation).
•
1.5‰ late Eocene/early Oligocene sponge δ30Si fall (increasing silicic acid concentration).
•
Upwelling related to proto-ACC facilitated late Eocene diatom proliferation.
•
Diatom driven carbon burial contributed to pCO2 decline before Antarctic glaciation.

Introduction
Diatoms are prolific phytoplankton, today accounting for up to 40% of global primary production (Tréguer et al., 1995).
The formation of opaline frustules by diatoms imposes an absolute requirement for silicon, fundamentally linking the modern marine silicon and carbon cycles.
Among the phytoplankton, diatoms are notably important for the export and burial of organic carbon for a number of reasons.
Firstly, diatoms efficiently export carbon from the surface ocean: their silica frustules provide ballast material, they are relatively large cells at the base of short food webs, and they form blooms that can terminate in aggregation and mass settling of cells (Baines et al., 2010; Buesseler, 1998; Ragueneau et al., 2000; Smetacek, 1999).
Secondly, diatoms may contribute up to 75% of primary production in upwelling regions of the world's oceans (Nelson et al., 1995).
In these areas, nutrients and CO2 sequestered in the deep ocean by the biological pump are resupplied to the surface.
Diatom domination of the resident phytoplankton community makes them critical in determining the magnitude of the CO2 flux to the atmosphere and the efficiency of the biological pump (Marinov et al., 2006; Sarmiento et al., 2004).
Today, diatom production in the Southern Ocean, coupled with the unique overturning circulation of the Antarctic Circumpolar Current (ACC), plays an important role in determining the efficiency of the biological pump and in setting the ocean-atmosphere CO2 balance.
The wind-driven ACC facilitates deep upwelling to the south of the Polar Front.
These upwelled waters are advected northwards by Ekman transport and thus provide nutrients that fuel diatom (as well as other phytoplankton) production, creating marked bands of opal and carbon export (Fig. 1; Speer et al., 2000; Ito et al., 2005).
These surface waters are eventually subducted to intermediate depths and supply nutrients to the lower latitude thermocline.
Thus, the amount and proportion of macronutrients used in this region play a major role in determining the efficiency of the biological pump globally (Sarmiento et al., 2004).
Travelling an alternative path, some of this upwelled water moves southwards, eventually forming Antarctic Bottom Water.
The magnitude of phytoplankton production and export along this path relative to the degree of nutrient supply sets the degree to which CO2 escapes from the ocean.
Today, incomplete nutrient utilisation makes this a region of CO2 venting to the atmosphere (Marinov et al., 2006).
Diatom radiation, coupled with the development of an ACC-type circulation, may have substantially increased the efficiency of the biological pump during the late Eocene and early Oligocene, facilitating organic carbon burial and contributing to the pCO2 drawdown hypothesised to have driven Antarctic Glaciation (Rabosky and Sorhannus, 2009; Scher and Martin, 2006).
Diatoms originated in the Jurassic, and open ocean diatom diversity and abundance increased through the late Cretaceous and Palaeogene, pumping silicic acid to depth and driving a concomitant decrease in shelf chert abundance (Maliva et al., 1989).
The late Eocene witnessed both a second diatom proliferation, culminating in an explosion of species diversity (Rabosky and Sorhannus, 2009) and enhanced Southern Ocean biogenic opal burial (Salamy and Zachos, 1999; Schumacher and Lazarus, 2004).
An increase in benthic δ13C, indicative of enhanced organic carbon burial, took place concomitantly (Cramer et al., 2009), particularly in the high latitude Southern Ocean.
The opening of the two tectonic gateways that allow an unimpeded current flow in the Southern Ocean also occurred gradually through the late Eocene and into the Oligocene.
Initial subsidence began between 50 (Drake Passage) and 35.5Ma (Tasman Gateway), with a deep connection completed by 33.5-30.2Ma (Livermore et al., 2007; Mackensen, 2004; Stickley et al., 2004).
From the time when these gateways opened, even to a relatively shallow depth, a complete pathway for wind-driven currents would have begun to affect Southern Ocean circulation, gradually intensifying upwelling as subsidence progressed (Livermore et al., 2007; Sijp et al., 2011).
The Late Eocene is thus hypothesised as the time when the important relationship between diatoms and the ACC developed.
Affirming the link between diatom radiation and late Eocene pCO2 drawdown requires evidence that diatom diversification and opal deposition corresponded to increased diatom productivity and organic carbon burial.
To this end, combined diatom and sponge silicon isotope records (δ30Si=[(30Si/28Si)Sample/(30Si/28Si)Standard-1]×103) are used here to characterise the late Eocene-early Oligocene marine silicon cycle.
Diatom δ30Si is a proxy for diatom silicic acid utilisation (Fig. 2a) because frustule growth preferentially incorporates 28Si over 30Si with 30ε∼-1.1‰ (De La Rocha et al., 1997).
Silicic acid utilisation is determined by both diatom uptake (broadly reflecting diatom productivity) and silicic acid supply to the surface ocean.
Although diatom δ30Si has previously been used to reconstruct silicic acid utilisation, efforts to convert this to productivity have been hampered by a lack of knowledge of the concentration of silicic acid supplied from depth.
Benthic siliceous sponges form spicules with a silicon isotope fractionation factor dependent on ambient silicic acid concentration (Hendry et al., 2010; Hendry and Robinson, 2012; Fig. 2b).
Sponge δ30Si constitutes a proxy for bottom-water silicic acid concentration and ocean circulation (Fig. 2b) which can be used as a constraint on the concentration of silicic acid supplied to the surface ocean, facilitating resolution of the effects upon diatom δ30Si of diatom uptake and silicic acid supply.
Materials and methods
Study sites and age models
Diatom and sponge δ30Si were analysed in sediments from ODP Leg 177, Site 1090B, and ODP Leg 113, Site 689B, respectively (Table 1 and Fig. 3).
Age control for our records comes from magnetostratigraphic age models (Site 689; Florindo and Roberts, 2005; Spiess, 1990 and Site 1090; Channell et al., 2003) on the CK95 GPTS (Cande and Kent, 1995), which yield satisfactory correlation of benthic foraminiferan/bulk δ13C and δ18O (Cramer et al., 2009; Pusz et al., 2011) between sites given our sample resolution of 200-500kyr (Fig. A1).
Methods
Sediment preparation for diatom δ30Si analysis
Initial sediment cleaning to extract biogenic opal followed well-established methodologies using H2O2, HCl and heavy liquid (sodium polytungstate) (Hendry and Rickaby, 2008; Morley et al., 2004; Shemesh et al., 1995).
Opal was then separated into narrow size ranges using microfiltration (Minoletti et al., 2009).
Recent work has shown the 2-20μm fraction is generally most representative of diatom δ30Si (Egan et al., 2012).
An initial test measuring the δ30Si of the 2-10μm, 10-20μm and 20-41μm size fractions from selected samples was conducted to establish the correct size fraction for targeting diatom δ30Si (Fig. 4).
Highest values in, and consistency between, the 2-10μm and 10-20μm fractions indicate these fractions best represent diatom δ30Si (Figs.
4 and 6), consistent with smear slide observations.
Consequently, the 2-10μm size fraction was always analysed for diatom δ30Si.
The 10-20μm fraction was additionally measured at given intervals to ensure diatom δ30Si values were not offset by low δ30Si silica components such as sponge spicules and radiolaria.
It is acknowledged that analysing smaller diatom size fractions may introduce a small seasonal or species-specific bias to measured diatom δ30Si values.
However it is clear from the work of Egan et al. (2012) that this is of minimum concern compared to the effect of including larger size fractions, which are highly likely to contain non-diatom opal in the Southern Ocean.
This is further attested to by the core top study of Ehlert et al. (2012), who found that opal in the 11-32μm size range in the Peruvian upwelling zone could be significantly offset from the δ30Si of handpicked diatom samples by non-diatom opal.
Selection of the smaller size fraction in this region is therefore essential to producing a δ30Si record that documents the target environmental parameter, silicic acid utilisation, rather than the amount of non-diatom contamination.
After microseparation, diatom opal was subjected to additional chemical cleaning following Ellwood and Hunter (1999), which included reduction (hydroxylamine hydrochloride/acetic acid solution), etching (NaF) and oxidation (strong acid solution of 50% HNO3+10% HCl).
Three samples from Site 1090 covering the age range investigated were studied by X-ray diffractometry.
All showed the diffuse peak representative of amorphous opal providing confirmation that the samples have not undergone diagenetic transformation to opal-CT.
Sponge spicule preparation
Sediments were prepared following existing techniques (Hendry et al., 2010), which were refined slightly to better suit the microseparation technique utilised here as a convenient alternative to sieving.
Initial cleaning with H2O2 and HCl was carried out to concentrate biogenic opal.
From this pre-cleaned sample the >100μm fraction was separated using microseparation (Minoletti et al., 2009) and between 50 and 100 spicules were hand-picked.
A range of spicule morphotypes were included, as it has been shown that neither spicule morphology nor species composition creates any consistent offset in δ30Si (Hendry et al., 2010, 2011).
Spicules were sonicated in reagent grade methanol and dried down in 200μL of concentrated HNO3.
Sponge δ30Si data presented here are generally in agreement with the data from this site produced by De la Rocha (2003), despite different methodologies and specific sampling intervals (Fig.
S2).
XRD analysis showed sponge spicules from the Eocene/Oligocene interval of this site to be amorphous opal (De la Rocha, 2003).
Opal dissolution and analysis
Cleaned sponge and diatom opal was dissolved via wet alkaline digestion (Cardinal et al., 2007; Ragueneau et al., 2005) in 0.2M NaOH at 100°C for 40min (diatoms) or up to 1 week (sponge spicules).
The samples were acidified to pH∼2 with 0.2M thermally distilled HCl and separated from major ions using cation exchange resin (BioRad AG50W-X12, Georg et al., 2006).
Silicon isotope analysis was carried out using a Nu Instruments Nu-Plasma HR multi-collector inductively coupled plasma mass spectrometer run in medium resolution mode (m/Δm∼3500 at 5% and 95%).
Samples were introduced via a self-aspirating PFA microconcentric nebuliser (ESI) in a Cetac Aridus II desolvating unit.
Measurements included six to eight standard-sample brackets (brackets where the rate of machine drift outstripped bracketing rate were disregarded), each composed of twenty eight-second integrations.
Samples were measured relative to the NIST RM 8546 standard.
The external diatomite standard (1.26±0.2‰, Reynolds et al., 2007) yielded a mean and 2SD of 1.23±0.25‰ (n=104).
Error bars in the figures and text are this 2SD external reproducibility unless the internal reproducibility of the standard sample brackets was larger, in which case this is quoted instead.
Diatom species-specific silicon isotope fractionation
Based on culture experiments (Sutton et al., 2013), it has recently been suggested that the diatom silicon isotope fractionation factor may be species dependent, and in particular that the genus Chaetoceros may have a significantly larger, and the species Fragilariopsis kerguelensis, a significantly smaller 30ε than the widely accepted value of ∼-1‰ (De la Rocha et al., 1997).
Our previous Southern Ocean core top study included Antarctic Peninsula sediments containing Chaetoceros sp.
and Pacific sector sediments containing F. kerguelensis (Egan et al., 2012).
The size fractions analysed had up to 40% differences in the abundance of these diatoms (Fig. 5).
However, there is no detectable δ30Si offset between size fractions in the core tops TAN127 or 385 (Fig. 5a and b), whilst in KC08A (Fig. 5c), there is a slight negative offset of the 12-20μm fraction which is opposite to that expected if the abundance of Chaetoceros was the cause (the suggested larger fractionation would act to lower δ30Si in the smaller size fractions).
Rather, the δ30Si values of size fractions between 2 and 20μm display a good correlation (r2=0.92, Fig. 2a) with surface silicic acid concentration and, where the water source δ30Si and silicic acid concentration are known or can be reasonably estimated, converge on an apparent 30ε of ∼-1‰ (Egan et al., 2012; Fripiat et al., 2011c).
This suggests that whilst inter-specific differences in fractionation factor exist in culture, in the natural environment the presence of such species does not yield a large offset in the δ30Si signature of the diatom population as a whole.
Although a down core effect cannot be completely ruled out, the good agreement between the 2-10μm and 10-20μm size fractions in our record from Site 1090 (Fig. 6) and our previous core top study (Figs.
2 and 5) suggest that changes in the species specific fractionation factor are unlikely to be the dominant driver of Site 1090 diatom δ30Si variation.
Results and discussion
Our new records of diatom and sponge δ30Si are shown in Fig. 6c and b (respectively) alongside benthic δ18O stacks for the main ocean basins (Fig. 6a) in which the 1-1.5‰ positive excursion centred around ∼33.7Ma marks the onset of Antarctic glaciation (data from Cramer et al., 2009).
Through the late Eocene, between the beginning of our record at ∼38Ma and ∼33.2Ma, diatom δ30Si rose by ∼0.6‰ (Fig. 6c), equivalent to a doubling of silicic acid utilisation in the modern Southern Ocean (Fig. 2a).
Peak utilisation indicated by high, near modern values (Fripiat et al., 2011a) between 33.8 and 33Ma coincides with the glaciation driven δ18O maximum (Fig. 6a).
Sponge δ30Si declines by around 1.5‰ between 37 and ∼31.5Ma.
This translates to at least a doubling in silicic acid concentration at the intermediate depth of ∼1500m at site 689 (10-50 to 50-100μmol/kg, Fig. 2b; Hendry and Robinson, 2012).
High diatom silicic acid utilisation may result either from limited silicic acid availability or from a high degree of diatom uptake.
Several lines of evidence suggest that the late Eocene diatom δ30Si increase resulted from enhanced diatom uptake.
Firstly, the rising silicic acid concentration in intermediate waters through the late Eocene, as indicated by sponge δ30Si decline (Fig. 6b), suggests the supply of silicic acid to the Southern Ocean surface is most likely to have increased across the period of diatom δ30Si rise.
Alternatively, lower silicic acid supply may have resulted from enhanced stratification.
However, this situation cannot easily be reconciled with increasing opal contribution to Southern Ocean sediments during the late Eocene and Oi1 (Anderson and Delaney, 2005; Salamy and Zachos, 1999; Schumacher and Lazarus, 2004); the lower nutrient supply would limit diatom production and opal supply to the seafloor.
Lastly, higher diatom uptake as the driver of rising δ30Si at this time is in good agreement with peak diatom diversity (Rabosky and Sorhannus, 2009).
Together, the diatom and sponge δ30Si records provide the first biogeochemical footprint of major late Eocene diatom expansion and proliferation to near modern levels by the Oi1 event in the Southern Ocean.
Further insight into late Eocene-early Oligocene silicon cycling may be gained by defining Δδ30Si1090 diatom-689 sponge=Site 1090 diatom δ30Si-Site 689 sponge δ30Si.
Diatom δ30Si reflects both the silicic acid isotopic composition and silicic acid utilisation in surface waters.
However, here we can account for variations in the concentration and isotopic composition of upwelling waters by normalising the δ30Si-1090 diatom values.
This is achieved by subtracting Site 689 sponge δ30Si values from Site 1090 diatom δ30Si values.
Assuming that (1) sponge spicule silicon isotopes reflect both the silicic acid concentration and isotopic composition, and (2) the intermediate waters bathing Site 689 and the surface waters near Site 1090 have a common source, in defining Δδ30Si we are approximating a measure of the ocean's silicic acid gradient between Sites 689 and 1090.
Δδ30Si is a particularly powerful parameter eliminating the influence of whole ocean silicon isotopic shifts, which should affect diatom and sponge δ30Si equally.
It is important to account for this, as modelling work suggests that changes in whole ocean δ30Si of up to 0.4‰ may result from declining riverine inputs over ka timescales (De la Rocha and Bickle, 2005).
Δδ30Si1090 diatom-689 sponge increases by ∼1.5‰ between 37 and ∼34.5Ma, reaching a high just prior to, and stabilising across, the Oi1 event (Fig. 6d).
Linear regressions fitted through the sponge and diatom data between 38 and 34Ma were found to diverge significantly, with a p value of 2.4×10-5 for non-divergence using a maximum likelihood method.
The divergence in the δ30Si records from Sites 1090 and 689 provides evidence for the development of a strong silicic acid gradient between high latitude intermediate waters and more northerly surface waters during the late Eocene.
Based on interpolation of diatom δ30Si across a hiatus at Site 1090 (Fig. 7a), Δδ30Si1090 diatom-689 sponge reaches around 5‰ at ∼31.5Ma, approaching the modern day estimate of ∼5.5‰.
Today Δδ30Si1090 diatom-689 sponge is determined by both the silicic acid concentration of waters upwelled in the southern Antarctic Circumpolar Current (ACC), affecting the signal more strongly at Site 689, and the degree of diatom silicic acid utilisation during northerly Ekman transport, having a greater impact on Site 1090, as outlined below (Figs.
1 and 3).
Site 689 is situated at the eastern extremity of the Weddell gyre (Barker et al., 1990), monitoring the nutrient status of upwelling and re-subducting waters in the Antarctic Zone of the Southern Ocean (Figs.
1 and 3b).
Site 1090 is situated between the Sub-Antarctic and Sub-Tropical fronts (Gersonde et al., 1999) such that it underlies silicic acid depleted surface waters resulting from extensive diatom production along the northern pathway (Fig. 1 and 3b).
Neither site moved significantly during the Cenozoic, implying the modern system of upwelling and surface water advection that is critically dependent on the presence of both the ACC and high diatom productivity was emergent through the late Eocene and fully developed by the early Oligocene.
Late Eocene diatom proliferation likely occurred in response to subsidence of Southern Ocean land bridges and the concurrent development of circum-Antarctic upwelling.
Steadily increasing silicic acid concentration at Site 689 from the beginning of our record until ∼31.5Ma (Fig. 6b) is coincident with the subsidence of two southern hemisphere land bridges.
Initial subsidence began between 50 (Drake Passage) and 35.5Ma (Tasman Gateway) with a deep connection in place by 33.5-30.2Ma (Livermore et al., 2007; Scher and Martin, 2006; Stickley et al., 2004).
Before the opening of these gateways to circum-Antarctic current flow, the Southern Ocean was dominated by surface water sinking (Diester-Haass, 1995).
We suggest that this sinking drove a southern overturning loop, analogous to modern North Atlantic deepwater formation, which drew in low latitude silicic acid/nutrient poor surface waters (Fig. 7c).
Without Southern Ocean upwelling, or exchange between the Pacific and Atlantic basins, overturning circulation would have been sluggish (Cramer et al., 2009) trapping silicic acid and other nutrients in the deep ocean, limiting surface primary productivity and hence organic carbon burial via the biological pump.
Comparatively low early Palaeocene benthic δ13C in the Pacific basin suggests nutrient-rich deepwater in this region (Cramer et al., 2009).
As gateway opening allowed the development of a circum-Antarctic current, the onset of upwelling, along with water flow from the Pacific to Atlantic, would have brought this deep store of nutrients to the surface (Fig. 7b).
Such conditions of upwelling and high nutrient availability would have favoured diatom proliferation and opal export; high diatom nutrient uptake and growth rates, along with their large storage vacuole, provide a competitive advantage over other phytoplankton when nutrient input to the surface is high and episodic (Falkowski et al.,1998; Rabosky and Sorhannus, 2009; Tozzi et al., 2004).
Globally, the response to a newly established 'diatom pump' is expected to have been silicic acid drawdown, counter to the observed regional increase at Site 689.
Increased diatom productivity would have led to higher opal burial, lowering the whole-ocean inventory over time.
However, high silicic acid concentration was maintained at Site 689 due to the establishment of the Antarctic overturning circulation (Fig. 7).
Diatom expansion into this new, turbulent, nutrient-rich niche would have increased the proportion of organic carbon exported from the surface ocean and, together with an increase in Southern Ocean primary production, could have led to the observed increase in organic carbon burial during the late Eocene and early Oligocene (Figs.
1 and 6; Anderson and Delaney, 2005; Diester-Haass, 1995; Salamy and Zachos, 1999).
Diatom production is characterised by high, rapid particle flux out of near surface waters, a process which enhances labile carbon concentration and organic carbon remineralisation in the deep ocean (Henson et al., 2012).
Although diatom production is not always linked to carbon export in the modern ocean, a strong coupling between production and export is observed in regions where nutrient supply is continuous, strong seasonal blooms occur and a diatom flora particularly efficient at carbon export exists, such as at Southern Ocean sites such near the Antarctic Polar Front (Buesseler et al., 2001).
As such, diatom proliferation may have lowered pCO2 by facilitating coupled biogenic silica-organic carbon export to the deep ocean and marine sediments.
Additionally, ocean alkalinity would have shifted as a result of increasing organic, relative to inorganic, carbon production and deposition (Diester-Haass, 1995; Henson et al., 2012), in keeping with pCO2 decline beginning ∼2Ma prior to the Oi1 event (Fig. 6f).
Eocene fluctuations in the equatorial Pacific carbonate compensation depth, coincident with increases in the accumulation of biogenic silica and organic carbon, have been linked to labile organic carbon input (Pälike et al., 2012), hinting that the effects of diatom radiation were not limited to the Southern Ocean.
The Oi1 global δ13C excursion, indicative of organic carbon burial, coincides with the final approach of diatom δ30Si towards consistently modern values (Fig. 6c) and peak Southern Ocean opal accumulation (Anderson and Delaney, 2005; Salamy and Zachos, 1999).
At this time, upwelling may have been intensified by enhanced meridional temperature gradients resulting from Antarctic Glaciation, leading to elevated nutrient supply and driving a further pulse of biological productivity (Salamy and Zachos, 1999).
Subsequently, δ13C and opal burial decline, whilst diatom δ30Si remains elevated (Fig. 6), suggesting a highly utilised but much reduced surface silicic acid pool.
This is consistent with productivity decline driven by nutrient limitation (Salamy and Zachos, 1999), in particular diatom silicon starvation.
The Southern Ocean attains surface and deepwater silicon isotopic compositions indistinguishable from the modern day at around 33Ma.
This implies that once established, upwelling and diatom production, coupled with the deep regeneration cycle of biogenic opal, trapped silicic acid within a rather stable Southern Ocean "loop".
Today such self-limitation confines the diatoms to upwelling regions with high silicic acid concentrations.
It is interesting to consider that this sequestration of silicic acid in the deep ocean, which has often facilitated shifts in the locus of opal deposition during the latter Cenozoic (Cortese et al., 2004) through changes in the location of silicic acid resupply to the surface and altered advection of deepwaters, began as far back as the Late Eocene.
It is acknowledged that there is considerable complexity in the relationship between the diatoms, the biological carbon pump, and atmospheric pCO2.
This is particularly well illustrated by a number of hypotheses invoking a role for diatoms in glacial interglacial CO2 cycles, predominantly through their ability to alter the partitioning of carbon between surface and deep ocean reservoirs (e.g. iron fertilisation, changes in the ratio of siliceous to calcareous primary producers, silicic acid leakage; e.g.
Kohfeld and Ridgewell, 2009) and consequent effects on carbonate compensation.
Some, such as iron fertilisation, may also alter the degree to which diatom opal export is coupled with organic carbon and other nutrient export (Ragueneau et al., 2000).
It is additionally noted that increased upwelling associated with the onset of a circum-Antarctic current may well have released carbon stored in the abyssal ocean to the atmosphere.
Although the hypothesis that diatom radiation contributed to Eocene CO2 drawdown by increasing organic carbon burial is one of a number of possible interpretations of our data, and maybe subject to the aforementioned complexities, it is favoured by the long time interval considered.
Over millions of years, processes such as organic carbon burial, which control the size of the surficial carbon reservoir, are likely to be of greater importance to atmospheric pCO2 than mechanisms such as ocean circulation or biological mediation of the deep ocean carbon reservoir, which largely alter the partitioning of this surficial carbon between the ocean and atmospheric reservoirs (Berner, 2003).
Diatom carbon fixation and silicification, which may be decoupled by changes in iron fertilisation or the relative silicifcation of the ambient species (Ragueneau et al., 2000), are suggested to have remained coupled through this time period based on the broad coincidence between increased diatom production (δ30Si, Southern Ocean opal burial), increased organic carbon burial, and positively trending δ13C (Anderson and Delaney, 2005; Diester-Haass, 1995; Salamy and Zachos, 1999).
Conclusions
Reconstruction of the Eocene-Oligocene marine silicon cycle in the Southern Ocean has been achieved through the novel combination of diatom and sponge δ30Si.
Increasing diatom δ30Si, interpreted in light of both opal accumulation rates and sponge δ30Si, provides the first biogeochemical evidence for diatom radiation during the late Eocene.
Sponge δ30Si records increasing silicic acid concentrations at intermediate depths in the Antarctic Zone throughout the late Eocene and the early Oligocene, coincident with the subsidence of Southern Ocean land bridges.
Rising diatom abundance concomitant with reorganisation of Southern Ocean circulation, which are both key in controlling the marine silicic acid distribution, had implications for organic carbon burial and global climate.
A mechanistic link between the development of this system and Eocene-Oligocene pCO2 drawdown exists through diatom-driven burial of organic carbon triggered by intensified upwelling, suggesting that the biological carbon pump played a significant role in the onset of Antarctic glaciation.
In particular, this mechanism implies a drawdown of CO2 associated with events of a precursor nature to the glaciation, as is required from both modelling and pCO2 proxy reconstructions.
Author contributions
K.E.E. undertook sediment preparation, analysis of silicon isotope ratios and wrote the paper.
R.E.M.R., K.R.H. and A.N.H. designed the study (with contributions from K.E.E), discussed the results and commented on/revised the manuscript.
R.E.M.R. and A.N.H. funded the project.
Acknowledgements
This work was carried out as part of the Natural Environmental Research Council (NERC) Grant NE/F005296/1.
The Oxford isotope geochemistry lab is supported by an ERC grant to Halliday.
Thanks to the Ocean Drilling Program for supplying sediments, and to Jorn Bruggeman and Michäel Hermoso for help with statistics and lab work.
Many thanks to two anonymous reviewers, whose comments and suggestions were extremely helpful in improving the manuscript, and more generally for providing insight into silicon cycle dynamics.
Data tables are included in the Supplementary materials.
supplementary materials
Supplementary data associated with this article can be found in the online version at doi:10.1016/j.epsl.2013.04.030.
Supporting information
Supplementary Data
Supplementary Data

The formation of sulfate, nitrate and perchlorate salts in the martian atmosphere

Highlights
•
We model a modified martian atmosphere to calculate deposition fluxes of salts.
•
Estimated sulfate soil concentrations are consistent with observed sulfate on Mars.
•
Pernitric acid forms more rapidly in the martian atmosphere than nitric acid.
•
An atmospheric origin for perchlorate is precluded (or supplemented by other processes).
Abstract
In extremely arid regions on Earth, such as the Atacama Desert, nitrate, sulfate and perchlorate salts form in the atmosphere and accumulate on the surface from dry deposition according to diagnostic evidence in their oxygen isotopes.
Salts of similar oxyanions should have formed in the atmosphere of Mars because of comparable photochemical reactions.
We use a 1-D photochemical model to calculate the deposition rates of sulfate, nitrogen oxyanions, and perchlorate from Mars' atmosphere, given a plausible range of volcanic fluxes of sulfur- and chlorine-containing gases in the past.
To calculate integrated fluxes over time, we assume that throughout the last 3byr (the Amazonian eon), the typical background atmosphere would have been similar to today's cold and dry environment.
If the soil has been mixed by impact perturbations to a characteristic depth of ∼2m during this time, given a time-average volcanic flux 0.1% of the modern terrestrial volcanic flux, the model suggests that the soil would have accumulated 1.0-1.7wt.% SO42- and 0.2-0.4wt.% N in the form of pernitrate (peroxynitrate) or nitrate.
The calculated sulfate concentration is consistent with in situ observations of soils from rovers and landers and orbital gamma ray spectroscopy.
However, nitrates or pernitrates are yet to be detected.
The modeled formation of perchlorate via purely gas-phase oxidation of volcanically-derived chlorine is insufficient by orders of magnitude to explain 0.4-0.6wt.% ClO4- measured by NASA's Phoenix Lander.
The far smaller amount of ozone in the martian atmosphere compared to the terrestrial atmosphere and the colder, drier conditions are the cause of lower rates of gas phase oxidation of chlorine volatiles to perchloric acid.
Our calculations imply that non-gas-phase processes not included in the photochemical model, such as heterogeneous reactions, are likely important for the formation of perchlorate and are yet to be identified.

Introduction
The elemental composition of the martian soil was first determined by X-ray fluorescence spectroscopy on the Viking Landers (VLs), which measured absolute concentrations (Clark et al., 1977).
Concentrations of elements in what was called the "average deep soil" at the VLs only summed to ∼89%.
The missing ∼11% was attributed to bound water (H2O, -OH), carbonates, nitrates, phosphates and oxides, given that some light elements (H, C, N, O plus Li, B, and F) were undetectable while other elements were obscured (Na, P, Cr and Mn) (Clark et al., 1982).
How the elements were bound into salts was uncertain, although Mg and S were correlated in duricrust soils, which was interpreted as a cement of magnesium sulfate (Clark, 1993).
Some of the "missing components" in VL soil analyses have since been detected.
Orbital thermal infrared spectroscopy showed that 2-5wt.% carbonates are present in the global dust (Bandfield et al., 2003), while thermal evolved gas analysis in soils at the site of the Phoenix Lander revealed 3-6wt.% carbonate (Boynton et al., 2009; Sutter et al., 2012).
Additionally, thermal evolved gas analysis in soils at the Rocknest location in Gale Crater was consistent with the presence of 1-2% fine-grained siderite and/or magnesite (Leshin et al., 2013).
Orbital gamma-spectroscopy have also revealed 1.5-7.5wt.% water-equivalent hydrogen near the martian surface (Boynton et al., 2007), and the Sample Analysis at Mars (SAM) instrument suite on the Mars Science Laboratory (MSL) detected evolved H2O consistent with ∼1.5-3wt.% H2O in sand grains at the Rocknest location (Leshin et al., 2013).
The Wet Chemistry Laboratory (WCL) on the Phoenix Lander provided in situ measurements of the composition of soluble salts in the martian soil.
Soluble sulfate was present at 1.3±0.5wt.% (Kounaves et al., 2010b), along with cations of sodium, potassium, calcium and magnesium.
The most surprising result was the presence of perchlorate (ClO4-) at an inferred concentration in the soil of ∼0.5wt.% (Hecht et al., 2009; Kounaves et al., 2010a).
The dominance of Mg(ClO4)2 is consistent with simulations of evaporation and freezing at the Phoenix landing site (Marion et al., 2010); however, further analysis of data from the WCL suggests that Ca(ClO4)2 may be the dominant form of perchlorate (Kounaves et al., 2012).
The perchlorate-sensitive electrode in the WCL experiment was also sensitive to nitrate, but it was 1000 times more sensitive to perchlorate.
Thus, the methodology precluded the detection of nitrate because the signal would have required a mass of nitrate that exceeded the mass of the sample (Hecht et al., 2009).
Recently, the MSL mission has also confirmed the presence of perchlorate using pyrolysis as part of the SAM experiment (Steininger et al., 2013).
Specifically, pyrolysis showed release of chloromethane and O2 from heated soil samples at the Rocknest location, which is consistent with the decomposition of perchlorate (Sutter et al., 2013).
If all of the evolved O2 was released from perchlorate, then the samples contained a comparable amount of perchlorate to the samples at the Phoenix landing site (Leshin et al., 2013).
Furthermore, reanalysis of the Viking thermal volatilization experiments suggest ⩽1.6% perchlorate at both Viking 1 and Viking 2 landing sites (Navarro-Gonzalez et al., 2010); however, this has been subject to some debate (Biemann and Bada, 2011).
Native perchlorate has also recently been measured in the martian meteorite EETA79001, albeit at a level <1ppm by mass (Kounaves et al., 2014).
Given the various locations of possible detection, perchlorate appears to be ubiquitous on the martian surface.
Alpha-proton X-ray fluorescence on landers and rovers after the VLs has led to the inference of a global soil unit (Blake et al., 2013; Bruckner et al., 2003; Morris et al., 2010; Rieder et al., 2004; Yen et al., 2013, 2005).
Soils can have components derived from local rocks but these are imprinted upon a global-scale soil that has characteristic ratios of the concentration of certain elements (Mg, Al, K, Ca and Fe) relative to silicon and also a positive correlation of Cl and S. Given the lander and rover detections of perchlorate, the Cl in this global soil may well be in the form of perchlorate.
Salts are relevant to the habitability of Mars.
First, perchlorate reduction is a metabolism used by some terrestrial bacteria in anaerobic conditions (Coates and Achenbach, 2004).
If such organisms exist (or did exist) on Mars, then they could gain energy by reducing perchlorate and oxidizing an electron donor such as organic carbon or ferrous iron.
This would require organic molecules to be present on Mars.
Indigenous organic molecules have yet to be confirmed on Mars; however the presence of perchlorate itself may have inhibited the detection of organics on Mars in pyrolysis experiments (Navarro-Gonzalez et al., 2010).
Second, perchlorate salts are highly deliquescent and significantly lower the freezing point of liquid water (Gough et al., 2011).
The eutectic point of Mg(ClO4)2 is -57°C (Stillman and Grimm, 2011), while that of Ca(ClO4)2 is -75°C (Pestova et al., 2005).
Given typical soil salt concentrations, small amounts of water (∼0.02g H2O per g soil) would permit a water activity sufficient for terrestrial life to be viable (Kounaves et al., 2010b).
Third, if all organisms require fixed nitrogen in proteins and nucleic acids, as they do on Earth, then the discovery of nitrogen oxyanions on Mars would be significant as well.
Perchlorate may be advantageous to microorganisms, but its impact on human exploration is more complicated.
Perchlorate may be harmful because it is potentially toxic to humans if ingested (Urbansky, 2002).
On the other hand, perchlorate could be useful for future exploration of Mars.
It is kinetically stable at typical planetary surface temperatures, but at high temperatures perchlorate is a powerful oxidant suitable for rocket propulsion (Trumpolt et al., 2005).
So perchlorate could be utilized as an in situ fuel resource for sample return missions and in the eventual human exploration of Mars.
On Earth, one source of sulfate, nitrate, and perchlorate salts is atmospheric deposition.
The salts can build up in extremely arid environments, such as the Atacama Desert in Chile (Catling et al., 2010; Ericksen, 1983) and the Antarctic Dry Valleys (Kounaves et al., 2010c).
Given the oxic arid environment of Mars, it is possible that the same salts, or similar ones, have formed in the atmosphere.
This work estimates the photochemical formation and deposition rates of martian oxyanions, and evaluates the plausibility of atmospheric chemistry as an important source of salts in the global martian soil.
This study builds upon the work of Catling et al. (2010), which used a one-dimensional photochemical model to investigate the formation of salts in the terrestrial atmosphere over the Atacama Desert.
Using purely gas-phase pathways, Catling et al. (2010) reproduced measured profiles of chlorine species in the terrestrial atmosphere and estimated deposition rates of both perchlorate and nitrate that were consistent with Atacama soil measurements.
Several profound differences, however, alter the photochemistry of Mars relative to Earth, including atmospheric composition, density, and temperature.
Our results reveal how these differences impact the formation of atmospheric salts on Mars as compared to Earth.
Background: atmospheric perchlorates, sulfates, and nitrates on Earth versus Mars
Perchlorates, sulfates, and nitrates are formed in the atmosphere on Earth and by analogy may have formed in the atmosphere of Mars.
In this section, we consider how terrestrial pathways of salt formation inform possible mechanisms on Mars.
Perchlorate formation
Perchlorate salts produced synthetically on Earth can be distinguished from those produced naturally in the atmosphere by virtue of their oxygen and chlorine isotope ratios.
Synthetic formation creates perchlorate with ordinary mass-dependent oxygen isotope fractionation (i.e. oxygen isotopes are distributed according to the linear relationship, δ17O≈0.52×δ18O).
Conversely, naturally occurring perchlorate typically contains mass-independently fractionated oxygen isotopes, a deviation defined as Δ17O, where Δ17O=δ17O-0.52×δ18O.
The known source for Δ17O occurs during ozone formation, leading to stratospheric ozone with 30-40‰ (Thiemens, 2006).
Perchlorate that contains Δ17O thus preserves a telltale signature of a naturally occurring formation pathway involving stratospheric photochemistry.
Natural perchlorate has been discovered in locations across the globe, including the Atacama Desert (Bao and Gu, 2004), the Antarctic Dry Valleys (Kounaves et al., 2010c), and the American southwest (Jackson et al., 2010; Rajagopalan et al., 2006).
The Mojave Desert in California and Atacama Desert in Chile have the highest Δ17O values.
In perchlorate samples from the Mojave Desert, Δ17O=8.6-18.4‰ (Jackson et al., 2010), and in samples from the Atacama Desert, Δ17O=4.2-9.6‰ (Bao and Gu, 2004).
The occurrence of radioactive 36Cl (produced by cosmic rays acting on atmospheric 36Ar) relative to 37Cl (Sturchio et al., 2009) further confirms a stratospheric source for the chlorine in natural terrestrial perchlorate.
A gas phase pathway for producing perchlorate is through the reaction of halogens with ozone.
Simonaitis and Heicklen (1975) proposed that perchloric acid, HClO4, forms by:(1)Cl+O3+M→ClO3+M(2)ClO3+OH+M→HClO4+M
For this study, we employ Eqs.
(1) and (2) as the major pathway to form HClO4.
However, the effectiveness of this mechanism has been questioned by Prasad and Lee (1994) based on the slow three-body kinetics.
In the same study, the authors presented an alternative method of HClO4 formation involving ClO·O3 and ClO·O2 (Prasad and Lee, 1994).
ClO·O2 has been detected (Kopitzky et al., 2002), but the existence of ClO·O3 is hypothetical.
Still, it is possible that other atmospheric reactions may form perchloric acid in addition to, or in place of, Eqs.
(1) and (2).
Regardless of the formation pathway, HClO4 has been shown to be stable by ab initio methods (Francisco, 1995), and it has been detected in the terrestrial stratosphere (Jaegle et al., 1996).
In the absence of rain, perchloric acid accumulates on the surface through dry deposition, where it reacts with minerals to form perchlorate salts (Catling et al., 2010).
Another photochemical pathway to ClO3 that was not investigated by Catling et al. (2010) involves the effect of bromine oxides:(3)BrO+ClO→Br+OClO
OClO then can form ClO3 by(4)OClO+O3→ClO3+O2
It is reasonable to assume that when Mars was volcanically active, the atmosphere contained the reactants for the above reactions.
There are multiple lines of evidence that volcanoes on Mars emitted both HCl and HBr.
First, the inferred composition of the martian mantle is enriched in Cl and Br by more than a factor of two compared with Earth (Wänke and Dreibus, 1994).
Second, the mineral composition of martian basaltic meteorites is consistent with an enrichment in Cl relative to terrestrial basalts (Filiberto and Treiman, 2009).
Third, there are elevated bromine levels in soils (Rieder et al., 2004), and recent analysis of Br/Cl ratios in soil profiles at Gusev Crater and Meridiani Planum indicate that bromine gas may have been released from soil to atmosphere by UV photolysis or chemical oxidation (Karunatillake et al., 2013).
Other ways to form perchlorate
In addition to Eqs.
(1) and (4), several other atmospheric reactions form precursors to HClO4; however, their relevance to Mars has not yet been demonstrated.
For example, laboratory experiments showed that oxidation of HCl gas by O3 gas produces both chlorate, ClO3-, and perchlorate, ClO4- (Wang, 2011).
The recovered ClO4- mass was two to three orders of magnitude greater than the recovered ClO3- mass (Wang, 2011).
However, HCl gas was exposed to O3 gas concentrations of 150-180mg/L (Wang, 2011), which are values that far exceed the concentration of O3 in both the terrestrial stratosphere and the martian atmosphere.
The Phoenix WCL was not designed to detect chlorate (the ion-sensitive electrode in the WCL has similar sensitivity to chlorate and nitrate, so a signal due mostly to chlorate is precluded).
However, it is possible that ClO3- contributed to the perchlorate signal (Hanley et al., 2012).
Lightning-induced oxidation of NaCl aerosols also produces ClO4-, but the recovered ratio of ClO4-/Cl- is only ∼10-4 by mass (Rao et al., 2012b).
Phoenix WCL measurements suggest that the ratio of ClO4-/Cl- is ∼15 by mass in the soil (Kounaves et al., 2010b), so the lightning production of perchlorate is not compatible with data.
Several authors have recently argued that ClO4- can be produced through aqueous and heterogeneous reactions; however, once again, it is unclear how effective such mechanisms would be on Mars.
Decomposition of chlorine-based bleach through UV exposure produces ClO4-, but at low yields ∼10-3% (Rao et al., 2012a).
Schuttlefield et al. (2011) show that perchlorate is produced by irradiation of titanium-containing crystals in aqueous solutions of chloride.
The perchlorate can initially be produced 2000 times faster than perchlorate produced via atmospheric oxidation of chlorine volatiles in Catling et al. (2010), but the reaction rate asymptotes and produces only ppm levels of perchlorate in 0.5M solutions of chloride.
Additionally, the radiation source used in their experiment, a 100-W xenon lamp, simulates the solar spectrum in space, which greatly exceeds the spectrum at Mars' surface because scattering and absorption by atmospheric gases and dust attenuates the ultraviolet solar flux (Patel et al., 2002).
In another recent study, CO2-doped chloride-bearing ices were bombarded with electrons, and chlorine oxides, such as ClO2- and ClO3-, formed (Kim et al., 2013).
These species can subsequently be oxidized to perchlorate; however, the yield of chlorine-oxides has not been quantified.
Atmospheric sulfates on Earth and Mars
On Earth and Venus, sulfate aerosols form when volcanic SO2 is oxidized and hydrated to form H2SO4 (McGouldrick et al., 2011).
On Earth, dry deposition of H2SO4 and subsequent reactions with minerals can form sulfates.
In fact, sulfates that are found within and near nitrate ore fields of the Atacama have Δ17O=0.4-4‰ (Michalski et al., 2004), which implies a stratospheric source for some of the oxygen molecules bound in the sulfate.
Settle (1979) suggested that sulfates on Mars would form in the atmosphere.
Mass independent fractionation of sulfur isotopes in secondary minerals in martian meteorites confirms this hypothesis, as it indicates an atmospheric source for sulfur (Farquhar et al., 2000).
Additionally, sulfates have been detected across the planet, which could be evidence for atmospheric deposition.
Apart from salts directly measured in the soil, large mounds of hydrated sulfate have been detected from orbit (Bibring et al., 2006; Murchie et al., 2009a, 2009b).
Atmospheric sources of the sulfate have been suggested for such sulfate-rich sedimentary deposits in Meridiani Planum (Niles and Michalski, 2009), Valles Marineris troughs, and Juventae Chasma (Catling et al., 2006).
Nitrates on Earth and Mars
In the terrestrial atmosphere, nitric acid (HNO3) can form by the reaction of NO2 with OH (Brasseur and Solomon, 2005).
Deposition of HNO3 forms nitrates.
Oxygen isotopes in nitrates from the Atacama Desert also indicate a stratospheric source, with Δ17O=14-21‰ (Michalski et al., 2004).
While nitrates have not yet been definitively detected on the martian surface, they should naturally form through oxidation of atmospheric N and accumulate on the surface (Mancinelli, 1996).
In photochemical models of the martian atmosphere, odd nitrogen (N and NO) forms in the thermosphere through photodissociation of N2, recombination of N2+ and NO+, the reaction of N2+ with O, and the reaction of O+(2P) with N2 (Krasnopolsky, 1993; Yung et al., 1977).
N and NO then flow into the lower atmosphere (Krasnopolsky, 1993; Yung et al., 1977).
These species can then be oxidized to NO2, which then can form nitric acid.
Previous work showed possible detections of indigenous nitrate in martian meteorite EETA79001 and Nakhla (Grady et al., 1995).
A more recent study of EETA79001 suggests a nitrate upper limit of ∼0.5wt.% in a sawdust sample that includes white-gray granular material known as "druse" (Kounaves et al., 2014).
Nitrate also may have been detected in Gale Crater by the Mars Science Laboratory (Navarro-Gonzalez et al., 2013).
However, the specific amount of nitrate in the soil remains largely unconstrained.Manning et al. (2008) have developed a steady-state model of nitrate decomposition and nitrogen escape in the martian atmosphere in an attempt to estimate the amount of nitrate in martian soil.
They calculated a ∼3m-thick global equivalent layer of pure NaNO3on Mars.
The actual inventory; however, is not known.
Photochemical model
To investigate chlorine, sulfur, and nitrogen chemistry in the martian atmosphere, we use a one-dimensional photochemical model derived from the model developed by Kasting (1979).
The model has recently been modified to study chlorine, sulfur, and nitrogen chemistry in the modern terrestrial atmosphere (Catling et al., 2010) and hydrogen, carbon, and oxygen chemistry in the modern martian atmosphere (Zahnle et al., 2008).
We combine the two models.
As part of model development, we matched the calculated mixing ratios of H2O, CO, H2O2, and O2 to those in the modern martian atmosphere (following Zahnle et al., 2008) and then added chlorine and sulfur chemistry from Catling et al. (2010), as discussed in Section 3.1.
The model uses a 1km vertical grid spacing and solves a time-dependent coupled transport-chemistry equation, as given in Catling et al. (2010).
The model is stepped forward to steady-state using the reverse Euler method.
Vertical transport occurs by eddy diffusion for all species, as well as molecular diffusion for H and H2.
Following the model of Zahnle et al. (2008), atmospheric transport, temperature, and relative humidity are initialized as follows.
The eddy diffusion coefficient K=106cm2s-1 for altitude z<20km, and K=106N(20km)/N(z)cm2s-1, where N is the atmospheric number density (moleculescm-3), for 20km<z<110km.
The surface temperature, T0, is set to 211K.
The atmospheric temperature follows a nominal profile of T=T0-1.4z for z<50km, and T is isothermal for z>50km.
The relative humidity is set to 17% throughout the entire atmosphere, and the atmospheric composition is set to 95% CO2.
The modern solar flux at Mars is applied at the top of the model grid.
A test to evaluate the effect of the evolution of the solar flux on salt deposition fluxes over the Amazonian eon (3Gyr-present) showed negligible effect (see Section 5.2).
Consequently, the nominal model assumes atmospheric Cl, S, and N chemistry is relatively insensitive to the evolving solar flux and uses the modern solar flux.
Updates to the photochemical model from Catling et al. (2010) include the incorporation of the delta 2-stream radiative transfer formulation of Toon et al. (1989) and a modification to the Rayleigh scattering cross-section.
The Rayleigh scattering cross-section is computed using the relative gas concentrations at each model height rather than assigning a global cross-section based on the ground level gas concentrations.
These improvements in realism were made primarily for model atmosphere studies involving variable pressure (Misra et al., 2014).
However, they make little difference to results in a thin atmosphere with well-mixed concentrations, such as modern Mars.
Model chemistry
The nominal chemistry includes gaseous species listed in Table 1 as well as two aerosols: H2SO4 (sulfate) aerosols and S8 (elemental sulfur) aerosols.
We neglected nitrate aerosols because the model runs in conditions where the partial pressure of HNO3 is insufficient to exceed its saturation vapor pressure based information in Luo et al. (1995).
The atmospheric chemistry allows for a version of the modern atmosphere, modified by volcanic fluxes, meant to simulate the atmosphere in the mid-Amazonian eon.
Volcanic fluxes are discussed in Section 3.2.
The 348 photochemical reactions include those used in Zahnle et al. (2008) and Catling et al. (2010), with the exception of reactions involving CH3Cl, which is produced biologically, and its photochemical products, CH2Cl, CH2ClO2, CH2ClO, and CHClO.
All reaction rates were updated where new rates were available from Sander et al. (2011).
For completeness, we also added one new chlorine reaction not included in Catling et al. (2010): Cl2O2+O3→ClO + ClOO+O2.
But the upper limit of the rate constant, k=1×10-19cm3s-1, is slow (Atkinson et al., 2007), so the effect on overall chemistry is negligible.
A background of 95% carbon dioxide allows for enhanced three-body reaction rates compared to terrestrial air (Lindner, 1988), so rate constants for three-body reactions measured in air are multiplied by a rate of 2.5 following Nair et al. (1994).
Atmospheric formation of salt-precursors occurs through the injection of volcanic gases (discussed in Section 3.2) and subsequent reactions in the gas-phase.
Formation of sulfuric acid in the gas-phase occurs by the following reactions (Seinfeld and Pandis, 2006):(5)SO2+OH+M→HOSO2+M(6)HOSO2+O2→HO2+SO3(7)SO3+H2O→H2SO4
Gas to particle conversion occurs when the saturation vapor pressure of H2SO4 is exceeded.
The H2O/H2SO4 ratio of the particles is self-consistently predicted.
Further details of aerosol formation are given in Pavlov et al. (2001).
S8 aerosols are formed when S2 polymerizes to form longer sulfur chains.
Polymerization stops at the stable S8 ring-molecule, which is assumed to condense.
S2 is formed by the reaction of HS and S, or, as discussed in Section 3.2, it is injected directly into the atmosphere through volcanism.
However, the nominal model has an oxidizing atmosphere, so S8 is not the relevant fate for S2 molecules.
Rather, S2 is oxidized to SO, which can be further oxidized to SO2 and H2SO4.
This is consistent with previous work showing that in CO2 atmospheres with low sulfur input and high SO2/H2S, like the atmosphere modeled here, primary sulfur is converted to sulfuric acid more efficiently than it is converted to elemental sulfur (Hu et al., 2013).
Gas-phase nitric acid is produced by a reaction analogous to Eq. (5):(8)NO2+OH+M→HNO3+M
Likewise, gas phase pernitric acid is formed by reaction of HO2 and NO2:(9)HO2+NO2+M→HNO4+M
Perchloric acid forms through reaction (2).
We also developed a version of the model with bromine chemistry to test whether the effects of interactions between the bromine and chlorine cycles, including Eq.
(3), are significant.
The bromine-version of the model is discussed in Section 6.
Volcanism and transport of species at the upper and lower boundaries
In the model, chlorine and sulfur gases are emitted from volcanoes.
These gases are the sources of Cl- and S-bearing gases.
Evidence for volcanism in the Amazonian eon is abundant.
Evidence includes lava flows dated by crater counts to be no older than 100myr old (Hartmann et al., 1999; Hartmann and Neukum, 2001) and basaltic meteorites that have crystallization ages of less than 1.3Gyr (Nyquist et al., 2001).
The composition and emission rates for martian volcanic gases are poorly constrained, so we use nominal estimates from the literature and test the model for sensitivity to their variations.
We assume a total emission rate that is ∼0.1% of the terrestrial volcanic emission rate used in Catling et al. (2010), which results in ∼106moleculescm-2s-1.
This factor is consistent with the estimate that the eruption rate of volcanic rocks was ∼103 times lower than contemporary terrestrial emission rates at 1-2Ga (Jakosky and Shock, 1998), assuming that volcanic gas emission rates scale directly with volcanic rock eruption rates.
We estimate the H-O-C-S composition of volcanic gases using the geochemical model results of Gaillard and Scaillet (2009).
For HCl, which is not considered by Gaillard and Scaillet (2009), we take the terrestrial arc volcano HCl/SO2 ratio of 0.3 (Pyle and Mather, 2009) and multiply by a factor of 2.
This factor of 2 is consistent with the chlorine enrichment observed on Mars (Wänke and Dreibus, 1994) and a Cl/La enrichment of ∼2.5 in martian meteorites compared to terrestrial basalts (Filiberto and Treiman, 2009).
The emission rates of volcanic gases in the nominal model are shown in Table 2.
S2 appears as the dominant sulfur gas because SO2 and S2 are favored over H2S at lower pressures (Gaillard and Scaillet, 2009).
Gaillard and Scaillet (2009) model martian magma as relatively water-poor (0.2wt.%) with a mantle oxygen fugacity of QFM-2 (2 log units below the quartz-fayalite-magnetite redox buffer).
These combined conditions favor S2 over SO2 (Gaillard and Scaillet, 2009).
Emissions are distributed evenly between the middle of the second atmospheric layer, 1.5km, and 20km in the model (the troposphere and lower stratosphere).
Most gases are removed from the atmosphere to the surface according to a prescribed deposition velocity.
The deposition velocity is a scaling factor that affects the transport of species from the bulk atmosphere to the surface in the absence of rain.
The deposition velocity is coupled to the gas concentrations computed by chemical kinetics.
Following Zahnle et al. (2008), we apply a deposition velocity of 0.02cms-1 to all species, with two exceptions.
First, the deposition velocities for O2, H2, and CO are set to zero (following Zahnle et al., 2008).
Second, all species with a zero deposition velocity in Catling et al. (2010) are prescribed a zero deposition velocity because we consider them to be nonreactive.
These species include NO3, N2O5, N2O, CH3O, CH3ONO, CH3ONO2, CH2ONO2, CH3O2, CH3OH, CH2OOH, Cl2, CH2OH, CH2O2, OClO, ClOO, ClONO, ClNO, ClNO2, CH3OCl, Cl2O2, Cl2O, ClO3, and Cl2O4.
The deposition velocity multiplied by the species number density at the lower boundary, in addition to the flux term from eddy diffusion, determines the flux of species to the surface.
The photochemical model simulates chemistry up to 110km, so upper boundary conditions, for certain species, must be applied to account for chemistry that takes place above the vertical bounds of the model.
As mentioned previously, both N and NO precipitate into the neutral atmosphere from the ionosphere.
The ionosphere is not explicitly modeled here, so the nominal input of odd-nitrogen at 110km is set to 2×106moleculesNcm-2s-1 and 2×107moleculesNOcm-2s-1 (Krasnopolsky, 1993).
We vary these rates later to test model sensitivity.
Additionally, following Zahnle et al. (2008), atomic oxygen is lost at the top of the model at a rate of 107moleculescm-2s-1.
This rate is consistent with literature estimates (Fox and Hac, 1997; Lammer et al., 2003).
Additionally, CO flows into the top of the model at a rate of 2×107moleculescm-2s-1 in order to maintain redox balance, and hydrogen escapes at the diffusion-limited rate (Zahnle et al., 2008).
Methods
We employ two groups of sensitivity tests to determine the effect of changing model parameters on the deposition rate of salt precursors.
The purpose of the first group of tests (Section 4.1) is to quantify the sensitivity of perchloric acid, (per)nitric acid, and sulfate aerosol deposition to a subset of upper and lower boundary conditions.
These include the solar flux; the input of Cl, N, and S atoms; and the deposition velocities of Cl and S atoms.
We will use the results to calculate the possible range of integrated salt deposition fluxes over the Amazonian eon.
The purpose of the second group of sensitivity tests (Section 4.2) is to figure out why the deposition rate of perchloric acid is different on Mars and Earth.
Several atmospheric parameters differ significantly between Mars and Earth.
We choose three parameters: surface temperature, pressure, and O2 mixing ratio, and vary them to see how each one affects the deposition rate of perchloric acid.
Sensitivity tests: upper and lower boundary conditions
We vary unconstrained boundary conditions, including volcanic emission rates, deposition velocities, and precipitation of odd nitrogen from above the model's upper boundary.
But first we test the model's sensitivity to the solar flux at the top of the atmosphere.
Sensitivity test #1: evolution of solar flux
We apply a parameterization of the evolution of the solar flux (from Claire et al., 2012) to the nominal model to quantify changes in deposition fluxes over time.
Sensitivity test #1: volcanic emissions
We vary the total volcanic emission rate from ∼106moleculescm-2s-1 to ∼108moleculescm-2s-1, or in other words, 0.1-10% of the terrestrial volcanic flux used in Catling et al. (2010).
Throughout this range, we also vary the HCl/SO2 from the nominal case of 0.6, up to 60.
Sensitivity test #2: deposition velocities
The deposition velocity used in the nominal model is not a measured value.
Rather, it is best understood as a fitting parameter tuned to reproduce CO, O2, and H2 abundances on modern Mars, valid as a global average on a 105year time scale.
Deposition velocities could have been different at other times and for other conditions.
We complete two model simulations in which two key deposition velocities are changed.
First, we set the deposition velocity of SO2, which competes with sulfate aerosol deposition, to zero.
Second, we set the deposition velocity of HCl, which competes with HClO4 deposition, to zero.
Sensitivity test #3: input of odd nitrogen
Nitrogen input from the upper atmosphere almost entirely controls the rate of nitrate deposition to the surface.
At the upper boundary of the model, we vary the odd nitrogen (N and NO) input from the ionosphere because the fluxes we assume in the nominal model are uncertain (Krasnopolsky, 1993).
We simulate the atmosphere under the following fluxes (in moleculescm-2s-1): NO=2.0×107 and N=2.0×106; NO=2×104 and N=2×103; NO=2×103 and N=2×102; and NO=2×102 and N=20.
Sensitivity tests: temperature, pressure, and oxygen mixing ratio
We vary the atmospheric pressure, temperature, and O2 mixing ratio to understand how these three parameters affect the deposition rate of perchloric acid.
Specifically, we want to answer the question: How does the different atmospheric state on Mars and Earth affect the perchloric acid deposition flux on each planet?
We start from the nominal model for Mars and then shift each parameter, separately, to more Earth-like values.
From here, we can see how the perchloric acid deposition flux either approaches or diverges from the calculated perchloric acid deposition flux on Earth of ∼105moleculescm-2s-1 (Catling et al., 2010).
These tests are not meant to represent physically plausible scenarios.
For example, temperature and pressure are varied independently, and the O2 mixing ratio is increased.
This latter change implicitly assumes there is a source of O2 at the surface.
Therefore, our interpretations are qualitative.
Sensitivity test #4: temperature
Atmospheric temperatures affect photochemical rate constants and atmospheric water vapor content.
These, in turn, affect the chain of reaction rates that lead to the oxidation of Cl to form HClO4.
To pinpoint how atmospheric temperatures alter reaction rates, we shift the nominal Mars temperature profile to higher surface temperatures.
The shape of the profile is preserved, but the surface temperature is increased from 211K to 284K (a temperature increase of ∼35%), with the latter temperature being more representative of surface temperatures on Earth.
Sensitivity test #5: surface pressure
Photochemical reaction rates are density-dependent; therefore, we vary the surface pressure from the nominal value of 6.3mb to 35mb.
This pressure is as high as the surface pressure can be taken without the model entering a CO-runaway as described in Zahnle et al. (2008).
Sensitivity test #6: oxygen concentration
The formation rate of perchloric acid in Eq.
(2) is dependent upon the atmospheric abundances of Cl and O3, in addition to the rate constant.
In Section 4.1.2, we described a test to determine the sensitivity of perchloric acid formation to Cl volcanic input, so for the last sensitivity test, we vary the O2 mixing ratio to determine sensitivity to the abundance of O2 and its photochemical product, O3.
In the nominal model, the O2 mixing ratio is calculated as model output; however for this test, the O2 mixing ratio is fixed as model input.
While holding the surface pressure constant, the O2 mixing ratio is increased by a factor of ten starting from its value in the nominal model, 1.6×10-3.
Results
Photochemical products of CO2 and H2O dominate the nominal model atmosphere.
The volcanic input is low, and therefore the atmosphere remains highly oxidizing.
The net redox imbalance of the atmosphere, pOx=2pO2-pCO-pH2, is 15μbar (as defined in Zahnle et al., 2008).
Dominant chlorine, sulfur, and nitrogen gases are HCl, SO2, and NO, respectively.
The mixing ratio profiles are plotted in Fig. 1.
Additionally, the production profiles of salt precursors are plotted in Fig. 2.
HNO3, HNO4, and sulfate aerosol (SO4 AER) production primarily occurs close to the surface, while HClO4 production primarily occurs aloft where the O3 abundance is highest.
Below we describe results from the sensitivity tests to assess the role of variables in salt formation and deposition on Mars.
Deposition fluxes in the nominal Mars model
The deposition fluxes of perchloric acid, sulfate aerosols, nitric acid, and pernitric acid in the nominal Mars model are listed in Table 3.
We assume that salts form from the acids upon deposition to the surface.
We next make several assumptions to calculate the concentration of salts that have accumulated in the soil during the Amazonian eon.
We first assume that perchloric acid, sulfate aerosols, nitric acid and pernitric acid have accumulated at a uniform rate.
This assumption is valid because the lack of aqueous minerals and very low weathering rates tell us the Amazonian eon on Mars has been characterized by a climate and atmosphere not greatly different from today (Bibring et al., 2006).
We next assume a range of soil mixing depths so that salts are distributed throughout the soil column.
According to Zent (1998), small post-Noachian impactors have churned the soil on Mars to a 1/e mixing depth of 0.51-0.85m.
Taking three e-folding depths, the range would be 1.5-2.6m depth, with a mean ∼2m.
The last assumption we make is that the soil density is 1gcm-3 (Moore and Jakosky, 1989).
Using these assumptions, we calculate the concentrations of anions in the soil for the nominal model and report them in Table 4.
These anions must be combined into salts.
For perchlorate, the salts may be Mg(ClO4)2 or Ca(ClO4)2 as discussed earlier.
The dominant nitrogen-bearing salt is unknown.
The sulfate deposition flux produced in the nominal model is compatible with estimates of the amount of sulfates on Mars.
The nominal range (1.0-1.7wt.% SO4) is consistent with 1.3wt.% soluble sulfate measured at the Phoenix landing site (Kounaves et al., 2010b).
The estimates also compare well with an average ∼6.8wt.% sulfur as SO3 (2.7wt.% S) in global soil inferred from elemental abundances measured at various locations on Mars by Spirit, Opportunity, Pathfinder, and Viking Landers.
We can also compare the average sulfur content in the top few tens of centimeters of the soil of ∼4.4wt.% inferred from Gamma Ray Spectrometer measurements (King and McLennan, 2010).
The agreement between model results and data suggest that 0.1% of the terrestrial volcanic gas flux is a good estimate for the volcanic emission rate on Mars 1-2Ga if soil salts derive from volcanic input.
Deposition of pernitric acid, HO2NO2 or HNO4, exceeds deposition of nitric acid, HNO3, contrary to the case on Earth.
Production of HNO4 is enhanced at lower temperatures because the rate constant of Eq.
(9) is inversely related to temperature.
The same is true for the rate constant of Eq.
(8), but the HNO3 abundance is depressed at lower temperatures due to the lower abundance of OH.
OH is produced by the photolysis of water vapor and the reaction of H2O with O(1D), but the water vapor abundance is lower at lower temperatures.
Implications of enhanced deposition of HNO4 on Mars, compared with Earth, will be discussed in a separate paper about N deposition on Mars (Catling et al., in preparation).
There is no confirmed nitrate on Mars, so we compare the results to an estimate of N on Mars.
The integrated deposition flux of HNO4 in the model corresponds to an evenly-distributed global layer of NO4 with a mass of 4.3×1018g, which is far smaller than the NO3 mass of 7.2±3.6 (×1020) calculated by Manning et al. (2008).
If the concentration of N predicted here are correct, then they could be below the detection limit for near-infrared orbital instruments, which may explain why nitrates have not yet been observed.
The model-inferred abundances of perchlorate are far smaller than observations, however.
If we assume the soil is mixed to an average depth of 2m and the parent salt is Ca(ClO4)2 or Mg(ClO4)2, then we calculate concentrations of 4.3×10-8wt.% and 4.0×10-10wt.%, respectively.
These concentrations are both many orders of magnitude lower than the observed abundance at the Phoenix landing site: 0.4-0.6wt.% (Hecht et al., 2009).
Also, the concentrations are far below what would be inferred from observations of perchlorate:nitrate ratios ∼1:60 measured in EETA79001 (Kounaves et al., 2014) and the Atacama perchlorate:nitrate of ∼1:1000.
Distinct from the results for Earth where gas phase reactions were sufficient to reproduce data (Catling et al., 2010), we conclude that additional heterogeneous reactions must be present to account for the efficient formation of perchlorate on Mars, a hypothesis we discuss further in Section 7.
But first we explore the sensitivity of deposition fluxes to model parameters.
Sensitivity of salt deposition to solar flux evolution
The effect of an evolving solar flux from Claire et al. (2012) on the salt deposition fluxes was negligible (Fig. 3).
Consequently, we set the solar flux to be the modern solar flux for the rest of the simulations.
Sensitivity of salt deposition fluxes to volcanic input
We next calculated the perchlorate, sulfate, and nitrate deposition fluxes on Mars as a function of volcanic input.
The total volcanic emission rate was increased from the nominal case (∼106moleculescm-2s-1) to one hundred times this value to reflect possible higher rates of volcanism on Mars in the past.
At the same time, we tested the effect of increasing the HCl/SO2 ratio, given that the value is uncertain.
We completed two simulations, one with HCl/SO2=0.6, and another with HCl/SO2=60.
The deposition fluxes of sulfate aerosols and perchloric acid are both sensitive to volcanic input (Fig. 4).
However, at the highest level of volcanic input, the deposition fluxes of both species are incompatible with observations.
First, we consider the largest deposition flux of sulfate aerosols.
This value occurs at a volcanic input of 100 times the nominal value.
We use the same range of mixing depths as we did in the previous section (1.5-2.6m) to calculate a concentration of 100-173wt.% sulfate in the soil.
This range is incompatible with observations of sulfur on Mars described in Section 5.1, and values greater than 100wt.% indicate that sulfate deposition is unrealistically high when the mixing depth is restricted to 1.5m.
Second, we consider the largest deposition flux of perchloric acid.
This value occurs at a volcanic input of 100 times the nominal value and the HCl/SO2 emission flux ratio of 60.
Once again, we use a range of mixing depths of 1.5-2.6m to calculate the concentration of ClO4- in the soil.
The range is 6.1×10-6-1.0×10-5wt.% ClO4-, which again is incompatible with the measurement of ∼0.5wt.% ClO4- made by the Phoenix Lander (Hecht et al., 2009).
Sensitivity of pernitrate and nitrate deposition flux to odd nitrogen input
As stated previously, there is considerable uncertainty in the input rate of odd nitrogen (N and NO) species from the martian ionosphere to the neutral atmosphere (Krasnopolsky, 1993).
In his own model of the neutral atmosphere, Krasnopolsky considers cases both with and without input of odd nitrogen from the upper atmosphere (Krasnopolsky, 1993).
We vary the input of N and NO into the model, using the nominal case as an upper limit.
As these values are decreased, the pernitrate deposition flux drops, which is shown in Fig. 5.
The lowest input of odd nitrogen corresponds to 3.5-6.1 (×10-4)wt.% N accumulated over 3byr and mixed into 1.5-2.6m of soil.
Sensitivity of salt deposition flux to deposition velocities
Perchloric acid and sulfate aerosols are minor sinks of chlorine and sulfur in the nominal model (Table 5).
To calculate upper limits on the deposition of sulfate aerosols and HClO4, we separately turned off the deposition velocity of its main competitors.
First we set the deposition velocity of SO2 to zero, and then we set the deposition velocity of HCl to zero.
The modified sinks of chlorine and sulfur are listed in Table 6.
When the SO2 deposition velocity is zero, ∼98% of the input sulfur exits the atmosphere as sulfate aerosols.
We repeat the calculation from Section 5.1 and find that this corresponds to 2.9-5.0wt.% SO4 accumulated over 3byr mixed into 1.5-2.6m of soil.
When the HCl deposition velocity is zero, the major surface sink of chlorine is HOCl.
HClO4 deposition increases by about four orders of magnitude from the nominal model.
This flux corresponds to 6.9×10-5-1.2×10-4wt.% ClO4- accumulated over 3byr mixed into 1.5-2.6m of soil.
Temperature
To semi-quantitatively assess the sensitivity of the deposition fluxes of salts to temperature, we forced the model temperature profile to higher values by increasing the temperature profile by 35% in 5% increments.
Warmer temperatures significantly increase the formation of perchloric acid.
Over the range tested, the deposition rate of perchloric acid increases by around six orders of magnitude.
As the atmospheric temperature increases, the chemistry begins to favor the deposition of HClO4 over HCl.
At higher temperatures, there is more H2O in the atmosphere, which is set by the atmospheric temperature.
More water vapor decreases CO and increases OH, while O2 concentrations are relatively insensitive to H2O abundance (Zahnle et al., 2008).
OH reacts with ClO3, through Eq.
(2), to produce HClO4.
HCl production increases at warmer temperatures because of increased amounts of HO2 (O+OH→H+O2 makes O2, H+O2+M→HO2+M makes HO2, and HO2+Cl→HCl+O2 makes HCl).
However, the destruction of HCl through HCl+OH→Cl+H2O also increases; hence the chemistry favors HClO4 deposition over HCl deposition.
This is shown in Fig. 6.
Pressure
Increasing the surface pressure from 6.3mb to 35mb decreases the deposition rate of atmospheric perchlorate from 4.6×10-3 to 7×10-5moleculescm-2s-1.
We did not continue to increase the surface pressure up to Earth-like pressures because a build-up of CO causes the atmosphere to become progressively more reduced, which only further decreases the formation of oxidized salt-precursors.
In thicker CO2 atmospheres, the significance of H2O vapor photolysis is reduced, and the OH mixing ratio decreases.
This, in turn, reduces the oxidation of CO via CO+OH→H+CO2 and the production of O2 via O+OH→H+O2.
As a consequence, the redox state of a more dense atmosphere becomes more reducing because the reducing gas CO is favored over the oxidizing gas O2.
This behavior was also recognized, and is detailed further, in Zahnle et al. (2008).
A more reduced atmosphere is less amenable to perchloric acid production (Fig. 7).
Chemistry: oxygen concentration
Changing the chemical composition of the atmosphere affects the perchlorate production and deposition rate.
A lower concentration of atmospheric ozone on Mars compared with Earth inhibits the production of perchloric acid.
As shown in Fig. 8, increasing the O2 mixing ratio from the level in the nominal model, while holding the total atmospheric pressure constant, causes a significant increase in the O3 column density.
This, in turn, causes the perchlorate deposition flux to increase by around four orders of magnitude.
While a larger O2 mixing ratio may not represent a specific past time on Mars, the results demonstrate that lower O2 and O3 concentrations limit the production of HClO4 in the nominal model.
For comparison, on Earth, a typical O3 column density is ∼8×1018moleculesO3cm-2.
Bromine chemistry
In one simulation, we included volcanogenic bromine, in the form of HBr.
Bromine interacts with chlorine cycles through Eq.
(3) and various other reactions.
The addition of gas phase bromine chemistry makes the model more complete and introduces known chemistry that may affect formation of perchloric acid.
Adding bromine chemistry involved a number of steps.
First we added bromine chemistry (see Appendix A) to the model of Catling et al. (2010) for the terrestrial atmosphere.
The model was validated against data, further details of which are given in Appendix A.
After validation, we then added inorganic bromine chemistry to the nominal Mars model.
The terrestrial arc volcano HCl/HBr emission flux ratio of ∼103 (Pyle and Mather, 2009) was applied, and two more cases with HCl/HBr=100 and HCl/HBr=10 were run, to determine if more HBr affected HClO4 production.
We note, again, that recent work indicates bromine gas may have been released to the atmosphere by volatilization of soil-derived bromine (Karunatillake et al., 2013), but the flux is not quantified, so we do not consider this source here.
When we include inorganic bromine chemistry, the effect on the deposition flux of perchloric acid is negligible.
This is because Eqs.
(3) and (4) occur at slower rates than other reactions, involving species from the nominal model, that form OClO and ClO3 (Table 7).
Discussion
The model results show that atmospheric dry deposition can plausibly explain typical abundances of a several wt.% sulfate observed in soils on Mars.
The model also predicts nitrogen amounts, as nitrates or pernitrates, at levels on the order of 0.1wt.%, assuming these species do not react or decompose.
Perchlorate is formed very slowly through the gas phase atmospheric chemistry that is modeled here.
For the nominal case, we calculate a ClO4- concentration of 2.8-4.8 (×10-8)wt.%, which is a factor of 107 less than the measurement made by the WCL instrument on the Phoenix Lander (Hecht et al., 2009).
With regard to perchlorate, the sensitivity tests revealed that changing the model boundary conditions did not increase perchloric acid deposition rates substantially.
The inclusion of bromine gas phase photochemistry has negligible effect on perchlorate fluxes.
Sensitivity tests revealed two primary causes of the lower perchlorate fluxes from gas phase reactions on Mars relative to Earth: lower amounts of ozone and colder, drier air.
The much smaller amounts of ozone in the martian atmosphere compared to the terrestrial atmosphere (Fig. 8) are important because ozone is critical for the gas phase formation reactions of precursors to perchloric acid, Eqs.
(1) and (4).
In addition, the colder and drier air results in a smaller concentration of OH radicals needed in the three-body reaction to make perchloric acid, Eq. (2).
We suggest that gas-solid reactions are probably critical for enhancing the transformation of chlorides to perchlorate on Mars.
Many important reactions in terrestrial atmospheric chemistry are heterogeneous.
The most famous example is that the Antarctic ozone hole could not be explained until models incorporated the heterogeneous chlorine chemistry that occurs on very cold particles (Solomon, 1999).
Another possibility is that perchlorate was formed in the past by heterogeneous reactions on atmospheric sulfate aerosols.
Small amounts of HClO4 have been produced in the laboratory by reactions of ClO with H2SO4 aerosols (Martin et al., 1979).
Jaegle et al. (1996) modeled the formation of HClO4, assuming a 40% yield from the reaction of ClO on the surface on sulfate aerosols.
The authors concluded that, on Earth, over 50% (2ppbv) of stratospheric inorganic chlorine could be sequestered into HClO4 when significant amounts of sulfate aerosols are present.
However, subsequent work questioned the result and found insignificant uptake of ClO on sulfuric acid (Abbatt, 1996).
Nonetheless, other heterogeneous reactions involving oxidizing radicals might be possible by analogy with ongoing research to understand the terrestrial atmosphere (George and Abbatt, 2010).
Improving model accuracy
Finally, we note that the accuracy of the photochemical model used here could be improved with new data.
NASA's MAVEN (Mars Atmosphere and Volatile EvolutioN) mission will characterize the upper atmosphere of the planet, which will include measurements of neutral species and thermal ions.
These data will be useful in constraining the odd-nitrogen flux into the neutral atmosphere, the loss of O from the neutral atmosphere, and the escape rate of hydrogen (Jakosky and MAVEN Science Team, 2011), all of which are adjustable parameters in the model.
Conclusions
We used a 1-dimensional photochemical model to calculate the atmospheric production and deposition rate of sulfate, nitrate and perchlorate salts in the Amazonian martian atmosphere.
The results suggest the following conclusions:•
Sulfate production through gas to particle conversion in the atmosphere is a viable means to account for the measured concentration of sulfates on the martian surface.
•
Nitrogen is continuously dry-deposited from the atmosphere of Mars even today mainly as pernitric acid.
During the Amazonian, 4.3×1018g NO4 could have been deposited across the martian surface if all of the nitrate is formed through atmospheric photochemistry and persists without decomposition or any further reactions.
This corresponds to a concentration of 0.3wt.% N if it is mixed uniformly to a depth of 2m.
This prediction can be confirmed or disproved by future in situ measurements.
•
The production of perchlorate via the gas phase formation process first modeled by Catling et al. (2010) for the driest regions of Earth's atmosphere is insufficient to account for 0.4-0.6wt.% ClO4- measured by NASA's Phoenix Lander.
Low atmospheric temperatures and a small inventory of oxygen and ozone limit the gas phase production rate of perchlorate.
Consequently, purely gas phase reactions do not appear to explain high perchlorate:chloride ratios in martian soil.
Instead, the efficient conversion of chloride to perchlorate may rely on heterogeneous reactions, i.e., gas-solid surface reactions.
The limitation of gas phase reactions may be analogous to problems encountered in terrestrial atmospheric chemistry, such as the 'ozone hole' issue.
Further theoretical and laboratory research into perchlorate production mechanisms under martian conditions is warranted.
Acknowledgments
This work was supported by NASA's Mars Fundamental Research Program through Grant NNX10AN67G awarded to DCC.
KJZ also acknowledges support from Mars Fundamental Research Program.
MLS acknowledges support from a National Science Foundation Integrative Graduate Education and Research Traineeship, under NSF-IGERT Grant DGE-9870713, Astrobiology: Life in and beyond Earth's Solar System.
MWC acknowledges support from a 2011 NAI Director's Discretionary Fund award titled "Perchlorate, Water, and Life", along with a NASA postdoctoral program award to work at the Virtual Planetary Laboratory.
MLS thanks Jim Kasting for helpful discussions about using the photochemical model.
We also thank two anonymous reviewers for improving the scientific content and clarity of the paper.
Bromine chemistry in an Earth photochemical model
There are many models for bromine chemistry in the troposphere and stratosphere, ranging from 3-D chemical transport models (e.g. von Glasow and Crutzen, 2006), to box models of the marine boundary layer (e.g. Sander and Crutzen, 1996), to 3-D stratospheric models (e.g. Hossaini et al., 2012).
One-dimensional models (Singh and Kasting, 1988; Yung et al., 1980) are able to match observed profiles of the most abundant bromine species.
Before we added bromine chemistry to the nominal Mars model, we added the chemistry to the terrestrial model of Catling et al. (2010).
Bromine reactions are listed in Table A.1.
Volcanic emission of HBr, release of bromine from sea salt aerosols, and emission of CH3Br from marine phytoplankton are all sources of atmospheric bromine on Earth (e.g. Lovelock, 1975).
Other sources of bromine, including anthropogenic and additional biological emissions, were not considered here because the sources are poorly constrained (von Glasow and Crutzen, 2006) and unlikely to apply for Mars.
We applied the terrestrial arc volcano HCl/HBr ratio of ∼103 to the terrestrial volcanic emissions (Pyle and Mather, 2009).
The HCl volcanic flux was set to 5×108moleculescm-2s-1 and the HBr volcanic flux was set to 5×105moleculescm-2s-1.
Bromine, in the form of BrCl and Br2, is released from sea salt aerosols at a rate of 1-10×1011gBryr-1 (von Glasow and Crutzen, 2006).
We took a rate of 5×1011gyr-1 and assumed all bromine is released as Br2.
For the third and last source of bromine, we assumed a CH3Br surface mixing ratio of 20pptv (Yung et al., 1980).
This value may be too high, compared with the nominal background concentration of CH3Br in the troposphere ∼10pptv (Quack and Wallace, 2003).
However, because we ignored other organic bromine sources, our assumption is justifiable.
The deposition velocities of bromine species were assumed to be equivalent to their chlorine counterparts, e.g., Br was given the same deposition velocity as Cl and HBr was given the same deposition velocity as HCl (Catling et al., 2010).
To validate the terrestrial model with bromine we compared satellite and in situ measurements of bromine species.
We compared measurements of four bromine species, BrO, BrONO2, HOBr, and HBr, with output from the 1-D model.
The model-derived BrO mixing ratio profile was compared with two separate measured profiles of BrO.
The first set of measurements was made with balloon-borne Differential Optical Absorption Spectroscopy (Dorf et al., 2008).
The balloon payload was launched at Teresina, Brazil (5.1°S, 42.9°W) on 17 June 2005 (Dorf et al., 2008).
The second set of measurements was taken by the Scanning Imaging Absorption Spectrometer for Atmospheric Chartography (SCIAMACHY) instrument aboard the European Envisat satellite (Sioris et al., 2006).
SCIAMACHY data plotted in Fig.
A.1 are from both low latitudes, 20-30°S, and high midlatitudes, 36-60°S, and correspond to early March 2003.
The data and model results compare relatively well, but it is clear that the modeled BrO concentrations are slightly lower than the observations (Fig.
A.1a).
This could be the result of several factors.
First, as noted previously, we have neglected some sources of organic and anthropogenic bromine.
Second, the model did not include heterogeneous chemistry.
In the model of von Glasow and Crutzen (2004), when only gas phase chemistry was considered, cycling of HOBr, HBr, and BrONO2 was slower than when heterogeneous chemistry was included, and as a result the BrO concentrations were systematically lower than they should have been.
We next compared the model output with measurements of BrONO2, HOBr, and HBr.
Measurements of BrONO2 were taken by the Michelson Interferometer for Passive Atmospheric Sounding aboard the European Envisat Satellite (Hopfner et al., 2009).
The measurements, both day and night, were taken in September 2003, across 40°S to 15°S.
The model accurately captured a diurnal average of BrONO2, within the limits of uncertainty (Fig.
A.1b).
Measurements of HOBr (Fig.
A.1c) and HBr (Fig.
A.1d) were from a far-infrared spectrometer aboard a balloon platform (Johnson et al., 1995).
The data reported in Fig.
A.1 are averages from several flights taken in 1988-1990.
There are only slight differences between the model results and observations, leading us to conclude that the calculation of bromine speciation is accurately captured by the addition of bromine reactions to the model of Catling et al. (2010).

The formation of sulfate, nitrate and perchlorate salts in the martian atmosphere

Highlights
•
We model a modified martian atmosphere to calculate deposition fluxes of salts.
•
Estimated sulfate soil concentrations are consistent with observed sulfate on Mars.
•
Pernitric acid forms more rapidly in the martian atmosphere than nitric acid.
•
An atmospheric origin for perchlorate is precluded (or supplemented by other processes).
Abstract
In extremely arid regions on Earth, such as the Atacama Desert, nitrate, sulfate and perchlorate salts form in the atmosphere and accumulate on the surface from dry deposition according to diagnostic evidence in their oxygen isotopes.
Salts of similar oxyanions should have formed in the atmosphere of Mars because of comparable photochemical reactions.
We use a 1-D photochemical model to calculate the deposition rates of sulfate, nitrogen oxyanions, and perchlorate from Mars' atmosphere, given a plausible range of volcanic fluxes of sulfur- and chlorine-containing gases in the past.
To calculate integrated fluxes over time, we assume that throughout the last 3byr (the Amazonian eon), the typical background atmosphere would have been similar to today's cold and dry environment.
If the soil has been mixed by impact perturbations to a characteristic depth of ∼2m during this time, given a time-average volcanic flux 0.1% of the modern terrestrial volcanic flux, the model suggests that the soil would have accumulated 1.0-1.7wt.% SO42- and 0.2-0.4wt.% N in the form of pernitrate (peroxynitrate) or nitrate.
The calculated sulfate concentration is consistent with in situ observations of soils from rovers and landers and orbital gamma ray spectroscopy.
However, nitrates or pernitrates are yet to be detected.
The modeled formation of perchlorate via purely gas-phase oxidation of volcanically-derived chlorine is insufficient by orders of magnitude to explain 0.4-0.6wt.% ClO4- measured by NASA's Phoenix Lander.
The far smaller amount of ozone in the martian atmosphere compared to the terrestrial atmosphere and the colder, drier conditions are the cause of lower rates of gas phase oxidation of chlorine volatiles to perchloric acid.
Our calculations imply that non-gas-phase processes not included in the photochemical model, such as heterogeneous reactions, are likely important for the formation of perchlorate and are yet to be identified.

Introduction
The elemental composition of the martian soil was first determined by X-ray fluorescence spectroscopy on the Viking Landers (VLs), which measured absolute concentrations (Clark et al., 1977).
Concentrations of elements in what was called the "average deep soil" at the VLs only summed to ∼89%.
The missing ∼11% was attributed to bound water (H2O, -OH), carbonates, nitrates, phosphates and oxides, given that some light elements (H, C, N, O plus Li, B, and F) were undetectable while other elements were obscured (Na, P, Cr and Mn) (Clark et al., 1982).
How the elements were bound into salts was uncertain, although Mg and S were correlated in duricrust soils, which was interpreted as a cement of magnesium sulfate (Clark, 1993).
Some of the "missing components" in VL soil analyses have since been detected.
Orbital thermal infrared spectroscopy showed that 2-5wt.% carbonates are present in the global dust (Bandfield et al., 2003), while thermal evolved gas analysis in soils at the site of the Phoenix Lander revealed 3-6wt.% carbonate (Boynton et al., 2009; Sutter et al., 2012).
Additionally, thermal evolved gas analysis in soils at the Rocknest location in Gale Crater was consistent with the presence of 1-2% fine-grained siderite and/or magnesite (Leshin et al., 2013).
Orbital gamma-spectroscopy have also revealed 1.5-7.5wt.% water-equivalent hydrogen near the martian surface (Boynton et al., 2007), and the Sample Analysis at Mars (SAM) instrument suite on the Mars Science Laboratory (MSL) detected evolved H2O consistent with ∼1.5-3wt.% H2O in sand grains at the Rocknest location (Leshin et al., 2013).
The Wet Chemistry Laboratory (WCL) on the Phoenix Lander provided in situ measurements of the composition of soluble salts in the martian soil.
Soluble sulfate was present at 1.3±0.5wt.% (Kounaves et al., 2010b), along with cations of sodium, potassium, calcium and magnesium.
The most surprising result was the presence of perchlorate (ClO4-) at an inferred concentration in the soil of ∼0.5wt.% (Hecht et al., 2009; Kounaves et al., 2010a).
The dominance of Mg(ClO4)2 is consistent with simulations of evaporation and freezing at the Phoenix landing site (Marion et al., 2010); however, further analysis of data from the WCL suggests that Ca(ClO4)2 may be the dominant form of perchlorate (Kounaves et al., 2012).
The perchlorate-sensitive electrode in the WCL experiment was also sensitive to nitrate, but it was 1000 times more sensitive to perchlorate.
Thus, the methodology precluded the detection of nitrate because the signal would have required a mass of nitrate that exceeded the mass of the sample (Hecht et al., 2009).
Recently, the MSL mission has also confirmed the presence of perchlorate using pyrolysis as part of the SAM experiment (Steininger et al., 2013).
Specifically, pyrolysis showed release of chloromethane and O2 from heated soil samples at the Rocknest location, which is consistent with the decomposition of perchlorate (Sutter et al., 2013).
If all of the evolved O2 was released from perchlorate, then the samples contained a comparable amount of perchlorate to the samples at the Phoenix landing site (Leshin et al., 2013).
Furthermore, reanalysis of the Viking thermal volatilization experiments suggest ⩽1.6% perchlorate at both Viking 1 and Viking 2 landing sites (Navarro-Gonzalez et al., 2010); however, this has been subject to some debate (Biemann and Bada, 2011).
Native perchlorate has also recently been measured in the martian meteorite EETA79001, albeit at a level <1ppm by mass (Kounaves et al., 2014).
Given the various locations of possible detection, perchlorate appears to be ubiquitous on the martian surface.
Alpha-proton X-ray fluorescence on landers and rovers after the VLs has led to the inference of a global soil unit (Blake et al., 2013; Bruckner et al., 2003; Morris et al., 2010; Rieder et al., 2004; Yen et al., 2013, 2005).
Soils can have components derived from local rocks but these are imprinted upon a global-scale soil that has characteristic ratios of the concentration of certain elements (Mg, Al, K, Ca and Fe) relative to silicon and also a positive correlation of Cl and S. Given the lander and rover detections of perchlorate, the Cl in this global soil may well be in the form of perchlorate.
Salts are relevant to the habitability of Mars.
First, perchlorate reduction is a metabolism used by some terrestrial bacteria in anaerobic conditions (Coates and Achenbach, 2004).
If such organisms exist (or did exist) on Mars, then they could gain energy by reducing perchlorate and oxidizing an electron donor such as organic carbon or ferrous iron.
This would require organic molecules to be present on Mars.
Indigenous organic molecules have yet to be confirmed on Mars; however the presence of perchlorate itself may have inhibited the detection of organics on Mars in pyrolysis experiments (Navarro-Gonzalez et al., 2010).
Second, perchlorate salts are highly deliquescent and significantly lower the freezing point of liquid water (Gough et al., 2011).
The eutectic point of Mg(ClO4)2 is -57°C (Stillman and Grimm, 2011), while that of Ca(ClO4)2 is -75°C (Pestova et al., 2005).
Given typical soil salt concentrations, small amounts of water (∼0.02g H2O per g soil) would permit a water activity sufficient for terrestrial life to be viable (Kounaves et al., 2010b).
Third, if all organisms require fixed nitrogen in proteins and nucleic acids, as they do on Earth, then the discovery of nitrogen oxyanions on Mars would be significant as well.
Perchlorate may be advantageous to microorganisms, but its impact on human exploration is more complicated.
Perchlorate may be harmful because it is potentially toxic to humans if ingested (Urbansky, 2002).
On the other hand, perchlorate could be useful for future exploration of Mars.
It is kinetically stable at typical planetary surface temperatures, but at high temperatures perchlorate is a powerful oxidant suitable for rocket propulsion (Trumpolt et al., 2005).
So perchlorate could be utilized as an in situ fuel resource for sample return missions and in the eventual human exploration of Mars.
On Earth, one source of sulfate, nitrate, and perchlorate salts is atmospheric deposition.
The salts can build up in extremely arid environments, such as the Atacama Desert in Chile (Catling et al., 2010; Ericksen, 1983) and the Antarctic Dry Valleys (Kounaves et al., 2010c).
Given the oxic arid environment of Mars, it is possible that the same salts, or similar ones, have formed in the atmosphere.
This work estimates the photochemical formation and deposition rates of martian oxyanions, and evaluates the plausibility of atmospheric chemistry as an important source of salts in the global martian soil.
This study builds upon the work of Catling et al. (2010), which used a one-dimensional photochemical model to investigate the formation of salts in the terrestrial atmosphere over the Atacama Desert.
Using purely gas-phase pathways, Catling et al. (2010) reproduced measured profiles of chlorine species in the terrestrial atmosphere and estimated deposition rates of both perchlorate and nitrate that were consistent with Atacama soil measurements.
Several profound differences, however, alter the photochemistry of Mars relative to Earth, including atmospheric composition, density, and temperature.
Our results reveal how these differences impact the formation of atmospheric salts on Mars as compared to Earth.
Background: atmospheric perchlorates, sulfates, and nitrates on Earth versus Mars
Perchlorates, sulfates, and nitrates are formed in the atmosphere on Earth and by analogy may have formed in the atmosphere of Mars.
In this section, we consider how terrestrial pathways of salt formation inform possible mechanisms on Mars.
Perchlorate formation
Perchlorate salts produced synthetically on Earth can be distinguished from those produced naturally in the atmosphere by virtue of their oxygen and chlorine isotope ratios.
Synthetic formation creates perchlorate with ordinary mass-dependent oxygen isotope fractionation (i.e. oxygen isotopes are distributed according to the linear relationship, δ17O≈0.52×δ18O).
Conversely, naturally occurring perchlorate typically contains mass-independently fractionated oxygen isotopes, a deviation defined as Δ17O, where Δ17O=δ17O-0.52×δ18O.
The known source for Δ17O occurs during ozone formation, leading to stratospheric ozone with 30-40‰ (Thiemens, 2006).
Perchlorate that contains Δ17O thus preserves a telltale signature of a naturally occurring formation pathway involving stratospheric photochemistry.
Natural perchlorate has been discovered in locations across the globe, including the Atacama Desert (Bao and Gu, 2004), the Antarctic Dry Valleys (Kounaves et al., 2010c), and the American southwest (Jackson et al., 2010; Rajagopalan et al., 2006).
The Mojave Desert in California and Atacama Desert in Chile have the highest Δ17O values.
In perchlorate samples from the Mojave Desert, Δ17O=8.6-18.4‰ (Jackson et al., 2010), and in samples from the Atacama Desert, Δ17O=4.2-9.6‰ (Bao and Gu, 2004).
The occurrence of radioactive 36Cl (produced by cosmic rays acting on atmospheric 36Ar) relative to 37Cl (Sturchio et al., 2009) further confirms a stratospheric source for the chlorine in natural terrestrial perchlorate.
A gas phase pathway for producing perchlorate is through the reaction of halogens with ozone.
Simonaitis and Heicklen (1975) proposed that perchloric acid, HClO4, forms by:(1)Cl+O3+M→ClO3+M(2)ClO3+OH+M→HClO4+M
For this study, we employ Eqs.
(1) and (2) as the major pathway to form HClO4.
However, the effectiveness of this mechanism has been questioned by Prasad and Lee (1994) based on the slow three-body kinetics.
In the same study, the authors presented an alternative method of HClO4 formation involving ClO·O3 and ClO·O2 (Prasad and Lee, 1994).
ClO·O2 has been detected (Kopitzky et al., 2002), but the existence of ClO·O3 is hypothetical.
Still, it is possible that other atmospheric reactions may form perchloric acid in addition to, or in place of, Eqs.
(1) and (2).
Regardless of the formation pathway, HClO4 has been shown to be stable by ab initio methods (Francisco, 1995), and it has been detected in the terrestrial stratosphere (Jaegle et al., 1996).
In the absence of rain, perchloric acid accumulates on the surface through dry deposition, where it reacts with minerals to form perchlorate salts (Catling et al., 2010).
Another photochemical pathway to ClO3 that was not investigated by Catling et al. (2010) involves the effect of bromine oxides:(3)BrO+ClO→Br+OClO
OClO then can form ClO3 by(4)OClO+O3→ClO3+O2
It is reasonable to assume that when Mars was volcanically active, the atmosphere contained the reactants for the above reactions.
There are multiple lines of evidence that volcanoes on Mars emitted both HCl and HBr.
First, the inferred composition of the martian mantle is enriched in Cl and Br by more than a factor of two compared with Earth (Wänke and Dreibus, 1994).
Second, the mineral composition of martian basaltic meteorites is consistent with an enrichment in Cl relative to terrestrial basalts (Filiberto and Treiman, 2009).
Third, there are elevated bromine levels in soils (Rieder et al., 2004), and recent analysis of Br/Cl ratios in soil profiles at Gusev Crater and Meridiani Planum indicate that bromine gas may have been released from soil to atmosphere by UV photolysis or chemical oxidation (Karunatillake et al., 2013).
Other ways to form perchlorate
In addition to Eqs.
(1) and (4), several other atmospheric reactions form precursors to HClO4; however, their relevance to Mars has not yet been demonstrated.
For example, laboratory experiments showed that oxidation of HCl gas by O3 gas produces both chlorate, ClO3-, and perchlorate, ClO4- (Wang, 2011).
The recovered ClO4- mass was two to three orders of magnitude greater than the recovered ClO3- mass (Wang, 2011).
However, HCl gas was exposed to O3 gas concentrations of 150-180mg/L (Wang, 2011), which are values that far exceed the concentration of O3 in both the terrestrial stratosphere and the martian atmosphere.
The Phoenix WCL was not designed to detect chlorate (the ion-sensitive electrode in the WCL has similar sensitivity to chlorate and nitrate, so a signal due mostly to chlorate is precluded).
However, it is possible that ClO3- contributed to the perchlorate signal (Hanley et al., 2012).
Lightning-induced oxidation of NaCl aerosols also produces ClO4-, but the recovered ratio of ClO4-/Cl- is only ∼10-4 by mass (Rao et al., 2012b).
Phoenix WCL measurements suggest that the ratio of ClO4-/Cl- is ∼15 by mass in the soil (Kounaves et al., 2010b), so the lightning production of perchlorate is not compatible with data.
Several authors have recently argued that ClO4- can be produced through aqueous and heterogeneous reactions; however, once again, it is unclear how effective such mechanisms would be on Mars.
Decomposition of chlorine-based bleach through UV exposure produces ClO4-, but at low yields ∼10-3% (Rao et al., 2012a).
Schuttlefield et al. (2011) show that perchlorate is produced by irradiation of titanium-containing crystals in aqueous solutions of chloride.
The perchlorate can initially be produced 2000 times faster than perchlorate produced via atmospheric oxidation of chlorine volatiles in Catling et al. (2010), but the reaction rate asymptotes and produces only ppm levels of perchlorate in 0.5M solutions of chloride.
Additionally, the radiation source used in their experiment, a 100-W xenon lamp, simulates the solar spectrum in space, which greatly exceeds the spectrum at Mars' surface because scattering and absorption by atmospheric gases and dust attenuates the ultraviolet solar flux (Patel et al., 2002).
In another recent study, CO2-doped chloride-bearing ices were bombarded with electrons, and chlorine oxides, such as ClO2- and ClO3-, formed (Kim et al., 2013).
These species can subsequently be oxidized to perchlorate; however, the yield of chlorine-oxides has not been quantified.
Atmospheric sulfates on Earth and Mars
On Earth and Venus, sulfate aerosols form when volcanic SO2 is oxidized and hydrated to form H2SO4 (McGouldrick et al., 2011).
On Earth, dry deposition of H2SO4 and subsequent reactions with minerals can form sulfates.
In fact, sulfates that are found within and near nitrate ore fields of the Atacama have Δ17O=0.4-4‰ (Michalski et al., 2004), which implies a stratospheric source for some of the oxygen molecules bound in the sulfate.
Settle (1979) suggested that sulfates on Mars would form in the atmosphere.
Mass independent fractionation of sulfur isotopes in secondary minerals in martian meteorites confirms this hypothesis, as it indicates an atmospheric source for sulfur (Farquhar et al., 2000).
Additionally, sulfates have been detected across the planet, which could be evidence for atmospheric deposition.
Apart from salts directly measured in the soil, large mounds of hydrated sulfate have been detected from orbit (Bibring et al., 2006; Murchie et al., 2009a, 2009b).
Atmospheric sources of the sulfate have been suggested for such sulfate-rich sedimentary deposits in Meridiani Planum (Niles and Michalski, 2009), Valles Marineris troughs, and Juventae Chasma (Catling et al., 2006).
Nitrates on Earth and Mars
In the terrestrial atmosphere, nitric acid (HNO3) can form by the reaction of NO2 with OH (Brasseur and Solomon, 2005).
Deposition of HNO3 forms nitrates.
Oxygen isotopes in nitrates from the Atacama Desert also indicate a stratospheric source, with Δ17O=14-21‰ (Michalski et al., 2004).
While nitrates have not yet been definitively detected on the martian surface, they should naturally form through oxidation of atmospheric N and accumulate on the surface (Mancinelli, 1996).
In photochemical models of the martian atmosphere, odd nitrogen (N and NO) forms in the thermosphere through photodissociation of N2, recombination of N2+ and NO+, the reaction of N2+ with O, and the reaction of O+(2P) with N2 (Krasnopolsky, 1993; Yung et al., 1977).
N and NO then flow into the lower atmosphere (Krasnopolsky, 1993; Yung et al., 1977).
These species can then be oxidized to NO2, which then can form nitric acid.
Previous work showed possible detections of indigenous nitrate in martian meteorite EETA79001 and Nakhla (Grady et al., 1995).
A more recent study of EETA79001 suggests a nitrate upper limit of ∼0.5wt.% in a sawdust sample that includes white-gray granular material known as "druse" (Kounaves et al., 2014).
Nitrate also may have been detected in Gale Crater by the Mars Science Laboratory (Navarro-Gonzalez et al., 2013).
However, the specific amount of nitrate in the soil remains largely unconstrained.Manning et al. (2008) have developed a steady-state model of nitrate decomposition and nitrogen escape in the martian atmosphere in an attempt to estimate the amount of nitrate in martian soil.
They calculated a ∼3m-thick global equivalent layer of pure NaNO3on Mars.
The actual inventory; however, is not known.
Photochemical model
To investigate chlorine, sulfur, and nitrogen chemistry in the martian atmosphere, we use a one-dimensional photochemical model derived from the model developed by Kasting (1979).
The model has recently been modified to study chlorine, sulfur, and nitrogen chemistry in the modern terrestrial atmosphere (Catling et al., 2010) and hydrogen, carbon, and oxygen chemistry in the modern martian atmosphere (Zahnle et al., 2008).
We combine the two models.
As part of model development, we matched the calculated mixing ratios of H2O, CO, H2O2, and O2 to those in the modern martian atmosphere (following Zahnle et al., 2008) and then added chlorine and sulfur chemistry from Catling et al. (2010), as discussed in Section 3.1.
The model uses a 1km vertical grid spacing and solves a time-dependent coupled transport-chemistry equation, as given in Catling et al. (2010).
The model is stepped forward to steady-state using the reverse Euler method.
Vertical transport occurs by eddy diffusion for all species, as well as molecular diffusion for H and H2.
Following the model of Zahnle et al. (2008), atmospheric transport, temperature, and relative humidity are initialized as follows.
The eddy diffusion coefficient K=106cm2s-1 for altitude z<20km, and K=106N(20km)/N(z)cm2s-1, where N is the atmospheric number density (moleculescm-3), for 20km<z<110km.
The surface temperature, T0, is set to 211K.
The atmospheric temperature follows a nominal profile of T=T0-1.4z for z<50km, and T is isothermal for z>50km.
The relative humidity is set to 17% throughout the entire atmosphere, and the atmospheric composition is set to 95% CO2.
The modern solar flux at Mars is applied at the top of the model grid.
A test to evaluate the effect of the evolution of the solar flux on salt deposition fluxes over the Amazonian eon (3Gyr-present) showed negligible effect (see Section 5.2).
Consequently, the nominal model assumes atmospheric Cl, S, and N chemistry is relatively insensitive to the evolving solar flux and uses the modern solar flux.
Updates to the photochemical model from Catling et al. (2010) include the incorporation of the delta 2-stream radiative transfer formulation of Toon et al. (1989) and a modification to the Rayleigh scattering cross-section.
The Rayleigh scattering cross-section is computed using the relative gas concentrations at each model height rather than assigning a global cross-section based on the ground level gas concentrations.
These improvements in realism were made primarily for model atmosphere studies involving variable pressure (Misra et al., 2014).
However, they make little difference to results in a thin atmosphere with well-mixed concentrations, such as modern Mars.
Model chemistry
The nominal chemistry includes gaseous species listed in Table 1 as well as two aerosols: H2SO4 (sulfate) aerosols and S8 (elemental sulfur) aerosols.
We neglected nitrate aerosols because the model runs in conditions where the partial pressure of HNO3 is insufficient to exceed its saturation vapor pressure based information in Luo et al. (1995).
The atmospheric chemistry allows for a version of the modern atmosphere, modified by volcanic fluxes, meant to simulate the atmosphere in the mid-Amazonian eon.
Volcanic fluxes are discussed in Section 3.2.
The 348 photochemical reactions include those used in Zahnle et al. (2008) and Catling et al. (2010), with the exception of reactions involving CH3Cl, which is produced biologically, and its photochemical products, CH2Cl, CH2ClO2, CH2ClO, and CHClO.
All reaction rates were updated where new rates were available from Sander et al. (2011).
For completeness, we also added one new chlorine reaction not included in Catling et al. (2010): Cl2O2+O3→ClO + ClOO+O2.
But the upper limit of the rate constant, k=1×10-19cm3s-1, is slow (Atkinson et al., 2007), so the effect on overall chemistry is negligible.
A background of 95% carbon dioxide allows for enhanced three-body reaction rates compared to terrestrial air (Lindner, 1988), so rate constants for three-body reactions measured in air are multiplied by a rate of 2.5 following Nair et al. (1994).
Atmospheric formation of salt-precursors occurs through the injection of volcanic gases (discussed in Section 3.2) and subsequent reactions in the gas-phase.
Formation of sulfuric acid in the gas-phase occurs by the following reactions (Seinfeld and Pandis, 2006):(5)SO2+OH+M→HOSO2+M(6)HOSO2+O2→HO2+SO3(7)SO3+H2O→H2SO4
Gas to particle conversion occurs when the saturation vapor pressure of H2SO4 is exceeded.
The H2O/H2SO4 ratio of the particles is self-consistently predicted.
Further details of aerosol formation are given in Pavlov et al. (2001).
S8 aerosols are formed when S2 polymerizes to form longer sulfur chains.
Polymerization stops at the stable S8 ring-molecule, which is assumed to condense.
S2 is formed by the reaction of HS and S, or, as discussed in Section 3.2, it is injected directly into the atmosphere through volcanism.
However, the nominal model has an oxidizing atmosphere, so S8 is not the relevant fate for S2 molecules.
Rather, S2 is oxidized to SO, which can be further oxidized to SO2 and H2SO4.
This is consistent with previous work showing that in CO2 atmospheres with low sulfur input and high SO2/H2S, like the atmosphere modeled here, primary sulfur is converted to sulfuric acid more efficiently than it is converted to elemental sulfur (Hu et al., 2013).
Gas-phase nitric acid is produced by a reaction analogous to Eq. (5):(8)NO2+OH+M→HNO3+M
Likewise, gas phase pernitric acid is formed by reaction of HO2 and NO2:(9)HO2+NO2+M→HNO4+M
Perchloric acid forms through reaction (2).
We also developed a version of the model with bromine chemistry to test whether the effects of interactions between the bromine and chlorine cycles, including Eq.
(3), are significant.
The bromine-version of the model is discussed in Section 6.
Volcanism and transport of species at the upper and lower boundaries
In the model, chlorine and sulfur gases are emitted from volcanoes.
These gases are the sources of Cl- and S-bearing gases.
Evidence for volcanism in the Amazonian eon is abundant.
Evidence includes lava flows dated by crater counts to be no older than 100myr old (Hartmann et al., 1999; Hartmann and Neukum, 2001) and basaltic meteorites that have crystallization ages of less than 1.3Gyr (Nyquist et al., 2001).
The composition and emission rates for martian volcanic gases are poorly constrained, so we use nominal estimates from the literature and test the model for sensitivity to their variations.
We assume a total emission rate that is ∼0.1% of the terrestrial volcanic emission rate used in Catling et al. (2010), which results in ∼106moleculescm-2s-1.
This factor is consistent with the estimate that the eruption rate of volcanic rocks was ∼103 times lower than contemporary terrestrial emission rates at 1-2Ga (Jakosky and Shock, 1998), assuming that volcanic gas emission rates scale directly with volcanic rock eruption rates.
We estimate the H-O-C-S composition of volcanic gases using the geochemical model results of Gaillard and Scaillet (2009).
For HCl, which is not considered by Gaillard and Scaillet (2009), we take the terrestrial arc volcano HCl/SO2 ratio of 0.3 (Pyle and Mather, 2009) and multiply by a factor of 2.
This factor of 2 is consistent with the chlorine enrichment observed on Mars (Wänke and Dreibus, 1994) and a Cl/La enrichment of ∼2.5 in martian meteorites compared to terrestrial basalts (Filiberto and Treiman, 2009).
The emission rates of volcanic gases in the nominal model are shown in Table 2.
S2 appears as the dominant sulfur gas because SO2 and S2 are favored over H2S at lower pressures (Gaillard and Scaillet, 2009).
Gaillard and Scaillet (2009) model martian magma as relatively water-poor (0.2wt.%) with a mantle oxygen fugacity of QFM-2 (2 log units below the quartz-fayalite-magnetite redox buffer).
These combined conditions favor S2 over SO2 (Gaillard and Scaillet, 2009).
Emissions are distributed evenly between the middle of the second atmospheric layer, 1.5km, and 20km in the model (the troposphere and lower stratosphere).
Most gases are removed from the atmosphere to the surface according to a prescribed deposition velocity.
The deposition velocity is a scaling factor that affects the transport of species from the bulk atmosphere to the surface in the absence of rain.
The deposition velocity is coupled to the gas concentrations computed by chemical kinetics.
Following Zahnle et al. (2008), we apply a deposition velocity of 0.02cms-1 to all species, with two exceptions.
First, the deposition velocities for O2, H2, and CO are set to zero (following Zahnle et al., 2008).
Second, all species with a zero deposition velocity in Catling et al. (2010) are prescribed a zero deposition velocity because we consider them to be nonreactive.
These species include NO3, N2O5, N2O, CH3O, CH3ONO, CH3ONO2, CH2ONO2, CH3O2, CH3OH, CH2OOH, Cl2, CH2OH, CH2O2, OClO, ClOO, ClONO, ClNO, ClNO2, CH3OCl, Cl2O2, Cl2O, ClO3, and Cl2O4.
The deposition velocity multiplied by the species number density at the lower boundary, in addition to the flux term from eddy diffusion, determines the flux of species to the surface.
The photochemical model simulates chemistry up to 110km, so upper boundary conditions, for certain species, must be applied to account for chemistry that takes place above the vertical bounds of the model.
As mentioned previously, both N and NO precipitate into the neutral atmosphere from the ionosphere.
The ionosphere is not explicitly modeled here, so the nominal input of odd-nitrogen at 110km is set to 2×106moleculesNcm-2s-1 and 2×107moleculesNOcm-2s-1 (Krasnopolsky, 1993).
We vary these rates later to test model sensitivity.
Additionally, following Zahnle et al. (2008), atomic oxygen is lost at the top of the model at a rate of 107moleculescm-2s-1.
This rate is consistent with literature estimates (Fox and Hac, 1997; Lammer et al., 2003).
Additionally, CO flows into the top of the model at a rate of 2×107moleculescm-2s-1 in order to maintain redox balance, and hydrogen escapes at the diffusion-limited rate (Zahnle et al., 2008).
Methods
We employ two groups of sensitivity tests to determine the effect of changing model parameters on the deposition rate of salt precursors.
The purpose of the first group of tests (Section 4.1) is to quantify the sensitivity of perchloric acid, (per)nitric acid, and sulfate aerosol deposition to a subset of upper and lower boundary conditions.
These include the solar flux; the input of Cl, N, and S atoms; and the deposition velocities of Cl and S atoms.
We will use the results to calculate the possible range of integrated salt deposition fluxes over the Amazonian eon.
The purpose of the second group of sensitivity tests (Section 4.2) is to figure out why the deposition rate of perchloric acid is different on Mars and Earth.
Several atmospheric parameters differ significantly between Mars and Earth.
We choose three parameters: surface temperature, pressure, and O2 mixing ratio, and vary them to see how each one affects the deposition rate of perchloric acid.
Sensitivity tests: upper and lower boundary conditions
We vary unconstrained boundary conditions, including volcanic emission rates, deposition velocities, and precipitation of odd nitrogen from above the model's upper boundary.
But first we test the model's sensitivity to the solar flux at the top of the atmosphere.
Sensitivity test #1: evolution of solar flux
We apply a parameterization of the evolution of the solar flux (from Claire et al., 2012) to the nominal model to quantify changes in deposition fluxes over time.
Sensitivity test #1: volcanic emissions
We vary the total volcanic emission rate from ∼106moleculescm-2s-1 to ∼108moleculescm-2s-1, or in other words, 0.1-10% of the terrestrial volcanic flux used in Catling et al. (2010).
Throughout this range, we also vary the HCl/SO2 from the nominal case of 0.6, up to 60.
Sensitivity test #2: deposition velocities
The deposition velocity used in the nominal model is not a measured value.
Rather, it is best understood as a fitting parameter tuned to reproduce CO, O2, and H2 abundances on modern Mars, valid as a global average on a 105year time scale.
Deposition velocities could have been different at other times and for other conditions.
We complete two model simulations in which two key deposition velocities are changed.
First, we set the deposition velocity of SO2, which competes with sulfate aerosol deposition, to zero.
Second, we set the deposition velocity of HCl, which competes with HClO4 deposition, to zero.
Sensitivity test #3: input of odd nitrogen
Nitrogen input from the upper atmosphere almost entirely controls the rate of nitrate deposition to the surface.
At the upper boundary of the model, we vary the odd nitrogen (N and NO) input from the ionosphere because the fluxes we assume in the nominal model are uncertain (Krasnopolsky, 1993).
We simulate the atmosphere under the following fluxes (in moleculescm-2s-1): NO=2.0×107 and N=2.0×106; NO=2×104 and N=2×103; NO=2×103 and N=2×102; and NO=2×102 and N=20.
Sensitivity tests: temperature, pressure, and oxygen mixing ratio
We vary the atmospheric pressure, temperature, and O2 mixing ratio to understand how these three parameters affect the deposition rate of perchloric acid.
Specifically, we want to answer the question: How does the different atmospheric state on Mars and Earth affect the perchloric acid deposition flux on each planet?
We start from the nominal model for Mars and then shift each parameter, separately, to more Earth-like values.
From here, we can see how the perchloric acid deposition flux either approaches or diverges from the calculated perchloric acid deposition flux on Earth of ∼105moleculescm-2s-1 (Catling et al., 2010).
These tests are not meant to represent physically plausible scenarios.
For example, temperature and pressure are varied independently, and the O2 mixing ratio is increased.
This latter change implicitly assumes there is a source of O2 at the surface.
Therefore, our interpretations are qualitative.
Sensitivity test #4: temperature
Atmospheric temperatures affect photochemical rate constants and atmospheric water vapor content.
These, in turn, affect the chain of reaction rates that lead to the oxidation of Cl to form HClO4.
To pinpoint how atmospheric temperatures alter reaction rates, we shift the nominal Mars temperature profile to higher surface temperatures.
The shape of the profile is preserved, but the surface temperature is increased from 211K to 284K (a temperature increase of ∼35%), with the latter temperature being more representative of surface temperatures on Earth.
Sensitivity test #5: surface pressure
Photochemical reaction rates are density-dependent; therefore, we vary the surface pressure from the nominal value of 6.3mb to 35mb.
This pressure is as high as the surface pressure can be taken without the model entering a CO-runaway as described in Zahnle et al. (2008).
Sensitivity test #6: oxygen concentration
The formation rate of perchloric acid in Eq.
(2) is dependent upon the atmospheric abundances of Cl and O3, in addition to the rate constant.
In Section 4.1.2, we described a test to determine the sensitivity of perchloric acid formation to Cl volcanic input, so for the last sensitivity test, we vary the O2 mixing ratio to determine sensitivity to the abundance of O2 and its photochemical product, O3.
In the nominal model, the O2 mixing ratio is calculated as model output; however for this test, the O2 mixing ratio is fixed as model input.
While holding the surface pressure constant, the O2 mixing ratio is increased by a factor of ten starting from its value in the nominal model, 1.6×10-3.
Results
Photochemical products of CO2 and H2O dominate the nominal model atmosphere.
The volcanic input is low, and therefore the atmosphere remains highly oxidizing.
The net redox imbalance of the atmosphere, pOx=2pO2-pCO-pH2, is 15μbar (as defined in Zahnle et al., 2008).
Dominant chlorine, sulfur, and nitrogen gases are HCl, SO2, and NO, respectively.
The mixing ratio profiles are plotted in Fig. 1.
Additionally, the production profiles of salt precursors are plotted in Fig. 2.
HNO3, HNO4, and sulfate aerosol (SO4 AER) production primarily occurs close to the surface, while HClO4 production primarily occurs aloft where the O3 abundance is highest.
Below we describe results from the sensitivity tests to assess the role of variables in salt formation and deposition on Mars.
Deposition fluxes in the nominal Mars model
The deposition fluxes of perchloric acid, sulfate aerosols, nitric acid, and pernitric acid in the nominal Mars model are listed in Table 3.
We assume that salts form from the acids upon deposition to the surface.
We next make several assumptions to calculate the concentration of salts that have accumulated in the soil during the Amazonian eon.
We first assume that perchloric acid, sulfate aerosols, nitric acid and pernitric acid have accumulated at a uniform rate.
This assumption is valid because the lack of aqueous minerals and very low weathering rates tell us the Amazonian eon on Mars has been characterized by a climate and atmosphere not greatly different from today (Bibring et al., 2006).
We next assume a range of soil mixing depths so that salts are distributed throughout the soil column.
According to Zent (1998), small post-Noachian impactors have churned the soil on Mars to a 1/e mixing depth of 0.51-0.85m.
Taking three e-folding depths, the range would be 1.5-2.6m depth, with a mean ∼2m.
The last assumption we make is that the soil density is 1gcm-3 (Moore and Jakosky, 1989).
Using these assumptions, we calculate the concentrations of anions in the soil for the nominal model and report them in Table 4.
These anions must be combined into salts.
For perchlorate, the salts may be Mg(ClO4)2 or Ca(ClO4)2 as discussed earlier.
The dominant nitrogen-bearing salt is unknown.
The sulfate deposition flux produced in the nominal model is compatible with estimates of the amount of sulfates on Mars.
The nominal range (1.0-1.7wt.% SO4) is consistent with 1.3wt.% soluble sulfate measured at the Phoenix landing site (Kounaves et al., 2010b).
The estimates also compare well with an average ∼6.8wt.% sulfur as SO3 (2.7wt.% S) in global soil inferred from elemental abundances measured at various locations on Mars by Spirit, Opportunity, Pathfinder, and Viking Landers.
We can also compare the average sulfur content in the top few tens of centimeters of the soil of ∼4.4wt.% inferred from Gamma Ray Spectrometer measurements (King and McLennan, 2010).
The agreement between model results and data suggest that 0.1% of the terrestrial volcanic gas flux is a good estimate for the volcanic emission rate on Mars 1-2Ga if soil salts derive from volcanic input.
Deposition of pernitric acid, HO2NO2 or HNO4, exceeds deposition of nitric acid, HNO3, contrary to the case on Earth.
Production of HNO4 is enhanced at lower temperatures because the rate constant of Eq.
(9) is inversely related to temperature.
The same is true for the rate constant of Eq.
(8), but the HNO3 abundance is depressed at lower temperatures due to the lower abundance of OH.
OH is produced by the photolysis of water vapor and the reaction of H2O with O(1D), but the water vapor abundance is lower at lower temperatures.
Implications of enhanced deposition of HNO4 on Mars, compared with Earth, will be discussed in a separate paper about N deposition on Mars (Catling et al., in preparation).
There is no confirmed nitrate on Mars, so we compare the results to an estimate of N on Mars.
The integrated deposition flux of HNO4 in the model corresponds to an evenly-distributed global layer of NO4 with a mass of 4.3×1018g, which is far smaller than the NO3 mass of 7.2±3.6 (×1020) calculated by Manning et al. (2008).
If the concentration of N predicted here are correct, then they could be below the detection limit for near-infrared orbital instruments, which may explain why nitrates have not yet been observed.
The model-inferred abundances of perchlorate are far smaller than observations, however.
If we assume the soil is mixed to an average depth of 2m and the parent salt is Ca(ClO4)2 or Mg(ClO4)2, then we calculate concentrations of 4.3×10-8wt.% and 4.0×10-10wt.%, respectively.
These concentrations are both many orders of magnitude lower than the observed abundance at the Phoenix landing site: 0.4-0.6wt.% (Hecht et al., 2009).
Also, the concentrations are far below what would be inferred from observations of perchlorate:nitrate ratios ∼1:60 measured in EETA79001 (Kounaves et al., 2014) and the Atacama perchlorate:nitrate of ∼1:1000.
Distinct from the results for Earth where gas phase reactions were sufficient to reproduce data (Catling et al., 2010), we conclude that additional heterogeneous reactions must be present to account for the efficient formation of perchlorate on Mars, a hypothesis we discuss further in Section 7.
But first we explore the sensitivity of deposition fluxes to model parameters.
Sensitivity of salt deposition to solar flux evolution
The effect of an evolving solar flux from Claire et al. (2012) on the salt deposition fluxes was negligible (Fig. 3).
Consequently, we set the solar flux to be the modern solar flux for the rest of the simulations.
Sensitivity of salt deposition fluxes to volcanic input
We next calculated the perchlorate, sulfate, and nitrate deposition fluxes on Mars as a function of volcanic input.
The total volcanic emission rate was increased from the nominal case (∼106moleculescm-2s-1) to one hundred times this value to reflect possible higher rates of volcanism on Mars in the past.
At the same time, we tested the effect of increasing the HCl/SO2 ratio, given that the value is uncertain.
We completed two simulations, one with HCl/SO2=0.6, and another with HCl/SO2=60.
The deposition fluxes of sulfate aerosols and perchloric acid are both sensitive to volcanic input (Fig. 4).
However, at the highest level of volcanic input, the deposition fluxes of both species are incompatible with observations.
First, we consider the largest deposition flux of sulfate aerosols.
This value occurs at a volcanic input of 100 times the nominal value.
We use the same range of mixing depths as we did in the previous section (1.5-2.6m) to calculate a concentration of 100-173wt.% sulfate in the soil.
This range is incompatible with observations of sulfur on Mars described in Section 5.1, and values greater than 100wt.% indicate that sulfate deposition is unrealistically high when the mixing depth is restricted to 1.5m.
Second, we consider the largest deposition flux of perchloric acid.
This value occurs at a volcanic input of 100 times the nominal value and the HCl/SO2 emission flux ratio of 60.
Once again, we use a range of mixing depths of 1.5-2.6m to calculate the concentration of ClO4- in the soil.
The range is 6.1×10-6-1.0×10-5wt.% ClO4-, which again is incompatible with the measurement of ∼0.5wt.% ClO4- made by the Phoenix Lander (Hecht et al., 2009).
Sensitivity of pernitrate and nitrate deposition flux to odd nitrogen input
As stated previously, there is considerable uncertainty in the input rate of odd nitrogen (N and NO) species from the martian ionosphere to the neutral atmosphere (Krasnopolsky, 1993).
In his own model of the neutral atmosphere, Krasnopolsky considers cases both with and without input of odd nitrogen from the upper atmosphere (Krasnopolsky, 1993).
We vary the input of N and NO into the model, using the nominal case as an upper limit.
As these values are decreased, the pernitrate deposition flux drops, which is shown in Fig. 5.
The lowest input of odd nitrogen corresponds to 3.5-6.1 (×10-4)wt.% N accumulated over 3byr and mixed into 1.5-2.6m of soil.
Sensitivity of salt deposition flux to deposition velocities
Perchloric acid and sulfate aerosols are minor sinks of chlorine and sulfur in the nominal model (Table 5).
To calculate upper limits on the deposition of sulfate aerosols and HClO4, we separately turned off the deposition velocity of its main competitors.
First we set the deposition velocity of SO2 to zero, and then we set the deposition velocity of HCl to zero.
The modified sinks of chlorine and sulfur are listed in Table 6.
When the SO2 deposition velocity is zero, ∼98% of the input sulfur exits the atmosphere as sulfate aerosols.
We repeat the calculation from Section 5.1 and find that this corresponds to 2.9-5.0wt.% SO4 accumulated over 3byr mixed into 1.5-2.6m of soil.
When the HCl deposition velocity is zero, the major surface sink of chlorine is HOCl.
HClO4 deposition increases by about four orders of magnitude from the nominal model.
This flux corresponds to 6.9×10-5-1.2×10-4wt.% ClO4- accumulated over 3byr mixed into 1.5-2.6m of soil.
Temperature
To semi-quantitatively assess the sensitivity of the deposition fluxes of salts to temperature, we forced the model temperature profile to higher values by increasing the temperature profile by 35% in 5% increments.
Warmer temperatures significantly increase the formation of perchloric acid.
Over the range tested, the deposition rate of perchloric acid increases by around six orders of magnitude.
As the atmospheric temperature increases, the chemistry begins to favor the deposition of HClO4 over HCl.
At higher temperatures, there is more H2O in the atmosphere, which is set by the atmospheric temperature.
More water vapor decreases CO and increases OH, while O2 concentrations are relatively insensitive to H2O abundance (Zahnle et al., 2008).
OH reacts with ClO3, through Eq.
(2), to produce HClO4.
HCl production increases at warmer temperatures because of increased amounts of HO2 (O+OH→H+O2 makes O2, H+O2+M→HO2+M makes HO2, and HO2+Cl→HCl+O2 makes HCl).
However, the destruction of HCl through HCl+OH→Cl+H2O also increases; hence the chemistry favors HClO4 deposition over HCl deposition.
This is shown in Fig. 6.
Pressure
Increasing the surface pressure from 6.3mb to 35mb decreases the deposition rate of atmospheric perchlorate from 4.6×10-3 to 7×10-5moleculescm-2s-1.
We did not continue to increase the surface pressure up to Earth-like pressures because a build-up of CO causes the atmosphere to become progressively more reduced, which only further decreases the formation of oxidized salt-precursors.
In thicker CO2 atmospheres, the significance of H2O vapor photolysis is reduced, and the OH mixing ratio decreases.
This, in turn, reduces the oxidation of CO via CO+OH→H+CO2 and the production of O2 via O+OH→H+O2.
As a consequence, the redox state of a more dense atmosphere becomes more reducing because the reducing gas CO is favored over the oxidizing gas O2.
This behavior was also recognized, and is detailed further, in Zahnle et al. (2008).
A more reduced atmosphere is less amenable to perchloric acid production (Fig. 7).
Chemistry: oxygen concentration
Changing the chemical composition of the atmosphere affects the perchlorate production and deposition rate.
A lower concentration of atmospheric ozone on Mars compared with Earth inhibits the production of perchloric acid.
As shown in Fig. 8, increasing the O2 mixing ratio from the level in the nominal model, while holding the total atmospheric pressure constant, causes a significant increase in the O3 column density.
This, in turn, causes the perchlorate deposition flux to increase by around four orders of magnitude.
While a larger O2 mixing ratio may not represent a specific past time on Mars, the results demonstrate that lower O2 and O3 concentrations limit the production of HClO4 in the nominal model.
For comparison, on Earth, a typical O3 column density is ∼8×1018moleculesO3cm-2.
Bromine chemistry
In one simulation, we included volcanogenic bromine, in the form of HBr.
Bromine interacts with chlorine cycles through Eq.
(3) and various other reactions.
The addition of gas phase bromine chemistry makes the model more complete and introduces known chemistry that may affect formation of perchloric acid.
Adding bromine chemistry involved a number of steps.
First we added bromine chemistry (see Appendix A) to the model of Catling et al. (2010) for the terrestrial atmosphere.
The model was validated against data, further details of which are given in Appendix A.
After validation, we then added inorganic bromine chemistry to the nominal Mars model.
The terrestrial arc volcano HCl/HBr emission flux ratio of ∼103 (Pyle and Mather, 2009) was applied, and two more cases with HCl/HBr=100 and HCl/HBr=10 were run, to determine if more HBr affected HClO4 production.
We note, again, that recent work indicates bromine gas may have been released to the atmosphere by volatilization of soil-derived bromine (Karunatillake et al., 2013), but the flux is not quantified, so we do not consider this source here.
When we include inorganic bromine chemistry, the effect on the deposition flux of perchloric acid is negligible.
This is because Eqs.
(3) and (4) occur at slower rates than other reactions, involving species from the nominal model, that form OClO and ClO3 (Table 7).
Discussion
The model results show that atmospheric dry deposition can plausibly explain typical abundances of a several wt.% sulfate observed in soils on Mars.
The model also predicts nitrogen amounts, as nitrates or pernitrates, at levels on the order of 0.1wt.%, assuming these species do not react or decompose.
Perchlorate is formed very slowly through the gas phase atmospheric chemistry that is modeled here.
For the nominal case, we calculate a ClO4- concentration of 2.8-4.8 (×10-8)wt.%, which is a factor of 107 less than the measurement made by the WCL instrument on the Phoenix Lander (Hecht et al., 2009).
With regard to perchlorate, the sensitivity tests revealed that changing the model boundary conditions did not increase perchloric acid deposition rates substantially.
The inclusion of bromine gas phase photochemistry has negligible effect on perchlorate fluxes.
Sensitivity tests revealed two primary causes of the lower perchlorate fluxes from gas phase reactions on Mars relative to Earth: lower amounts of ozone and colder, drier air.
The much smaller amounts of ozone in the martian atmosphere compared to the terrestrial atmosphere (Fig. 8) are important because ozone is critical for the gas phase formation reactions of precursors to perchloric acid, Eqs.
(1) and (4).
In addition, the colder and drier air results in a smaller concentration of OH radicals needed in the three-body reaction to make perchloric acid, Eq. (2).
We suggest that gas-solid reactions are probably critical for enhancing the transformation of chlorides to perchlorate on Mars.
Many important reactions in terrestrial atmospheric chemistry are heterogeneous.
The most famous example is that the Antarctic ozone hole could not be explained until models incorporated the heterogeneous chlorine chemistry that occurs on very cold particles (Solomon, 1999).
Another possibility is that perchlorate was formed in the past by heterogeneous reactions on atmospheric sulfate aerosols.
Small amounts of HClO4 have been produced in the laboratory by reactions of ClO with H2SO4 aerosols (Martin et al., 1979).
Jaegle et al. (1996) modeled the formation of HClO4, assuming a 40% yield from the reaction of ClO on the surface on sulfate aerosols.
The authors concluded that, on Earth, over 50% (2ppbv) of stratospheric inorganic chlorine could be sequestered into HClO4 when significant amounts of sulfate aerosols are present.
However, subsequent work questioned the result and found insignificant uptake of ClO on sulfuric acid (Abbatt, 1996).
Nonetheless, other heterogeneous reactions involving oxidizing radicals might be possible by analogy with ongoing research to understand the terrestrial atmosphere (George and Abbatt, 2010).
Improving model accuracy
Finally, we note that the accuracy of the photochemical model used here could be improved with new data.
NASA's MAVEN (Mars Atmosphere and Volatile EvolutioN) mission will characterize the upper atmosphere of the planet, which will include measurements of neutral species and thermal ions.
These data will be useful in constraining the odd-nitrogen flux into the neutral atmosphere, the loss of O from the neutral atmosphere, and the escape rate of hydrogen (Jakosky and MAVEN Science Team, 2011), all of which are adjustable parameters in the model.
Conclusions
We used a 1-dimensional photochemical model to calculate the atmospheric production and deposition rate of sulfate, nitrate and perchlorate salts in the Amazonian martian atmosphere.
The results suggest the following conclusions:•
Sulfate production through gas to particle conversion in the atmosphere is a viable means to account for the measured concentration of sulfates on the martian surface.
•
Nitrogen is continuously dry-deposited from the atmosphere of Mars even today mainly as pernitric acid.
During the Amazonian, 4.3×1018g NO4 could have been deposited across the martian surface if all of the nitrate is formed through atmospheric photochemistry and persists without decomposition or any further reactions.
This corresponds to a concentration of 0.3wt.% N if it is mixed uniformly to a depth of 2m.
This prediction can be confirmed or disproved by future in situ measurements.
•
The production of perchlorate via the gas phase formation process first modeled by Catling et al. (2010) for the driest regions of Earth's atmosphere is insufficient to account for 0.4-0.6wt.% ClO4- measured by NASA's Phoenix Lander.
Low atmospheric temperatures and a small inventory of oxygen and ozone limit the gas phase production rate of perchlorate.
Consequently, purely gas phase reactions do not appear to explain high perchlorate:chloride ratios in martian soil.
Instead, the efficient conversion of chloride to perchlorate may rely on heterogeneous reactions, i.e., gas-solid surface reactions.
The limitation of gas phase reactions may be analogous to problems encountered in terrestrial atmospheric chemistry, such as the 'ozone hole' issue.
Further theoretical and laboratory research into perchlorate production mechanisms under martian conditions is warranted.
Acknowledgments
This work was supported by NASA's Mars Fundamental Research Program through Grant NNX10AN67G awarded to DCC.
KJZ also acknowledges support from Mars Fundamental Research Program.
MLS acknowledges support from a National Science Foundation Integrative Graduate Education and Research Traineeship, under NSF-IGERT Grant DGE-9870713, Astrobiology: Life in and beyond Earth's Solar System.
MWC acknowledges support from a 2011 NAI Director's Discretionary Fund award titled "Perchlorate, Water, and Life", along with a NASA postdoctoral program award to work at the Virtual Planetary Laboratory.
MLS thanks Jim Kasting for helpful discussions about using the photochemical model.
We also thank two anonymous reviewers for improving the scientific content and clarity of the paper.
Bromine chemistry in an Earth photochemical model
There are many models for bromine chemistry in the troposphere and stratosphere, ranging from 3-D chemical transport models (e.g. von Glasow and Crutzen, 2006), to box models of the marine boundary layer (e.g. Sander and Crutzen, 1996), to 3-D stratospheric models (e.g. Hossaini et al., 2012).
One-dimensional models (Singh and Kasting, 1988; Yung et al., 1980) are able to match observed profiles of the most abundant bromine species.
Before we added bromine chemistry to the nominal Mars model, we added the chemistry to the terrestrial model of Catling et al. (2010).
Bromine reactions are listed in Table A.1.
Volcanic emission of HBr, release of bromine from sea salt aerosols, and emission of CH3Br from marine phytoplankton are all sources of atmospheric bromine on Earth (e.g. Lovelock, 1975).
Other sources of bromine, including anthropogenic and additional biological emissions, were not considered here because the sources are poorly constrained (von Glasow and Crutzen, 2006) and unlikely to apply for Mars.
We applied the terrestrial arc volcano HCl/HBr ratio of ∼103 to the terrestrial volcanic emissions (Pyle and Mather, 2009).
The HCl volcanic flux was set to 5×108moleculescm-2s-1 and the HBr volcanic flux was set to 5×105moleculescm-2s-1.
Bromine, in the form of BrCl and Br2, is released from sea salt aerosols at a rate of 1-10×1011gBryr-1 (von Glasow and Crutzen, 2006).
We took a rate of 5×1011gyr-1 and assumed all bromine is released as Br2.
For the third and last source of bromine, we assumed a CH3Br surface mixing ratio of 20pptv (Yung et al., 1980).
This value may be too high, compared with the nominal background concentration of CH3Br in the troposphere ∼10pptv (Quack and Wallace, 2003).
However, because we ignored other organic bromine sources, our assumption is justifiable.
The deposition velocities of bromine species were assumed to be equivalent to their chlorine counterparts, e.g., Br was given the same deposition velocity as Cl and HBr was given the same deposition velocity as HCl (Catling et al., 2010).
To validate the terrestrial model with bromine we compared satellite and in situ measurements of bromine species.
We compared measurements of four bromine species, BrO, BrONO2, HOBr, and HBr, with output from the 1-D model.
The model-derived BrO mixing ratio profile was compared with two separate measured profiles of BrO.
The first set of measurements was made with balloon-borne Differential Optical Absorption Spectroscopy (Dorf et al., 2008).
The balloon payload was launched at Teresina, Brazil (5.1°S, 42.9°W) on 17 June 2005 (Dorf et al., 2008).
The second set of measurements was taken by the Scanning Imaging Absorption Spectrometer for Atmospheric Chartography (SCIAMACHY) instrument aboard the European Envisat satellite (Sioris et al., 2006).
SCIAMACHY data plotted in Fig.
A.1 are from both low latitudes, 20-30°S, and high midlatitudes, 36-60°S, and correspond to early March 2003.
The data and model results compare relatively well, but it is clear that the modeled BrO concentrations are slightly lower than the observations (Fig.
A.1a).
This could be the result of several factors.
First, as noted previously, we have neglected some sources of organic and anthropogenic bromine.
Second, the model did not include heterogeneous chemistry.
In the model of von Glasow and Crutzen (2004), when only gas phase chemistry was considered, cycling of HOBr, HBr, and BrONO2 was slower than when heterogeneous chemistry was included, and as a result the BrO concentrations were systematically lower than they should have been.
We next compared the model output with measurements of BrONO2, HOBr, and HBr.
Measurements of BrONO2 were taken by the Michelson Interferometer for Passive Atmospheric Sounding aboard the European Envisat Satellite (Hopfner et al., 2009).
The measurements, both day and night, were taken in September 2003, across 40°S to 15°S.
The model accurately captured a diurnal average of BrONO2, within the limits of uncertainty (Fig.
A.1b).
Measurements of HOBr (Fig.
A.1c) and HBr (Fig.
A.1d) were from a far-infrared spectrometer aboard a balloon platform (Johnson et al., 1995).
The data reported in Fig.
A.1 are averages from several flights taken in 1988-1990.
There are only slight differences between the model results and observations, leading us to conclude that the calculation of bromine speciation is accurately captured by the addition of bromine reactions to the model of Catling et al. (2010).

Degradation of the beta-blocker propranolol by electrochemical advanced oxidation processes based on Fenton's reaction chemistry using a boron-doped diamond anode
Propranolol hydrochloride of 99% purity was supplied by the pharmaceutical AstraZeneca Espana (Madrid, Spain). 1-Naphthol was reactive reagent from BDH Chemical Ltd. and phthalic acid was analytical reagent from Aldrich. Oxamic and oxalic acids were analytical grade from Panreac. Sulfuric acid, anhydrous sodium sulfate and ferrous sulfate heptahydrate were analytical grade from Merck and Fluka. Solutions were prepared with pure water obtained from a Millipore Milli-Q system with TOC content < 1 μg dm-3 and resistivity > 18 MΩ cm at 25 degC. Organic solvents and other chemicals used were either HPLC or analytical grade from Merck, Fluka, Panreac and Aldrich.Super high-rate, long cycle life of europium-modified, carbon-coated, hierarchical mesoporous lithium-titanate anode materials for lithium ion batteries

Europium doping hierarchical mesoporous Li4-x/2Ti5-x/2EuxO12@C microspheres were prepared by the co-precipitation method. In a typical synthesis, 12 mL of tetrabutyl titanate and an appropriate amount of Eu(NO3)3*6H2O were added to anhydrous ethanol to form a faint yellow homogeneous solution A. Then, 1.208 g of LiOH*H2O were dissolved in deionized water to form solution B. Finally, solution B was slowly dropped into solution A with stirring. The yellow transparent solution gradually turned into a white suspension. The mixture was stirred for 4 h, dried at 80 degC overnight to fully remove the solvent, with subsequent heating to 600 degC for 5 h under 5 vol% N2/H2 atmosphere. Excess Li was used to compensate for volatilization of Li at high temperatures. The Li4-x/2Ti5-x/2EuxO12@C (x = 0, 0.002, 0.004, 0.006, 0.008 and 0.01) samples were obtained and denoted as LTO1, LTO2, LTO3, LTO4, LTO5 and LTO6, respectively.


Exploring the working mechanism of Li + in O3-type NaLi 0.1 Ni 0.35 Mn 0.55 O 2 cathode materials for rechargeable Na-ion batteries
Both O3-type NLNMO and NNMO were synthesized by a co-precipitation method. NiSO4*6H2O and MnSO4*H2O were titrated into a stoichiometric NaOH solution to form a precursor TM(OH)2. The precursor was washed and dried at 120 degC for 12 h, and then stoichiometric amounts of Li2CO3 (only in NLNMO), Na2CO3, and TM(OH)2 were mixed with acetone as a dispersant via ball milling for 6 h and dried at 120 degC. The mixture was pressed into pellets, calcined at 900 degC for 12 h in air and then cooled down to room temperature in the furnace. The samples were then stored in an Ar-filled glove box until characterization or cell construction for protection from moisture and air, since otherwise Na+ would have been extracted from the material.24Rigorously modeling self-stabilizing fault-tolerant circuits: An ultra-robust clocking scheme for systems-on-chip

Abstract
We present the first implementation of a distributed clock generation scheme for Systems-on-Chip that recovers from an unbounded number of arbitrary transient faults despite a large number of arbitrary permanent faults.
We devise self-stabilizing hardware building blocks and a hybrid synchronous/asynchronous state machine enabling metastability-free transitions of the algorithm's states.
We provide a comprehensive modeling approach that permits to prove, given correctness of the constructed low-level building blocks, the high-level properties of the synchronization algorithm (which have been established in a more abstract model).
We believe this approach to be of interest in its own right, since this is the first technique permitting to mathematically verify, at manageable complexity, high-level properties of a fault-prone system in terms of its very basic components.
We evaluate a prototype implementation, which has been designed in VHDL, using the Petrify tool in conjunction with some extensions, and synthesized for an Altera Cyclone FPGA.
Highlights
•
We introduce a novel modeling framework for fault-tolerant VLSI circuits.
•
We cast a self-stabilizing clocking scheme from a companion article in this model.
•
We discuss the implications of theory and model for the resulting implementation.
•
We present the measures taken to avoid metastable upsets despite faults.
•
We provide experimental data from a prototype FPGA implementation of the algorithm.

Introduction & related work
In the past, computers have essentially been viewed as monolithic, synchronous, fault-free systems.
If at all, fault-tolerance has been introduced (i) to deal with limited, specific failures (e.g. errors in communication or data read from storage, which are usually handled via error-correcting codes), and (ii) at the level of distributed systems comprised of multiple machines that are fault-prone or subject to attacks (e.g. data centers or peer-to-peer applications, which use some form of replication).
Except for critical systems and extreme operational conditions (e.g. medical or aerospace applications [1]), there has been little motivation to build systems that are robust on all levels from scratch, a process that involves redesigning-or even reinventing-the very basics of how computations are organized and performed.
Due to the tremendous advances of Very Large Scale Integration (VLSI) technology, this situation has changed.
Enabled by ever decreasing feature sizes and supply voltages, modern circuits nowadays accommodate billions of transistors running at GHz speeds [2].
As a consequence, the assumption of chip-global (not to speak of system-global) synchrony [3] and no (or restricted) faults gradually became outdated [4].
Improved process technology and architectural-level fault-tolerance measures are common nowadays, and the lack of global synchrony has been tackled by accepting a certain level of asynchrony between different parts of the system.
In the most extreme form of this approach, computations are completely unsynchronized at all levels [5], which requires to synchronize all dependent activities (like sending and receiving of data) explicitly via handshaking.
In contrast, Globally Asynchronous Locally Synchronous (GALS) systems [6] make use of local clock sources to drive synchronous computations within each clock domain.
Note that, in the wider sense, most multiprocessors fall into this category, as there is usually no single common clock that drives all processors.
GALS systems again can be divided into two general classes: One that operates asynchronously at the inter-domain level, and the other consisting of multi-synchronous systems [7,8] that provide some, albeit reduced, degree of synchronization among clock domains.
The former class suffers from the drawback that, for inter-domain communication, either strong synchronizers or stoppable clocks must be foreseen [9].
After all, every bit of the sender's data must have stabilized at the receiver before the clock edge used for reading the data occurs.
This is avoided in multi-synchronous systems, where high-speed inter-domain communication via FIFO buffers can be implemented due to the available global synchronization [10].
Since the latter abstraction is also very useful for other purposes, multi-synchronous GALS is preferable from the viewpoint of a system-level designer.
Naturally, establishing inter-domain synchronization comes at additional costs.
While it is not too difficult to achieve and maintain in the absence of faults [11,12], the issue becomes highly challenging once faults of clocking system components enter the picture.
Contribution
We present an FPGA prototype implementation of a distributed clock generation scheme for SoC that self-stabilizes in the presence of up to f<n/3 faulty nodes.
It incorporates the pulse algorithm from [13] that tolerates arbitrary clock drifts and allows for deterministic recovery and (re)joining in constant time if n-f nodes are synchronized; it stabilizes within time O(n) with probability 1-2-n from any arbitrary state.
An additional algorithmic layer that interacts weakly with the former provides bounded high-frequency clocks atop of it.
Nodes executing the compound algorithm broadcast a mere constant number of bits in constant time.
The formal proofs of the properties of the pulse synchronization algorithm and the derived high-frequency clocks are given in [13].
Deriving an implementation from the specification of the algorithm in [13] proved to be challenging, as the high-level theoretical model and formulation of the algorithm in [13] abstracts away many details.
Firstly, it assumes a number of basic self-stabilizing modules above the level of gates and wires to be given.
We devise and discuss self-stabilizing implementations of these building blocks meeting the specifications required by the high-level algorithm.
Secondly, the algorithm's description is in terms of state machines performing transitions that are non-trivial in the sense that they do not consist of switching a single binary signal or memory bit.
This requires careful consideration of metastability issues, since these state transitions are triggered by information from different clock domains.
In order to resolve this issue, we introduce a generic Hybrid State Transition Machine (HSTM) that asynchronously starts a local synchronous execution of a state transition satisfying the model specification from [13].
Related to this matter, we thirdly discuss in detail how the algorithm and its implementation make a best effort to guard against metastable upsets.
Here, we try to get the best out of the design decisions and rely on synchronizers only where absolutely necessary.
These non-trivial implementation issues and the complex interactions between the basic building blocks raise the question under which circumstances the high-level properties of the algorithm shown in [13] indeed hold for the presented implementation.
To answer this question, we devised a model that is able to capture the behavior of the constructed modules, including faults, resilience to faults, and self-stabilization, in a hierarchical fashion.
By specifying the desired behavior of modules in terms of the feasible output generated in response to their inputs, we can also reason about the behavior of (implementations of) modules in a hierarchical manner.
This property is crucial, as it permits to determine conditions under which our implementation indeed satisfies the requirements by the abstract model used in [13], and then soundly conclude that if these conditions are met, all statements made in [13] apply to our implementation.
Since our approach is highly generic and permits to adjust the granularity of the description in order to focus on specific aspects of the system, we believe it to be of general and independent interest in the context of devising fault-tolerant systems.
In order to verify the predictions from theory,33
Or, to be scientifically accurate, we rather successfully failed at falsifying them.
Our implementation primarily serves as a proof of concept, as clearly an FPGA implementation can merely hint at the properties of an ASIC.
 we carried out several experiments incorporating drifting clocks, varying delays, and both transient and permanent faults.
This necessitated the development of a testbed that can be efficiently controlled and set up for executing a large number of test runs quickly.
In our 8-node prototype implementation, the compound algorithm generates 8-bit clocks that in all runs stabilized within 1.9⋅106d time (where d is the maximal end-to-end communication delay).
In our testbed, which runs at roughly 100 kHz, this amounts to less than 12 s.
For a system running at GHz speed, this translates to about a millisecond.
We also observed that the deterministic stabilization mechanism designed for more benign conditions operates as expected, recovering nodes by about two orders of magnitude faster.
Organization of the article
In the next section, we summarize the obstacles and design goals that need to be considered for clock synchronization in our setting; we also introduce the basic building blocks assumed in [13], which perform typical operations used by fault-tolerant synchronization algorithms.
Section 3 introduces the formal model, alongside illustrating examples and proofs of some basic properties.
Subsequently, in Section 4 we cast the modules informally discussed earlier in our formal framework, and interpret nodes, protocols, and the synchronization problem as modules as well.
In Section 5, we move on to the description of the algorithm from [13] in terms of this framework.
We provide high-level intution on the purpose of its various components and summarize the main statements proved in [13].
Section 6 follows up with presenting our implementations of the basic modules specified in Section 2.2, including the HSTM.
In this context, we will also cover our efforts to minimize the probability for metastable upsets.
In Section 7 we describe the testbed setup, the experiments, and their results.
Finally, in Section 8 we evaluate to what extent our design goals are met and give an outlook on future work.
On-chip clock synchronization
Our goal is to design a scalable hardware clock generation scheme that is resilient to arbitrary transient and permanent faults and carefully minimizes the risk of metastability.
We will now discuss our objectives in more detail and explain why tackling them in conjunction proves to be much harder than achieving them individually.
In accordance with standard notions, in the following we will refer to clock domains as nodes, as they represent the smallest "independent" algorithmic building block we use.
This is to be understood in the sense that we consider a node faulty if any one of its components is faulty, and non-faulty otherwise (irrespectively of whether other nodes behave correctly or not).
Denoting by [i..j] the set {k∈N|i⩽k⩽j}, ultimately, each correct node i∈[1..n] must at all times t output a (discrete) logical clock Li(t)∈N that fulfills certain properties despite the aforementioned obstacles; most obviously, we strive for minimizing maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)}.
Challenges
Inexact local clocks and unknown message delays
When synchronizing clocks, one needs to face that clocks are not perfect and that it cannot be exactly determined how much time it takes to communicate a clock reading.
These fundamental uncertainties entail that synchronization can never be perfectly accurate and must be an ongoing process [14].
We formalize these notions as follows.
Each node i∈[1..n] can make use of local clocks that are inexact and therefore drift (i.e., do not progress at the same rate).
Since we are only concerned with synchronizing clock domains with each other, we do not care about Newtonian time.
Instead, we describe the system in terms of a reference time satisfying that any correctly operating clock progresses at a speed between 1 and some constant ϑ with respect to the reference time t∈R.
A (local) clock C:R→R that is correct during a period of reference time [t-,t+]⊆R guarantees that ∀t, t′∈[t-,t+], t<t′: t′-t⩽C(t′)-C(t)⩽ϑ(t′-t) (in particular, C is continuous and strictly increasing during [t-,t+]).44
We use real-valued, unbounded clocks here to simplify the presentation.
It will later become clear that the algorithm can indeed operate with discrete bounded clocks, as it does not need to access absolute clock values, but rather approximately measures bounded differences in time.
 In contrast to many "traditional" synchronization settings, we would like to tolerate quite large relative clock drifts ϑ-1 of up to about 20%, as accurate and stable oscillators are not available in a System-on-Chip (SoC) at low costs.
Tolerating such large drifts permits to utilize very simple ring oscillators even under heavily varying conditions (temperature, supply voltage, etc.) [15].
Node i communicates with node j via an abstract FIFO channel that (if correct) continuously makes i's state available to j, albeit delayed by an unknown value between 0 and the maximal delay d.
We denote the input port of the channel from node i to node j by Si and its output port by Sj,i.
Node i also loops back its own state to itself on a channel.
The time required for computations that are triggered by some communicated information is accounted for by d as well, i.e., d is an end-to-end delay.55
This is the reason why we speak of an abstract channel.
We will later introduce the (physical) channels that essentially represent the wires on the chip; the maximal delay d is then the sum of the maximal delay of the physical channels and the computing elements of the nodes.
 For the sake of a straightforward presentation, throughout this article we assume that all channels from node i to some node j are part of node i, i.e., faults of the channel are mapped to the sender node.
We remark, however, that a more detailed treatment (as e.g.
in [16]) can be beneficial and is supported by the modeling framework underlying this work.
Transient faults
Increasing soft error rates of modern VLSI circuits [17], originating in ionizing radiation [18-21], cross-talk, and ground bouncing [22,23], make it vital to allow for recovery from transient faults.
The most extreme transient fault scenario is that the entire system undergoes a period of an unbounded number of arbitrary faults.66
The only restriction is that transient faults do not affect the non-volatile memory (and in particular not the algorithm itself), as this would induce a permanent fault.
 Algorithms that are capable of re-establishing regular operation after transient faults cease are called self-stabilizing [24].
This requirement is equivalent to stating that, if the system is fault-free, the algorithm converges to a valid state from an arbitrary initial configuration within a bounded time; we refer to this period as stabilization time.
Due to this equivalency, self-stabilizing algorithms have the additional advantage of requiring no initialization, i.e., a self-stabilizing clocking system does not need to be booted with any initial synchrony.
For self-stabilizing algorithms, stabilization time is obviously an important quality measure.
As the fundamental time unit of the system is d, i.e., the time span it takes to effectively communicate and process any piece of information with certainty, guarantees on the stabilization time are clearly always some multiple of d; the respective prefactor typically is a function of the number of nodes n, the number of sustainable or actual permanent faults, and the clock drift ϑ.
In our context, the stabilization time is not only of relevance to whether waiting for stabilization is bearable in terms of the down-time of the system; it is important to understand that a failure of the synchronization layer will quickly result in incoherencies of operations on higher layers, entailing the threat of data loss or corruption, potentially without any possibility of future recovery.
Because of the need of maintaining accurate synchronization in the presence of drifting clocks, quite a few clock synchronization algorithms are self-stabilizing.
In fact, conventional clock trees [3] are trivially self-stabilizing-after all, they simply disseminate the signal of a single oscillator throughout a chip.
However, they cannot cope with any permanent fault of the clock source or the network distributing the clock.
Similarly, one could easily make a system comprising several clock sources self-stabilizing, by picking one master clock and letting all other clocks synchronize to it.
Again, this simplistic approach will fail if the master or its outgoing communication channels become faulty.
Permanent faults
Sustaining functionality in the presence of permanent faults necessitates redundancy.
More precisely, it is known that tolerating f worst-case faults (traditionally called Byzantine faults in this context) is impossible if n⩽3f (without cryptographic assumptions) [14,25].77
Allowing cryptography would still necessitate n>2f [26,27]; we hence discard this option due to the additional complexity incurred.
 Hence, natural questions are whether assuming worst-case failures is too demanding and whether the fault model could be relaxed in order to circumvent the lower bound.
Unfortunately, examining the lower bound reveals that it originates in the ability of a faulty node to communicate conflicting information to different receivers.
This behavior can easily emerge from a faulty output stage in a circuit: If an analog voltage level in between the range for a valid "1" and that for a valid "0" is evaluated (for example due to a timing fault, a glitch on a signal line, or a defective driver output) by more than one receiver, some might read a "1" while others read a "0".
Note that this is a fundamental problem, as mapping the continuous range of possible voltages to discrete binary values entails that there is always a critical threshold close to which it is impossible to ensure that all receivers observe the same binary value.
It is still an option to argue about the spatial distribution of (permanent) faults within the system, though, as we discuss in Section 8.
However, in this article, we consider the worst case, which also motivates the choice of full connectivity88
We are aware that this constitutes a serious scalability issue; again we refer to the discussion in Section 8.
 between the nodes due to a respective impossibility result [26,27].
This lower bound entails that, due to their low connectivity, most existing distributed clock generation schemes [11,12,28,29] cannot cope with a reasonable number of worst-case faults.
Nonetheless, dealing with up to f faults in a fully connected system of n⩾3f+1 nodes is-at least from a high-level perspective-still fairly easy, provided that we can rely on synchronization already being established.
To illustrate this, consider the simple state machine of a node given in Fig. 1.
In the figure, the node's states are depicted in circles and the feasible state transitions are indicated by arrows.
A node switches, for example, from state ready to state propose if the condition next to the arrow is satisfied.
In this example, this means that either 3ϑ2d time has passed on its local clock since it switched to state ready or its incoming channels (including its loop-back channel) showed at least f+1 other nodes in state propose since it switched to state ready.
This behavior is realized by each node i∈[1..n] having (binary) memory flags proposei,j for each node j∈[1..n]: Node i's flag proposei,j is set to 1 at a time t iff Si,j(t)=propose and the flag was in state 0 before.
The flag is reset to 0 on node i's state transition to ready (in the figure indicated by the rectangular box on the respective arrow).
Deciding whether the transition condition is satisfied at time t thus boils down to checking whether the timeout condition is satisfied or at least f+1 of the propose memory flags are in state 1.
Now assume that each node runs a copy of this state machine, and at least n-f non-faulty nodes enter state increase during some time window [t,t+2d).
As local clocks run at speeds between 1 and ϑ, all nodes will switch to state ready during [t+3d,t+2d+3ϑd).
Hence, at the time when a node switches to ready, the delayed state information on the channels will not show non-faulty nodes in state propose any more.
Therefore, no non-faulty node will switch to propose again due to memorizing f+1 nodes (at least one of which must be non-faulty) in state propose before the first non-faulty node switches to propose.
Thus, the latter must happen because 3ϑ2d local time passed on a local clock, which takes at least until time t+3d+3ϑd>t+2d+3ϑd.
By this time, all nodes will have switched to ready.
This implies that at the time when the first node switches to increase again (which eventually happens because all n-f non-faulty nodes switch to propose), all nodes will already have switched to ready.
Given that n⩾3f+1, we have that n-2f⩾f+1, i.e., if at some non-faulty node n-f channels show state propose, any node will observe f+1 channels in this state (though due to delayed communication maybe not at exactly the same instance in time).
This implies that at most d time after the first node switched to increase again, all non-faulty nodes have switched to propose.
Another d time later, all n-f non-faulty nodes will have become aware of this and have switched to increase, i.e., within a time window of 2d.
Repeating this reasoning inductively and assuming that the nodes increase their logical clocks (that initially are 0) by 1 whenever they switch to increase, well-synchronized logical clocks are obtained: The maximum difference in time between any two correct nodes performing their kth clock tick, the skew, is at most 2d for the above algorithm.
A variation of this simple technique [30] is known for long and a closely related approach called DARTS has been implemented in hardware [31,32].
However, all these algorithms are not self-stabilizing.
In fact, even if clocks would not drift, the delay d was arbitrarily small, and there was only a single faulty node (i.e., even if we allow for f>1, only one node is actually faulty), they still would not stabilize.
To see this for the algorithm given in Fig. 1, first consider the following execution with n=3f+1, part of which is depicted in Fig. 2.
The correct nodes are split evenly, into three subsets Ai, i∈{1,2,3}, of size f.
Set A1 initially is in state ready, with all memory flags corresponding to nodes in A3 in state 1 and all other flags in state 0.
The nodes in A2 and A3 are in state increase, with the timers of nodes in A2 having progressed halfway towards expiring and the timers in A3 just started (i.e., these nodes just left propose), and their propose signals are memorized by nodes in A1.
Just when the nodes in A2 are about to switch to ready, the faulty node sends propose signals to the nodes in A1, causing them to switch to propose.
They will send propose signals, once receiving them memorize 2f+1=n-f nodes in state propose, and thus proceed to state increase.
However, the nodes in A2 will still observe the propose signals of the nodes in A1 after resetting their memory flags upon switching to ready.
Thus, we end up in the same situation, except that Ai (indices modulo 3) takes the role of Ai-1.
Repetition yields an execution that never stabilizes and has 3 sets of grossly desynchronized nodes that are not faulty.
This execution can be generalized to n=kf+1 for integers k⩾3: we split the correct nodes in k sets of size f and make them proceed equidistantly spread in time through the cycle.
The difference is that now more than one group will linger in states ready or propose upon arrival of the next; the crucial point is that the single faulty node retains control over when groups proceed to state increase.
The cases n=kf+2 and n=kf+3 require more involved constructions; it should be intuitive, though, that with 2 actually failing nodes the above construction can be modified to operate with one or two of the sets containing f+1 nodes.
Combining transient and permanent faults
Combining self-stabilization and resilience to permanent faults results in much more robust systems.
Both properties synergize in that, as long as at all times there is some sufficiently large set (not necessarily the same!) of nodes that is non-faulty, an arbitrary number of transient faults is transparently masked, i.e., the system remains operational even though over time each individual component may repeatedly undergo transient failures and recover from them.
This drastically increases the mean time until overall system failure: In a system that is not resilient to permanent faults, any fault will result in an immediate breakdown of guaranteed properties, whereas a system that is not self-stabilizing will fail (and might not recover without an external reboot) once the sum of faults exceeds one third of the nodes.99
One could compromise by guaranteeing that nodes recover in bounded time, provided that the number of faults is never overwhelming.
In fact, the algorithm presented in this article has the property that in this case nodes will recover faster and deterministically (in contrast to the slower, probabilistic stabilization from arbitrary system states).
However, sacrificing stabilization from arbitrary states will not reduce the complexity of the algorithm significantly, and theory strongly indicates that the respective gain is limited to a constant factor in general.
There is a considerable body of work on distributed synchronization algorithms that are self-stabilizing as well as resilient to permanent faults.
However, until recently, there has been no solution worth considering for hardware implementation.
Known algorithms exhibit a prohibitively large communication complexity (i.e., nodes send Ω(n) bits over each channel in constant time) [33,34], incur an exponential stabilization time [35], require exponentially small clock drifts [36], or require much stronger assumptions on the system's behavior [37].
Recently, we proposed an approach that does not suffer from such drawbacks [13,38,39], whose implementation is the subject of this work.
Metastability
In our specific setting, minimizing the potential for metastability is particularly demanding.
Metastability results from violating a stateful circuit's input timing constraints, e.g., by changing the data input of a flip-flop at the time of the clock transition.
While this can be safely avoided during normal operation, a faulty node might exhibit arbitrary timing and hence cause such a violation.
As this can never be prevented in the first place if worst-case faults are considered, it is mandatory to guard the channels against propagating metastability, e.g.
by using synchronizers.
In order to minimize the required length of synchronizer chains, decreasing latency and area consumption (the latter also on higher layers of the system), however, it is beneficial to avoid the potential for upsets by construction wherever possible.
Apart from the (unavoidable) threat originating from faulty nodes, safely preventing timing violations is hindered by the lack of a common time base during the stabilization phase after an excessive number of transient faults.
It has been shown that it is impossible to guarantee with certainty that no metastable upsets occur if the system is in an arbitrary initial state, even if all nodes adhere to the protocol [40].
Careful design is thus required in order to minimize the probability of upsets during stabilization, in particular since such upsets might obstruct the stabilization process.
Once the system stabilized, i.e., the non-faulty nodes are synchronized, the algorithm can use this synchronization to structure communication in a way that entirely avoids metastable upsets caused by non-faulty nodes.
Thus, in the absence of faults, we require that the system operates metastability-free.
Note that even this seemingly simple task is not trivial, as one cannot employ the classical wait-for-all paradigm: Doing so would imply that just a single non-responsive node would cause the entire system to deadlock.
Therefore, when depending on other nodes in the decision to take a state transition, it is necessary to wait for at most n-f signals.
Safely reading signals thus cannot rely on handshaking, but must be based on suitable monotonicity and/or timing conditions (guaranteed by the use of memory flags and local clocks, for example).
The bounded-delay "interlocking condition" used in DARTS [32] and the simple algorithm in Fig. 1 are showcases for such techniques.
Operating frequency vs. clock precision
In order to be practical, the logical clocks need to run at a frequency in the GHz range.
While one could obviously utilize frequency multiplication to achieve this goal, this is not straightforward to build in the self-stabilizing context.
After all, clock multipliers involve complex devices like phase-locked loops and are hence not obviously self-stabilizing.
Moreover, for a fixed guaranteed skew (of say 2d), naive frequency amplification also increases the logical clock imprecision maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)} by the scaling factor, which may adversely affect certain services.
For example, the size of the FIFO buffers used for inter-domain communication in [10] depends on the clock imprecision and must hence be adapted accordingly.
On the other hand, by dividing frequencies, it is clearly possible to guarantee that maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)}=1.
Therefore, it is an important design goal to minimize clock imprecision while at the same time maximizing the frequency at which clocks run.
Naturally, this becomes much more involved due to the design goals already presented.
Scalability
Being able to meet all the above design goals is meaningless if one cannot control the amount of resources devoted to the task of clock generation.
Pivotal issues are the following:•
Area consumption: The chip area used by the components of the synchronization algorithm decomposes into the area consumed by the nodes and their interconnections.
The former can be captured by the gate complexity, i.e., the number of (constant fan-in) gates required to perform the algorithm's computations.
The latter significantly depends on the chip layout, which is highly application-dependent and hence outside our scope of control.
It is clear, however, that the number of channels and their bandwidth play a crucial role.
•
Communication complexity: Apart from whether two nodes are connected or not, it is of interest how many wires are required.
This is well-represented by the bit complexity of an algorithm, i.e., the number of bits it exchanges per time unit between communication partners.
Note that while the number of wires can be reduced by means of time division, this will require additional memory and computational resources on the receiver's side and increase the communication delay.
In any case, it is highly desirable to devise algorithms of (small) constant bit complexity.
Moreover, broadcasting the same information to all nodes instead of different information to different receivers is to be preferred, as it allows us to use communication buses.
•
Stabilization time: For reasons stated earlier, we would like to minimize the stabilization time.
In particular, it is not good enough to know that an algorithm eventually stabilizes, as the required time might be well above what makes the algorithm self-stabilizing in any practical sense.
•
Resilience: The number f of faults that can be concurrently sustained without losing synchronization or the capability to stabilize should grow with system size, as otherwise a larger system will suffer from more frequent outages.
Note that while we must accept that stabilization is a random process (due to the unavoidable probability of metastable upsets), we demand that a system that is stable will always remain so as long as there are not too many faults (including upsets).
As mentioned earlier, a lower bound shows that always f<n/3, giving a precise meaning to "too many" here.
•
Delays: As the maximal delay d accounts both for the delay incurred by communication as well as computation, it is vital to minimize both.
Notwithstanding the fact that the communication delay and computing speed is mostly determined by parameters outside our control (technology, spatial distances, number of nodes, etc.), minimizing the gate complexity and, in particular, the depth of the circuits implementing the nodes' algorithms (that determine the computing delays) is important.
•
Metastability: In larger and faster systems, the number of events per time unit that could cause metastable upsets is obviously larger.
Therefore, it is vital to safely exclude metastability from occurring during regular operation by construction.1010
Note that in this regard our approach is superior to standard GALS systems using synchronizers, where the risk of metastability is immanent (at every clock transition) also in normal operation.
 We admit metastability only during rare exceptional phases of system operation where it cannot be avoided in principle, like during stabilization or in case of faults.
As the probabilities for metastable upsets are hard to quantify even in a final product, we do not use a "hard" measure here.1111
It is worth mentioning, though, that the asymptotic increase in the number of events per time unit that could cause metastable upsets is clearly at least polynomial in n.
As synchronizer chains decrease the probability of upsets exponentially, the required length of synchronizer chains will asymptotically grow as Ω(logn), increasing the system cost and (effectively) decreasing its operational frequency.
•
Connectivity: In order to facilitate efficient placement and routing on a chip, it is vital to ensure that the communication network is sparse.
Also, a sparse network will consume less area and is beneficial to fault containment.1212
If a single event such as e.g.
an ionizing particle hit can render multiple nodes faulty, even tolerating a large number of faulty nodes is of little use w.r.t.
the overall resilience of the system.
 Tackling this issue is subject to our future work and hence beyond the scope of this article, however.
•
Clock size: If the logical clocks have too few bits, i.e., overflow too frequently, they might be unsuitable for the application logic of the SoC.
The algorithm we present in this article can in principle provide clocks of arbitrary bounded size.
However, its stabilization time would grow linearly with the maximum clock value once we scale above 8-bit clocks.
In a recent publication, we show how to construct larger clocks efficiently [41].
Typical modules for clock synchronization protocols
We next introduce the basic modules that are assumed by the model used in [13].
We first give an intuitive description of the required modules.
Subsequently, we introduce a novel formal framework for specifying self-stabilizing fault-tolerant modules and specify our basic modules in this framework.
Any implementation satisfying this specification can be plugged into the high-level algorithm in order to yield a system guaranteeing the properties proved in [13].
We now list the building blocks beyond standard logic gates that will explicitly or implicitly be used by the algorithm presented in Section 5.
Each of these building blocks computes output signals that are constrained by (the history of) its input signals.
If the logic function implies an output transition in reaction to an input change, this transition is not required to occur immediately; it must occur within a known time bound, however.
Given the time bounds for the individual modules and the connecting wires, one can compute the maximum delay d.
Moreover, informally speaking, it must be avoided that a single change in the input(s) causes multiple transitions of the output signal, as this could undermine the high-level algorithm's logic.
Note also that statefulness, i.e., any sort of memory (including positive feedback loops), bears the potential for metastable upsets and requires careful attention in order to ensure self-stabilization.1313
In other words, the module must recover from arbitrary corruptions of its memory.
 Purely combinational elements, on the other hand, differ in their ability to prevent metastable inputs from reaching the output under certain conditions.
Each node will be a union of state machines that communicate via channels (both among each other and with remote nodes) and are composed of standard logic gates and all other modules we describe below.
Fig. 5 depicts such a state machine.•
Communication channels.
We previously introduced the communication channels from node i to node j as abstract devices that convey the states with a delay of at most d that also accounts for computations.
Viewed as a module, the (physical) communication channels do account for the time to communicate the state information only, whereas computations are performed by standard logic gates and the modules we will describe next.
A communication channel of this type simply maps its input signal to its output signal.
The reason why communication channels are nonetheless listed as modules here is that encoding a non-binary state signal in a glitch- and metastability-free manner is a non-trivial task, as in the absence of (reliable) synchrony both parallel and sequential communication present challenges.
In our abstraction, this encoding is performed by the channel, which requires additional logic and thus potentially results in delays beyond the mere wire delays as well as the necessity to consider issues concerning metastability and self-stabilization.
•
Memory flags.
These are just simple binary storage elements that can be set to 1 by means of one input signal and can be reset to 0 by a second input signal; their state is externally accessible via an output signal.
Simply put, a memory flag just "remembers" which input signal was 1 most recently.
In our algorithms, memory flags will be used to memorize wether an input signal from a remote node was in state 1 at some time after the most recent reset upon a state transition of one of the node's state machines (in Fig. 1, e.g., a node resets its propose flags when switching from increase to ready).
•
Threshold gates.
Frequently, nodes will need to decide whether a certain threshold number (f+1 or n-f) of signals (or sets of signals) satisfy some Boolean predicate (e.g., the conditions for switching to propose and increase in Fig. 1 involve such a threshold).
A threshold gate takes the respective binary input signals and outputs 1 if the threshold is reached and 0 otherwise.
•
•
Randomized watchdog timers.
A randomized watchdog timer (D,s,C) is a module with input port Si,i and output port TimeD,s,C, where D is a bounded random distribution on (0,D]⊂R+, s is a state, and C a clock.
The module specification of (D,s,C) is analogous to the module specification of a watchdog timer, except that property (Expire) is replaced by:-
(Expire') Denote by R⊂R the set of times when (T,s,C) is reset.
For each time tR∈R, denote by tE(tR) the unique time satisfying that C(tE)-C(tR)=T(tR), where T(tR) is drawn (independently) from D.
Then, for each t∈R, TimeT,s,C(t)=0 iff t∈⋃tR∈R[tR,tE(tR)).
 We apply the same notational conventions as for watchdog timers.
•
State transition modules.
Node i's state transition module has input ports Si,j for each node j∈[1..n] as well as one binary input port for each of the memory flags, (randomized) watchdog timers and threshold gates it uses.
Furthermore it has an output port Si as well as one binary Reset output port for each of the memory flags it uses.
A node's state transition module executes a state machine specified by (i) a finite set S of states, (ii) a function tr, called the transition function, from T⊆S2 to the set of Boolean predicates on the alphabet consisting of expressions of the form "p=s" (used for expressing guards), where p is from the state transition module's input ports and s is from the set of possible states of signal p, and (iii) a function re, called the reset function, from T to the power set of the node's memory flags.
Intuitively, the transition function specifies the conditions (guards) under which a node switches states, and the reset function determines which memory flags to reset upon the state change.
Formally, let P be a predicate on the input ports of node i's state transition module.
We define P holds at time t by structural induction: If P is equal to p=s, where p is an input port of node i's state transition module and s is one of the states signal p can obtain, then P holds at time t iff p(t)=s.
Otherwise, if P is of the form ¬P1, P1∧P2, or P1∨P2, we define P holds at time t in the straightforward manner.
For a given transition delay dTrans>0, the module specification ΦSTM of node i's state transition module is defined as follows.
Let Ein be an execution of the state transition module's input ports and Eout an execution of its output ports.
Then Eout∈ΦSTM(Ein) iff there is some ε>0 and a signal locked such that the following requirements are met.
(The intuition is that locked(t)=0 means that the node is ready to perform the next state transition once a guard becomes true, whereas in case of locked(t)=1 the node is currently executing a previously "locked" transition.)-
(Safety) The node (i.e., Si) does not switch states at any time t with locked(t)=0.
In every maximal interval [tl,tu)⊆R satisfying that locked≡1 on [tl,tu), it switches states exactly once.
-
(Delay) For each interval [tl,tu) as above, tu-tl⩽dTrans-ε.
-
(Guard) For each interval [tl,tu) as above, (Si(tl),Si(tu))∈T and tr(Si(t),Si(tu)) is satisfied at some time t∈[tl-ε,tl].
-
(Responsiveness) If locked(t)=0 and there is a state s∈S such that (Si(t),s)∈T and tr(Si(t),s) holds at time t, then locked(t+ε)=1.
-
(Flags) For an arbitrary interval [tl,tu) as above, suppose that the node switches from state Si(tl) to state Si(tu) at time ts∈[tl,tu).
Then for each memory flag specified by re(Si(tl),Si(tu)), the corresponding reset output port of the state transition module is in state 1 at some time in (tl,ts] (and therefore the flag is reset).
Outside these time intervals, reset ports are in state 0.
 A node may run multiple, say k∈N, state machines in parallel (i.e., contain several state machines as submodules).
In this case, its state signal is the joint signal Si=(Si1,…,Sik), where Sil, l∈[1..k], denotes the lth state machine of the node.
Throughout this article, the different state machines of each node i have disjoint state spaces.
For simplicity, we hence may say "node i is in state s at time t" instead of "state machine l of node i is in state s at time t" when referring to Sil(t)=s, etc.
To account for the latency of the memory flags, threshold gates and (randomized) watchdog timers, their ports are not directly connected to the state transition module's ports, but via binary communication channels with respective delays.
The resulting structure of the compound module node i is depicted in Fig. 5.
Note that additional communication channels at the threshold gates' and memory flags' input ports allow to model the fact that memory flags are not necessarily reset at the same time, and signals may arrive shifted in time at the threshold gates.
As mentioned earlier, for simplicity we consider the outgoing channels to remote nodes as part of the node.
Hence, the output ports of node i comprise the output ports Sj,i, j∈[1..n], of the channels disseminating its state Si.
In addition, in order to solve the actual problem of clock generation, we include the locally computed discrete clock value Li as an output port.
Protocols and problem formulation
We next formalize the concept of a protocol, like the one presented in Section 5, followed by what it means for a protocol to solve self-stabilizing clock synchronization in spite of f faults.
Formally, a protocol (for an n-node system) is a compound module consisting of n modules referred to as nodes.
The nodes are to be specified as modules themselves, and in our case will follow the layout we just described.
It thus remains to state in Section 5 which (randomized) watchdog timers, memory flags and threshold gates our protocol uses as well as the state transition modules' transition and reset functions.
A clock synchronization module with n∈N nodes, clock imprecision Σ, amortized frequency bounds A-,A+∈R+, slacks τ-,τ+∈R0+, maximum frequency F+, and at most f∈N faults is a module without input ports and with output ports Li, i∈[1..n].
Its module specification is extendable.
An execution of the module on R is feasible, iff there exists a subset C of [1..n] of size at least n-f satisfying that•
∀t∈R, i,j∈C: |Li(t)-Lj(t)|⩽Σ,
•
∀t, t′∈R, t<t′, i∈C: A-(t′-t)-τ-⩽Li(t′)-Li(t)⩽A+(t′-t)+τ+, and
•
∀t, t′∈R,t<t′, i∈C: Li(t′)-Li(t)⩽⌈F+(t′-t)⌉.
We say a protocol Π (for an n-node system) with no input ports and output ports Li, i∈[1..n], solves self-stabilizing clock synchronization with clock imprecision Σ, amortized frequency bounds A-,A+, slacks τ-,τ+∈R0+, maximum frequency F+, at most f faults, and stabilization time T (with probability p) iff it is an f-tolerant, (with probability at least p) T-stabilizing implementation of the clock synchronization module with the respective parameters.
A (real-world) implementation will output bounded clocks of size K∈N only.
In this case the output ports do not yield Li(t), but only Li(t)modK.
Nevertheless, we introduced the signals Li as abstract functions in this setting, as they allow to state the frequency bounds concisely.
Note that there is no physical counterpart of these values in the real-world system; to be strictly accurate, it would be necessary to qualify the above definitions further by "with bounded clocks of size K" in order to distinguish this version of the problem from the abstract one with unbounded clocks.
Practical implementability issues
Our formal model incorporates a precise semantics of what it means for a module to implement some other module, namely, inclusion of all feasible (sub-)executions.
Unfortunately, however, this strong requirement must often be relaxed when it comes to real implementations.
This is primarily a consequence of the fact that there is no physical implementation of a circuit that can avoid metastability.
Since preventing certain inputs to a module requires output guarantees from others, this is a challenging problem to systems that are self-stabilizing or tolerate persistent faults; combining these properties complicates this issue further.
More specifically, in order to faithfully implement their specifications, basic modules must be able to (i) deal with all possible inputs and (ii) recover reliably from transient faults.
Unfortunately, (i) is often impossible to achieve with real circuits.
For example, simultaneous input changes may drive any implementation of a Muller C-gate into a metastable state, which implies that its output ports do not even carry signals according to our definition, and are hence not feasible.
Of course, metastability can also be caused by physical faults affecting the module; such faults can obviously not be analyzed within our model either.
This possibility obviously invalidates any guarantees that compound implementations containing this instance may provide, unless they can mask the error due to fault-tolerance.
Moreover, real circuits cannot guarantee (ii) under all circumstances either, as it is impossible to always prohibit the propagation of metastable inputs to the outputs and the system may contain feedback-loops.
In principle, it would be possible to extend the presented model to cover also generation and propagation of metastability explicitly, by replacing the finite alphabet S and discrete events with a continuous range of signal values (the voltages) [43].
Since this would dramatically increase the complexity of any analysis, we choose a different approach that also allows us to handle other implementation intricacies in a pragmatic way.
In fact, even in the absence of metastability, it is not necessarily simple and even possible for real implementations to guarantee (ii) under all circumstances.
Apart from the fact that transient faults may lead to permanent errors by damaging physical components,1616
We remark that, technically speaking, excessively high voltages on the input wires could also be interpreted as an "input violation", as this violates the definition of our signals.
However, it makes sense to interpret such (hopefully exceptional) events as a fault of the module.
 our model does not prohibit that temporarily infeasible inputs result in permanent infeasibility, i.e., even when inputs become benign again at a later state of the execution of the module in question, there is no suffix of the execution that is feasible.
The oscillator implementation given in Example 3.6 demonstrates this issue, and further modules exhibiting persistently faulty behavior after temporary violations of input constraints are easily conceived.
As we aim for self-stabilization, it is clear that we cannot allow implementations that suffer from such drawbacks: Neither transient faults nor their consequences, i.e., temporarily arbitrary executions, may result in permanent faults.
Clearly, both recovery from transient failures and resilience of a basic module to erroneous inputs, and hence the whole definition of what actually constitutes a transient fault in our model, is implicitly defined by the physical realization of an implementation.
These observations have important consequences.
On the one hand, careful design of the basic modules is of paramount importance.
For instance, in a final product, a watchdog timer must not have its duration stored in a memory register that can be corrupted by a temporary charge injection (e.g. due to a particle hit), a ring oscillator should not be able to run unchecked at e.g.
twice its frequency indefinitely (e.g. triggered by a voltage pulse), and one has to make sure that stateful components like memory flags or state transition modules eventually "forget" about potentially erroneous inputs in the past, and eventually behave according to their specification again.
As discussed above, however, this cannot usually be perfect: There will always be (rare) scenarios, where an implemented circuit will not work like an ideal one, i.e., violate its specification.
We incorporate this in our model, in a pragmatic well-known from critical system design, by means of the notion of imperfect implementation coverage.
For a given module implementation, the coverage implicitly or explicitly determines the fraction of all possibly executions in which the implementation works as specified.
Since exceptional scenarios like metastability are usually extremely rare, we do not bother with defining the notion of coverage formally here: The coverage should be very close to 100% anyway.
In Section 6, we will argue that each of our basic modules will work as specified, except for very rare situations that may trigger metastability due to a violation of input timing constraints.
Thanks to this approach, algorithms and proofs can rely on sufficiently simple specifications of basic modules, which usually also admit robust and efficient implementations in practice.
Any unhandled scenarios are relegated to imperfect implementation coverage.
This feature is essential for devising proofs of reasonable complexity that show self-stabilization of all compound modules, implying that the system indeed will recover once transient faults of (basic) modules cease.
Due to the hierarchical composition of modules, compound modules fully derive their behavior from their submodules and can therefore be analyzed based on the properties of their submodules, while we may switch at will between viewing a module as given (i.e., basic), analyzing it in more detail as a compound implementation, or (for low-level modules) analyzing it in an even more detailed model.
This way, our approach also inherently supports tight interaction between algorithmic design and design of the basic building blocks used in the algorithms.
The FATAL+ clock synchronization protocol
In this section, we recast the self-stabilizing clock synchronization algorithm introduced in [13] in the modeling framework of the previous section and summarize its most important properties.
Since the main focus of our paper is on the implementation of our algorithm in this model, there is no need to provide a detailed description of the stabilization mechanism, let alone formal proofs of the stated claims; the analysis of the correctness and performance of the algorithm in [13] is based on a simpler abstract system model, assuming a globally valid end-to-end delay bound d covering any (local and remote) communication and processing action, which is fully compatible with our modeling framework.
More specifically, all that is needed in order to reuse the results of the analysis in [13] is to compute the maximum end-to-end delay occurring in the implementation of our algorithm in the modeling framework introduced in Section 2.2.
Recall from Section 2.2 that our top-level clock synchronization module is implemented as a compound module consisting of n nodes and their connecting top-level channels (with maximum delay dChan).
Every node, in turn, is a compound module made up of a state transition module, watchdog timers, memory flags, and threshold modules interconnected by channels (modeling various delays) as shown in Fig. 5.
Finally, a state transition module represents several communicating concurrent asynchronous state machines (with maximum transition time dTrans).
It ensures that state transitions of every constituent state machine occur in an orderly fashion, i.e., that every transition happens exactly once and, if need be, memory flags are consistently reset.
The state of each state machine is encoded in a few bits and conveyed via the top-level channels to all other modules in the system that need to receive it on some input port.
Given this simple internal structure, computing the resulting end-to-end delay bound d (or, for the quick cycle, dmin+ and dmax+, see below) from the constituent delay bounds is straightforward, see Section 6 for details.
State machine representation
Obviously, the entire logic of our algorithm is encoded in the state machines of a node.
In [13], we use a graphical representation that also reveals the layered structure imposed by their communication.
We already employed this description in Fig. 1.
With the definitions from the previous section at hand, we can now give our graphical representation a precise formal meaning that will allow us to translate the results from [13] to our modeling framework.
Our graphical representation defines the set of possible states S of a state machine (in Fig. 1 ready, propose, and increase) and, by means of the arrows between the states, the set of possible state transitions T⊆S2 (here ready to propose, propose to increase, and increase to ready).
If, for a state transition from s to s′, re(s,s′)≠∅, i.e., there are memory flags that need to be reset, re(s,s′) is given in a rectangular box on the arrow.
Since for each node i and state s we will always reset all memory flags Memi,j,s for j∈{1,…,n} together, we simply write s1,…,sk in such a box to represent the fact that all flags Memi,j,s, j∈{1,…,n}, s∈{s1,…,sk}, are to be reset.
Note that some of these states may be from a different state machine, i.e., the states s1,…,sk need not all be from S.
Completing the description, for each (s,s′)∈T, tr(s,s′) is given by the label next to the respective arrow.
Again, we make use of a condensed notation.
Assume that the state machine in question is part of node i.
We will employ threshold conditions like "⩾f+1 s1", whereby we refer to at least f+1 of i's memory flags Memi,j,s1 being in state 1, or "⩾n-f s1 or s2", which is true if ∑j∈Nmax{Memi,j,s1,Memi,j,s2}⩾n-f, i.e., for at least n-f nodes j flag Memi,j,s1 or flag Memi,j,s2 is in state 1.
An example for such a rule is the transition from propose to increase in Fig. 1.
Such conditions will be translated to a binary signal by feeding the memory flags' signals (or, in the latter case, the output of n OR-gates with inputs Memi,j,s1 and Memi,j,s2) into a threshold gate (of threshold f+1 or n-f, respectively).
Further abbreviations we use for timeouts.
Recall that for a timeout (T,s,C), we omit the clock C from the notation, i.e., write (T,s) instead of (T,s,C).
Timeout (T,s) switches to 1 after T local time units (i.e., between T/ϑ and T+dTrans reference time) has passed since the last switch to state s was triggered.
In case it is part of a transition rule, we write (T,s) for the condition TimeT,s,C=1, and if the transition goes from the state s to which the timeout corresponds to some state s′, we simply write T.
For instance, the condition "3ϑd local time has passed" in Fig. 1 is concisely stated as "3ϑd".
Finally, as for memory flag resets, transition rules may also refer to a state s of another state machine.
In the special case that a predicate solely depends on the current state of another of the node's state machines, we write "in s" or "not in s" to indicate the predicates p=s and ¬(p=s), respectively, where p is the input port connected to the channel communicating the other state machine's state to the state transition module.
Finally, the above rules can be composed by logical AND or OR, which we display by connecting expressions with and or or, respectively.
In Fig. 1, such a composition occurs in tr(ready,propose).
Overview of the algorithm
Each node is a collection of several state machines that are organized in a layered structure.
On each layer, the state machines of the (at least n-f) non-faulty nodes cooperate in order to establish certain synchronization properties of their output signals.
The higher is a state machine in the hierarchy, the stronger are these guarantees; the lower it is, the weaker are the synchronization properties its input signals need to satisfy for stabilization.
The lowest-layer state machine utilizes randomization to recover from any configuration (provided its basic modules are correct (again), i.e., guarantee feasible executions).
Each other layer utilizes auxiliary information from the layer below to stabilize.
Finally, the top level state machine outputs the logical clocks Li.
More specifically, we have the following state machines.•
At the top level, we have the quick cycle state machine (Fig. 6) that outputs Li.
The quick cycle is very similar to the algorithm given in Fig. 1, except that it is coupled to the state machine beneath it in order to ensure eventual stabilization.
Once the system is stabilized, it consistently and deterministically increases Li at a high frequency while guaranteeing small clock imprecision.
•
The main state machine (Fig. 8) is the centerpiece of the stabilization mechanism.
Once stabilized, it generates slow, roughly synchronized "pulses" within certain frequency bounds.
These pulses can be seen as a "heartbeat" of the system; at each pulse, the quick cycle's clocks are reset to 0 and the quick cycle's state machines are forced into state accept+ (corresponding to the increase state in Fig. 1).
This enforces exactly the initial synchrony that we explained to be necessary for the correct operation of the algorithm from Fig. 1.
By itself, however, the main state machine is not capable of recovering from every possible initial configuration of the non-faulty nodes.
In certain cases, it requires some coarse synchrony to be established first in order to stabilize, which is probabilistically provided by the underlying layer.
We remark that, once stabilized, the main state machine operates fully independently of this layer (and thus deterministically).
•
The auxiliary information potentially required for stabilization by the main state machine is provided by a simple intermediate layer we refer to as extension of the main state machine (Fig. 9).
Essentially, it is supposed to be consistently reset by the underlying layer and then communicate information vital for stabilization to the main state machine.
This information depends both on the time of reset and the current states of the n main state machines, which it therefore monitors.
•
Finally, the resynchronization routine (Fig. 10) utilizes randomized timeouts to consistently generate events at all non-faulty nodes that could be understood as "randomized pulses".
Such a pulse is correct for our purposes if all non-faulty nodes generate a respective event in coarse synchrony and no non-faulty node generates another such event within a time window of a certain length.
The crux of the matter is that a single such pulse suffices to achieve stabilization deterministically.
Relying on (pseudo-)randomness on this layer greatly simplifies the task of overcoming the interference by faulty nodes at low costs in both time and communication.
We note that the main state machine masks this randomness once stabilization is achieved, facilitating deterministic behavior of the higher levels and, ultimately, the nodes' clocks Li.
We will now present the individual state machines.
We refrain from a discussion of choosing appropriate durations for the timers, confining ourselves to stating a feasible family of choices later on.
The quick cycle
The quick cycle state machine is depicted in Fig. 6.
It introduces an additional notation: As the states ready+ and accept+ are not distinguished in any of the transition conditions in the other state machines, the same state none+ can be communicated here.
This allows for a very efficient single-bit representation of the communicated states.
In Fig. 6, this is expressed by dividing the circles representing states, putting the state names in the upper part and the communicated states in the lower part.
Apart from saving a wire, this permits to use trivial encoding and decoding of the signal, a simplification of the logic that minimizes delays and therefore maximizes the clock frequency that can be achieved.
Essentially, the quick cycle works as the algorithm given in Fig. 1, where the logical clock is increased whenever the machine switches to state accept+.
However, the quick cycle differs from the algorithm in Fig. 1 in that there is an interface to the main state machine given in Fig. 8.
These state machines communicate by means of two signals only, one for each direction of the communication: (i) The quick cycle state machine of node i generates the nexti signal by which it exerts some limited influence on the time between two successive pulses generated by the main state machine, and (ii) it observes the (T2+,accept) timer.
This timer is coupled to the state accept of Fig. 8, in which the pulse synchronization algorithm generates a new pulse.
The signal's purpose is to enforce a consistent reset of the quick cycle state machine (once the main state machine has stabilized).
The feedback mechanism (i) makes sure that, during regular operation, the reset of the quick cycle does not have any effect on the clocks.
This is guaranteed by triggering pulses (by means of the non-faulty nodes briefly changing the nexti signal to 1 and back to 0 again) exactly at the wrap-around of the logical clock Li, i.e., at the time when Li is "increased" from the maximal clock value K-1=2b-1 (of a b-bit clock) to 0=KmodK.
Similar to Fig. 1, the transition conditions of the quick cycle ensure that the logical clocks never have a clock imprecision of more than one.
To increase the frequency further, each node could increase the number of clock "ticks" generated in each iteration of the quick cycle by means of a high-frequency local clock (essentially, a watchdog timer together with a counter), at the expense of larger clock imprecision (see [13]).
Main state machine
Before we show the complete main state machine, consider its basic cycle depicted in Fig. 7.
Once the main state machines have stabilized, all non-faulty nodes will undergo the states of the basic cycle in rough synchrony.
The states sleep, sleep→waking, and waking serve diagnostic purposes related to the stabilization process.
The duration T2 of the timer (T2,accept) triggering the transition from waking to ready is so large that the node will always be in state waking long before the timer expires.
Thus, we can see that the basic cycle has an underlying structure that is very similar to the quick cycle.
Due to the more complicated logic and conditions on the duration of timers required for the stabilization mechanism, it is however executed at a frequency that is by orders of magnitude smaller than that of the quick cycle.
The difference in the rules for switching to propose and accept, respectively, are also mostly related to the stabilization process.
An exception is the condition "T3 and nexti=1" that can trigger a transition from ready to propose.
Choosing T3 smaller than T4 and taking the signal nexti into account, we permit the quick cycle to adjust the time between pulses (i.e., switches to accept) triggered by the main state machine: Once both state machines are roughly synchronized among all non-faulty nodes, the main state machines will always be in state ready before the logical clocks Li maintained by the quick cycle reach the wrap-around (i.e., become 0 modulo K) and trigger the nexti signals.
Moreover, this happens at all nodes at close times and before any timer (T4,ready) expires at one of the non-faulty nodes.
Hence, by a reasoning similar as for Fig. 1, all non-faulty nodes will switch to propose and subsequently accept in a well-synchronized fashion, caused by the wrap-around of the logical clocks.
An important observation that is proved in [13] is that, once the main state machines stabilized, the nodes execute the basic cycle deterministically and any state transition is certainly completed before one of the conditions for leaving the basic cycle can be satisfied.
Apart from small additional slacks in the timer durations, this is a consequence of the fact that none of the transition conditions of the basic cycle refer to the probabilistic lower layers of the protocol; all evaluated timers and memory flags solely involve states of the basic cycle only, and the nexti signal is provided by the quick cycle.
As we will discuss in Section 6, this property prevents non-faulty nodes from introducing metastability once stabilization is achieved.
We now turn our attention to the full main state machine that is shown in Fig. 8.
Compared to the basic cycle, we have two additional states, resync and join, that can be occupied by non-faulty nodes during the stabilization process only, and an additional reset of memory flags on the transition from sleep→waking to waking.
The various conditions for leaving the basic cycle and switching to recover are consistency checks.
A node will only leave the basic cycle if it is certain that the system is not operating as desired.
As the high-level operation of the algorithm is not the subject of this article, we limit our exposition to briefly discussing the two possible ways to re-enter the basic cycle, corresponding to two different stabilization mechanisms.
The first stabilization mechanism is very simple, and it is much faster than the second one.
Assuming that at least n-f non-faulty nodes are executing the basic cycle (i.e., the main state machines have already stabilized if we consider the remaining nodes faulty), a recovering node just needs to "jump on the train" and start executing the basic cycle as well.
This is realized by the condition for switching from recover to accept.
It is not hard to see that due to this condition, the node will switch to accept in sufficient synchrony with the majority of n-f synchronized, non-faulty nodes within at most two consecutive pulses and subsequently follow the basic cycle as well.
Note that this condition makes direct use of the state signals instead of using memory flags.
This potentially induces metastability at the joining node, but we will explain in Section 6 why the risk is low.1717
Recall that during stabilization we cannot exclude metastability with certainty even in the absence of any further faults.
 On the plus side, this simplifies the algorithm, as the node does not need to implement frequent resets of the respective memory flags to ensure consistent observation of others' states; the sending nodes will just do this implicitly by leaving state accept.
Clearly, the first stabilization mechanism will fail in certain settings.
Most obviously, it cannot "restart" the system if all nodes are in state recover.
Hence it may not surprise that the second stabilization mechanism, which deals with such cases, is much more involved.
Careful attention has to be paid to avoiding the potential for system-wide dead- or live-locks.
In view of our design goals, state-of-the-art deterministic solutions for this problem are not sufficiently efficient.
Hence, the main state machine relies on a probabilistic lower layer that provides certain guarantees with a very large probability.
Extension of the main state machine
The extension of the main state machine, given in Fig. 9, can be seen as a simple control structure for the phases of stabilization.
The intricacy lies in designing the interface such that this control does not interfere with the basic cycle if the system is stable.
Consequently, the influence of the extension of the main state machine is limited to (i) resetting the join and sleep→waking flags upon "initializing" the stabilization process (by switching from dormant to passive) and (ii) providing the signals of the timers (T6,active) and (T7,passive) the main state machine utilizes in the transition rule from recover to join.
Roughly speaking, the main state machines will stabilize deterministically under the condition that their extensions switch at all non-faulty nodes from dormant to passive in rough synchrony and then do not switch back to dormant too quickly, i.e., before the second stabilization mechanism of the main state machine completes its work.
Putting it simply, we require a single, coarsely synchronized pulse, whose generation is the purpose of the lowest layer we present now.
Resynchronization state machine
The resynchronization state machine is specified in Fig. 10.
Strictly speaking, it actually consists of two separate state machines, one of which is however extremely simple.
Every now and then, each node will briefly switch to the init state, seeking to induce the generation of a "pulse" (where the pulse here is locally triggered by switching to resync) that causes a consistent switch of all non-faulty nodes from dormant to passive.
Leaving resync will force the extension state machine back into state dormant.
This is the only interaction with the above layer, which is sufficient if a pulse is successfully generated once.
The generation of a pulse is achieved by all non-faulty nodes following the advice of a single node switching to init, thus establishing the common time base required for a synchronized pulse.
Two obstacles are to be overcome: possibly some of the non-faulty nodes already believe that the system is in the middle of an attempt to stabilize (i.e., they are already in state resync and thus not ready to follow the advice given by another node) and possibly inconsistent information by nodes that remain faulty (causing only some of the non-faulty nodes to switch to resync).
In contrast to the higher levels, however, we are satisfied if only occasionally a successful pulse is generated.
Hence, the above issues can be overcome by randomization.
The source of randomness here is the randomized timer (R3,wait).
The distribution R3 and the logic of the second, more complicated state machine including the state resync are designed such that there is a large probability that within time O(n) all non-faulty nodes will consistently switch to state resync.
This O(n) is essentially the factor by which the second stabilization mechanism of the main state machine is slower than the first one.
Timer durations
Clearly, in order for the protocol to operate as desired, the timer durations need to satisfy certain constraints.
We state a feasible family of durations here; the minimal constraints that are required by the proofs are given in [13].
Recall that ϑ>1 and that d bounds the maximal end-to-end delay incurred between the time when a state transition condition is met and the time when the respective signal transition is observed at all receivers.
As the logic of the quick cycle is much simpler than that of the other state machines, it typically permits much tighter upper and lower bounds on this end-to-end delay.
As in [13], these bounds are denoted by dmin+ and dmax+⩽d.
In Section 6, we will discuss how d, dmin+, and dmax+ can be computed out of the constituent delays incurred in our basic modules.
Definingλ:=(25ϑ-9)/(25ϑ)∈(4/5,1)andα:=(T2+T4)/(ϑ(T2+T3+4d)), for any ϑ>1, α⩾1, the following family of timeout durations meets the requirements stated in [13] (see the reference for a proof):T1+:=6ϑ2d+6ϑ2dmax+-ϑdmin+T2+:=3ϑd+3ϑdmax+T3+:=6ϑ3d+6ϑ3dmax+-ϑ2dmin+T1:=4ϑdT2:=46ϑ3d/(1-λ)T3:=(ϑ2-1)46ϑ3d/(1-λ)+31ϑ3dT4:=46ϑ3(αϑ3-1)d/(1-λ)+35αϑ4dT5:=46ϑ4(αϑ3-1)d/(1-λ)+39αϑ5dT6:=46ϑ4d/(1-λ)T7:=92αϑ8d/(1-λ)+78αϑ5d and further,R1:=46ϑ6(3αϑ3-1)d/(1-λ)+109αϑ6dR2:=(92ϑ7(3αϑ3-1)/(1-λ)2+(218αϑ7+108ϑ3)/(1-λ))(n-f)dR3:=uniformly distributed random variable on[3ϑd+(92ϑ8(3αϑ3-1)/(1-λ)2+(218αϑ8+108ϑ4)/(1-λ))(n-f)d,3ϑd+(8(1-λ)+ϑ)(92ϑ7(3αϑ3-1)/(1-λ)2+(218αϑ7+108ϑ3)/(1-λ))(n-f)d].
Finally, the maximal logical clock value K-1 is not arbitary, as we require(1)K∈[(46ϑ4/(1-λ)+52ϑ2)/(12+10dmax+/d),α(46ϑ4/(1-λ)+32ϑ2)/(12+12dmax+/d)].
Note that, by manipulating α, we can make K arbitrarily large, but this comes at the expense of a proportional increase in the timer durations of the main state machine and its underlying layers, increasing the overall stabilization time.
Summary of results from theory
We conclude the section with a summary of the most important statements proved in [13], expressed in terms of the model employed in this article.
To this end, we need to specify the protocol as a compound implementation about that we will formulate our theorems.
Definition 5.1
The FATAL+ Protocol
The FATAL+ protocol is a compound module consisting of nodes i∈{1,…,n}.
It has no input ports and an output port Li for each node i.
The n input ports of node i are connected to the output ports of the channels Si,j, j∈{1,…,n}.
Each node is comprised of one copy of each of the state machines presented in this section, and the implementation of each node is derived from the implementations (given in Section 6) of the basic modules defined in Section 2.2 that are connected as specified in this section.
The output port Li of node i is the output port of its quick cycle state machine.
The first theorem states a probabilistic stabilization result.
Since we did not formally define probabilistically stabilizing implementations, its formulation is somewhat cumbersome.
Intuitively (and slightly inaccurately), the statement is to be read as "no matter what the initial state and the execution, the protocol stabilizes almost certainly within Tslow time".
Theorem 5.2
Fix any f′⩽f:=⌊(n-1)/3⌋ and feasible α, setTslow:=(24(1-λ)+3ϑ)R2+R1/ϑ+T1++T3++(9ϑ+8)d+5dmax+-dmax-∈Θ(αn), and pick K∈Θ(αn) in accordance with inequality (1).
Consider an execution on [t-,t+] of the FATAL+ protocol where (at least) n-f′ nodes are feasible.
Assume that an adversary that knows everything about the system except that it does not learn about the durations of randomized watchdog timers before they expire controls all other aspects of the execution (clock drifts and delays of feasible submodules within the admissible bounds as well as the output ports' signals of faulty modules).
Then the execution restricted to [t-+Tslow,t+] is with probability at least 1-2-(n-f) a feasible execution of a clock synchronization module with clock imprecision Σ=1, amortized frequency bounds A-=1/(T1++T3++3dmax+) and A+=1/(ϑ(T1++T3+)), slacks τ-=τ+=2, maximum frequency F+=1/(ϑ(T1++T3+-2dmax++dmin+)), at most f′ faults, and clocks of size K∈Θ(αn).
In this sense, for each f′⩽f, the FATAL+ protocol is an f′-tolerant implementation of a clock synchronization module with the respective parameters that stabilizes with probability at least 1-2-(n-f) within time Tslow∈O(αn).
The above theorem corresponds to the slow, but robust, second stabilization mechanism.
The next theorem, which corresponds to the faster first stabilization mechanism, essentially states that in an execution where n-f nodes already stabilized, any further non-faulty nodes recover quickly and deterministically, within O(α) time.
Theorem 5.3
We use the notation of the previous theorem. Moreover,Tfast:=T2+T4+(1+5/(2ϑ))R1+5d∈Θ(α).
Suppose an execution of the FATAL+ protocol is feasible on [t-,t+] with respect to the clock synchronization module specified in Theorem 5.2.
Consider the set of nodes W⊆N whose restricted executions on [t-,t+] are feasible.
Then the execution restricted to [t-+Tfast,t+] is feasible with respect to a clock synchronization module with the same parameters, except that it tolerates n-|W| faults only.
We should like to mention that in [13] a number of further results on stabilization are given.
In particular, if the faulty nodes exhibit only little coordination among themselves or do not tune their operations to the non-faulty nodes' states, also the "slow" stabilization mechanism will succeed quickly, granted that the resynchronization state machines are not in a "too bad" configuration, i.e., most timers of type R2 are expired and timeouts of type R3 are in (roughly) random states.
We will informally discuss some of these scenarios in Section 7.
Finally, we emphasize again that the power of the above theorems severely depends on the quality of basic implementations (cf.
Section 4.2).
While compound modules' properties can be formally analyzed, e.g.
giving rise to the theorems above, these results are meaningless if too many basic implementations are infeasible too frequently.
Hence it is vital to come up with robust implementations of the basic modules, which is the subject of the next section.
Implementation
In this section, we present the cornerstones of our FPGA prototype implementation of the FATAL+ protocol.
The objectives of this implementation are (i) to serve as a proof of concept, (ii) to validate the predictions of the theoretical analysis, and (iii) to form a basis for the future development of protocol variants and engineering improvements.
Rather than striving for optimizing performance, area, or power efficiency, our primary goal is hence to essentially provide a direct mapping of the algorithmic description to hardware, and to evaluate its properties in various operating scenarios.
Not surprisingly, traditional design principles for digital circuits are not adequate for our purposes.
This is true for three major reasons:•
Asynchrony: Targeting ultra-reliable clock generation in SoCs, the implementation of FATAL+ itself cannot rely on the availability of a synchronous clock.
Moreover, many guards, like the one of the transition from propose to accept in Fig. 8, depend on remote nodes' states and should hence not be synchronized to a local clock in order to maximize performance.
Testing for activated guards synchronized to a local clock source also increases the risk of generating metastability, as remote signals originate in different clock domains.
On the other hand, conventional asynchronous state machines (ASM) are not well-suited for implementing the state machines from Fig. 6-Fig. 10 due to the possibility of choice w.r.t.
successor states and continuously enabled (i.e., non-alternating) guards.
Our prototype hence relies on hybrid state machines (HSM) that combine ASM with synchronous transition state machines (TSM) that are started on demand only.
•
Fault tolerance: The consideration of Byzantine faulty nodes forced us to abandon the classic "wait for all" paradigm traditionally used for enforcing the indication principle in asynchronous designs: Failures may easily inhibit the completion of the request/acknowledge cycles typically used for transition-based flow control.
A few timing constraints, established by our theoretical analysis, in conjunction with state-based communication are resorted to in order to establish event ordering and synchronized executions in FATAL+.
•
Self-stabilization: In sharp contrast to non-stabilizing algorithms, which can always assume that there is a (substantial) number of non-faulty nodes that run approximately synchronously and hence jointly adhere to certain timing constraints, self-stabilizing algorithms cannot even assume this.
Although FATAL+ guarantees that non-faulty nodes will eventually execute synchronously, even when started from an arbitrary state, the violation of timing constraints and hence metastability cannot be avoided during stabilization [44].
For example, state accept in Fig. 8 has two successors sleep and recover, the guards of which could become true arbitrarily close to each other in certain stabilization scenarios.
This is acceptable, though, as long as such problematic events are neither systematic nor frequent, which is ensured by the design and implementation of FATAL+ (see Section 6.1).
 Inspecting Figs.
6-10 reveals that the state transitions of the FATAL+ state machines are triggered by AND/OR combinations of the following different types of conditions:(1)
A watchdog timer expires ["(T2,accept)"].
(2)
The state machines of a certain number (1, ⩾f+1, or ⩾n-f) of nodes reached a particular (subset of) state(s) at least once since the reset of the corresponding memory flags ["⩾n-f accept"].
(3)
The state machines of a certain number (1, ⩾f+1, or ⩾n-f) of nodes are currently in (one of) a particular (subset of) state(s) ["in resync"].
(4)
Always ["true"].
These requirements reveal the need for the following major building blocks (cf.
Section 4):•
Concurrent HSMs, implementing the states and transitions specified in the protocol.
An ideal HSM would always provide feasible executions of its state transition module.
•
Communication infrastructure between these state machines, continuously conveying state information.
This is simply done by the channels Si,j propagating the signal Si to all receivers.
•
Watchdog timers (also with random timeouts) for implementing type (1) guards.
•
Threshold modules and memory flags for implementing type (2) and type (3) guards.
If we could provide implementations of all these building blocks that match the specifications of the formal model in Section 2.2 under all circumstances, in the sense that all executions at non-faulty nodes are always feasible, the theoretical guarantees derived in [13] would apply without restriction.
As already noted, however, this is impossible to guarantee, since there is no way to rule out metastable upsets with complete certainty, and there are no elements available for our purpose whose behavior is specified for metastable inputs.
Nevertheless, it is possible to design our basic modules in a way that keeps the probability of such events acceptably low.
Moreover, all stateful components must be implemented in a self-stabilizing way: They must be able to eventually recover from an arbitrary erroneous internal state, including metastability, when facing sufficiently long executions on their input ports that do not induce metastability.
Before we proceed with a description of the implementations of the required basic modules, we discuss how FATAL+ deals with the threat of metastability arising from our extreme fault scenarios.
Metastability issues
Reducing the potential for both metastability generation and metastability propagation are important goals in the design and implementation of FATAL+.
Although it is impossible to completely rule out metastability generation in the presence of Byzantine faulty nodes (which may issue signal transitions at arbitrary times anyway) and during self-stabilization (where all nodes may be completely unsynchronized), we nevertheless achieve the following properties.
Robustness against metastable upsets and their propagation:(I)
Guaranteed metastability-freedom in fault-free executions after stabilization.
(II)
Low probability of metastable upsets: We have taken care to keep the windows of vulnerability of our implementations of basic modules as small as possible.
Thus, desynchronized or faulty nodes must be very lucky to actually trigger a metastable upset.
In addition, mechanisms for decreasing the upset probability even further could be incorporated, if required in particularly critical applications.
(III)
Metastability containment: Non-faulty nodes are very robust against propagation of metastable upsets due to the algorithm's control flow.
Limited consequences of metastable upsets:(IV)
Limited impact of metastable upsets during stabilization: Metastable upsets that occur at non-faulty nodes during the stabilization phase can only delay stabilization.
Since these are rare events even then, the respective effect on the (average) stabilization time is very small.
(V)
Fast recovery from metastability after stabilization: As long as n-f non-faulty nodes remain synchronized, a metastable upset at a node may disrupt its synchrony towards the other nodes only for a short time.
Due to the fast stabilization mechanism the node will fully recover within O(1) time once metastability ceases.
(VI)
Masking of metastable upsets as faults: Provided that the measures ensuring (II) and (III) are effective (i.e., metastability does not spread) and the system-level fault-tolerance of f nodes operating outside their specification is not exhausted, metastable upsets at some nodes do not affect the correctness of other nodes.
The following approaches have been used in FATAL+ to accomplish these goals (additional details will be given in the subsequent sections):(I)
If all nodes are synchronized and fault-free, we can satisfy timing constraints on the modules' input ports' signals that ensure that even our (necessarily imperfect) implementations of the abstract modules maintain feasibility at all times.
Essentially, the argument is that since there is no initial violation of the constraints and no faults are imposed by external events, we can conclude that the constraints will be satisfied at later points in time as well.
This property is formally proved in [13].
(II)
All building blocks that are susceptible to metastable upsets, like memory flags, are implemented in a way that minimizes the time span during which they are vulnerable.
Moreover, elastic pipelines acting as metastability filters [40] or synchronizers could be added easily to our design to further protect such elements.
(III)
We enforce (standard) error containment by avoiding any explicit control flow between ASMs: Since the communication is exclusively performed by virtue of states, a faulty receiver cannot impact a non-faulty sender, and a faulty sender, in turn, cannot directly interfere with the operation of a non-faulty receiver (apart from conveying an incorrect state, of course).
To extend error containment to also cover metastability to the best possible extent, several forms of logical masking are employed.
One example is the combination of memory flags and threshold gates, which ensure that possibly upset memory flags are always overruled quickly by correct ones at the threshold output.1818
It is not self-evident that this type of masking is very effective for metastability as well.
Later on we will discuss why this is indeed the case.
 A higher-level form of logical masking occurs due to the fact that, after stabilization, all non-faulty nodes execute the outer cycle of the main state machine (Fig. 8) only.
The outer cycle's guards do not involve any of the timeouts, states, or flags accessed by the resynchronization routine (Fig. 10) or the extension of the main state machine (Fig. 9); hence any metastability of the corresponding signals does not affect the logic of the main state machine and the layers on top of it (including the logical clocks).
(IV)
The measures outlined in (II) and (III) are complemented by adding time masking using randomization.
The resynchronization routine (Fig. 10) tries to initialize recovery from arbitrary states at random, sufficiently sparse points in time.
Hence non-faulty nodes cannot be systematically kept from stabilizing.
The proofs in [13] reveal that within O(n) time in fact it is likely that there are multiple events that will imply subsequent stabilization.
Considering that metastable upsets are rare events in our setting, their impact thus becomes negligible.
(V)
This property directly follows from the results shown in [13]: If n-f nodes faithfully execute the basic cycle, any non-faulty node will (re)synchronize within O(1) time, irrespectively of its current state.
(VI)
If metastability does not spread to a given receiver, the latter will observe for each sender some execution, even if the sender does not send a valid signal in terms of our system model.
Since we assume that faulty nodes may output arbitrary signals at their output ports, our model thus makes no distinction between a "conventionally" faulty node and one that behaves erratically due to metastable upsets.1919
To match our model, invalid signal states are simply mapped to some default state, e.g.
resync for the main state machine.
 As the algorithm is resilient to up to f faults, such upsets are masked as long as the total number of nodes operating outside their module specification is at most f.
State machine communication
According to our system model, an HSM of node i must continuously communicate its current state system-wide via the channels Sj,i.
For simplicity, we use parallel communication, by means of a suitably sized data bus, in our implementation.2020
It is, however, possible to replace parallel communication by serial communication, e.g., by extending the (synchronous) TSM (see Section 6.3) appropriately.
 A complete receiver as described below is employed for every state machine in the system.
Since a node treats itself like any other node in type (2) and type (3) guards with thresholds, every node receives its own state as well.
Channels
Fig. 11 shows the circuitry used for communicating the current state of the main algorithm in Fig. 8.
The sender consists of a simple array of flip-flops, which drive the parallel data bus that thus continuously reflects the current state of the sender's HSM.
Technically speaking, the flip-flops are not part of the channel but rather the sender's HSM; they are the "physical location" of the HSM's state in the sense of our model.
The channel thus "begins" with the wires conveying the stored values.2121
Note that there is some freedom with respect to the mapping of module ports to the physical system, which also affects which module(s) become(s) infeasible due to a (physical) fault.
However, no matter what the precise mapping, care has to be taken to avoid correlated failures.
For instance, if all channels meet in a single spot due to bad routing, manufacturing defects or electromigration could connect several channels, therefore rendering our system-level fault-tolerance (i.e., the resilience to f node failures) ineffective.
In sharp contrast to handshake-based communication, reading at the receiver occurs without any direct coordination with the sender.
To avoid the unacceptable risk of reading and capturing false intermediate sender states, which might be perceived by the receiver upon a sender state transition in case of different delays on the data bus wires, delay-insensitive state coding [45] must be used.
We have chosen the following encoding for the main state machine in Fig. 8:propose0000accept1001sleep1011sleep→waking0011waking0101ready0110recover1100join1010
The receiver comprises a simple combinational decoder consisting of AND gates, which generate a 1-out-of-m encoding of the binary representation of the state communicated via the data bus.
The decoded signals correspond to a single sender state each.
This information is directly used for type (3) guards, and fed into memory flags for type (2) guards.
For the other state machines making up FATAL+, it suffices to communicate only a single bit of state information (supp or none in Fig. 9, init or wait in Fig. 10, and propose+ or none+ in Fig. 6).
Hence, every bus consists of a single sender flip-flop plus a wire here, and the decoder in the receiver becomes trivial.
In the sequel, we restrict our discussion to the main state machine's channel, as the simpler single-bit channels clearly meet the specification of a channel.
Note that in both cases the (physical) channels used in our implementation trivially recover from any inputs and transient faults, as they are obviously forgetful.
The memory flags at the receiver's side contain feedback-loops, however, which do not allow us to apply Theorem 3.7 and Lemma 3.8.
Correctness.
We now argue informally2222
Our basic modules appear simple enough to be amenable to formal verification.
Still, there are complications: Besides the fact that we assume not only continuous time but also continuous computations, which rules out using standard verification approaches, there is the challenge of finding and expressing suitable input port execution constraints required for implementation correctness.
Exploring this avenue is part of our future work.
 why and when the above implementation matches the specifications given in Section 4.
Note that when affected by faults or provided with illegal inputs, modules may of course exhibit arbitrary behavior.
In that case we rely on (a) the system-level fault tolerance properties (for fault masking), (b) the self-stabilization properties of the affected modules (for recovery), and (c) the rare occurrence of these situations (in order to not exhaust the system-level fault tolerance limits).
In addition to considering the fault-free behavior, it hence suffices to restrict our attention to (b) and (c) here.
For fault-free operation, the described implementation essentially realizes a channel as specified in Section 4 with some maximum delay dChan, granted that changes of the input provided by the sender are separated in time sufficiently well.
To see this, consider an input switch from state s to s′ (note that not all flip-flops will switch their output signals at exactly the same instant), where initially the signal is stable also on the receiver's side.
Once the signal change propagated through the wires and the AND gates, the decoder output signal corresponding to state s′ will be 1, while all other signals will be 0.
Due to the use of delay-insensitive state encoding, there are no glitches and the signals for all other states s″∉{s,s′} will continuously be 0.
Nevertheless, formally, this behavior does not yet fully match the definition of our communication channels in Section 4: It is possible that temporarily both s and s′ are 1.
Since our algorithms are completely oblivious to the exact point in time when the perceived Si,j changes after the sender's state Sj changed (the analysis in [13] only requires that this happens within d time), however, this problem can easily be abstracted away.2323
Formally, this abstraction builds upon a weakened definition of lower-level channels, which attain values from S∪(S2).
Alternatively, it would also be possible to use an explicit transition state ⊥ (encoded by any bit sequence not corresponding to a state), and force the sender to always perform state transition via ⊥.
 All that is needed here is to interpret, in a static way, the situation where both s and s′ are valid as, say, s.
The attentive reader will have noticed that the 1-out-of-m decoder outputs (i.e., the state signals at the inputs of the memory flags) may temporarily be all 0 during the reception of a sender state transition as well.
Fortunately, this behavior is completely masked from becoming visible to our algorithms: The memory flags prohibit this from becoming visible in type (2) guards at all, and all state transition conditions involving type (3) guards refer to a single state only.
Hence, in terms of the transition condition, a similar abstraction as above is valid (i.e., for a remote state transition from s to s′ with a "gap" we can define an equivalent execution without gap in which the node in question behaves identically).
The above arguments critically rely on the assumption that states change not too rapidly.
Otherwise, the receiver could e.g.
fail to observe states that the sender assumed for a too short period of time only, or even decode a state that has not been attained.
For non-faulty nodes, this is guaranteed in our implementation because the minimal amount of time an HSM needs to complete a state transition is greater than the maximum end-to-end delay variation of the signals employed in the communication channel.
This constraint is easy to ensure by proper circuit design rules.
Metastability.
Within the communication channels themselves, metastable upsets could only occur in the senders' flip-flops and in the receivers' memory flags; everything else is stateless combinational logic.
The flip-flops are clocked by the sender's own clock, hence could become metastable only in case of a faulty sender.
The issue of upsets of the memory flags is discussed in Section 6.2.2.
Viewed at the node level, it is obvious that if the sender's state signal becomes metastable or changes too quickly (which can only happen if the sender is faulty), this can also induce metastability at the receiver side by propagation over the channel.
During the stabilization phase, the receiver could also experience a channel-induced metastable upset in memory flags and/or in its HSMs due to the arbitrary desynchronization between sender and receiver; since the windows of vulnerability are very small, the upset probability is very low, though.
Eventually, after stabilization, the synchrony between non-faulty nodes guaranteed by the FATAL+ protocol ensures that the received state data will always be stable when read in a transition condition in the main algorithm's outer cycle, recall item (I) in Section 6.1.
Memory flags
Every memory flag is just an SR-latch with dominant reset, whose functional equivalents are also depicted in Fig. 11.
Note that a memory flag is set depending on the state communicated by the sender, but (dominantly) cleared under the receiver's control.
Metastability.
A memory flag may become metastable when the inputs change during stabilization of its feedback loop, which can occur due to (a) input glitches and/or (b) simultaneous falling transitions on both inputs.
However, for correct receivers, (a) can only occur in case of a faulty sender, and (b) is again only possible during stabilization: Once non-faulty nodes execute the outer cycle of Fig. 8, it is guaranteed that e.g.
all non-faulty nodes enter accept before the first one leaves.
Overall, the upset probability is thus very small.
It could be further reduced by diverse known means for metastability filtering, like using an elastic pipeline or Schmitt-trigger stages (which must be accounted for in the delay bounds, though).
Finally, it is well-known that a metastable flip-flop will recover in finite time with probability one [44].
Any SR latch matches the specification of a memory flag according to Section 4 followed by a channel with some maximum delay dMem, provided that it starts from a clean initial state and the set/reset signals avoid (a) and (b) above.
As argued above, the latter is guaranteed by our algorithm except in case of a metastable upset.
In case of the memory flag implementation shown in Fig. 11, dMem is primarily determined by the end-to-end settling time of the feedback loop.
This delay also determines the vulnerability window with respect to metastability (i.e. critical glitch length, and "simultaneity" of transitions).
Hence, making dMem small, which is easy to achieve by design, contributes to both speed and robustness.
Except in case of metastability, discussed before, our memory flag implementation is self-stabilizing since it is dMem-forgetful in the presence of input executions that avoid (a) and (b).
Threshold modules
The most straightforward implementation of the threshold modules used for generating the ⩾f+1 and ⩾n-f thresholds in type (2) and type (3) guards is a simple sum-of-product network, which just builds the OR of all AND combinations of f+1 respectively n-f inputs.
This implementation however quickly becomes highly expensive, as it requires Θ((nf)) gates.
A more efficient alternative is a sorting network, where the kth output indicates whether a threshold of k is reached.
For simplicity, in our FPGA implementation, threshold modules are built by means of lookup-tables (LUT).
Correctness.
Similar to our memory flag implementation, it is impossible to implement the properties of a threshold module as stated in Section 4, followed by a channel with some maximum delay dTh, in case of arbitrary inputs: Finding out whether a certain number of inputs is 1 exactly at the same time cannot be implemented with real circuits.
All implementations proposed above are forgetful and their outputs will stabilize quickly if their inputs do not change.
Moreover, after stabilization type (2) guards are irrelevant, since neither the basic cycle of the main state machine nor the quick cycle evaluate such guards.
Hence, in this case we can restrict our attention to input executions where inputs may change from 0 to 1 only, not back.
The reset of the memory flags to 0 is performed during state transitions (when the guards' signals are suppressed by the locked signal) and therefore safe.
As any of the proposed threshold module implementations involve combinational logic only, they are trivially self-stabilizing: According to Theorem 3.7, they are forgetful and hence, by Lemma 3.8, self-stabilizing.
Therefore, provided that the longest path delay does not exceed dTh, the properties stated in Section 4 are satisfied for monotonic inputs.2424
Some dedicated experiments confirmed that even our LUT implementation on an FPGA, for which we have no control over the placement, operates glitch-free on monotonic inputs.
Metastability.
As discussed above, type (2) guards cannot be safely evaluated by threshold gates and may cause glitches or metastable upsets.
Since this is of relevance before stabilization only, this risk is considered acceptable.
Like our channel implementations, threshold modules can propagate metastability: A metastable input could be propagated to the output when there are exactly k-1 non-faulty inputs in state 1 and the metastable input therefore makes the difference between output 0 and 1.
In all other cases, however, the metastable input will simply be masked.
Thus, albeit not perfect, threshold gates are an efficient means for metastability containment.
Hybrid state machines
Our prototype implementation of FATAL+ relies on hybrid state machines (HSM): An asynchronous state machine (ASM) is used for determining, by asynchronously evaluating the guards, the points in time when a state transition shall occur.
Our ASMs have been built by deriving a state transition graph (STG) specification directly from Figs.
6-10 and generating the delay-insensitive implementation via Petrify [46].
The actual state transition of an HSM is governed by an underlying synchronous transition state machine (TSM).
The TSM resolves a possibly non-deterministic choice of the successor state and then sequentially performs the required transition actions:1.
"Locking" the transition, i.e., disabling any other transitions of the ASM (despite possibly satisfied guards); this happens at the start of the TSM and is thus asynchronously triggered.
2.
Reset of memory flags and watchdog timers.
3.
Communication of the new state, i.e., writing its representation into the flip-flops whose output is fed into the channels Sj,i.
4.
Completing the transition to the new state by enabling further transitions of the ASM.
 The TSM is driven by a pausable clock (see Section 6.4), which is started dynamically by the ASM upon triggering the transition.
Note that this avoids the need for synchronization with a free-running clock and hence preserves the ASM's continuous time scale.
The TSM works as follows (see Fig. 12): Assume that the ASM is in state A, and that the guard G for the transition from A to B becomes true.
If no other transition is currently being taken (indicated by the locked signal being 0), the TSM clock is started and the TSM sequence counter is released.
With every rising edge of TSMClock, the TSM moves through a sequence of three states: synchronize (Syn), commit (Cmt), and terminate (Trm) shown in the rectangular box in Fig. 12.
In Syn, the locked signal is activated to prevent other choices from being executed in case of more than one guard becoming true.
Once the TSM has reached Syn, it has decided to actually take the transition to B and hence moves on to state Cmt.
Here the watchdog timer associated with B and possibly some memory flags are cleared according to the FATAL+ state machine, and the new state B is captured by the output flip-flops driving the state communication data bus (recall Section 6.2).
Note that the resulting delay must be accounted for in the communication delay bounds d, dmax+ and dmin+.
Finally, the TSM moves on to state Trm, in which the reset signals are inactivated again and the TSM clock is halted (and the TSM sequence counter forced to the reset state).
The locked signal is also cleared here, which effectively moves the ASM to state B.
It is only now that guards pertaining to state B may become true.
Metastability.
Whereas any ambiguity of state transitions due to multiple activated guards can easily be resolved via some priority rule, metastability due to (a) enabled guards that become immediately disabled again or (b) new guards that are enabled close to "locking" time cannot be ruled out in general.
However, as argued in Section 6.1, in FATAL+ (a) could only occur during stabilization, due to type (3) guards, or due to faulty nodes successfully inducing metastability of memory flags; recall that otherwise type (1) and type (2) guards are always monotonic, with the reset (of watchdog timers and memory flags) being under the control of the local state machine.
Similarly, our proofs in [13] reveal that upsets due to (b) do not occur after stabilization in the main state machine and the quick cycle (Figs.
6 and 8).
As the main state machine is logically independent of the lower layers (Figs.
9 and 10) after stabilization, any metastability in these layers is fully masked.
Thus, after stabilization, metastability of the TSMs we care about can only occur due to unstable inputs, i.e., upsets in memory flags, that are in addition filtered through threshold gates (type (1) guards use local timeouts and are thus considered non-faulty, and all type (2) guards employed by the main state machine and the quick cycle use thresholds).
Note that due to the logical masking of metastability provided by the threshold gates (cf.
Section 6.2.3) any memory flag acts as an implicit synchronizer: If a faulty node successfully induces metastability in the flag, this does not matter until the threshold can actually be reached.
If the respective time span is large, the memory flag is likely to have stabilized again already.
Therefore, in addition to succeeding in creating metastability, faulty nodes must do so within a specific window of time.
Due to the asynchronously triggered transitions, this window of vulnerability of the synchronizing stage for Syn is very small.
The resulting very low probability of a metastable upset due to a fault is considered acceptable.
The residual probability of metastable upsets could be further reduced by introducing synchronizer stages.
Considering their performance penalty of one extra clock cycle on the one hand and the low initial risk of metastable upsets (that are handled by the system level fault tolerance with much lower average performance penalty) on the other hand, however, the introduction of synchronizers does not seem beneficial in general.
Correctness.
Thanks to the synchronous TSM described above, the maximum state transition time dTrans can easily be expressed in terms of the frequency of the pausable clock.
Hence, it is reasonably easy to see that the HSM satisfies the specification given in Section 4, when it starts from a proper initial state and avoids the above scenarios (a) and (b) of unstable guards.
A careful simulation analysis of the overall HSM design confirms that it can in fact recover from arbitrary initial states, except metastable ones.
With respect to metastable initial states, we conjecture that eventual recovery occurs with probability 1 due to the fact that the only devices used in the implementation that are not forgetful are flip-flops with dominant reset (in the TSM sequence counter) and Muller C-gates (in the control logic of the ASM), for both of which it is known that metastability eventually resolves.
Clocks and watchdog timers
Pausable oscillator
The TSM clock is an asynchronously startable and synchronously stoppable ring oscillator, which provides a clock signal TSMClock that is 0 when the clock is stopped via an active 1 input signal TSMCStop.
A variant that is also asynchronously stoppable (under certain timing constraints) is used for driving the watchdog timers (see below).
The frequency of the ring oscillator is primarily determined by the (odd) number of inverters in the feedback loop.2525
In our FPGA implementation, the oscillator frequency is so high that, to reduce the hardware overhead for this proof-of-concept implementation, we also employ a frequency divider at the output.
 It varies heavily with the operating conditions, in particular with supply voltage and temperature: The resulting (two-sided) clock drift ξ is typically in the range of 7 to 9% for uncompensated ring oscillators like ours; in ASICs, it could be lowered down of 1 to 2% by special compensation techniques [15].
Note that the two-sided clock drifts map to ϑ=(1+ξ)/(1-ξ) bounds roughly between 1.15 and 1.19 or 1.02 and 1.04, respectively.
Correctness.
The operation of the TSM clock circuit shown in Fig. 13 is straightforward: In its initial state, TSMCStop=1 and the Muller C-gate has 1 at its output, so TSMClock=0.
Note that the circuit also stabilizes to the initial state if the Muller C-gate was erroneously initialized to 0, as the ring oscillator would eventually generate TSMClock=1, enforcing the correct initial value 1 of the C-gate.
When the ASM requests a state transition, at some arbitrary time when a transition guard becomes true, it just sets TSMCStop=LOW.
This starts the TSM clock and produces the first rising edge of TSMClock half a clock cycle time later.
As long as TSMCStop remains 0, the ring oscillator runs freely.
The stopping of TSMClock is regularly initiated by the TSM itself: With the rising edge of TSMClock that moves the TSM into Trm, TSMCStop is set to 1.
Since TSMClock is also 1 after the rising edge,2626
Obviously, we only have to take care in the timing analysis that setting TSMCStop=1 occurs well within the first half period.
 the output of the C-gate is forced to 1 as well.
Hence, after having finished the half period of this final clock cycle, the feedback loop is frozen and TSMClock remains 0.
Metastability.
The problem of devising a proof that the pausable clock will eventually recover when it starts from a metastable initial state is intricate (and outside the scope of this paper); this is not obvious due to the quite complex feedback loop involved in this circuit.
We conjecture that similar arguments as in [44] can be used to show that this will happen with probability 1; with this result established one could hope to infer that the HSM as a whole recovers from arbitrary metastable states with probability 1.
For metastability-free operation of the C-gate in Fig. 13, (a) the falling transition of TSMCStop must not occur simultaneously with a rising edge of TSMClock, and (b) the rising transition of TSMCStop must not occur simultaneously with the falling edge of TSMClock.
(a) is guaranteed by stopping the clock in state Trm of the TSM, since the output of the C-gate is permanently forced to 1 on this occasion; TSMClock cannot hence generate a rising transition before TSMCStop goes to 0 again.
Whereas this synchronous stopping normally also ensures (b), we cannot always rule out the possibility of getting TSMCStop=1 close to the first rising edge of TSMClock: (b) could thus occur due to prematurely disabled type (3) guards, which we discussed already with respect to their potential to create metastability in the TSM, recall Section 6.3.
Besides being a rare event, this can only do harm during stabilization, however.
Watchdog timer design
Every ASM state, except for accept in Fig. 8, is associated with at most one watchdog timer required for type (1) guards; accept is associated with three timers (for T1 and T2 as well as for T2+ in Fig. 6).
Recall that a timer is reset by the TSM when its associated state is entered, which does not necessarily happen synchronously with its counting clock.
According to Fig. 14, every watchdog timer consists of a synchronous, dominantly resettable up-counter that is clocked by its own pausable oscillator (as shown in Fig. 13) and a timeout register that holds the timeout value TO.2727
Note that these values must be hard-coded in order to avoid that a fault that intuitively should be transient (e.g. a bit flip in volatile memory) becomes permanent by "altering the algorithm".
 A comparator raises an output signal if the counter value is equal to the TO register value.
A "capture flip-flop" with dominant reset memorizes the expired condition until the timer is re-triggered.
Note that using a (synchronous) flip-flop instead of an SR latch here allows us to completely mask glitches at the comparator output, which may originate from intermediate inconsistent bit patterns at the counter output.
The reset signal TSMresWD, supplied by the TSM, (re-)triggers the watchdog as follows: The counter is reset to zero, the capture flip-flop is cleared, and the oscillator is temporarily stopped.
Stopping the oscillator is necessary to avoid metastability effects due to the unsynchronized release of the reset signal (recall that this signal originates from the clock domain of the TSM!) and the watchdog's local oscillator.
Note carefully, however, that the Muller C-gate in Fig. 13 must be extended by a dominant reset input connected to its stop input (TSMCStop) to prevent metastable upsets.
Moreover, to ensure a proper reset, one has to make sure that the reset duration is sufficiently large.
To guarantee this, TSMresWD is fed into a pulse shaping circuitry (bottom left part of Fig. 14) that makes sure that the reset pulse is longer than one period of the local clock.2828
This is why seven inverters are shown in Fig. 14, indicating that the output pulse needs to remain active for more than two half periods of the local clock, assuming three inverters in its oscillator's loop.
 At the end of this shaped reset pulse, counter and flip-flop have attained a clean reset state, and the local oscillator has safely been brought to a stable stopped state (with its output at 0).
When reset is finally released (to 0), the oscillator starts running.
As soon as the comparator detects a match between the current count and the timeout register, it will set match to 1.
This rising edge is captured by the flip-flop, thus keeping the watchdog timeout signal WDexpired at 1 even when the comparator reverts its output to 0 later on again (note that the counter keeps on running).
This construction ensures that the oscillator continues to operate also after the timeout expires, which is crucial for self-stabilization; in a system where the clocks driving the timeouts can be permanently halted, there is no way to avoid deadlocks for all possible states.
As for the watchdog timer with random timeout R3 in Fig. 10, our implementation uses a linear feedback shift register (LFSR) that is continuously clocked by the watchdog's oscillator: A uniformly distributed random value from the specified range, sampled from the LFSR, is loaded into the timeout register whenever the watchdog timer is re-triggered.2929
Note that for many settings, it is reasonable to assume that the new random value remains a secret until the timeout expires, as it is not read or in any other way considered by the node until then.
Under this condition, FATAL+ is resilient against the strong adversary specified in [13].
As our prototype implementation is not meant for studying security issues, however, the simple LFSR implementation is sufficient here.
 If both the watchdog timer and the LFSR are clocked by the same oscillator, this can be done in a synchronous way.
In order to avoid metastable upsets of the LFSR, which might occur when stopping the clock upon retriggering the watchdog as described above, we use a standard pausable oscillator for R3: Since R3 is guaranteed to timeout before it is re-triggered (see Fig. 10), we can stop the oscillator synchronously (as in the TSM) when the timeout occurs, i.e., tie its stop input to the OR of WDexpired and the pulse-shaped reset signal.
Another add-on is needed for the random timer R3 in order to guarantee that the LFSR recovers from an arbitrary state after a fault: Since an LFSR has a forbidden internal state (all-0 in our case), we use an additional comparator that detects an all-0 LFSR output and forces a (synchronized) reset of the LFSR to a proper initial state.
Metastability.
Using a dominant reset in conjunction with a reset pulse of sufficient length guarantees that pausable oscillator, counter, and flip-flop cannot become metastable when a watchdog timer is re-triggered.
Since all other activities are driven by the local oscillator and hence trivially metastability-free, this leaves the pulse-shaping unit as the only component that could possibly suffer from a metastable upset.
However, TSMresWD is generated by the TSM, which is guaranteed to generate a clean pulse at every correct node.
Hence, the pulse shaping unit could become metastable only at a faulty node.
With respect to the recovery from a metastable state, similar considerations as for the memory flags in Section 6.2.2 suggest that the pulse shaping unit will stabilize to an initial state with reset set to 0 eventually with probability 1.
Correctness.
Combining the implementations of the pausable oscillator (with additional reset) and the watchdog timer, it is not too difficult to verify that the specification given in Section 4 is met, provided all circuits start from a non-metastable initial state.
With respect to self-stabilization, the pulse shaping unit can be guaranteed to stabilize to an initial state with its reset output 0 from an arbitrary internal state.
Hence, the pausable oscillator and hence the counter will eventually run.
Provided that the counter implementation guarantees that it cycles through the full (finite) sequence of possible states (unless reset earlier), i.e., there are no deadlock states or alternative cyclic sequences that might be entered in case of a fault, our implementation ensures that WDexpired will eventually be set to 1, even if started from an arbitrary initial state.
One should bear in mind, though, that the time to recover a watchdog timer contributes to the overall stabilization time of the system.
It is hence advisable to make sure that recovering a watchdog timer does not take much longer than the largest timeout value in the system, e.g.
by avoiding oversized counter registers.
Computing the end-to-end delay bounds
From the implementations of the individual components, it is straightforward to compute the delays d, dmin+, and dmax+.
Recall that d bounds, for any node i, the maximal time that passes between a state transition of a remote node and a possibly triggered corresponding state change, i.e., the transition of Si.
This is done by computing the maximal sum of delays of any possible computing path, ranging over all possible state transitions (cf.
Fig. 5), taking into account the delay of the channels Sj,i.
Clearly, the channel delay for the remote channels Sj,i exceeds the delays of the local channels; hence, the longest path to the input ports of the state transition module is bounded by dChan+dMem+dTh>dTime.
Subsequently, the HSM locks the state transition and the TSM executes, which takes about two and a half clock cycles C of the pausable oscillator.
Note, however, that the new state is written into the flip-flops holding the state already during the commit cycle, i.e., after at most 1.5C.
A more accurate bound on d than dChan+dMem+dTh+2.5C is thusd⩽1.5C+max{C,dChan+dMem+dTh}.
For our approach, dmax+≈d, since the only difference to d is that dChan is replaced by dChan+, the delay of the simpler 1-bit channels (cf.
Section 6.2.1).
If the main state machine's channels would utilize serial encoding, though, one might well have that dChan≫max{dChan+,C}.
Finally, dmin+>C/ϑ, since this is the minimal time the HSM allows between locking a state transition and actually performing the transition at the port Si.3030
Clearly, a more precise analysis would yield tighter bounds.
Experimental evaluation
Our prototype implementation has been written in VHDL and compiled for an Altera Cyclone IV FPGA using the Quartus tool, see [47].
Since FPGAs neither natively provide the basic elements required for asynchronous designs nor allow the designer to exercise control over the actual mapping of functions to the available LUTs (we implemented threshold modules via LUTs rather than via combinational AND-OR networks for complexity reasons), we had to make sure that properties that hold naturally in "real" asynchronous implementations also hold here.
Apart from standard functional and timing verification via Modelsim, we therefore conducted some preliminary experiments for verifying the assumed properties (glitch-freeness, monotonicity, etc.) of the synthesized implementations of our core building blocks.
Backed up by the (positive) results of these experiments, complete systems consisting of n=4 respectively n=8 nodes (tolerating at most f=1 respectively f=2 Byzantine faulty nodes) have been built and verified to work as expected; overall, they consume 23000 respectively 55000 logic blocks.
Note however, that both designs also include the test environment, which makes up a significant part of the setup.
To facilitate systematic experiments, we developed a custom test bench that provides the following functionality:(1)
Measurement of pulse frequency and skew at different nodes.
(2)
Continuous monitoring of the potential for generating metastability in HSM state transitions.
(3)
Starting the entire system from an arbitrary state (including memory flags and timers), either specified deterministically or chosen at random.
(4)
Resetting a single node to some initial state, at arbitrary times.
(5)
Varying the clock frequency of any oscillator, at arbitrary times.
(6)
Choosing the communication delay between each pair of sender and receiver.
 All these experiments can be performed with Byzantine nodes.
To this end, the HSMs of the Byzantine nodes can be replaced by special devices that allow to (possibly inconsistently) communicate, via the communication data buses, any HSM state to any receiver HSM at any time.
Points (1) to (6) are achieved as follows:
(1) is accomplished using standard measurement equipment (logic analyzer, oscilloscope, frequency counter) attached to the appropriate signals routed via output pins.
(2) is implemented by memorizing any event where more than one guard is enabled (at the time when the TSM locks a state transition) in a flag that can be externally monitored.
(3) is realized by adding a scan-chain to the FPGA implementation, which allows us to serially shift-in arbitrary initial system states at run-time.
Repeated random experiments are controlled via a Python script executed at a PC workstation, which is connected via USB to an ATMega 16 microcontroller (μC) that acts as a scan-controller towards the FPGA.
For each experiment, the Python script generates a bit-stream representing an initial configuration.
The μC takes this stream, sends it to the FPGA via the serial scan-chain interface, and finally signals the FPGA to start execution of FATAL+.
Simultaneously, it starts a timer.
When a timeout occurs or the FPGA signals completion of the experiment, the μC informs the Python script which records the time until completion together with the outcome of the experiment and proceeds with sending the next initial configuration.
To enable (4) to (6), the testbench provides a global high-resolution clock that can be used for triggering mode changes.
To ensure its synchrony w.r.t.
the various nodes' clocks, we replaced all start/stoppable ring oscillators by start/stoppable oscillators that derive their output from the global clock signal.
Point (4) is achieved by just forcing a node to reset to its initial state for this run at any time during the current execution.
In order to facilitate (5), dividers combined with clock multipliers (PLLs) are used: For any oscillator, it is possible to choose one of five different frequencies (0, excessively slow, slow, fast, excessively fast) at any time.
For (6), a variable delay line implemented as a synchronous shift register of length X∈[0,15], driven by the global clock, can be inserted in any data bus connecting different HSMs individually.
In order to exercise also complex test scenarios in a reproducible way, a dedicated testbed execution state machine (TESM), driven by the global clock, is used to control the times and nodes when and where clock speeds, transmission delays, and communicated fault states are changed and when a single node is reset throughout an execution of the system.
Transition guards may involve global time and any combinatorial expression of signals used in the implementation of FATAL+, i.e., any predicate on the current system state.
Using our testbench, it was not too difficult to get our FATAL+ implementation up and running.
With the implementation parameters ϑ=1.3, d=13T, dmin+=dmax+=3T, where T=400ns (2.5 MHz) is the experimental clock period, and minimal timeouts according to the constraints listed in [13] (cf.
Section 5.7), pulses of an 8 node FATAL respectively FATAL+ system (including the quick cycle) occur at a frequency of about 62 Hz respectively 10 kHz.
A logic analyzer screenshot is depicted in Fig. 15.
Note that the quite low values for the frequency stem from the fact that we were intentionally slowing down the system, enabling better control of the execution.
As to be expected from such a fairly complex setup, we spotted several hidden design errors that showed up during our experiments, but also some minor, yet problematic errors in our theoretical analysis (like a missing factor of ϑ in one of our timeouts due to a typo).
In the original setup, these issues manifested in deviations of the measured w.r.t.
the predicted performance.
After resolving them, we conducted the following experiments, observing the behavior of both the overall FATAL+ and the underlying FATAL pulse generation protocol.
Worst-case skew experiment
To drive an 8-node FATAL system into a worst-case skew scenario,3131
The maximal imprecision is meaningful in connection with the system's frequency only.
In contrast, the skew captures the maximal time difference between corresponding signal transitions at the nodes, which reflects the quality of synchronization without requiring additional context.
 the set of nodes was split into four sets:•
A set A of two nodes with slow clock sources.
All communication to these nodes is maximally delayed.
•
A set B of two nodes with fast clock sources.
All communication to these nodes is minimally delayed.
•
Another set C of two nodes with fast clock sources.
All communication to these nodes is maximally delayed.
•
A set D of two faulty nodes.
These nodes always send propose to the nodes in B and do not send any other signals.3232
In our model, this behavior is mapped to a default signal at the receivers, e.g.
resync in the main state machine.
This setup leads to the following behavior of the main state machine (Fig. 8) once the system is stabilized.
The nodes in B∪C will always switch to propose first because their timeouts T3 expire (it is shown in [13] that at this time nexti=1 at all non-faulty nodes), and due to the "help" of the faulty nodes, the threshold of n-f=6 for switching to accept is reached at the nodes in B after the minimal delay.
It takes the maximal delay until the nodes in A realize that 4⩾f+1 nodes reached state propose and switch to this state.
Since 4<n-f, the nodes in A∪C require the support of the nodes in A to follow to state accept.
Hence, this happens another maximal delay later.
The resulting scenario is depicted in Fig. 16.
Assuming that the communication delay is at most d and at least dmin⩾0, we predict a skew of at most 2d-dmin between the nodes in B switching to accept and the nodes in A∪C catching up.
The experimental results confirmed the analytic predictions as being essentially tight: Letting the fast nodes run at a speed of 3 MHz and the slow nodes at 2.5 MHz, and setting the maximum delay d to about 3.6 μs (9 clock cycles), we observed a skew of about 6 μs.
This is consistent with the relatively large minimum delay dmin arising in our testbed.
A logic analyzer screenshot is depicted in Fig. 17.
The figure also demonstrates the capability of FATAL+ to generate pulses with significantly less skew (1 μs) on top of the FATAL pulses with worst-case skew.
Metastability experiments
We run a series of experiments dedicated to finding situations that potentially lead to metastable upsets.
We repeatedly set up 8-node systems with randomly chosen clock speeds between 2.5 MHz and 3.25 MHz and communication delays of at most 16 clock cycles.
While the system stabilized from these random initial states, we monitored the nodes' HSM state transitions after stabilization for multiple active conflicting state transitions during a period of over 60 h in total.
As predicted by our theoretical findings, in none of the experiments two conflicting guards were ever active at the same (global testbench) time after stabilization.
Stabilization time experiments
We evaluated stabilization time both in the absence and in the presence of faulty nodes.
In the latter case, we demonstrated the influence of the choice of the random timeout R3 on the stabilization time.
Stabilization in the absence of faulty nodes
To evaluate stabilization times in the absence of faulty nodes, we set up an 8 node system and run over 250000 experiments in each of which the nodes booted from random initial states, with randomly chosen clock speeds between 2.5 MHz and 3.25MHz=2.5ϑMHz, and message delays of up to d=16 clock cycles.
As soon as all nodes switched to state accept within 2d time, the FPGA signaled the μC to record the elapsed time and start the next experiment.
A considerable fraction of the scenarios (over 45%) stabilizes within less than 0.035 s (less than 5500d), which can be credited to the fast stabilization mechanism intended for individual nodes resynchronizing to a running system (see Fig. 18).
The remaining runs (see Fig. 19; please mind the different y-axis scale) stabilize, supported by the resynchronization routine, in less than 12 s (about 1.9⋅106d), which is less than the system's upper bound on R3 of approximately 14.9 s (about 2.3⋅106d) and significantly less than the system's upper bound on Tslow given in Theorem 5.2, which is no more than 44.5 s (about 7⋅106d) in this scenario.
Note that the stabilization time is inversely proportional to the frequency, i.e., in a system that is not artificially slowed down, stabilization is orders of magnitude faster.
For example, assuming d=1ns, we obtain that over 45% of the experiments stabilize within 5.5 μs, and all experiments stabilize within 1.9 ms.
Experimental results carried out for a 4-node system were analogous.
Either the main algorithm was capable to stabilize by itself (as for a large fraction the experiments in the head of the distribution), or once the resynchronization algorithm provided support after R3 expired at some node and n-f nodes switched to resync in approximate synchrony (the experiments in the tail of the distribution).
Fig. 20 shows stabilization by the resynchronization algorithm in a 4-node system: Eventually, all nodes switch to state none.
A node whose timeout R3 expires at a time when all timeouts (R2,supp) are expired, say node 1, forces all nodes from none into supp1.
Additional R3 timers, expiring at other nodes, may only force nodes into suppj, with j≠1, but do not prevent nodes from eventually communicating supp to all other nodes.
Thus nodes finally switch to supp→resync and from there to resync, in synchrony.
This again suffices to deterministically stabilize the nodes' main algorithm (as shown in [13]).
Note that the condition that all corresponding R2 timeouts are expired when a timeout R3 expires (actually, n-f suffice) will eventually be satisfied.
This happens at the latest when R3 expires for the second time at some node, simply because the distribution of the randomized timeout R3 guarantees that the picked duration is always larger than (roughly) ϑR2.
Stabilization with Byzantine nodes and deterministic timeouts
The importance of timeout R3 being randomly distributed is demonstrated in the following experiment.
We set up a 4-node FATAL+ system with one Byzantine faulty node, say node 4, and chose all R3 to be equal and initially synchronized, i.e., all R3 timeouts expire at about the same time at all correct nodes.
If the Byzantine node knows when R3 is going to expire, it can prohibit correct nodes from simultaneously switching to resync, thereby preventing synchronization of the Main Algorithm and thus stabilization: Shortly before R3 expires at the correct nodes, it sends init to two nodes, say 1 and 3, making them switch to supp4.
Subsequently, however, it only supports node 1 by sending supp to it.
This forces node 1 to switch to supp→resync and then resync alone.
While node 1 is in resync (i.e., while R1 is running), it does not support other nodes by sending supp.
Specifically, it does not support nodes 2 and 3 when they switch to supp1.
Eventually all nodes switch back to none, and the scenario can be repeated.
Fig. 21 depicts the scenario and Fig. 22 shows a logic analyzer screenshot of this experiment.
Note, however, that by definition of the probability distribution of R3, executions where R3 expires in synchrony at all correct nodes forever occur with probability 0.
We remark that there is always a nonzero probability that the randomly chosen durations of the timeouts R3 at non-faulty nodes align in a fortunate manner, so that stabilization could not be prevented even by an omniscient adversary orchestrating clock drifts, message delays, and faulty nodes.
While the probability of such a convenient event occurring in O(n) time decreases exponentially in the number of nodes n, it is reasonably likely for n=7 and in particular n=4 (i.e., systems that tolerate f=2 or f=1 faults, respectively).
This observation has been verified for n=4 by the first of the two experiments below.
Stabilization with Byzantine nodes and probabilistic timeouts
Two experimental setups were chosen to test stabilization in the presence of Byzantine nodes, using probabilistic timeouts R3 for correct nodes.
In the first experiment, a Byzantine node has access to the timeout values of all nodes as soon as they start their R3 timers.
In this case, the Byzantine node followed the strategy from before, obstructing any stabilization attempt that would otherwise be successful.
We observed that the Byzantine node was able to block at most one stabilization attempt of each non-faulty node.
Then it failed to prevent stabilization because the R2 timeouts corresponding to the Byzantine node did not expire on time before some non-faulty node successfully initialized stabilization.
In the second experiment, the Byzantine node has no access to timeout values, and therefore simply sends inconsistent init and supp signals as often as allowed by the timeouts R2 corresponding to it.
We did not observe any inhibited synchronized switch to resync when R3 expired at a correct node, however.
It should be noted that weaker adversaries and "better" initial configurations result in a constant stabilization time, irrespectively of the number of nodes n (see [13] for details).
The second experiment above demonstrates such a case; Theorem 5.3 states another.
The common-case stabilization time will therefore be considerably smaller than the (probabilistic) worst-case bound that is linear in n.
Conclusions
In this work, we introduced a novel modeling framework for self-stabilizing, fault-tolerant asynchronous digital circuits and demonstrated its applicability to our recently introduced FATAL+ clock generation scheme for multi-synchronous GALS architectures.
Our framework enables to reason about high-level properties of the system based on the behavior of basic building blocks, at arbitrary granularity and in a seamless manner.
At the same time, the hierarchical structure of the model permits to do this in a fashion amenable to formal analysis.
We believe this to be the first approach concurrently providing all these features, and therefore consider it as a promising foundation for future research in the area of fault-tolerant digital circuits.
As the conclusion of our paper, we now assess to which extent the properties of our implementation of the FATAL+ algorithm, which have been expressed and verified within our modeling framework and tested experimentally, meet our design goals.
Furthermore, we will discuss a number of potential improvements and future research avenues.
Our exposition will follow the optimization criteria listed in Section 2.1.7.•
Area consumption: For a suitable implementation, the total number of gates is O(nlogn) per node.
This can be seen by observing that the complexity of the threshold gates is dominating the asymptotic number of gates, since the O(n) remaining components of a node have a constant number of gates each; using sorting networks to implement threshold gates, the stated complexity bound follows [48].
Trivially, this number of gates is a factor of O(logn) from optimal.
We conjecture that in fact this complexity is asymptotically optimal, unless one is willing to sacrifice other desirable properties of the algorithm (e.g. optimal resilience).
Assuming that the gate complexity of the nodes adequately represents the area consumption of our algorithm, we conclude that our solution is satisfactory in that regard.
•
Communication complexity: Our implementation uses 7 (1-bit) wires per channel, and sequential encoding of the states of the main state machine would reduce this number to 5.
All communication are broadcasts.
Considering the complexity of the task, there seems to be very limited room for improvement.
•
Stabilization time: Our algorithm has a stabilization time of O(n) in the worst case.
Recent findings [49] show that a polylogarithmic stabilization time can be achieved at a low communication complexity; however, this comes at the expense of suboptimal resilience, a weaker adversarial model, and, most importantly, constants in the complexity bounds that make the resulting algorithm inferior to our solution for any practical range of parameters.
Moreover, as formalized in [13] and demonstrated in Section 7, for a wide range of scenarios our algorithm achieves constant stabilization time.
Considering the severe fault model, we conclude that despite not being optimal, our algorithm performs satisfactory with respect to this quality measure.
•
Resilience: It is known that 3f+1 nodes are necessary to tolerate f faults [25,14] unless cryptographic tools are available.
Since the complexity incurred by cryptographic tools is prohibitive in our setting, our algorithm features optimal resilience.
•
Delays: As mentioned, the delay of wires is outside our control.
Taking dmin+ and dmax+ into account in the quick cycle machine, we make best use of the available bounds in terms of the final frequency/synchrony trade-off.
The delays incurred by the computations performed at nodes are proportional to the depths of the involved circuits.
Again, the implementation of the threshold gates is the dominant cost factor here.
The sorting network by Ajtai et al. [48] exhibits depth O(logn).
Assuming constant fan-in of gates, this is clearly asymptotically optimal if the decision when to increase the logical clock Lv next indeed depends on all n-1 input signals of v from remote nodes.
We conclude that, so far as within our control, the design goal of minimizing the incurred delays is met by our algorithm.
•
Metastability: We discussed several effective measures to prevent metastability in Section 6.
Our experiments support our theoretical finding that, after stabilization, metastability may not occur in absence of further faults.
However, since metastability is an elusive problem for which it is difficult to transfer insights and observations to other modes of operation of a given system-let alone to different implementation technology-a mathematical treatment of metastability is highly desirable.
Our model opens up various possible approaches to this issue.
For one, it is feasible to switch to a more accurate description of signals in terms of signals' voltages as continuous functions of time.
Another option choosing an intermediate level of complexity would be to add an additional signal state (e.g. ⊥) for "invalid" signals, representing e.g.
creeping or oscillating signals.
Assigning appropriate probabilities of metastability propagation and decay to modules, this would enable a unified probabilistic analysis of metastability generation, propagation, and decay within a modeling framework using discrete state representations.
Such an approach could yield entirely unconditional guarantees on system recovery; in contrast, our current description requires an a priori guarantee that metastability is sufficiently contained during the stabilization process.
•
Connectivity: The algorithm presented in this work requires to connect all pairs of nodes and is therefore not scalable.
Unfortunately, it is known that Ω(n2) links are required for tolerating f∈Ω(n) faults in the worst case [26,27].
We argued for the assumption of worst-case behavior of faulty nodes; however, it appears reasonable that typical systems will not exhibit a worst-case distribution of faults within the system.
Indeed, many interesting scenarios justify to assume a much more benign distribution of faults.
In the extreme case where faults are distributed uniformly and independently at random with a constant probability, say, 10%, of a node being faulty, node degrees of Δ∈O(clogn) would suffice to guarantee (at a given point in time) that the probability that more than Δ/9 neighbors of any node are faulty, is at most 1-1/nc.
Note that this implies that the mean time until this property is violated polynomially grows with system size.
Using the FATAL+ protocol in small subsystems (of less than Δ nodes), system-wide synchronization will be much easier to achieve than if one would start from scratch.
In this setting, Δ∈O(logn) would replace n in all complexity bounds of the FATAL+ algorithm, resulting in particular in gate complexity O(lognloglogn) per node, computational delay O(loglogn), and stabilization time O(clogn) with probability 1-1/nc.
Thus, this approach promises "local" fault-tolerance of Ω(Δ) faults in each neighborhood in combination with excellent scalability in all complexity measures, and realizing this is a major goal of our future work.
•
Clock size: The constraint (1) entails that either clock size is bounded or large clocks result in larger stabilization time.
This restriction can be overcome if we use the clocks of bounded size generated by FATAL+ as input to another layer that runs a synchronous consensus algorithm in order to agree on exponentially larger clocks [41].
 Finally, we would like to mention two more prospective extensions of our work.
First, building on our modeling framework, it seems feasible to tackle an even more strict verification of the algorithm's properties than "standard" mathematical analysis.
The hierarchical structure and formal specifications of modules seem amenable to formal verification methods.
Such an approach should benefit from the possibilities to adjust the granularity of the model by the distinction between basic and compound modules as well as the restrictions imposed by the module specifications; more restrictive modules may be simpler to analyze, yet will guarantee the same properties as the stated variants.
Second, it should be noted that it is straightforward to derive clocks of even higher frequency from the FATAL+ clocks.
This is essentially done by frequency multiplication, at the expense of increasing the clock skew.
We refer to Dolev et al. [13] for details.
Overall, we consider the present work an important step towards a practical, ultra-robust clocking scheme for SoC.
We plan to address the open problems discussed above in the future, and hope that this will ultimately lead to dependable real-world systems clocked by variants of the scheme proposed in this article.
Acknowledgments
We would like to thank the anonymous reviewers for their valuable comments.

Rigorously modeling self-stabilizing fault-tolerant circuits: An ultra-robust clocking scheme for systems-on-chip

Abstract
We present the first implementation of a distributed clock generation scheme for Systems-on-Chip that recovers from an unbounded number of arbitrary transient faults despite a large number of arbitrary permanent faults.
We devise self-stabilizing hardware building blocks and a hybrid synchronous/asynchronous state machine enabling metastability-free transitions of the algorithm's states.
We provide a comprehensive modeling approach that permits to prove, given correctness of the constructed low-level building blocks, the high-level properties of the synchronization algorithm (which have been established in a more abstract model).
We believe this approach to be of interest in its own right, since this is the first technique permitting to mathematically verify, at manageable complexity, high-level properties of a fault-prone system in terms of its very basic components.
We evaluate a prototype implementation, which has been designed in VHDL, using the Petrify tool in conjunction with some extensions, and synthesized for an Altera Cyclone FPGA.
Highlights
•
We introduce a novel modeling framework for fault-tolerant VLSI circuits.
•
We cast a self-stabilizing clocking scheme from a companion article in this model.
•
We discuss the implications of theory and model for the resulting implementation.
•
We present the measures taken to avoid metastable upsets despite faults.
•
We provide experimental data from a prototype FPGA implementation of the algorithm.

Introduction & related work
In the past, computers have essentially been viewed as monolithic, synchronous, fault-free systems.
If at all, fault-tolerance has been introduced (i) to deal with limited, specific failures (e.g. errors in communication or data read from storage, which are usually handled via error-correcting codes), and (ii) at the level of distributed systems comprised of multiple machines that are fault-prone or subject to attacks (e.g. data centers or peer-to-peer applications, which use some form of replication).
Except for critical systems and extreme operational conditions (e.g. medical or aerospace applications [1]), there has been little motivation to build systems that are robust on all levels from scratch, a process that involves redesigning-or even reinventing-the very basics of how computations are organized and performed.
Due to the tremendous advances of Very Large Scale Integration (VLSI) technology, this situation has changed.
Enabled by ever decreasing feature sizes and supply voltages, modern circuits nowadays accommodate billions of transistors running at GHz speeds [2].
As a consequence, the assumption of chip-global (not to speak of system-global) synchrony [3] and no (or restricted) faults gradually became outdated [4].
Improved process technology and architectural-level fault-tolerance measures are common nowadays, and the lack of global synchrony has been tackled by accepting a certain level of asynchrony between different parts of the system.
In the most extreme form of this approach, computations are completely unsynchronized at all levels [5], which requires to synchronize all dependent activities (like sending and receiving of data) explicitly via handshaking.
In contrast, Globally Asynchronous Locally Synchronous (GALS) systems [6] make use of local clock sources to drive synchronous computations within each clock domain.
Note that, in the wider sense, most multiprocessors fall into this category, as there is usually no single common clock that drives all processors.
GALS systems again can be divided into two general classes: One that operates asynchronously at the inter-domain level, and the other consisting of multi-synchronous systems [7,8] that provide some, albeit reduced, degree of synchronization among clock domains.
The former class suffers from the drawback that, for inter-domain communication, either strong synchronizers or stoppable clocks must be foreseen [9].
After all, every bit of the sender's data must have stabilized at the receiver before the clock edge used for reading the data occurs.
This is avoided in multi-synchronous systems, where high-speed inter-domain communication via FIFO buffers can be implemented due to the available global synchronization [10].
Since the latter abstraction is also very useful for other purposes, multi-synchronous GALS is preferable from the viewpoint of a system-level designer.
Naturally, establishing inter-domain synchronization comes at additional costs.
While it is not too difficult to achieve and maintain in the absence of faults [11,12], the issue becomes highly challenging once faults of clocking system components enter the picture.
Contribution
We present an FPGA prototype implementation of a distributed clock generation scheme for SoC that self-stabilizes in the presence of up to f<n/3 faulty nodes.
It incorporates the pulse algorithm from [13] that tolerates arbitrary clock drifts and allows for deterministic recovery and (re)joining in constant time if n-f nodes are synchronized; it stabilizes within time O(n) with probability 1-2-n from any arbitrary state.
An additional algorithmic layer that interacts weakly with the former provides bounded high-frequency clocks atop of it.
Nodes executing the compound algorithm broadcast a mere constant number of bits in constant time.
The formal proofs of the properties of the pulse synchronization algorithm and the derived high-frequency clocks are given in [13].
Deriving an implementation from the specification of the algorithm in [13] proved to be challenging, as the high-level theoretical model and formulation of the algorithm in [13] abstracts away many details.
Firstly, it assumes a number of basic self-stabilizing modules above the level of gates and wires to be given.
We devise and discuss self-stabilizing implementations of these building blocks meeting the specifications required by the high-level algorithm.
Secondly, the algorithm's description is in terms of state machines performing transitions that are non-trivial in the sense that they do not consist of switching a single binary signal or memory bit.
This requires careful consideration of metastability issues, since these state transitions are triggered by information from different clock domains.
In order to resolve this issue, we introduce a generic Hybrid State Transition Machine (HSTM) that asynchronously starts a local synchronous execution of a state transition satisfying the model specification from [13].
Related to this matter, we thirdly discuss in detail how the algorithm and its implementation make a best effort to guard against metastable upsets.
Here, we try to get the best out of the design decisions and rely on synchronizers only where absolutely necessary.
These non-trivial implementation issues and the complex interactions between the basic building blocks raise the question under which circumstances the high-level properties of the algorithm shown in [13] indeed hold for the presented implementation.
To answer this question, we devised a model that is able to capture the behavior of the constructed modules, including faults, resilience to faults, and self-stabilization, in a hierarchical fashion.
By specifying the desired behavior of modules in terms of the feasible output generated in response to their inputs, we can also reason about the behavior of (implementations of) modules in a hierarchical manner.
This property is crucial, as it permits to determine conditions under which our implementation indeed satisfies the requirements by the abstract model used in [13], and then soundly conclude that if these conditions are met, all statements made in [13] apply to our implementation.
Since our approach is highly generic and permits to adjust the granularity of the description in order to focus on specific aspects of the system, we believe it to be of general and independent interest in the context of devising fault-tolerant systems.
In order to verify the predictions from theory,33
Or, to be scientifically accurate, we rather successfully failed at falsifying them.
Our implementation primarily serves as a proof of concept, as clearly an FPGA implementation can merely hint at the properties of an ASIC.
 we carried out several experiments incorporating drifting clocks, varying delays, and both transient and permanent faults.
This necessitated the development of a testbed that can be efficiently controlled and set up for executing a large number of test runs quickly.
In our 8-node prototype implementation, the compound algorithm generates 8-bit clocks that in all runs stabilized within 1.9⋅106d time (where d is the maximal end-to-end communication delay).
In our testbed, which runs at roughly 100 kHz, this amounts to less than 12 s.
For a system running at GHz speed, this translates to about a millisecond.
We also observed that the deterministic stabilization mechanism designed for more benign conditions operates as expected, recovering nodes by about two orders of magnitude faster.
Organization of the article
In the next section, we summarize the obstacles and design goals that need to be considered for clock synchronization in our setting; we also introduce the basic building blocks assumed in [13], which perform typical operations used by fault-tolerant synchronization algorithms.
Section 3 introduces the formal model, alongside illustrating examples and proofs of some basic properties.
Subsequently, in Section 4 we cast the modules informally discussed earlier in our formal framework, and interpret nodes, protocols, and the synchronization problem as modules as well.
In Section 5, we move on to the description of the algorithm from [13] in terms of this framework.
We provide high-level intution on the purpose of its various components and summarize the main statements proved in [13].
Section 6 follows up with presenting our implementations of the basic modules specified in Section 2.2, including the HSTM.
In this context, we will also cover our efforts to minimize the probability for metastable upsets.
In Section 7 we describe the testbed setup, the experiments, and their results.
Finally, in Section 8 we evaluate to what extent our design goals are met and give an outlook on future work.
On-chip clock synchronization
Our goal is to design a scalable hardware clock generation scheme that is resilient to arbitrary transient and permanent faults and carefully minimizes the risk of metastability.
We will now discuss our objectives in more detail and explain why tackling them in conjunction proves to be much harder than achieving them individually.
In accordance with standard notions, in the following we will refer to clock domains as nodes, as they represent the smallest "independent" algorithmic building block we use.
This is to be understood in the sense that we consider a node faulty if any one of its components is faulty, and non-faulty otherwise (irrespectively of whether other nodes behave correctly or not).
Denoting by [i..j] the set {k∈N|i⩽k⩽j}, ultimately, each correct node i∈[1..n] must at all times t output a (discrete) logical clock Li(t)∈N that fulfills certain properties despite the aforementioned obstacles; most obviously, we strive for minimizing maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)}.
Challenges
Inexact local clocks and unknown message delays
When synchronizing clocks, one needs to face that clocks are not perfect and that it cannot be exactly determined how much time it takes to communicate a clock reading.
These fundamental uncertainties entail that synchronization can never be perfectly accurate and must be an ongoing process [14].
We formalize these notions as follows.
Each node i∈[1..n] can make use of local clocks that are inexact and therefore drift (i.e., do not progress at the same rate).
Since we are only concerned with synchronizing clock domains with each other, we do not care about Newtonian time.
Instead, we describe the system in terms of a reference time satisfying that any correctly operating clock progresses at a speed between 1 and some constant ϑ with respect to the reference time t∈R.
A (local) clock C:R→R that is correct during a period of reference time [t-,t+]⊆R guarantees that ∀t, t′∈[t-,t+], t<t′: t′-t⩽C(t′)-C(t)⩽ϑ(t′-t) (in particular, C is continuous and strictly increasing during [t-,t+]).44
We use real-valued, unbounded clocks here to simplify the presentation.
It will later become clear that the algorithm can indeed operate with discrete bounded clocks, as it does not need to access absolute clock values, but rather approximately measures bounded differences in time.
 In contrast to many "traditional" synchronization settings, we would like to tolerate quite large relative clock drifts ϑ-1 of up to about 20%, as accurate and stable oscillators are not available in a System-on-Chip (SoC) at low costs.
Tolerating such large drifts permits to utilize very simple ring oscillators even under heavily varying conditions (temperature, supply voltage, etc.) [15].
Node i communicates with node j via an abstract FIFO channel that (if correct) continuously makes i's state available to j, albeit delayed by an unknown value between 0 and the maximal delay d.
We denote the input port of the channel from node i to node j by Si and its output port by Sj,i.
Node i also loops back its own state to itself on a channel.
The time required for computations that are triggered by some communicated information is accounted for by d as well, i.e., d is an end-to-end delay.55
This is the reason why we speak of an abstract channel.
We will later introduce the (physical) channels that essentially represent the wires on the chip; the maximal delay d is then the sum of the maximal delay of the physical channels and the computing elements of the nodes.
 For the sake of a straightforward presentation, throughout this article we assume that all channels from node i to some node j are part of node i, i.e., faults of the channel are mapped to the sender node.
We remark, however, that a more detailed treatment (as e.g.
in [16]) can be beneficial and is supported by the modeling framework underlying this work.
Transient faults
Increasing soft error rates of modern VLSI circuits [17], originating in ionizing radiation [18-21], cross-talk, and ground bouncing [22,23], make it vital to allow for recovery from transient faults.
The most extreme transient fault scenario is that the entire system undergoes a period of an unbounded number of arbitrary faults.66
The only restriction is that transient faults do not affect the non-volatile memory (and in particular not the algorithm itself), as this would induce a permanent fault.
 Algorithms that are capable of re-establishing regular operation after transient faults cease are called self-stabilizing [24].
This requirement is equivalent to stating that, if the system is fault-free, the algorithm converges to a valid state from an arbitrary initial configuration within a bounded time; we refer to this period as stabilization time.
Due to this equivalency, self-stabilizing algorithms have the additional advantage of requiring no initialization, i.e., a self-stabilizing clocking system does not need to be booted with any initial synchrony.
For self-stabilizing algorithms, stabilization time is obviously an important quality measure.
As the fundamental time unit of the system is d, i.e., the time span it takes to effectively communicate and process any piece of information with certainty, guarantees on the stabilization time are clearly always some multiple of d; the respective prefactor typically is a function of the number of nodes n, the number of sustainable or actual permanent faults, and the clock drift ϑ.
In our context, the stabilization time is not only of relevance to whether waiting for stabilization is bearable in terms of the down-time of the system; it is important to understand that a failure of the synchronization layer will quickly result in incoherencies of operations on higher layers, entailing the threat of data loss or corruption, potentially without any possibility of future recovery.
Because of the need of maintaining accurate synchronization in the presence of drifting clocks, quite a few clock synchronization algorithms are self-stabilizing.
In fact, conventional clock trees [3] are trivially self-stabilizing-after all, they simply disseminate the signal of a single oscillator throughout a chip.
However, they cannot cope with any permanent fault of the clock source or the network distributing the clock.
Similarly, one could easily make a system comprising several clock sources self-stabilizing, by picking one master clock and letting all other clocks synchronize to it.
Again, this simplistic approach will fail if the master or its outgoing communication channels become faulty.
Permanent faults
Sustaining functionality in the presence of permanent faults necessitates redundancy.
More precisely, it is known that tolerating f worst-case faults (traditionally called Byzantine faults in this context) is impossible if n⩽3f (without cryptographic assumptions) [14,25].77
Allowing cryptography would still necessitate n>2f [26,27]; we hence discard this option due to the additional complexity incurred.
 Hence, natural questions are whether assuming worst-case failures is too demanding and whether the fault model could be relaxed in order to circumvent the lower bound.
Unfortunately, examining the lower bound reveals that it originates in the ability of a faulty node to communicate conflicting information to different receivers.
This behavior can easily emerge from a faulty output stage in a circuit: If an analog voltage level in between the range for a valid "1" and that for a valid "0" is evaluated (for example due to a timing fault, a glitch on a signal line, or a defective driver output) by more than one receiver, some might read a "1" while others read a "0".
Note that this is a fundamental problem, as mapping the continuous range of possible voltages to discrete binary values entails that there is always a critical threshold close to which it is impossible to ensure that all receivers observe the same binary value.
It is still an option to argue about the spatial distribution of (permanent) faults within the system, though, as we discuss in Section 8.
However, in this article, we consider the worst case, which also motivates the choice of full connectivity88
We are aware that this constitutes a serious scalability issue; again we refer to the discussion in Section 8.
 between the nodes due to a respective impossibility result [26,27].
This lower bound entails that, due to their low connectivity, most existing distributed clock generation schemes [11,12,28,29] cannot cope with a reasonable number of worst-case faults.
Nonetheless, dealing with up to f faults in a fully connected system of n⩾3f+1 nodes is-at least from a high-level perspective-still fairly easy, provided that we can rely on synchronization already being established.
To illustrate this, consider the simple state machine of a node given in Fig. 1.
In the figure, the node's states are depicted in circles and the feasible state transitions are indicated by arrows.
A node switches, for example, from state ready to state propose if the condition next to the arrow is satisfied.
In this example, this means that either 3ϑ2d time has passed on its local clock since it switched to state ready or its incoming channels (including its loop-back channel) showed at least f+1 other nodes in state propose since it switched to state ready.
This behavior is realized by each node i∈[1..n] having (binary) memory flags proposei,j for each node j∈[1..n]: Node i's flag proposei,j is set to 1 at a time t iff Si,j(t)=propose and the flag was in state 0 before.
The flag is reset to 0 on node i's state transition to ready (in the figure indicated by the rectangular box on the respective arrow).
Deciding whether the transition condition is satisfied at time t thus boils down to checking whether the timeout condition is satisfied or at least f+1 of the propose memory flags are in state 1.
Now assume that each node runs a copy of this state machine, and at least n-f non-faulty nodes enter state increase during some time window [t,t+2d).
As local clocks run at speeds between 1 and ϑ, all nodes will switch to state ready during [t+3d,t+2d+3ϑd).
Hence, at the time when a node switches to ready, the delayed state information on the channels will not show non-faulty nodes in state propose any more.
Therefore, no non-faulty node will switch to propose again due to memorizing f+1 nodes (at least one of which must be non-faulty) in state propose before the first non-faulty node switches to propose.
Thus, the latter must happen because 3ϑ2d local time passed on a local clock, which takes at least until time t+3d+3ϑd>t+2d+3ϑd.
By this time, all nodes will have switched to ready.
This implies that at the time when the first node switches to increase again (which eventually happens because all n-f non-faulty nodes switch to propose), all nodes will already have switched to ready.
Given that n⩾3f+1, we have that n-2f⩾f+1, i.e., if at some non-faulty node n-f channels show state propose, any node will observe f+1 channels in this state (though due to delayed communication maybe not at exactly the same instance in time).
This implies that at most d time after the first node switched to increase again, all non-faulty nodes have switched to propose.
Another d time later, all n-f non-faulty nodes will have become aware of this and have switched to increase, i.e., within a time window of 2d.
Repeating this reasoning inductively and assuming that the nodes increase their logical clocks (that initially are 0) by 1 whenever they switch to increase, well-synchronized logical clocks are obtained: The maximum difference in time between any two correct nodes performing their kth clock tick, the skew, is at most 2d for the above algorithm.
A variation of this simple technique [30] is known for long and a closely related approach called DARTS has been implemented in hardware [31,32].
However, all these algorithms are not self-stabilizing.
In fact, even if clocks would not drift, the delay d was arbitrarily small, and there was only a single faulty node (i.e., even if we allow for f>1, only one node is actually faulty), they still would not stabilize.
To see this for the algorithm given in Fig. 1, first consider the following execution with n=3f+1, part of which is depicted in Fig. 2.
The correct nodes are split evenly, into three subsets Ai, i∈{1,2,3}, of size f.
Set A1 initially is in state ready, with all memory flags corresponding to nodes in A3 in state 1 and all other flags in state 0.
The nodes in A2 and A3 are in state increase, with the timers of nodes in A2 having progressed halfway towards expiring and the timers in A3 just started (i.e., these nodes just left propose), and their propose signals are memorized by nodes in A1.
Just when the nodes in A2 are about to switch to ready, the faulty node sends propose signals to the nodes in A1, causing them to switch to propose.
They will send propose signals, once receiving them memorize 2f+1=n-f nodes in state propose, and thus proceed to state increase.
However, the nodes in A2 will still observe the propose signals of the nodes in A1 after resetting their memory flags upon switching to ready.
Thus, we end up in the same situation, except that Ai (indices modulo 3) takes the role of Ai-1.
Repetition yields an execution that never stabilizes and has 3 sets of grossly desynchronized nodes that are not faulty.
This execution can be generalized to n=kf+1 for integers k⩾3: we split the correct nodes in k sets of size f and make them proceed equidistantly spread in time through the cycle.
The difference is that now more than one group will linger in states ready or propose upon arrival of the next; the crucial point is that the single faulty node retains control over when groups proceed to state increase.
The cases n=kf+2 and n=kf+3 require more involved constructions; it should be intuitive, though, that with 2 actually failing nodes the above construction can be modified to operate with one or two of the sets containing f+1 nodes.
Combining transient and permanent faults
Combining self-stabilization and resilience to permanent faults results in much more robust systems.
Both properties synergize in that, as long as at all times there is some sufficiently large set (not necessarily the same!) of nodes that is non-faulty, an arbitrary number of transient faults is transparently masked, i.e., the system remains operational even though over time each individual component may repeatedly undergo transient failures and recover from them.
This drastically increases the mean time until overall system failure: In a system that is not resilient to permanent faults, any fault will result in an immediate breakdown of guaranteed properties, whereas a system that is not self-stabilizing will fail (and might not recover without an external reboot) once the sum of faults exceeds one third of the nodes.99
One could compromise by guaranteeing that nodes recover in bounded time, provided that the number of faults is never overwhelming.
In fact, the algorithm presented in this article has the property that in this case nodes will recover faster and deterministically (in contrast to the slower, probabilistic stabilization from arbitrary system states).
However, sacrificing stabilization from arbitrary states will not reduce the complexity of the algorithm significantly, and theory strongly indicates that the respective gain is limited to a constant factor in general.
There is a considerable body of work on distributed synchronization algorithms that are self-stabilizing as well as resilient to permanent faults.
However, until recently, there has been no solution worth considering for hardware implementation.
Known algorithms exhibit a prohibitively large communication complexity (i.e., nodes send Ω(n) bits over each channel in constant time) [33,34], incur an exponential stabilization time [35], require exponentially small clock drifts [36], or require much stronger assumptions on the system's behavior [37].
Recently, we proposed an approach that does not suffer from such drawbacks [13,38,39], whose implementation is the subject of this work.
Metastability
In our specific setting, minimizing the potential for metastability is particularly demanding.
Metastability results from violating a stateful circuit's input timing constraints, e.g., by changing the data input of a flip-flop at the time of the clock transition.
While this can be safely avoided during normal operation, a faulty node might exhibit arbitrary timing and hence cause such a violation.
As this can never be prevented in the first place if worst-case faults are considered, it is mandatory to guard the channels against propagating metastability, e.g.
by using synchronizers.
In order to minimize the required length of synchronizer chains, decreasing latency and area consumption (the latter also on higher layers of the system), however, it is beneficial to avoid the potential for upsets by construction wherever possible.
Apart from the (unavoidable) threat originating from faulty nodes, safely preventing timing violations is hindered by the lack of a common time base during the stabilization phase after an excessive number of transient faults.
It has been shown that it is impossible to guarantee with certainty that no metastable upsets occur if the system is in an arbitrary initial state, even if all nodes adhere to the protocol [40].
Careful design is thus required in order to minimize the probability of upsets during stabilization, in particular since such upsets might obstruct the stabilization process.
Once the system stabilized, i.e., the non-faulty nodes are synchronized, the algorithm can use this synchronization to structure communication in a way that entirely avoids metastable upsets caused by non-faulty nodes.
Thus, in the absence of faults, we require that the system operates metastability-free.
Note that even this seemingly simple task is not trivial, as one cannot employ the classical wait-for-all paradigm: Doing so would imply that just a single non-responsive node would cause the entire system to deadlock.
Therefore, when depending on other nodes in the decision to take a state transition, it is necessary to wait for at most n-f signals.
Safely reading signals thus cannot rely on handshaking, but must be based on suitable monotonicity and/or timing conditions (guaranteed by the use of memory flags and local clocks, for example).
The bounded-delay "interlocking condition" used in DARTS [32] and the simple algorithm in Fig. 1 are showcases for such techniques.
Operating frequency vs. clock precision
In order to be practical, the logical clocks need to run at a frequency in the GHz range.
While one could obviously utilize frequency multiplication to achieve this goal, this is not straightforward to build in the self-stabilizing context.
After all, clock multipliers involve complex devices like phase-locked loops and are hence not obviously self-stabilizing.
Moreover, for a fixed guaranteed skew (of say 2d), naive frequency amplification also increases the logical clock imprecision maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)} by the scaling factor, which may adversely affect certain services.
For example, the size of the FIFO buffers used for inter-domain communication in [10] depends on the clock imprecision and must hence be adapted accordingly.
On the other hand, by dividing frequencies, it is clearly possible to guarantee that maxi,j∈[1..n],t⩾0{Li(t)-Lj(t)}=1.
Therefore, it is an important design goal to minimize clock imprecision while at the same time maximizing the frequency at which clocks run.
Naturally, this becomes much more involved due to the design goals already presented.
Scalability
Being able to meet all the above design goals is meaningless if one cannot control the amount of resources devoted to the task of clock generation.
Pivotal issues are the following:•
Area consumption: The chip area used by the components of the synchronization algorithm decomposes into the area consumed by the nodes and their interconnections.
The former can be captured by the gate complexity, i.e., the number of (constant fan-in) gates required to perform the algorithm's computations.
The latter significantly depends on the chip layout, which is highly application-dependent and hence outside our scope of control.
It is clear, however, that the number of channels and their bandwidth play a crucial role.
•
Communication complexity: Apart from whether two nodes are connected or not, it is of interest how many wires are required.
This is well-represented by the bit complexity of an algorithm, i.e., the number of bits it exchanges per time unit between communication partners.
Note that while the number of wires can be reduced by means of time division, this will require additional memory and computational resources on the receiver's side and increase the communication delay.
In any case, it is highly desirable to devise algorithms of (small) constant bit complexity.
Moreover, broadcasting the same information to all nodes instead of different information to different receivers is to be preferred, as it allows us to use communication buses.
•
Stabilization time: For reasons stated earlier, we would like to minimize the stabilization time.
In particular, it is not good enough to know that an algorithm eventually stabilizes, as the required time might be well above what makes the algorithm self-stabilizing in any practical sense.
•
Resilience: The number f of faults that can be concurrently sustained without losing synchronization or the capability to stabilize should grow with system size, as otherwise a larger system will suffer from more frequent outages.
Note that while we must accept that stabilization is a random process (due to the unavoidable probability of metastable upsets), we demand that a system that is stable will always remain so as long as there are not too many faults (including upsets).
As mentioned earlier, a lower bound shows that always f<n/3, giving a precise meaning to "too many" here.
•
Delays: As the maximal delay d accounts both for the delay incurred by communication as well as computation, it is vital to minimize both.
Notwithstanding the fact that the communication delay and computing speed is mostly determined by parameters outside our control (technology, spatial distances, number of nodes, etc.), minimizing the gate complexity and, in particular, the depth of the circuits implementing the nodes' algorithms (that determine the computing delays) is important.
•
Metastability: In larger and faster systems, the number of events per time unit that could cause metastable upsets is obviously larger.
Therefore, it is vital to safely exclude metastability from occurring during regular operation by construction.1010
Note that in this regard our approach is superior to standard GALS systems using synchronizers, where the risk of metastability is immanent (at every clock transition) also in normal operation.
 We admit metastability only during rare exceptional phases of system operation where it cannot be avoided in principle, like during stabilization or in case of faults.
As the probabilities for metastable upsets are hard to quantify even in a final product, we do not use a "hard" measure here.1111
It is worth mentioning, though, that the asymptotic increase in the number of events per time unit that could cause metastable upsets is clearly at least polynomial in n.
As synchronizer chains decrease the probability of upsets exponentially, the required length of synchronizer chains will asymptotically grow as Ω(logn), increasing the system cost and (effectively) decreasing its operational frequency.
•
Connectivity: In order to facilitate efficient placement and routing on a chip, it is vital to ensure that the communication network is sparse.
Also, a sparse network will consume less area and is beneficial to fault containment.1212
If a single event such as e.g.
an ionizing particle hit can render multiple nodes faulty, even tolerating a large number of faulty nodes is of little use w.r.t.
the overall resilience of the system.
 Tackling this issue is subject to our future work and hence beyond the scope of this article, however.
•
Clock size: If the logical clocks have too few bits, i.e., overflow too frequently, they might be unsuitable for the application logic of the SoC.
The algorithm we present in this article can in principle provide clocks of arbitrary bounded size.
However, its stabilization time would grow linearly with the maximum clock value once we scale above 8-bit clocks.
In a recent publication, we show how to construct larger clocks efficiently [41].
Typical modules for clock synchronization protocols
We next introduce the basic modules that are assumed by the model used in [13].
We first give an intuitive description of the required modules.
Subsequently, we introduce a novel formal framework for specifying self-stabilizing fault-tolerant modules and specify our basic modules in this framework.
Any implementation satisfying this specification can be plugged into the high-level algorithm in order to yield a system guaranteeing the properties proved in [13].
We now list the building blocks beyond standard logic gates that will explicitly or implicitly be used by the algorithm presented in Section 5.
Each of these building blocks computes output signals that are constrained by (the history of) its input signals.
If the logic function implies an output transition in reaction to an input change, this transition is not required to occur immediately; it must occur within a known time bound, however.
Given the time bounds for the individual modules and the connecting wires, one can compute the maximum delay d.
Moreover, informally speaking, it must be avoided that a single change in the input(s) causes multiple transitions of the output signal, as this could undermine the high-level algorithm's logic.
Note also that statefulness, i.e., any sort of memory (including positive feedback loops), bears the potential for metastable upsets and requires careful attention in order to ensure self-stabilization.1313
In other words, the module must recover from arbitrary corruptions of its memory.
 Purely combinational elements, on the other hand, differ in their ability to prevent metastable inputs from reaching the output under certain conditions.
Each node will be a union of state machines that communicate via channels (both among each other and with remote nodes) and are composed of standard logic gates and all other modules we describe below.
Fig. 5 depicts such a state machine.•
Communication channels.
We previously introduced the communication channels from node i to node j as abstract devices that convey the states with a delay of at most d that also accounts for computations.
Viewed as a module, the (physical) communication channels do account for the time to communicate the state information only, whereas computations are performed by standard logic gates and the modules we will describe next.
A communication channel of this type simply maps its input signal to its output signal.
The reason why communication channels are nonetheless listed as modules here is that encoding a non-binary state signal in a glitch- and metastability-free manner is a non-trivial task, as in the absence of (reliable) synchrony both parallel and sequential communication present challenges.
In our abstraction, this encoding is performed by the channel, which requires additional logic and thus potentially results in delays beyond the mere wire delays as well as the necessity to consider issues concerning metastability and self-stabilization.
•
Memory flags.
These are just simple binary storage elements that can be set to 1 by means of one input signal and can be reset to 0 by a second input signal; their state is externally accessible via an output signal.
Simply put, a memory flag just "remembers" which input signal was 1 most recently.
In our algorithms, memory flags will be used to memorize wether an input signal from a remote node was in state 1 at some time after the most recent reset upon a state transition of one of the node's state machines (in Fig. 1, e.g., a node resets its propose flags when switching from increase to ready).
•
Threshold gates.
Frequently, nodes will need to decide whether a certain threshold number (f+1 or n-f) of signals (or sets of signals) satisfy some Boolean predicate (e.g., the conditions for switching to propose and increase in Fig. 1 involve such a threshold).
A threshold gate takes the respective binary input signals and outputs 1 if the threshold is reached and 0 otherwise.
•
•
Randomized watchdog timers.
A randomized watchdog timer (D,s,C) is a module with input port Si,i and output port TimeD,s,C, where D is a bounded random distribution on (0,D]⊂R+, s is a state, and C a clock.
The module specification of (D,s,C) is analogous to the module specification of a watchdog timer, except that property (Expire) is replaced by:-
(Expire') Denote by R⊂R the set of times when (T,s,C) is reset.
For each time tR∈R, denote by tE(tR) the unique time satisfying that C(tE)-C(tR)=T(tR), where T(tR) is drawn (independently) from D.
Then, for each t∈R, TimeT,s,C(t)=0 iff t∈⋃tR∈R[tR,tE(tR)).
 We apply the same notational conventions as for watchdog timers.
•
State transition modules.
Node i's state transition module has input ports Si,j for each node j∈[1..n] as well as one binary input port for each of the memory flags, (randomized) watchdog timers and threshold gates it uses.
Furthermore it has an output port Si as well as one binary Reset output port for each of the memory flags it uses.
A node's state transition module executes a state machine specified by (i) a finite set S of states, (ii) a function tr, called the transition function, from T⊆S2 to the set of Boolean predicates on the alphabet consisting of expressions of the form "p=s" (used for expressing guards), where p is from the state transition module's input ports and s is from the set of possible states of signal p, and (iii) a function re, called the reset function, from T to the power set of the node's memory flags.
Intuitively, the transition function specifies the conditions (guards) under which a node switches states, and the reset function determines which memory flags to reset upon the state change.
Formally, let P be a predicate on the input ports of node i's state transition module.
We define P holds at time t by structural induction: If P is equal to p=s, where p is an input port of node i's state transition module and s is one of the states signal p can obtain, then P holds at time t iff p(t)=s.
Otherwise, if P is of the form ¬P1, P1∧P2, or P1∨P2, we define P holds at time t in the straightforward manner.
For a given transition delay dTrans>0, the module specification ΦSTM of node i's state transition module is defined as follows.
Let Ein be an execution of the state transition module's input ports and Eout an execution of its output ports.
Then Eout∈ΦSTM(Ein) iff there is some ε>0 and a signal locked such that the following requirements are met.
(The intuition is that locked(t)=0 means that the node is ready to perform the next state transition once a guard becomes true, whereas in case of locked(t)=1 the node is currently executing a previously "locked" transition.)-
(Safety) The node (i.e., Si) does not switch states at any time t with locked(t)=0.
In every maximal interval [tl,tu)⊆R satisfying that locked≡1 on [tl,tu), it switches states exactly once.
-
(Delay) For each interval [tl,tu) as above, tu-tl⩽dTrans-ε.
-
(Guard) For each interval [tl,tu) as above, (Si(tl),Si(tu))∈T and tr(Si(t),Si(tu)) is satisfied at some time t∈[tl-ε,tl].
-
(Responsiveness) If locked(t)=0 and there is a state s∈S such that (Si(t),s)∈T and tr(Si(t),s) holds at time t, then locked(t+ε)=1.
-
(Flags) For an arbitrary interval [tl,tu) as above, suppose that the node switches from state Si(tl) to state Si(tu) at time ts∈[tl,tu).
Then for each memory flag specified by re(Si(tl),Si(tu)), the corresponding reset output port of the state transition module is in state 1 at some time in (tl,ts] (and therefore the flag is reset).
Outside these time intervals, reset ports are in state 0.
 A node may run multiple, say k∈N, state machines in parallel (i.e., contain several state machines as submodules).
In this case, its state signal is the joint signal Si=(Si1,…,Sik), where Sil, l∈[1..k], denotes the lth state machine of the node.
Throughout this article, the different state machines of each node i have disjoint state spaces.
For simplicity, we hence may say "node i is in state s at time t" instead of "state machine l of node i is in state s at time t" when referring to Sil(t)=s, etc.
To account for the latency of the memory flags, threshold gates and (randomized) watchdog timers, their ports are not directly connected to the state transition module's ports, but via binary communication channels with respective delays.
The resulting structure of the compound module node i is depicted in Fig. 5.
Note that additional communication channels at the threshold gates' and memory flags' input ports allow to model the fact that memory flags are not necessarily reset at the same time, and signals may arrive shifted in time at the threshold gates.
As mentioned earlier, for simplicity we consider the outgoing channels to remote nodes as part of the node.
Hence, the output ports of node i comprise the output ports Sj,i, j∈[1..n], of the channels disseminating its state Si.
In addition, in order to solve the actual problem of clock generation, we include the locally computed discrete clock value Li as an output port.
Protocols and problem formulation
We next formalize the concept of a protocol, like the one presented in Section 5, followed by what it means for a protocol to solve self-stabilizing clock synchronization in spite of f faults.
Formally, a protocol (for an n-node system) is a compound module consisting of n modules referred to as nodes.
The nodes are to be specified as modules themselves, and in our case will follow the layout we just described.
It thus remains to state in Section 5 which (randomized) watchdog timers, memory flags and threshold gates our protocol uses as well as the state transition modules' transition and reset functions.
A clock synchronization module with n∈N nodes, clock imprecision Σ, amortized frequency bounds A-,A+∈R+, slacks τ-,τ+∈R0+, maximum frequency F+, and at most f∈N faults is a module without input ports and with output ports Li, i∈[1..n].
Its module specification is extendable.
An execution of the module on R is feasible, iff there exists a subset C of [1..n] of size at least n-f satisfying that•
∀t∈R, i,j∈C: |Li(t)-Lj(t)|⩽Σ,
•
∀t, t′∈R, t<t′, i∈C: A-(t′-t)-τ-⩽Li(t′)-Li(t)⩽A+(t′-t)+τ+, and
•
∀t, t′∈R,t<t′, i∈C: Li(t′)-Li(t)⩽⌈F+(t′-t)⌉.
We say a protocol Π (for an n-node system) with no input ports and output ports Li, i∈[1..n], solves self-stabilizing clock synchronization with clock imprecision Σ, amortized frequency bounds A-,A+, slacks τ-,τ+∈R0+, maximum frequency F+, at most f faults, and stabilization time T (with probability p) iff it is an f-tolerant, (with probability at least p) T-stabilizing implementation of the clock synchronization module with the respective parameters.
A (real-world) implementation will output bounded clocks of size K∈N only.
In this case the output ports do not yield Li(t), but only Li(t)modK.
Nevertheless, we introduced the signals Li as abstract functions in this setting, as they allow to state the frequency bounds concisely.
Note that there is no physical counterpart of these values in the real-world system; to be strictly accurate, it would be necessary to qualify the above definitions further by "with bounded clocks of size K" in order to distinguish this version of the problem from the abstract one with unbounded clocks.
Practical implementability issues
Our formal model incorporates a precise semantics of what it means for a module to implement some other module, namely, inclusion of all feasible (sub-)executions.
Unfortunately, however, this strong requirement must often be relaxed when it comes to real implementations.
This is primarily a consequence of the fact that there is no physical implementation of a circuit that can avoid metastability.
Since preventing certain inputs to a module requires output guarantees from others, this is a challenging problem to systems that are self-stabilizing or tolerate persistent faults; combining these properties complicates this issue further.
More specifically, in order to faithfully implement their specifications, basic modules must be able to (i) deal with all possible inputs and (ii) recover reliably from transient faults.
Unfortunately, (i) is often impossible to achieve with real circuits.
For example, simultaneous input changes may drive any implementation of a Muller C-gate into a metastable state, which implies that its output ports do not even carry signals according to our definition, and are hence not feasible.
Of course, metastability can also be caused by physical faults affecting the module; such faults can obviously not be analyzed within our model either.
This possibility obviously invalidates any guarantees that compound implementations containing this instance may provide, unless they can mask the error due to fault-tolerance.
Moreover, real circuits cannot guarantee (ii) under all circumstances either, as it is impossible to always prohibit the propagation of metastable inputs to the outputs and the system may contain feedback-loops.
In principle, it would be possible to extend the presented model to cover also generation and propagation of metastability explicitly, by replacing the finite alphabet S and discrete events with a continuous range of signal values (the voltages) [43].
Since this would dramatically increase the complexity of any analysis, we choose a different approach that also allows us to handle other implementation intricacies in a pragmatic way.
In fact, even in the absence of metastability, it is not necessarily simple and even possible for real implementations to guarantee (ii) under all circumstances.
Apart from the fact that transient faults may lead to permanent errors by damaging physical components,1616
We remark that, technically speaking, excessively high voltages on the input wires could also be interpreted as an "input violation", as this violates the definition of our signals.
However, it makes sense to interpret such (hopefully exceptional) events as a fault of the module.
 our model does not prohibit that temporarily infeasible inputs result in permanent infeasibility, i.e., even when inputs become benign again at a later state of the execution of the module in question, there is no suffix of the execution that is feasible.
The oscillator implementation given in Example 3.6 demonstrates this issue, and further modules exhibiting persistently faulty behavior after temporary violations of input constraints are easily conceived.
As we aim for self-stabilization, it is clear that we cannot allow implementations that suffer from such drawbacks: Neither transient faults nor their consequences, i.e., temporarily arbitrary executions, may result in permanent faults.
Clearly, both recovery from transient failures and resilience of a basic module to erroneous inputs, and hence the whole definition of what actually constitutes a transient fault in our model, is implicitly defined by the physical realization of an implementation.
These observations have important consequences.
On the one hand, careful design of the basic modules is of paramount importance.
For instance, in a final product, a watchdog timer must not have its duration stored in a memory register that can be corrupted by a temporary charge injection (e.g. due to a particle hit), a ring oscillator should not be able to run unchecked at e.g.
twice its frequency indefinitely (e.g. triggered by a voltage pulse), and one has to make sure that stateful components like memory flags or state transition modules eventually "forget" about potentially erroneous inputs in the past, and eventually behave according to their specification again.
As discussed above, however, this cannot usually be perfect: There will always be (rare) scenarios, where an implemented circuit will not work like an ideal one, i.e., violate its specification.
We incorporate this in our model, in a pragmatic well-known from critical system design, by means of the notion of imperfect implementation coverage.
For a given module implementation, the coverage implicitly or explicitly determines the fraction of all possibly executions in which the implementation works as specified.
Since exceptional scenarios like metastability are usually extremely rare, we do not bother with defining the notion of coverage formally here: The coverage should be very close to 100% anyway.
In Section 6, we will argue that each of our basic modules will work as specified, except for very rare situations that may trigger metastability due to a violation of input timing constraints.
Thanks to this approach, algorithms and proofs can rely on sufficiently simple specifications of basic modules, which usually also admit robust and efficient implementations in practice.
Any unhandled scenarios are relegated to imperfect implementation coverage.
This feature is essential for devising proofs of reasonable complexity that show self-stabilization of all compound modules, implying that the system indeed will recover once transient faults of (basic) modules cease.
Due to the hierarchical composition of modules, compound modules fully derive their behavior from their submodules and can therefore be analyzed based on the properties of their submodules, while we may switch at will between viewing a module as given (i.e., basic), analyzing it in more detail as a compound implementation, or (for low-level modules) analyzing it in an even more detailed model.
This way, our approach also inherently supports tight interaction between algorithmic design and design of the basic building blocks used in the algorithms.
The FATAL+ clock synchronization protocol
In this section, we recast the self-stabilizing clock synchronization algorithm introduced in [13] in the modeling framework of the previous section and summarize its most important properties.
Since the main focus of our paper is on the implementation of our algorithm in this model, there is no need to provide a detailed description of the stabilization mechanism, let alone formal proofs of the stated claims; the analysis of the correctness and performance of the algorithm in [13] is based on a simpler abstract system model, assuming a globally valid end-to-end delay bound d covering any (local and remote) communication and processing action, which is fully compatible with our modeling framework.
More specifically, all that is needed in order to reuse the results of the analysis in [13] is to compute the maximum end-to-end delay occurring in the implementation of our algorithm in the modeling framework introduced in Section 2.2.
Recall from Section 2.2 that our top-level clock synchronization module is implemented as a compound module consisting of n nodes and their connecting top-level channels (with maximum delay dChan).
Every node, in turn, is a compound module made up of a state transition module, watchdog timers, memory flags, and threshold modules interconnected by channels (modeling various delays) as shown in Fig. 5.
Finally, a state transition module represents several communicating concurrent asynchronous state machines (with maximum transition time dTrans).
It ensures that state transitions of every constituent state machine occur in an orderly fashion, i.e., that every transition happens exactly once and, if need be, memory flags are consistently reset.
The state of each state machine is encoded in a few bits and conveyed via the top-level channels to all other modules in the system that need to receive it on some input port.
Given this simple internal structure, computing the resulting end-to-end delay bound d (or, for the quick cycle, dmin+ and dmax+, see below) from the constituent delay bounds is straightforward, see Section 6 for details.
State machine representation
Obviously, the entire logic of our algorithm is encoded in the state machines of a node.
In [13], we use a graphical representation that also reveals the layered structure imposed by their communication.
We already employed this description in Fig. 1.
With the definitions from the previous section at hand, we can now give our graphical representation a precise formal meaning that will allow us to translate the results from [13] to our modeling framework.
Our graphical representation defines the set of possible states S of a state machine (in Fig. 1 ready, propose, and increase) and, by means of the arrows between the states, the set of possible state transitions T⊆S2 (here ready to propose, propose to increase, and increase to ready).
If, for a state transition from s to s′, re(s,s′)≠∅, i.e., there are memory flags that need to be reset, re(s,s′) is given in a rectangular box on the arrow.
Since for each node i and state s we will always reset all memory flags Memi,j,s for j∈{1,…,n} together, we simply write s1,…,sk in such a box to represent the fact that all flags Memi,j,s, j∈{1,…,n}, s∈{s1,…,sk}, are to be reset.
Note that some of these states may be from a different state machine, i.e., the states s1,…,sk need not all be from S.
Completing the description, for each (s,s′)∈T, tr(s,s′) is given by the label next to the respective arrow.
Again, we make use of a condensed notation.
Assume that the state machine in question is part of node i.
We will employ threshold conditions like "⩾f+1 s1", whereby we refer to at least f+1 of i's memory flags Memi,j,s1 being in state 1, or "⩾n-f s1 or s2", which is true if ∑j∈Nmax{Memi,j,s1,Memi,j,s2}⩾n-f, i.e., for at least n-f nodes j flag Memi,j,s1 or flag Memi,j,s2 is in state 1.
An example for such a rule is the transition from propose to increase in Fig. 1.
Such conditions will be translated to a binary signal by feeding the memory flags' signals (or, in the latter case, the output of n OR-gates with inputs Memi,j,s1 and Memi,j,s2) into a threshold gate (of threshold f+1 or n-f, respectively).
Further abbreviations we use for timeouts.
Recall that for a timeout (T,s,C), we omit the clock C from the notation, i.e., write (T,s) instead of (T,s,C).
Timeout (T,s) switches to 1 after T local time units (i.e., between T/ϑ and T+dTrans reference time) has passed since the last switch to state s was triggered.
In case it is part of a transition rule, we write (T,s) for the condition TimeT,s,C=1, and if the transition goes from the state s to which the timeout corresponds to some state s′, we simply write T.
For instance, the condition "3ϑd local time has passed" in Fig. 1 is concisely stated as "3ϑd".
Finally, as for memory flag resets, transition rules may also refer to a state s of another state machine.
In the special case that a predicate solely depends on the current state of another of the node's state machines, we write "in s" or "not in s" to indicate the predicates p=s and ¬(p=s), respectively, where p is the input port connected to the channel communicating the other state machine's state to the state transition module.
Finally, the above rules can be composed by logical AND or OR, which we display by connecting expressions with and or or, respectively.
In Fig. 1, such a composition occurs in tr(ready,propose).
Overview of the algorithm
Each node is a collection of several state machines that are organized in a layered structure.
On each layer, the state machines of the (at least n-f) non-faulty nodes cooperate in order to establish certain synchronization properties of their output signals.
The higher is a state machine in the hierarchy, the stronger are these guarantees; the lower it is, the weaker are the synchronization properties its input signals need to satisfy for stabilization.
The lowest-layer state machine utilizes randomization to recover from any configuration (provided its basic modules are correct (again), i.e., guarantee feasible executions).
Each other layer utilizes auxiliary information from the layer below to stabilize.
Finally, the top level state machine outputs the logical clocks Li.
More specifically, we have the following state machines.•
At the top level, we have the quick cycle state machine (Fig. 6) that outputs Li.
The quick cycle is very similar to the algorithm given in Fig. 1, except that it is coupled to the state machine beneath it in order to ensure eventual stabilization.
Once the system is stabilized, it consistently and deterministically increases Li at a high frequency while guaranteeing small clock imprecision.
•
The main state machine (Fig. 8) is the centerpiece of the stabilization mechanism.
Once stabilized, it generates slow, roughly synchronized "pulses" within certain frequency bounds.
These pulses can be seen as a "heartbeat" of the system; at each pulse, the quick cycle's clocks are reset to 0 and the quick cycle's state machines are forced into state accept+ (corresponding to the increase state in Fig. 1).
This enforces exactly the initial synchrony that we explained to be necessary for the correct operation of the algorithm from Fig. 1.
By itself, however, the main state machine is not capable of recovering from every possible initial configuration of the non-faulty nodes.
In certain cases, it requires some coarse synchrony to be established first in order to stabilize, which is probabilistically provided by the underlying layer.
We remark that, once stabilized, the main state machine operates fully independently of this layer (and thus deterministically).
•
The auxiliary information potentially required for stabilization by the main state machine is provided by a simple intermediate layer we refer to as extension of the main state machine (Fig. 9).
Essentially, it is supposed to be consistently reset by the underlying layer and then communicate information vital for stabilization to the main state machine.
This information depends both on the time of reset and the current states of the n main state machines, which it therefore monitors.
•
Finally, the resynchronization routine (Fig. 10) utilizes randomized timeouts to consistently generate events at all non-faulty nodes that could be understood as "randomized pulses".
Such a pulse is correct for our purposes if all non-faulty nodes generate a respective event in coarse synchrony and no non-faulty node generates another such event within a time window of a certain length.
The crux of the matter is that a single such pulse suffices to achieve stabilization deterministically.
Relying on (pseudo-)randomness on this layer greatly simplifies the task of overcoming the interference by faulty nodes at low costs in both time and communication.
We note that the main state machine masks this randomness once stabilization is achieved, facilitating deterministic behavior of the higher levels and, ultimately, the nodes' clocks Li.
We will now present the individual state machines.
We refrain from a discussion of choosing appropriate durations for the timers, confining ourselves to stating a feasible family of choices later on.
The quick cycle
The quick cycle state machine is depicted in Fig. 6.
It introduces an additional notation: As the states ready+ and accept+ are not distinguished in any of the transition conditions in the other state machines, the same state none+ can be communicated here.
This allows for a very efficient single-bit representation of the communicated states.
In Fig. 6, this is expressed by dividing the circles representing states, putting the state names in the upper part and the communicated states in the lower part.
Apart from saving a wire, this permits to use trivial encoding and decoding of the signal, a simplification of the logic that minimizes delays and therefore maximizes the clock frequency that can be achieved.
Essentially, the quick cycle works as the algorithm given in Fig. 1, where the logical clock is increased whenever the machine switches to state accept+.
However, the quick cycle differs from the algorithm in Fig. 1 in that there is an interface to the main state machine given in Fig. 8.
These state machines communicate by means of two signals only, one for each direction of the communication: (i) The quick cycle state machine of node i generates the nexti signal by which it exerts some limited influence on the time between two successive pulses generated by the main state machine, and (ii) it observes the (T2+,accept) timer.
This timer is coupled to the state accept of Fig. 8, in which the pulse synchronization algorithm generates a new pulse.
The signal's purpose is to enforce a consistent reset of the quick cycle state machine (once the main state machine has stabilized).
The feedback mechanism (i) makes sure that, during regular operation, the reset of the quick cycle does not have any effect on the clocks.
This is guaranteed by triggering pulses (by means of the non-faulty nodes briefly changing the nexti signal to 1 and back to 0 again) exactly at the wrap-around of the logical clock Li, i.e., at the time when Li is "increased" from the maximal clock value K-1=2b-1 (of a b-bit clock) to 0=KmodK.
Similar to Fig. 1, the transition conditions of the quick cycle ensure that the logical clocks never have a clock imprecision of more than one.
To increase the frequency further, each node could increase the number of clock "ticks" generated in each iteration of the quick cycle by means of a high-frequency local clock (essentially, a watchdog timer together with a counter), at the expense of larger clock imprecision (see [13]).
Main state machine
Before we show the complete main state machine, consider its basic cycle depicted in Fig. 7.
Once the main state machines have stabilized, all non-faulty nodes will undergo the states of the basic cycle in rough synchrony.
The states sleep, sleep→waking, and waking serve diagnostic purposes related to the stabilization process.
The duration T2 of the timer (T2,accept) triggering the transition from waking to ready is so large that the node will always be in state waking long before the timer expires.
Thus, we can see that the basic cycle has an underlying structure that is very similar to the quick cycle.
Due to the more complicated logic and conditions on the duration of timers required for the stabilization mechanism, it is however executed at a frequency that is by orders of magnitude smaller than that of the quick cycle.
The difference in the rules for switching to propose and accept, respectively, are also mostly related to the stabilization process.
An exception is the condition "T3 and nexti=1" that can trigger a transition from ready to propose.
Choosing T3 smaller than T4 and taking the signal nexti into account, we permit the quick cycle to adjust the time between pulses (i.e., switches to accept) triggered by the main state machine: Once both state machines are roughly synchronized among all non-faulty nodes, the main state machines will always be in state ready before the logical clocks Li maintained by the quick cycle reach the wrap-around (i.e., become 0 modulo K) and trigger the nexti signals.
Moreover, this happens at all nodes at close times and before any timer (T4,ready) expires at one of the non-faulty nodes.
Hence, by a reasoning similar as for Fig. 1, all non-faulty nodes will switch to propose and subsequently accept in a well-synchronized fashion, caused by the wrap-around of the logical clocks.
An important observation that is proved in [13] is that, once the main state machines stabilized, the nodes execute the basic cycle deterministically and any state transition is certainly completed before one of the conditions for leaving the basic cycle can be satisfied.
Apart from small additional slacks in the timer durations, this is a consequence of the fact that none of the transition conditions of the basic cycle refer to the probabilistic lower layers of the protocol; all evaluated timers and memory flags solely involve states of the basic cycle only, and the nexti signal is provided by the quick cycle.
As we will discuss in Section 6, this property prevents non-faulty nodes from introducing metastability once stabilization is achieved.
We now turn our attention to the full main state machine that is shown in Fig. 8.
Compared to the basic cycle, we have two additional states, resync and join, that can be occupied by non-faulty nodes during the stabilization process only, and an additional reset of memory flags on the transition from sleep→waking to waking.
The various conditions for leaving the basic cycle and switching to recover are consistency checks.
A node will only leave the basic cycle if it is certain that the system is not operating as desired.
As the high-level operation of the algorithm is not the subject of this article, we limit our exposition to briefly discussing the two possible ways to re-enter the basic cycle, corresponding to two different stabilization mechanisms.
The first stabilization mechanism is very simple, and it is much faster than the second one.
Assuming that at least n-f non-faulty nodes are executing the basic cycle (i.e., the main state machines have already stabilized if we consider the remaining nodes faulty), a recovering node just needs to "jump on the train" and start executing the basic cycle as well.
This is realized by the condition for switching from recover to accept.
It is not hard to see that due to this condition, the node will switch to accept in sufficient synchrony with the majority of n-f synchronized, non-faulty nodes within at most two consecutive pulses and subsequently follow the basic cycle as well.
Note that this condition makes direct use of the state signals instead of using memory flags.
This potentially induces metastability at the joining node, but we will explain in Section 6 why the risk is low.1717
Recall that during stabilization we cannot exclude metastability with certainty even in the absence of any further faults.
 On the plus side, this simplifies the algorithm, as the node does not need to implement frequent resets of the respective memory flags to ensure consistent observation of others' states; the sending nodes will just do this implicitly by leaving state accept.
Clearly, the first stabilization mechanism will fail in certain settings.
Most obviously, it cannot "restart" the system if all nodes are in state recover.
Hence it may not surprise that the second stabilization mechanism, which deals with such cases, is much more involved.
Careful attention has to be paid to avoiding the potential for system-wide dead- or live-locks.
In view of our design goals, state-of-the-art deterministic solutions for this problem are not sufficiently efficient.
Hence, the main state machine relies on a probabilistic lower layer that provides certain guarantees with a very large probability.
Extension of the main state machine
The extension of the main state machine, given in Fig. 9, can be seen as a simple control structure for the phases of stabilization.
The intricacy lies in designing the interface such that this control does not interfere with the basic cycle if the system is stable.
Consequently, the influence of the extension of the main state machine is limited to (i) resetting the join and sleep→waking flags upon "initializing" the stabilization process (by switching from dormant to passive) and (ii) providing the signals of the timers (T6,active) and (T7,passive) the main state machine utilizes in the transition rule from recover to join.
Roughly speaking, the main state machines will stabilize deterministically under the condition that their extensions switch at all non-faulty nodes from dormant to passive in rough synchrony and then do not switch back to dormant too quickly, i.e., before the second stabilization mechanism of the main state machine completes its work.
Putting it simply, we require a single, coarsely synchronized pulse, whose generation is the purpose of the lowest layer we present now.
Resynchronization state machine
The resynchronization state machine is specified in Fig. 10.
Strictly speaking, it actually consists of two separate state machines, one of which is however extremely simple.
Every now and then, each node will briefly switch to the init state, seeking to induce the generation of a "pulse" (where the pulse here is locally triggered by switching to resync) that causes a consistent switch of all non-faulty nodes from dormant to passive.
Leaving resync will force the extension state machine back into state dormant.
This is the only interaction with the above layer, which is sufficient if a pulse is successfully generated once.
The generation of a pulse is achieved by all non-faulty nodes following the advice of a single node switching to init, thus establishing the common time base required for a synchronized pulse.
Two obstacles are to be overcome: possibly some of the non-faulty nodes already believe that the system is in the middle of an attempt to stabilize (i.e., they are already in state resync and thus not ready to follow the advice given by another node) and possibly inconsistent information by nodes that remain faulty (causing only some of the non-faulty nodes to switch to resync).
In contrast to the higher levels, however, we are satisfied if only occasionally a successful pulse is generated.
Hence, the above issues can be overcome by randomization.
The source of randomness here is the randomized timer (R3,wait).
The distribution R3 and the logic of the second, more complicated state machine including the state resync are designed such that there is a large probability that within time O(n) all non-faulty nodes will consistently switch to state resync.
This O(n) is essentially the factor by which the second stabilization mechanism of the main state machine is slower than the first one.
Timer durations
Clearly, in order for the protocol to operate as desired, the timer durations need to satisfy certain constraints.
We state a feasible family of durations here; the minimal constraints that are required by the proofs are given in [13].
Recall that ϑ>1 and that d bounds the maximal end-to-end delay incurred between the time when a state transition condition is met and the time when the respective signal transition is observed at all receivers.
As the logic of the quick cycle is much simpler than that of the other state machines, it typically permits much tighter upper and lower bounds on this end-to-end delay.
As in [13], these bounds are denoted by dmin+ and dmax+⩽d.
In Section 6, we will discuss how d, dmin+, and dmax+ can be computed out of the constituent delays incurred in our basic modules.
Definingλ:=(25ϑ-9)/(25ϑ)∈(4/5,1)andα:=(T2+T4)/(ϑ(T2+T3+4d)), for any ϑ>1, α⩾1, the following family of timeout durations meets the requirements stated in [13] (see the reference for a proof):T1+:=6ϑ2d+6ϑ2dmax+-ϑdmin+T2+:=3ϑd+3ϑdmax+T3+:=6ϑ3d+6ϑ3dmax+-ϑ2dmin+T1:=4ϑdT2:=46ϑ3d/(1-λ)T3:=(ϑ2-1)46ϑ3d/(1-λ)+31ϑ3dT4:=46ϑ3(αϑ3-1)d/(1-λ)+35αϑ4dT5:=46ϑ4(αϑ3-1)d/(1-λ)+39αϑ5dT6:=46ϑ4d/(1-λ)T7:=92αϑ8d/(1-λ)+78αϑ5d and further,R1:=46ϑ6(3αϑ3-1)d/(1-λ)+109αϑ6dR2:=(92ϑ7(3αϑ3-1)/(1-λ)2+(218αϑ7+108ϑ3)/(1-λ))(n-f)dR3:=uniformly distributed random variable on[3ϑd+(92ϑ8(3αϑ3-1)/(1-λ)2+(218αϑ8+108ϑ4)/(1-λ))(n-f)d,3ϑd+(8(1-λ)+ϑ)(92ϑ7(3αϑ3-1)/(1-λ)2+(218αϑ7+108ϑ3)/(1-λ))(n-f)d].
Finally, the maximal logical clock value K-1 is not arbitary, as we require(1)K∈[(46ϑ4/(1-λ)+52ϑ2)/(12+10dmax+/d),α(46ϑ4/(1-λ)+32ϑ2)/(12+12dmax+/d)].
Note that, by manipulating α, we can make K arbitrarily large, but this comes at the expense of a proportional increase in the timer durations of the main state machine and its underlying layers, increasing the overall stabilization time.
Summary of results from theory
We conclude the section with a summary of the most important statements proved in [13], expressed in terms of the model employed in this article.
To this end, we need to specify the protocol as a compound implementation about that we will formulate our theorems.
Definition 5.1
The FATAL+ Protocol
The FATAL+ protocol is a compound module consisting of nodes i∈{1,…,n}.
It has no input ports and an output port Li for each node i.
The n input ports of node i are connected to the output ports of the channels Si,j, j∈{1,…,n}.
Each node is comprised of one copy of each of the state machines presented in this section, and the implementation of each node is derived from the implementations (given in Section 6) of the basic modules defined in Section 2.2 that are connected as specified in this section.
The output port Li of node i is the output port of its quick cycle state machine.
The first theorem states a probabilistic stabilization result.
Since we did not formally define probabilistically stabilizing implementations, its formulation is somewhat cumbersome.
Intuitively (and slightly inaccurately), the statement is to be read as "no matter what the initial state and the execution, the protocol stabilizes almost certainly within Tslow time".
Theorem 5.2
Fix any f′⩽f:=⌊(n-1)/3⌋ and feasible α, setTslow:=(24(1-λ)+3ϑ)R2+R1/ϑ+T1++T3++(9ϑ+8)d+5dmax+-dmax-∈Θ(αn), and pick K∈Θ(αn) in accordance with inequality (1).
Consider an execution on [t-,t+] of the FATAL+ protocol where (at least) n-f′ nodes are feasible.
Assume that an adversary that knows everything about the system except that it does not learn about the durations of randomized watchdog timers before they expire controls all other aspects of the execution (clock drifts and delays of feasible submodules within the admissible bounds as well as the output ports' signals of faulty modules).
Then the execution restricted to [t-+Tslow,t+] is with probability at least 1-2-(n-f) a feasible execution of a clock synchronization module with clock imprecision Σ=1, amortized frequency bounds A-=1/(T1++T3++3dmax+) and A+=1/(ϑ(T1++T3+)), slacks τ-=τ+=2, maximum frequency F+=1/(ϑ(T1++T3+-2dmax++dmin+)), at most f′ faults, and clocks of size K∈Θ(αn).
In this sense, for each f′⩽f, the FATAL+ protocol is an f′-tolerant implementation of a clock synchronization module with the respective parameters that stabilizes with probability at least 1-2-(n-f) within time Tslow∈O(αn).
The above theorem corresponds to the slow, but robust, second stabilization mechanism.
The next theorem, which corresponds to the faster first stabilization mechanism, essentially states that in an execution where n-f nodes already stabilized, any further non-faulty nodes recover quickly and deterministically, within O(α) time.
Theorem 5.3
We use the notation of the previous theorem. Moreover,Tfast:=T2+T4+(1+5/(2ϑ))R1+5d∈Θ(α).
Suppose an execution of the FATAL+ protocol is feasible on [t-,t+] with respect to the clock synchronization module specified in Theorem 5.2.
Consider the set of nodes W⊆N whose restricted executions on [t-,t+] are feasible.
Then the execution restricted to [t-+Tfast,t+] is feasible with respect to a clock synchronization module with the same parameters, except that it tolerates n-|W| faults only.
We should like to mention that in [13] a number of further results on stabilization are given.
In particular, if the faulty nodes exhibit only little coordination among themselves or do not tune their operations to the non-faulty nodes' states, also the "slow" stabilization mechanism will succeed quickly, granted that the resynchronization state machines are not in a "too bad" configuration, i.e., most timers of type R2 are expired and timeouts of type R3 are in (roughly) random states.
We will informally discuss some of these scenarios in Section 7.
Finally, we emphasize again that the power of the above theorems severely depends on the quality of basic implementations (cf.
Section 4.2).
While compound modules' properties can be formally analyzed, e.g.
giving rise to the theorems above, these results are meaningless if too many basic implementations are infeasible too frequently.
Hence it is vital to come up with robust implementations of the basic modules, which is the subject of the next section.
Implementation
In this section, we present the cornerstones of our FPGA prototype implementation of the FATAL+ protocol.
The objectives of this implementation are (i) to serve as a proof of concept, (ii) to validate the predictions of the theoretical analysis, and (iii) to form a basis for the future development of protocol variants and engineering improvements.
Rather than striving for optimizing performance, area, or power efficiency, our primary goal is hence to essentially provide a direct mapping of the algorithmic description to hardware, and to evaluate its properties in various operating scenarios.
Not surprisingly, traditional design principles for digital circuits are not adequate for our purposes.
This is true for three major reasons:•
Asynchrony: Targeting ultra-reliable clock generation in SoCs, the implementation of FATAL+ itself cannot rely on the availability of a synchronous clock.
Moreover, many guards, like the one of the transition from propose to accept in Fig. 8, depend on remote nodes' states and should hence not be synchronized to a local clock in order to maximize performance.
Testing for activated guards synchronized to a local clock source also increases the risk of generating metastability, as remote signals originate in different clock domains.
On the other hand, conventional asynchronous state machines (ASM) are not well-suited for implementing the state machines from Fig. 6-Fig. 10 due to the possibility of choice w.r.t.
successor states and continuously enabled (i.e., non-alternating) guards.
Our prototype hence relies on hybrid state machines (HSM) that combine ASM with synchronous transition state machines (TSM) that are started on demand only.
•
Fault tolerance: The consideration of Byzantine faulty nodes forced us to abandon the classic "wait for all" paradigm traditionally used for enforcing the indication principle in asynchronous designs: Failures may easily inhibit the completion of the request/acknowledge cycles typically used for transition-based flow control.
A few timing constraints, established by our theoretical analysis, in conjunction with state-based communication are resorted to in order to establish event ordering and synchronized executions in FATAL+.
•
Self-stabilization: In sharp contrast to non-stabilizing algorithms, which can always assume that there is a (substantial) number of non-faulty nodes that run approximately synchronously and hence jointly adhere to certain timing constraints, self-stabilizing algorithms cannot even assume this.
Although FATAL+ guarantees that non-faulty nodes will eventually execute synchronously, even when started from an arbitrary state, the violation of timing constraints and hence metastability cannot be avoided during stabilization [44].
For example, state accept in Fig. 8 has two successors sleep and recover, the guards of which could become true arbitrarily close to each other in certain stabilization scenarios.
This is acceptable, though, as long as such problematic events are neither systematic nor frequent, which is ensured by the design and implementation of FATAL+ (see Section 6.1).
 Inspecting Figs.
6-10 reveals that the state transitions of the FATAL+ state machines are triggered by AND/OR combinations of the following different types of conditions:(1)
A watchdog timer expires ["(T2,accept)"].
(2)
The state machines of a certain number (1, ⩾f+1, or ⩾n-f) of nodes reached a particular (subset of) state(s) at least once since the reset of the corresponding memory flags ["⩾n-f accept"].
(3)
The state machines of a certain number (1, ⩾f+1, or ⩾n-f) of nodes are currently in (one of) a particular (subset of) state(s) ["in resync"].
(4)
Always ["true"].
These requirements reveal the need for the following major building blocks (cf.
Section 4):•
Concurrent HSMs, implementing the states and transitions specified in the protocol.
An ideal HSM would always provide feasible executions of its state transition module.
•
Communication infrastructure between these state machines, continuously conveying state information.
This is simply done by the channels Si,j propagating the signal Si to all receivers.
•
Watchdog timers (also with random timeouts) for implementing type (1) guards.
•
Threshold modules and memory flags for implementing type (2) and type (3) guards.
If we could provide implementations of all these building blocks that match the specifications of the formal model in Section 2.2 under all circumstances, in the sense that all executions at non-faulty nodes are always feasible, the theoretical guarantees derived in [13] would apply without restriction.
As already noted, however, this is impossible to guarantee, since there is no way to rule out metastable upsets with complete certainty, and there are no elements available for our purpose whose behavior is specified for metastable inputs.
Nevertheless, it is possible to design our basic modules in a way that keeps the probability of such events acceptably low.
Moreover, all stateful components must be implemented in a self-stabilizing way: They must be able to eventually recover from an arbitrary erroneous internal state, including metastability, when facing sufficiently long executions on their input ports that do not induce metastability.
Before we proceed with a description of the implementations of the required basic modules, we discuss how FATAL+ deals with the threat of metastability arising from our extreme fault scenarios.
Metastability issues
Reducing the potential for both metastability generation and metastability propagation are important goals in the design and implementation of FATAL+.
Although it is impossible to completely rule out metastability generation in the presence of Byzantine faulty nodes (which may issue signal transitions at arbitrary times anyway) and during self-stabilization (where all nodes may be completely unsynchronized), we nevertheless achieve the following properties.
Robustness against metastable upsets and their propagation:(I)
Guaranteed metastability-freedom in fault-free executions after stabilization.
(II)
Low probability of metastable upsets: We have taken care to keep the windows of vulnerability of our implementations of basic modules as small as possible.
Thus, desynchronized or faulty nodes must be very lucky to actually trigger a metastable upset.
In addition, mechanisms for decreasing the upset probability even further could be incorporated, if required in particularly critical applications.
(III)
Metastability containment: Non-faulty nodes are very robust against propagation of metastable upsets due to the algorithm's control flow.
Limited consequences of metastable upsets:(IV)
Limited impact of metastable upsets during stabilization: Metastable upsets that occur at non-faulty nodes during the stabilization phase can only delay stabilization.
Since these are rare events even then, the respective effect on the (average) stabilization time is very small.
(V)
Fast recovery from metastability after stabilization: As long as n-f non-faulty nodes remain synchronized, a metastable upset at a node may disrupt its synchrony towards the other nodes only for a short time.
Due to the fast stabilization mechanism the node will fully recover within O(1) time once metastability ceases.
(VI)
Masking of metastable upsets as faults: Provided that the measures ensuring (II) and (III) are effective (i.e., metastability does not spread) and the system-level fault-tolerance of f nodes operating outside their specification is not exhausted, metastable upsets at some nodes do not affect the correctness of other nodes.
The following approaches have been used in FATAL+ to accomplish these goals (additional details will be given in the subsequent sections):(I)
If all nodes are synchronized and fault-free, we can satisfy timing constraints on the modules' input ports' signals that ensure that even our (necessarily imperfect) implementations of the abstract modules maintain feasibility at all times.
Essentially, the argument is that since there is no initial violation of the constraints and no faults are imposed by external events, we can conclude that the constraints will be satisfied at later points in time as well.
This property is formally proved in [13].
(II)
All building blocks that are susceptible to metastable upsets, like memory flags, are implemented in a way that minimizes the time span during which they are vulnerable.
Moreover, elastic pipelines acting as metastability filters [40] or synchronizers could be added easily to our design to further protect such elements.
(III)
We enforce (standard) error containment by avoiding any explicit control flow between ASMs: Since the communication is exclusively performed by virtue of states, a faulty receiver cannot impact a non-faulty sender, and a faulty sender, in turn, cannot directly interfere with the operation of a non-faulty receiver (apart from conveying an incorrect state, of course).
To extend error containment to also cover metastability to the best possible extent, several forms of logical masking are employed.
One example is the combination of memory flags and threshold gates, which ensure that possibly upset memory flags are always overruled quickly by correct ones at the threshold output.1818
It is not self-evident that this type of masking is very effective for metastability as well.
Later on we will discuss why this is indeed the case.
 A higher-level form of logical masking occurs due to the fact that, after stabilization, all non-faulty nodes execute the outer cycle of the main state machine (Fig. 8) only.
The outer cycle's guards do not involve any of the timeouts, states, or flags accessed by the resynchronization routine (Fig. 10) or the extension of the main state machine (Fig. 9); hence any metastability of the corresponding signals does not affect the logic of the main state machine and the layers on top of it (including the logical clocks).
(IV)
The measures outlined in (II) and (III) are complemented by adding time masking using randomization.
The resynchronization routine (Fig. 10) tries to initialize recovery from arbitrary states at random, sufficiently sparse points in time.
Hence non-faulty nodes cannot be systematically kept from stabilizing.
The proofs in [13] reveal that within O(n) time in fact it is likely that there are multiple events that will imply subsequent stabilization.
Considering that metastable upsets are rare events in our setting, their impact thus becomes negligible.
(V)
This property directly follows from the results shown in [13]: If n-f nodes faithfully execute the basic cycle, any non-faulty node will (re)synchronize within O(1) time, irrespectively of its current state.
(VI)
If metastability does not spread to a given receiver, the latter will observe for each sender some execution, even if the sender does not send a valid signal in terms of our system model.
Since we assume that faulty nodes may output arbitrary signals at their output ports, our model thus makes no distinction between a "conventionally" faulty node and one that behaves erratically due to metastable upsets.1919
To match our model, invalid signal states are simply mapped to some default state, e.g.
resync for the main state machine.
 As the algorithm is resilient to up to f faults, such upsets are masked as long as the total number of nodes operating outside their module specification is at most f.
State machine communication
According to our system model, an HSM of node i must continuously communicate its current state system-wide via the channels Sj,i.
For simplicity, we use parallel communication, by means of a suitably sized data bus, in our implementation.2020
It is, however, possible to replace parallel communication by serial communication, e.g., by extending the (synchronous) TSM (see Section 6.3) appropriately.
 A complete receiver as described below is employed for every state machine in the system.
Since a node treats itself like any other node in type (2) and type (3) guards with thresholds, every node receives its own state as well.
Channels
Fig. 11 shows the circuitry used for communicating the current state of the main algorithm in Fig. 8.
The sender consists of a simple array of flip-flops, which drive the parallel data bus that thus continuously reflects the current state of the sender's HSM.
Technically speaking, the flip-flops are not part of the channel but rather the sender's HSM; they are the "physical location" of the HSM's state in the sense of our model.
The channel thus "begins" with the wires conveying the stored values.2121
Note that there is some freedom with respect to the mapping of module ports to the physical system, which also affects which module(s) become(s) infeasible due to a (physical) fault.
However, no matter what the precise mapping, care has to be taken to avoid correlated failures.
For instance, if all channels meet in a single spot due to bad routing, manufacturing defects or electromigration could connect several channels, therefore rendering our system-level fault-tolerance (i.e., the resilience to f node failures) ineffective.
In sharp contrast to handshake-based communication, reading at the receiver occurs without any direct coordination with the sender.
To avoid the unacceptable risk of reading and capturing false intermediate sender states, which might be perceived by the receiver upon a sender state transition in case of different delays on the data bus wires, delay-insensitive state coding [45] must be used.
We have chosen the following encoding for the main state machine in Fig. 8:propose0000accept1001sleep1011sleep→waking0011waking0101ready0110recover1100join1010
The receiver comprises a simple combinational decoder consisting of AND gates, which generate a 1-out-of-m encoding of the binary representation of the state communicated via the data bus.
The decoded signals correspond to a single sender state each.
This information is directly used for type (3) guards, and fed into memory flags for type (2) guards.
For the other state machines making up FATAL+, it suffices to communicate only a single bit of state information (supp or none in Fig. 9, init or wait in Fig. 10, and propose+ or none+ in Fig. 6).
Hence, every bus consists of a single sender flip-flop plus a wire here, and the decoder in the receiver becomes trivial.
In the sequel, we restrict our discussion to the main state machine's channel, as the simpler single-bit channels clearly meet the specification of a channel.
Note that in both cases the (physical) channels used in our implementation trivially recover from any inputs and transient faults, as they are obviously forgetful.
The memory flags at the receiver's side contain feedback-loops, however, which do not allow us to apply Theorem 3.7 and Lemma 3.8.
Correctness.
We now argue informally2222
Our basic modules appear simple enough to be amenable to formal verification.
Still, there are complications: Besides the fact that we assume not only continuous time but also continuous computations, which rules out using standard verification approaches, there is the challenge of finding and expressing suitable input port execution constraints required for implementation correctness.
Exploring this avenue is part of our future work.
 why and when the above implementation matches the specifications given in Section 4.
Note that when affected by faults or provided with illegal inputs, modules may of course exhibit arbitrary behavior.
In that case we rely on (a) the system-level fault tolerance properties (for fault masking), (b) the self-stabilization properties of the affected modules (for recovery), and (c) the rare occurrence of these situations (in order to not exhaust the system-level fault tolerance limits).
In addition to considering the fault-free behavior, it hence suffices to restrict our attention to (b) and (c) here.
For fault-free operation, the described implementation essentially realizes a channel as specified in Section 4 with some maximum delay dChan, granted that changes of the input provided by the sender are separated in time sufficiently well.
To see this, consider an input switch from state s to s′ (note that not all flip-flops will switch their output signals at exactly the same instant), where initially the signal is stable also on the receiver's side.
Once the signal change propagated through the wires and the AND gates, the decoder output signal corresponding to state s′ will be 1, while all other signals will be 0.
Due to the use of delay-insensitive state encoding, there are no glitches and the signals for all other states s″∉{s,s′} will continuously be 0.
Nevertheless, formally, this behavior does not yet fully match the definition of our communication channels in Section 4: It is possible that temporarily both s and s′ are 1.
Since our algorithms are completely oblivious to the exact point in time when the perceived Si,j changes after the sender's state Sj changed (the analysis in [13] only requires that this happens within d time), however, this problem can easily be abstracted away.2323
Formally, this abstraction builds upon a weakened definition of lower-level channels, which attain values from S∪(S2).
Alternatively, it would also be possible to use an explicit transition state ⊥ (encoded by any bit sequence not corresponding to a state), and force the sender to always perform state transition via ⊥.
 All that is needed here is to interpret, in a static way, the situation where both s and s′ are valid as, say, s.
The attentive reader will have noticed that the 1-out-of-m decoder outputs (i.e., the state signals at the inputs of the memory flags) may temporarily be all 0 during the reception of a sender state transition as well.
Fortunately, this behavior is completely masked from becoming visible to our algorithms: The memory flags prohibit this from becoming visible in type (2) guards at all, and all state transition conditions involving type (3) guards refer to a single state only.
Hence, in terms of the transition condition, a similar abstraction as above is valid (i.e., for a remote state transition from s to s′ with a "gap" we can define an equivalent execution without gap in which the node in question behaves identically).
The above arguments critically rely on the assumption that states change not too rapidly.
Otherwise, the receiver could e.g.
fail to observe states that the sender assumed for a too short period of time only, or even decode a state that has not been attained.
For non-faulty nodes, this is guaranteed in our implementation because the minimal amount of time an HSM needs to complete a state transition is greater than the maximum end-to-end delay variation of the signals employed in the communication channel.
This constraint is easy to ensure by proper circuit design rules.
Metastability.
Within the communication channels themselves, metastable upsets could only occur in the senders' flip-flops and in the receivers' memory flags; everything else is stateless combinational logic.
The flip-flops are clocked by the sender's own clock, hence could become metastable only in case of a faulty sender.
The issue of upsets of the memory flags is discussed in Section 6.2.2.
Viewed at the node level, it is obvious that if the sender's state signal becomes metastable or changes too quickly (which can only happen if the sender is faulty), this can also induce metastability at the receiver side by propagation over the channel.
During the stabilization phase, the receiver could also experience a channel-induced metastable upset in memory flags and/or in its HSMs due to the arbitrary desynchronization between sender and receiver; since the windows of vulnerability are very small, the upset probability is very low, though.
Eventually, after stabilization, the synchrony between non-faulty nodes guaranteed by the FATAL+ protocol ensures that the received state data will always be stable when read in a transition condition in the main algorithm's outer cycle, recall item (I) in Section 6.1.
Memory flags
Every memory flag is just an SR-latch with dominant reset, whose functional equivalents are also depicted in Fig. 11.
Note that a memory flag is set depending on the state communicated by the sender, but (dominantly) cleared under the receiver's control.
Metastability.
A memory flag may become metastable when the inputs change during stabilization of its feedback loop, which can occur due to (a) input glitches and/or (b) simultaneous falling transitions on both inputs.
However, for correct receivers, (a) can only occur in case of a faulty sender, and (b) is again only possible during stabilization: Once non-faulty nodes execute the outer cycle of Fig. 8, it is guaranteed that e.g.
all non-faulty nodes enter accept before the first one leaves.
Overall, the upset probability is thus very small.
It could be further reduced by diverse known means for metastability filtering, like using an elastic pipeline or Schmitt-trigger stages (which must be accounted for in the delay bounds, though).
Finally, it is well-known that a metastable flip-flop will recover in finite time with probability one [44].
Any SR latch matches the specification of a memory flag according to Section 4 followed by a channel with some maximum delay dMem, provided that it starts from a clean initial state and the set/reset signals avoid (a) and (b) above.
As argued above, the latter is guaranteed by our algorithm except in case of a metastable upset.
In case of the memory flag implementation shown in Fig. 11, dMem is primarily determined by the end-to-end settling time of the feedback loop.
This delay also determines the vulnerability window with respect to metastability (i.e. critical glitch length, and "simultaneity" of transitions).
Hence, making dMem small, which is easy to achieve by design, contributes to both speed and robustness.
Except in case of metastability, discussed before, our memory flag implementation is self-stabilizing since it is dMem-forgetful in the presence of input executions that avoid (a) and (b).
Threshold modules
The most straightforward implementation of the threshold modules used for generating the ⩾f+1 and ⩾n-f thresholds in type (2) and type (3) guards is a simple sum-of-product network, which just builds the OR of all AND combinations of f+1 respectively n-f inputs.
This implementation however quickly becomes highly expensive, as it requires Θ((nf)) gates.
A more efficient alternative is a sorting network, where the kth output indicates whether a threshold of k is reached.
For simplicity, in our FPGA implementation, threshold modules are built by means of lookup-tables (LUT).
Correctness.
Similar to our memory flag implementation, it is impossible to implement the properties of a threshold module as stated in Section 4, followed by a channel with some maximum delay dTh, in case of arbitrary inputs: Finding out whether a certain number of inputs is 1 exactly at the same time cannot be implemented with real circuits.
All implementations proposed above are forgetful and their outputs will stabilize quickly if their inputs do not change.
Moreover, after stabilization type (2) guards are irrelevant, since neither the basic cycle of the main state machine nor the quick cycle evaluate such guards.
Hence, in this case we can restrict our attention to input executions where inputs may change from 0 to 1 only, not back.
The reset of the memory flags to 0 is performed during state transitions (when the guards' signals are suppressed by the locked signal) and therefore safe.
As any of the proposed threshold module implementations involve combinational logic only, they are trivially self-stabilizing: According to Theorem 3.7, they are forgetful and hence, by Lemma 3.8, self-stabilizing.
Therefore, provided that the longest path delay does not exceed dTh, the properties stated in Section 4 are satisfied for monotonic inputs.2424
Some dedicated experiments confirmed that even our LUT implementation on an FPGA, for which we have no control over the placement, operates glitch-free on monotonic inputs.
Metastability.
As discussed above, type (2) guards cannot be safely evaluated by threshold gates and may cause glitches or metastable upsets.
Since this is of relevance before stabilization only, this risk is considered acceptable.
Like our channel implementations, threshold modules can propagate metastability: A metastable input could be propagated to the output when there are exactly k-1 non-faulty inputs in state 1 and the metastable input therefore makes the difference between output 0 and 1.
In all other cases, however, the metastable input will simply be masked.
Thus, albeit not perfect, threshold gates are an efficient means for metastability containment.
Hybrid state machines
Our prototype implementation of FATAL+ relies on hybrid state machines (HSM): An asynchronous state machine (ASM) is used for determining, by asynchronously evaluating the guards, the points in time when a state transition shall occur.
Our ASMs have been built by deriving a state transition graph (STG) specification directly from Figs.
6-10 and generating the delay-insensitive implementation via Petrify [46].
The actual state transition of an HSM is governed by an underlying synchronous transition state machine (TSM).
The TSM resolves a possibly non-deterministic choice of the successor state and then sequentially performs the required transition actions:1.
"Locking" the transition, i.e., disabling any other transitions of the ASM (despite possibly satisfied guards); this happens at the start of the TSM and is thus asynchronously triggered.
2.
Reset of memory flags and watchdog timers.
3.
Communication of the new state, i.e., writing its representation into the flip-flops whose output is fed into the channels Sj,i.
4.
Completing the transition to the new state by enabling further transitions of the ASM.
 The TSM is driven by a pausable clock (see Section 6.4), which is started dynamically by the ASM upon triggering the transition.
Note that this avoids the need for synchronization with a free-running clock and hence preserves the ASM's continuous time scale.
The TSM works as follows (see Fig. 12): Assume that the ASM is in state A, and that the guard G for the transition from A to B becomes true.
If no other transition is currently being taken (indicated by the locked signal being 0), the TSM clock is started and the TSM sequence counter is released.
With every rising edge of TSMClock, the TSM moves through a sequence of three states: synchronize (Syn), commit (Cmt), and terminate (Trm) shown in the rectangular box in Fig. 12.
In Syn, the locked signal is activated to prevent other choices from being executed in case of more than one guard becoming true.
Once the TSM has reached Syn, it has decided to actually take the transition to B and hence moves on to state Cmt.
Here the watchdog timer associated with B and possibly some memory flags are cleared according to the FATAL+ state machine, and the new state B is captured by the output flip-flops driving the state communication data bus (recall Section 6.2).
Note that the resulting delay must be accounted for in the communication delay bounds d, dmax+ and dmin+.
Finally, the TSM moves on to state Trm, in which the reset signals are inactivated again and the TSM clock is halted (and the TSM sequence counter forced to the reset state).
The locked signal is also cleared here, which effectively moves the ASM to state B.
It is only now that guards pertaining to state B may become true.
Metastability.
Whereas any ambiguity of state transitions due to multiple activated guards can easily be resolved via some priority rule, metastability due to (a) enabled guards that become immediately disabled again or (b) new guards that are enabled close to "locking" time cannot be ruled out in general.
However, as argued in Section 6.1, in FATAL+ (a) could only occur during stabilization, due to type (3) guards, or due to faulty nodes successfully inducing metastability of memory flags; recall that otherwise type (1) and type (2) guards are always monotonic, with the reset (of watchdog timers and memory flags) being under the control of the local state machine.
Similarly, our proofs in [13] reveal that upsets due to (b) do not occur after stabilization in the main state machine and the quick cycle (Figs.
6 and 8).
As the main state machine is logically independent of the lower layers (Figs.
9 and 10) after stabilization, any metastability in these layers is fully masked.
Thus, after stabilization, metastability of the TSMs we care about can only occur due to unstable inputs, i.e., upsets in memory flags, that are in addition filtered through threshold gates (type (1) guards use local timeouts and are thus considered non-faulty, and all type (2) guards employed by the main state machine and the quick cycle use thresholds).
Note that due to the logical masking of metastability provided by the threshold gates (cf.
Section 6.2.3) any memory flag acts as an implicit synchronizer: If a faulty node successfully induces metastability in the flag, this does not matter until the threshold can actually be reached.
If the respective time span is large, the memory flag is likely to have stabilized again already.
Therefore, in addition to succeeding in creating metastability, faulty nodes must do so within a specific window of time.
Due to the asynchronously triggered transitions, this window of vulnerability of the synchronizing stage for Syn is very small.
The resulting very low probability of a metastable upset due to a fault is considered acceptable.
The residual probability of metastable upsets could be further reduced by introducing synchronizer stages.
Considering their performance penalty of one extra clock cycle on the one hand and the low initial risk of metastable upsets (that are handled by the system level fault tolerance with much lower average performance penalty) on the other hand, however, the introduction of synchronizers does not seem beneficial in general.
Correctness.
Thanks to the synchronous TSM described above, the maximum state transition time dTrans can easily be expressed in terms of the frequency of the pausable clock.
Hence, it is reasonably easy to see that the HSM satisfies the specification given in Section 4, when it starts from a proper initial state and avoids the above scenarios (a) and (b) of unstable guards.
A careful simulation analysis of the overall HSM design confirms that it can in fact recover from arbitrary initial states, except metastable ones.
With respect to metastable initial states, we conjecture that eventual recovery occurs with probability 1 due to the fact that the only devices used in the implementation that are not forgetful are flip-flops with dominant reset (in the TSM sequence counter) and Muller C-gates (in the control logic of the ASM), for both of which it is known that metastability eventually resolves.
Clocks and watchdog timers
Pausable oscillator
The TSM clock is an asynchronously startable and synchronously stoppable ring oscillator, which provides a clock signal TSMClock that is 0 when the clock is stopped via an active 1 input signal TSMCStop.
A variant that is also asynchronously stoppable (under certain timing constraints) is used for driving the watchdog timers (see below).
The frequency of the ring oscillator is primarily determined by the (odd) number of inverters in the feedback loop.2525
In our FPGA implementation, the oscillator frequency is so high that, to reduce the hardware overhead for this proof-of-concept implementation, we also employ a frequency divider at the output.
 It varies heavily with the operating conditions, in particular with supply voltage and temperature: The resulting (two-sided) clock drift ξ is typically in the range of 7 to 9% for uncompensated ring oscillators like ours; in ASICs, it could be lowered down of 1 to 2% by special compensation techniques [15].
Note that the two-sided clock drifts map to ϑ=(1+ξ)/(1-ξ) bounds roughly between 1.15 and 1.19 or 1.02 and 1.04, respectively.
Correctness.
The operation of the TSM clock circuit shown in Fig. 13 is straightforward: In its initial state, TSMCStop=1 and the Muller C-gate has 1 at its output, so TSMClock=0.
Note that the circuit also stabilizes to the initial state if the Muller C-gate was erroneously initialized to 0, as the ring oscillator would eventually generate TSMClock=1, enforcing the correct initial value 1 of the C-gate.
When the ASM requests a state transition, at some arbitrary time when a transition guard becomes true, it just sets TSMCStop=LOW.
This starts the TSM clock and produces the first rising edge of TSMClock half a clock cycle time later.
As long as TSMCStop remains 0, the ring oscillator runs freely.
The stopping of TSMClock is regularly initiated by the TSM itself: With the rising edge of TSMClock that moves the TSM into Trm, TSMCStop is set to 1.
Since TSMClock is also 1 after the rising edge,2626
Obviously, we only have to take care in the timing analysis that setting TSMCStop=1 occurs well within the first half period.
 the output of the C-gate is forced to 1 as well.
Hence, after having finished the half period of this final clock cycle, the feedback loop is frozen and TSMClock remains 0.
Metastability.
The problem of devising a proof that the pausable clock will eventually recover when it starts from a metastable initial state is intricate (and outside the scope of this paper); this is not obvious due to the quite complex feedback loop involved in this circuit.
We conjecture that similar arguments as in [44] can be used to show that this will happen with probability 1; with this result established one could hope to infer that the HSM as a whole recovers from arbitrary metastable states with probability 1.
For metastability-free operation of the C-gate in Fig. 13, (a) the falling transition of TSMCStop must not occur simultaneously with a rising edge of TSMClock, and (b) the rising transition of TSMCStop must not occur simultaneously with the falling edge of TSMClock.
(a) is guaranteed by stopping the clock in state Trm of the TSM, since the output of the C-gate is permanently forced to 1 on this occasion; TSMClock cannot hence generate a rising transition before TSMCStop goes to 0 again.
Whereas this synchronous stopping normally also ensures (b), we cannot always rule out the possibility of getting TSMCStop=1 close to the first rising edge of TSMClock: (b) could thus occur due to prematurely disabled type (3) guards, which we discussed already with respect to their potential to create metastability in the TSM, recall Section 6.3.
Besides being a rare event, this can only do harm during stabilization, however.
Watchdog timer design
Every ASM state, except for accept in Fig. 8, is associated with at most one watchdog timer required for type (1) guards; accept is associated with three timers (for T1 and T2 as well as for T2+ in Fig. 6).
Recall that a timer is reset by the TSM when its associated state is entered, which does not necessarily happen synchronously with its counting clock.
According to Fig. 14, every watchdog timer consists of a synchronous, dominantly resettable up-counter that is clocked by its own pausable oscillator (as shown in Fig. 13) and a timeout register that holds the timeout value TO.2727
Note that these values must be hard-coded in order to avoid that a fault that intuitively should be transient (e.g. a bit flip in volatile memory) becomes permanent by "altering the algorithm".
 A comparator raises an output signal if the counter value is equal to the TO register value.
A "capture flip-flop" with dominant reset memorizes the expired condition until the timer is re-triggered.
Note that using a (synchronous) flip-flop instead of an SR latch here allows us to completely mask glitches at the comparator output, which may originate from intermediate inconsistent bit patterns at the counter output.
The reset signal TSMresWD, supplied by the TSM, (re-)triggers the watchdog as follows: The counter is reset to zero, the capture flip-flop is cleared, and the oscillator is temporarily stopped.
Stopping the oscillator is necessary to avoid metastability effects due to the unsynchronized release of the reset signal (recall that this signal originates from the clock domain of the TSM!) and the watchdog's local oscillator.
Note carefully, however, that the Muller C-gate in Fig. 13 must be extended by a dominant reset input connected to its stop input (TSMCStop) to prevent metastable upsets.
Moreover, to ensure a proper reset, one has to make sure that the reset duration is sufficiently large.
To guarantee this, TSMresWD is fed into a pulse shaping circuitry (bottom left part of Fig. 14) that makes sure that the reset pulse is longer than one period of the local clock.2828
This is why seven inverters are shown in Fig. 14, indicating that the output pulse needs to remain active for more than two half periods of the local clock, assuming three inverters in its oscillator's loop.
 At the end of this shaped reset pulse, counter and flip-flop have attained a clean reset state, and the local oscillator has safely been brought to a stable stopped state (with its output at 0).
When reset is finally released (to 0), the oscillator starts running.
As soon as the comparator detects a match between the current count and the timeout register, it will set match to 1.
This rising edge is captured by the flip-flop, thus keeping the watchdog timeout signal WDexpired at 1 even when the comparator reverts its output to 0 later on again (note that the counter keeps on running).
This construction ensures that the oscillator continues to operate also after the timeout expires, which is crucial for self-stabilization; in a system where the clocks driving the timeouts can be permanently halted, there is no way to avoid deadlocks for all possible states.
As for the watchdog timer with random timeout R3 in Fig. 10, our implementation uses a linear feedback shift register (LFSR) that is continuously clocked by the watchdog's oscillator: A uniformly distributed random value from the specified range, sampled from the LFSR, is loaded into the timeout register whenever the watchdog timer is re-triggered.2929
Note that for many settings, it is reasonable to assume that the new random value remains a secret until the timeout expires, as it is not read or in any other way considered by the node until then.
Under this condition, FATAL+ is resilient against the strong adversary specified in [13].
As our prototype implementation is not meant for studying security issues, however, the simple LFSR implementation is sufficient here.
 If both the watchdog timer and the LFSR are clocked by the same oscillator, this can be done in a synchronous way.
In order to avoid metastable upsets of the LFSR, which might occur when stopping the clock upon retriggering the watchdog as described above, we use a standard pausable oscillator for R3: Since R3 is guaranteed to timeout before it is re-triggered (see Fig. 10), we can stop the oscillator synchronously (as in the TSM) when the timeout occurs, i.e., tie its stop input to the OR of WDexpired and the pulse-shaped reset signal.
Another add-on is needed for the random timer R3 in order to guarantee that the LFSR recovers from an arbitrary state after a fault: Since an LFSR has a forbidden internal state (all-0 in our case), we use an additional comparator that detects an all-0 LFSR output and forces a (synchronized) reset of the LFSR to a proper initial state.
Metastability.
Using a dominant reset in conjunction with a reset pulse of sufficient length guarantees that pausable oscillator, counter, and flip-flop cannot become metastable when a watchdog timer is re-triggered.
Since all other activities are driven by the local oscillator and hence trivially metastability-free, this leaves the pulse-shaping unit as the only component that could possibly suffer from a metastable upset.
However, TSMresWD is generated by the TSM, which is guaranteed to generate a clean pulse at every correct node.
Hence, the pulse shaping unit could become metastable only at a faulty node.
With respect to the recovery from a metastable state, similar considerations as for the memory flags in Section 6.2.2 suggest that the pulse shaping unit will stabilize to an initial state with reset set to 0 eventually with probability 1.
Correctness.
Combining the implementations of the pausable oscillator (with additional reset) and the watchdog timer, it is not too difficult to verify that the specification given in Section 4 is met, provided all circuits start from a non-metastable initial state.
With respect to self-stabilization, the pulse shaping unit can be guaranteed to stabilize to an initial state with its reset output 0 from an arbitrary internal state.
Hence, the pausable oscillator and hence the counter will eventually run.
Provided that the counter implementation guarantees that it cycles through the full (finite) sequence of possible states (unless reset earlier), i.e., there are no deadlock states or alternative cyclic sequences that might be entered in case of a fault, our implementation ensures that WDexpired will eventually be set to 1, even if started from an arbitrary initial state.
One should bear in mind, though, that the time to recover a watchdog timer contributes to the overall stabilization time of the system.
It is hence advisable to make sure that recovering a watchdog timer does not take much longer than the largest timeout value in the system, e.g.
by avoiding oversized counter registers.
Computing the end-to-end delay bounds
From the implementations of the individual components, it is straightforward to compute the delays d, dmin+, and dmax+.
Recall that d bounds, for any node i, the maximal time that passes between a state transition of a remote node and a possibly triggered corresponding state change, i.e., the transition of Si.
This is done by computing the maximal sum of delays of any possible computing path, ranging over all possible state transitions (cf.
Fig. 5), taking into account the delay of the channels Sj,i.
Clearly, the channel delay for the remote channels Sj,i exceeds the delays of the local channels; hence, the longest path to the input ports of the state transition module is bounded by dChan+dMem+dTh>dTime.
Subsequently, the HSM locks the state transition and the TSM executes, which takes about two and a half clock cycles C of the pausable oscillator.
Note, however, that the new state is written into the flip-flops holding the state already during the commit cycle, i.e., after at most 1.5C.
A more accurate bound on d than dChan+dMem+dTh+2.5C is thusd⩽1.5C+max{C,dChan+dMem+dTh}.
For our approach, dmax+≈d, since the only difference to d is that dChan is replaced by dChan+, the delay of the simpler 1-bit channels (cf.
Section 6.2.1).
If the main state machine's channels would utilize serial encoding, though, one might well have that dChan≫max{dChan+,C}.
Finally, dmin+>C/ϑ, since this is the minimal time the HSM allows between locking a state transition and actually performing the transition at the port Si.3030
Clearly, a more precise analysis would yield tighter bounds.
Experimental evaluation
Our prototype implementation has been written in VHDL and compiled for an Altera Cyclone IV FPGA using the Quartus tool, see [47].
Since FPGAs neither natively provide the basic elements required for asynchronous designs nor allow the designer to exercise control over the actual mapping of functions to the available LUTs (we implemented threshold modules via LUTs rather than via combinational AND-OR networks for complexity reasons), we had to make sure that properties that hold naturally in "real" asynchronous implementations also hold here.
Apart from standard functional and timing verification via Modelsim, we therefore conducted some preliminary experiments for verifying the assumed properties (glitch-freeness, monotonicity, etc.) of the synthesized implementations of our core building blocks.
Backed up by the (positive) results of these experiments, complete systems consisting of n=4 respectively n=8 nodes (tolerating at most f=1 respectively f=2 Byzantine faulty nodes) have been built and verified to work as expected; overall, they consume 23000 respectively 55000 logic blocks.
Note however, that both designs also include the test environment, which makes up a significant part of the setup.
To facilitate systematic experiments, we developed a custom test bench that provides the following functionality:(1)
Measurement of pulse frequency and skew at different nodes.
(2)
Continuous monitoring of the potential for generating metastability in HSM state transitions.
(3)
Starting the entire system from an arbitrary state (including memory flags and timers), either specified deterministically or chosen at random.
(4)
Resetting a single node to some initial state, at arbitrary times.
(5)
Varying the clock frequency of any oscillator, at arbitrary times.
(6)
Choosing the communication delay between each pair of sender and receiver.
 All these experiments can be performed with Byzantine nodes.
To this end, the HSMs of the Byzantine nodes can be replaced by special devices that allow to (possibly inconsistently) communicate, via the communication data buses, any HSM state to any receiver HSM at any time.
Points (1) to (6) are achieved as follows:
(1) is accomplished using standard measurement equipment (logic analyzer, oscilloscope, frequency counter) attached to the appropriate signals routed via output pins.
(2) is implemented by memorizing any event where more than one guard is enabled (at the time when the TSM locks a state transition) in a flag that can be externally monitored.
(3) is realized by adding a scan-chain to the FPGA implementation, which allows us to serially shift-in arbitrary initial system states at run-time.
Repeated random experiments are controlled via a Python script executed at a PC workstation, which is connected via USB to an ATMega 16 microcontroller (μC) that acts as a scan-controller towards the FPGA.
For each experiment, the Python script generates a bit-stream representing an initial configuration.
The μC takes this stream, sends it to the FPGA via the serial scan-chain interface, and finally signals the FPGA to start execution of FATAL+.
Simultaneously, it starts a timer.
When a timeout occurs or the FPGA signals completion of the experiment, the μC informs the Python script which records the time until completion together with the outcome of the experiment and proceeds with sending the next initial configuration.
To enable (4) to (6), the testbench provides a global high-resolution clock that can be used for triggering mode changes.
To ensure its synchrony w.r.t.
the various nodes' clocks, we replaced all start/stoppable ring oscillators by start/stoppable oscillators that derive their output from the global clock signal.
Point (4) is achieved by just forcing a node to reset to its initial state for this run at any time during the current execution.
In order to facilitate (5), dividers combined with clock multipliers (PLLs) are used: For any oscillator, it is possible to choose one of five different frequencies (0, excessively slow, slow, fast, excessively fast) at any time.
For (6), a variable delay line implemented as a synchronous shift register of length X∈[0,15], driven by the global clock, can be inserted in any data bus connecting different HSMs individually.
In order to exercise also complex test scenarios in a reproducible way, a dedicated testbed execution state machine (TESM), driven by the global clock, is used to control the times and nodes when and where clock speeds, transmission delays, and communicated fault states are changed and when a single node is reset throughout an execution of the system.
Transition guards may involve global time and any combinatorial expression of signals used in the implementation of FATAL+, i.e., any predicate on the current system state.
Using our testbench, it was not too difficult to get our FATAL+ implementation up and running.
With the implementation parameters ϑ=1.3, d=13T, dmin+=dmax+=3T, where T=400ns (2.5 MHz) is the experimental clock period, and minimal timeouts according to the constraints listed in [13] (cf.
Section 5.7), pulses of an 8 node FATAL respectively FATAL+ system (including the quick cycle) occur at a frequency of about 62 Hz respectively 10 kHz.
A logic analyzer screenshot is depicted in Fig. 15.
Note that the quite low values for the frequency stem from the fact that we were intentionally slowing down the system, enabling better control of the execution.
As to be expected from such a fairly complex setup, we spotted several hidden design errors that showed up during our experiments, but also some minor, yet problematic errors in our theoretical analysis (like a missing factor of ϑ in one of our timeouts due to a typo).
In the original setup, these issues manifested in deviations of the measured w.r.t.
the predicted performance.
After resolving them, we conducted the following experiments, observing the behavior of both the overall FATAL+ and the underlying FATAL pulse generation protocol.
Worst-case skew experiment
To drive an 8-node FATAL system into a worst-case skew scenario,3131
The maximal imprecision is meaningful in connection with the system's frequency only.
In contrast, the skew captures the maximal time difference between corresponding signal transitions at the nodes, which reflects the quality of synchronization without requiring additional context.
 the set of nodes was split into four sets:•
A set A of two nodes with slow clock sources.
All communication to these nodes is maximally delayed.
•
A set B of two nodes with fast clock sources.
All communication to these nodes is minimally delayed.
•
Another set C of two nodes with fast clock sources.
All communication to these nodes is maximally delayed.
•
A set D of two faulty nodes.
These nodes always send propose to the nodes in B and do not send any other signals.3232
In our model, this behavior is mapped to a default signal at the receivers, e.g.
resync in the main state machine.
This setup leads to the following behavior of the main state machine (Fig. 8) once the system is stabilized.
The nodes in B∪C will always switch to propose first because their timeouts T3 expire (it is shown in [13] that at this time nexti=1 at all non-faulty nodes), and due to the "help" of the faulty nodes, the threshold of n-f=6 for switching to accept is reached at the nodes in B after the minimal delay.
It takes the maximal delay until the nodes in A realize that 4⩾f+1 nodes reached state propose and switch to this state.
Since 4<n-f, the nodes in A∪C require the support of the nodes in A to follow to state accept.
Hence, this happens another maximal delay later.
The resulting scenario is depicted in Fig. 16.
Assuming that the communication delay is at most d and at least dmin⩾0, we predict a skew of at most 2d-dmin between the nodes in B switching to accept and the nodes in A∪C catching up.
The experimental results confirmed the analytic predictions as being essentially tight: Letting the fast nodes run at a speed of 3 MHz and the slow nodes at 2.5 MHz, and setting the maximum delay d to about 3.6 μs (9 clock cycles), we observed a skew of about 6 μs.
This is consistent with the relatively large minimum delay dmin arising in our testbed.
A logic analyzer screenshot is depicted in Fig. 17.
Metastability experiments
We run a series of experiments dedicated to finding situations that potentially lead to metastable upsets.
We repeatedly set up 8-node systems with randomly chosen clock speeds between 2.5 MHz and 3.25 MHz and communication delays of at most 16 clock cycles.
While the system stabilized from these random initial states, we monitored the nodes' HSM state transitions after stabilization for multiple active conflicting state transitions during a period of over 60 h in total.
As predicted by our theoretical findings, in none of the experiments two conflicting guards were ever active at the same (global testbench) time after stabilization.
Stabilization time experiments
We evaluated stabilization time both in the absence and in the presence of faulty nodes.
In the latter case, we demonstrated the influence of the choice of the random timeout R3 on the stabilization time.
Stabilization in the absence of faulty nodes
To evaluate stabilization times in the absence of faulty nodes, we set up an 8 node system and run over 250000 experiments in each of which the nodes booted from random initial states, with randomly chosen clock speeds between 2.5 MHz and 3.25MHz=2.5ϑMHz, and message delays of up to d=16 clock cycles.
As soon as all nodes switched to state accept within 2d time, the FPGA signaled the μC to record the elapsed time and start the next experiment.
A considerable fraction of the scenarios (over 45%) stabilizes within less than 0.035 s (less than 5500d), which can be credited to the fast stabilization mechanism intended for individual nodes resynchronizing to a running system (see Fig. 18).
The remaining runs (see Fig. 19; please mind the different y-axis scale) stabilize, supported by the resynchronization routine, in less than 12 s (about 1.9⋅106d), which is less than the system's upper bound on R3 of approximately 14.9 s (about 2.3⋅106d) and significantly less than the system's upper bound on Tslow given in Theorem 5.2, which is no more than 44.5 s (about 7⋅106d) in this scenario.
Note that the stabilization time is inversely proportional to the frequency, i.e., in a system that is not artificially slowed down, stabilization is orders of magnitude faster.
For example, assuming d=1ns, we obtain that over 45% of the experiments stabilize within 5.5 μs, and all experiments stabilize within 1.9 ms.
Experimental results carried out for a 4-node system were analogous.
Either the main algorithm was capable to stabilize by itself (as for a large fraction the experiments in the head of the distribution), or once the resynchronization algorithm provided support after R3 expired at some node and n-f nodes switched to resync in approximate synchrony (the experiments in the tail of the distribution).
Fig. 20 shows stabilization by the resynchronization algorithm in a 4-node system: Eventually, all nodes switch to state none.
A node whose timeout R3 expires at a time when all timeouts (R2,supp) are expired, say node 1, forces all nodes from none into supp1.
Additional R3 timers, expiring at other nodes, may only force nodes into suppj, with j≠1, but do not prevent nodes from eventually communicating supp to all other nodes.
Thus nodes finally switch to supp→resync and from there to resync, in synchrony.
This again suffices to deterministically stabilize the nodes' main algorithm (as shown in [13]).
Note that the condition that all corresponding R2 timeouts are expired when a timeout R3 expires (actually, n-f suffice) will eventually be satisfied.
This happens at the latest when R3 expires for the second time at some node, simply because the distribution of the randomized timeout R3 guarantees that the picked duration is always larger than (roughly) ϑR2.
Stabilization with Byzantine nodes and deterministic timeouts
The importance of timeout R3 being randomly distributed is demonstrated in the following experiment.
We set up a 4-node FATAL+ system with one Byzantine faulty node, say node 4, and chose all R3 to be equal and initially synchronized, i.e., all R3 timeouts expire at about the same time at all correct nodes.
If the Byzantine node knows when R3 is going to expire, it can prohibit correct nodes from simultaneously switching to resync, thereby preventing synchronization of the Main Algorithm and thus stabilization: Shortly before R3 expires at the correct nodes, it sends init to two nodes, say 1 and 3, making them switch to supp4.
Subsequently, however, it only supports node 1 by sending supp to it.
This forces node 1 to switch to supp→resync and then resync alone.
While node 1 is in resync (i.e., while R1 is running), it does not support other nodes by sending supp.
Specifically, it does not support nodes 2 and 3 when they switch to supp1.
Eventually all nodes switch back to none, and the scenario can be repeated.
Fig. 21 depicts the scenario and Fig. 22 shows a logic analyzer screenshot of this experiment.
Note, however, that by definition of the probability distribution of R3, executions where R3 expires in synchrony at all correct nodes forever occur with probability 0.
We remark that there is always a nonzero probability that the randomly chosen durations of the timeouts R3 at non-faulty nodes align in a fortunate manner, so that stabilization could not be prevented even by an omniscient adversary orchestrating clock drifts, message delays, and faulty nodes.
While the probability of such a convenient event occurring in O(n) time decreases exponentially in the number of nodes n, it is reasonably likely for n=7 and in particular n=4 (i.e., systems that tolerate f=2 or f=1 faults, respectively).
This observation has been verified for n=4 by the first of the two experiments below.
Stabilization with Byzantine nodes and probabilistic timeouts
Two experimental setups were chosen to test stabilization in the presence of Byzantine nodes, using probabilistic timeouts R3 for correct nodes.
In the first experiment, a Byzantine node has access to the timeout values of all nodes as soon as they start their R3 timers.
In this case, the Byzantine node followed the strategy from before, obstructing any stabilization attempt that would otherwise be successful.
We observed that the Byzantine node was able to block at most one stabilization attempt of each non-faulty node.
Then it failed to prevent stabilization because the R2 timeouts corresponding to the Byzantine node did not expire on time before some non-faulty node successfully initialized stabilization.
In the second experiment, the Byzantine node has no access to timeout values, and therefore simply sends inconsistent init and supp signals as often as allowed by the timeouts R2 corresponding to it.
We did not observe any inhibited synchronized switch to resync when R3 expired at a correct node, however.
It should be noted that weaker adversaries and "better" initial configurations result in a constant stabilization time, irrespectively of the number of nodes n (see [13] for details).
The second experiment above demonstrates such a case; Theorem 5.3 states another.
The common-case stabilization time will therefore be considerably smaller than the (probabilistic) worst-case bound that is linear in n.
Conclusions
In this work, we introduced a novel modeling framework for self-stabilizing, fault-tolerant asynchronous digital circuits and demonstrated its applicability to our recently introduced FATAL+ clock generation scheme for multi-synchronous GALS architectures.
Our framework enables to reason about high-level properties of the system based on the behavior of basic building blocks, at arbitrary granularity and in a seamless manner.
At the same time, the hierarchical structure of the model permits to do this in a fashion amenable to formal analysis.
We believe this to be the first approach concurrently providing all these features, and therefore consider it as a promising foundation for future research in the area of fault-tolerant digital circuits.
As the conclusion of our paper, we now assess to which extent the properties of our implementation of the FATAL+ algorithm, which have been expressed and verified within our modeling framework and tested experimentally, meet our design goals.
Furthermore, we will discuss a number of potential improvements and future research avenues.
Our exposition will follow the optimization criteria listed in Section 2.1.7.•
Area consumption: For a suitable implementation, the total number of gates is O(nlogn) per node.
This can be seen by observing that the complexity of the threshold gates is dominating the asymptotic number of gates, since the O(n) remaining components of a node have a constant number of gates each; using sorting networks to implement threshold gates, the stated complexity bound follows [48].
Trivially, this number of gates is a factor of O(logn) from optimal.
We conjecture that in fact this complexity is asymptotically optimal, unless one is willing to sacrifice other desirable properties of the algorithm (e.g. optimal resilience).
Assuming that the gate complexity of the nodes adequately represents the area consumption of our algorithm, we conclude that our solution is satisfactory in that regard.
•
Communication complexity: Our implementation uses 7 (1-bit) wires per channel, and sequential encoding of the states of the main state machine would reduce this number to 5.
All communication are broadcasts.
Considering the complexity of the task, there seems to be very limited room for improvement.
•
Stabilization time: Our algorithm has a stabilization time of O(n) in the worst case.
Recent findings [49] show that a polylogarithmic stabilization time can be achieved at a low communication complexity; however, this comes at the expense of suboptimal resilience, a weaker adversarial model, and, most importantly, constants in the complexity bounds that make the resulting algorithm inferior to our solution for any practical range of parameters.
Moreover, as formalized in [13] and demonstrated in Section 7, for a wide range of scenarios our algorithm achieves constant stabilization time.
Considering the severe fault model, we conclude that despite not being optimal, our algorithm performs satisfactory with respect to this quality measure.
•
Resilience: It is known that 3f+1 nodes are necessary to tolerate f faults [25,14] unless cryptographic tools are available.
Since the complexity incurred by cryptographic tools is prohibitive in our setting, our algorithm features optimal resilience.
•
Delays: As mentioned, the delay of wires is outside our control.
Taking dmin+ and dmax+ into account in the quick cycle machine, we make best use of the available bounds in terms of the final frequency/synchrony trade-off.
The delays incurred by the computations performed at nodes are proportional to the depths of the involved circuits.
Again, the implementation of the threshold gates is the dominant cost factor here.
The sorting network by Ajtai et al. [48] exhibits depth O(logn).
Assuming constant fan-in of gates, this is clearly asymptotically optimal if the decision when to increase the logical clock Lv next indeed depends on all n-1 input signals of v from remote nodes.
We conclude that, so far as within our control, the design goal of minimizing the incurred delays is met by our algorithm.
•
Metastability: We discussed several effective measures to prevent metastability in Section 6.
Our experiments support our theoretical finding that, after stabilization, metastability may not occur in absence of further faults.
However, since metastability is an elusive problem for which it is difficult to transfer insights and observations to other modes of operation of a given system-let alone to different implementation technology-a mathematical treatment of metastability is highly desirable.
Our model opens up various possible approaches to this issue.
For one, it is feasible to switch to a more accurate description of signals in terms of signals' voltages as continuous functions of time.
Another option choosing an intermediate level of complexity would be to add an additional signal state (e.g. ⊥) for "invalid" signals, representing e.g.
creeping or oscillating signals.
Assigning appropriate probabilities of metastability propagation and decay to modules, this would enable a unified probabilistic analysis of metastability generation, propagation, and decay within a modeling framework using discrete state representations.
Such an approach could yield entirely unconditional guarantees on system recovery; in contrast, our current description requires an a priori guarantee that metastability is sufficiently contained during the stabilization process.
•
Connectivity: The algorithm presented in this work requires to connect all pairs of nodes and is therefore not scalable.
Unfortunately, it is known that Ω(n2) links are required for tolerating f∈Ω(n) faults in the worst case [26,27].
We argued for the assumption of worst-case behavior of faulty nodes; however, it appears reasonable that typical systems will not exhibit a worst-case distribution of faults within the system.
Indeed, many interesting scenarios justify to assume a much more benign distribution of faults.
In the extreme case where faults are distributed uniformly and independently at random with a constant probability, say, 10%, of a node being faulty, node degrees of Δ∈O(clogn) would suffice to guarantee (at a given point in time) that the probability that more than Δ/9 neighbors of any node are faulty, is at most 1-1/nc.
Note that this implies that the mean time until this property is violated polynomially grows with system size.
Using the FATAL+ protocol in small subsystems (of less than Δ nodes), system-wide synchronization will be much easier to achieve than if one would start from scratch.
In this setting, Δ∈O(logn) would replace n in all complexity bounds of the FATAL+ algorithm, resulting in particular in gate complexity O(lognloglogn) per node, computational delay O(loglogn), and stabilization time O(clogn) with probability 1-1/nc.
Thus, this approach promises "local" fault-tolerance of Ω(Δ) faults in each neighborhood in combination with excellent scalability in all complexity measures, and realizing this is a major goal of our future work.
•
Clock size: The constraint (1) entails that either clock size is bounded or large clocks result in larger stabilization time.
This restriction can be overcome if we use the clocks of bounded size generated by FATAL+ as input to another layer that runs a synchronous consensus algorithm in order to agree on exponentially larger clocks [41].
 Finally, we would like to mention two more prospective extensions of our work.
First, building on our modeling framework, it seems feasible to tackle an even more strict verification of the algorithm's properties than "standard" mathematical analysis.
The hierarchical structure and formal specifications of modules seem amenable to formal verification methods.
Such an approach should benefit from the possibilities to adjust the granularity of the model by the distinction between basic and compound modules as well as the restrictions imposed by the module specifications; more restrictive modules may be simpler to analyze, yet will guarantee the same properties as the stated variants.
Second, it should be noted that it is straightforward to derive clocks of even higher frequency from the FATAL+ clocks.
This is essentially done by frequency multiplication, at the expense of increasing the clock skew.
We refer to Dolev et al. [13] for details.
Overall, we consider the present work an important step towards a practical, ultra-robust clocking scheme for SoC.
We plan to address the open problems discussed above in the future, and hope that this will ultimately lead to dependable real-world systems clocked by variants of the scheme proposed in this article.
Acknowledgments
We would like to thank the anonymous reviewers for their valuable comments.

Disordered spinel LiNi0.5Mn1.5O4 cathode with improved rate performance for lithium-ion batteries
To prepare LiNi0.5Mn1.5O4, 0.65 g of NiCl2 (Aldrich, 98%), 2.97 g of MnCl2.4H2O (Sigma-Aldrich, 99%), 0.42 g of LiOH*H2O (Sigma-Aldrich, 99%), and 2.50 g of glycine (Sigma, 99%) were homogeneously dissolved in deionized water. The solution was then heated at 90 degC for 8 h under vacuum until blue suspension was formed. The as-obtained precursor was heat-treated at 580 degC for 5 h and then further heated at 750 degC, 850 degC, and 950 degC, respectively for 12 h in air to yield the final products. The synthesized powders obtained were marked as S580, S750, S850, and S950, respectively.Silver nanoparticle plasmonic effects on hole-transport material-free mesoporous heterojunction perovskite solar cells

To fabricate the silver NPs, a modified two-step reduction synthesis procedure was implemented, which was developed based on the conventional reduction method (Agnihotri et al., 2014). First, 90 mL of an aqueous solution containing sodium borohydride (NaBH4) and tri-sodium citrate (TSC) at the ratio of 2:7 (1 x 10-3 mol dm-3:3.5 x 10-3 mol dm-3) was heated to 60 degC for 30 min under vigorous stirring at 300 rpm to ensure a homogenous solution. After 30 min, 4 ml of an aqueous solution of AgNO3 (4 x 10-3 mol dm-3) was added drop-wise to the mixture, and the temperature was further raised to 95 degC to make the solution boil quickly. The reaction was allowed to continue for another 30 min. Finally, the solution was cooled down to room temperature with stirring, and the NPs were collected by centrifugation at 5000 rpm and redispersed in ethanol via sonicating for 15 min.



Regeneration of native broadleaved species on clearfelled conifer plantations in upland Britain

Highlights
•
We examine native tree regeneration on clearfelled conifer plantations.
•
Mean regeneration density exceeded 1000stems/ha and was dominated by birch.
•
Regeneration is increased by the absence of ground flora after clearfelling.
•
Proximity to a wind-dispersed seed source increased natural regeneration.
•
Brash piles reduced regeneration density.
Abstract
In upland areas of Great Britain, large tracts of non-native conifer plantations have been established on poor quality agricultural land.
There is now considerable interest in the conversion of some of these plantations to a more natural woodland comprised of native tree species.
We studied the tree regeneration and ground flora on 15 upland sites (altitudes ranging from 120m to 380m above sea level) that had been clearfelled of conifers.
Regeneration of native tree species was successful where a clearcut site was adjacent to mature native trees, which acted as a seed source.
Mean regeneration densities of native tree species on clearcut sites were typically greater than 1000stems/ha, exceeding minimum recommended planting densities for the establishment of new native woodland.
Whilst 10 native woody tree species were recorded, the regeneration was dominated by birch species.
Regeneration densities were significantly higher on clearcut sites than on adjacent areas of unplanted moorland, probably due to the lack of a dense ground flora following the clearfelling operations.
Our results indicate that where local native seed sources exist, clearfelling upland conifer plantation sites to allow natural regeneration has the potential to be an effective method of establishing native woodland.

Introduction
Timber plantations have been widely established across Northern Hemisphere mid-latitudes (Zerbe, 2002; Yamagawa et al., 2010) with plantation forests now making up 14% of total forest area in western European countries (Forest Europe, 2011) and about 70% of total forest area in Britain (Brockerhoff et al., 2008).
These plantation forests usually consist of fast-growing, non-native conifer species located on marginal agricultural land in the uplands (Humphrey et al., 2006).
They are typically intensively managed for timber production with substantial site preparation before planting (e.g., ploughing, drainage, and occasional use of fertiliser) and harvesting of timber occurring by clearfelling after a relatively short rotation.
Whilst plantation forests can provide habitat for a range of species (Humphrey et al., 2000; Quine and Humphrey, 2010; Bremer and Farley, 2010; Coote et al., 2012), semi-natural woodlands typically contain greater biological diversity (Brockerhoff et al., 2008; Bremer and Farley, 2010).
Furthermore, plantation forests can result in soil and stream acidification (Carling et al., 2001) as well as potential negative impacts on water resources.
Recently, a greater interest in woodlands for their ecological and recreational value means that semi-natural and mixed forests consisting of native species are becoming increasingly valued (Felton et al., 2010).
As many plantations are now reaching the end of their rotations, there is considerable potential for establishment of semi-natural woodland on former plantation forest sites (Spiecker et al., 2004; Dedrick et al., 2007).
The restoration of plantation forests to semi-natural woodland can be carried out through a range of methods.
The conifer crop can either be clearfelled or the trees can be removed more gradually through multiple thinning operations.
There are also a range of methods for establishing native trees including planting, direct seeding or natural regeneration.
Natural regeneration is the establishment of trees from seeds produced in situ (Harmer and Kerr, 1995) and is the preferred means of achieving native woodland expansion in Great Britain (Forestry Commission, 1994).
Potential advantages of natural regeneration include the preservation of local genotypes and greater structural diversity of the resulting woodland (Peterken, 1996), high seedling density (Holgén and Hånell, 2000) as well as increased cost-effectiveness (Tarp et al., 2000; Jonásová et al., 2006).
Natural regeneration has been studied in a range of environments including degraded lowland tropical pasture (Parrotta et al., 1997), tropical mountain forests (Holl et al., 2000), boreal forest (Peltzer et al., 2000; Holgén and Hånell, 2000; Hanssen, 2003; Man et al., 2008; Man et al., 2009), lowland European forests (Madsen and Larsen, 1997; Emborg, 1998; Olesen and Madsen, 2008; Modrý et al., 2004; Swagrzyk et al., 2001; Harmer and Morgan, 2009; Wagner et al., 2010; Smit et al., 2012) and European mountain forests (Jonásová et al., 2010; Bace et al., 2012).
However, the regeneration of native species on clearfelled conifer plantations is still poorly understood (Zerbe, 2002) with Wallace (1998)'s study of birch regeneration in clearfelled spruce plantations the only previous study in upland Britain.
Here we report the first extensive study of natural regeneration of native hardwood species on clearfelled upland conifer plantations in Britain.
We addressed the following questions: (i) How well do native tree species regenerate on clearfelled upland conifer plantations? (ii) How does regeneration on clearfelled conifer plantations compare to regeneration on improved farmland and open moorland? (iii) What are the dominant factors controlling regeneration? (iv) How does the ground flora develop in the years following clearfelling and how does this impact tree regeneration?
Materials and methods
Experimental sites
We surveyed a total of 21 sites at 4 different upland locations: Hardknott forest and Rainsbarrow wood in the Lake District, north-west England and Clashindarroch forest and Bin forest in Aberdeenshire, north-east Scotland.
All forests surveyed were managed by the Forestry Commission.
The soil type, obtained from Forestry Commission soil maps, was used to predict the natural woodland community that would be expected to develop (Rodwell and Patterson, 1994).
Details of the sites selected are given in Table 1 and locations are shown in Fig. 1.
Hardknott forest was planted on upland moorland between 1940 and 1955 (N.
Williams 2008, Forestry Commission, personal communication).
There are several broadleaf woodland fragments of Quercus spp.
(oak spp.), Betula spp.
(birch), Sorbus aucuparia (rowan), Ilex aquifolium (holly) and Salix spp.
(willow).
Nearby Rainsbarrow woodland was planted with conifers between 1959 and 1962 and is designated as a Planted Ancient Woodland Site (PAWS) (Thompson et al., 2003).
PAWS are sites with a long history of forest cover, with the original semi-natural woodland cleared and replaced by a plantation, a practice that was widespread in the UK before around 1980 (Thompson et al., 2003).
Clashindarroch forest was established from 1930 onwards (Forestry Commission, 1964).
Prior to afforestation, the land was mostly upland moorland with a dense flora of Calluna vulgaris (ling heather) and Vaccinium myrtillus (bilberry) with limited areas of Pteridium aquilinium (bracken) on the lower elevations (Forestry Commission, 1952).
Bin forest was established from 1926 onwards when most of the land was upland moorland with dense ling heather vegetation (Forestry Commission, 1964).
Both Clashindarroch and Bin forests retained small fragments of semi-natural woodland consisting largely of birch and rowan as well as Alnus glutinosa (common alder) and willow on the wetter ground.
At these 4 locations we surveyed 15 sites that had been afforested with conifers, clearfelled and then left to regenerate naturally.
Table 1 details the species of the felled conifer crop, which was generally dominated by Picea sitchensis (Sitka spruce), matching the dominant conifer species used across Britain (Forestry Commission, 2012).
The harvesting residues, known as brash, were typically windrowed - that is, gathered into regularly spaced linear mounds known as brash mats or windrows.
Date of afforestation ranged from 1926 to 1942 and the date of clearfelling ranged from 1988 to 2009.
At the time of our surveys the time since clearfelling varied from 1 to 15years.
Table 1 details the date surveys were carried out.
The area of clearfells was estimated using digitized maps and varied between 0.9 and 35.2ha.
We compared the rates of native tree regeneration on these clearfelled sites to nearby areas which had not been previously planted with conifers (control sites).
We surveyed 6 control sites.
The control sites were typically situated less than 1km from the study sites.
At a number of the sites former agricultural use had resulted in considerable alteration to the vegetation and the physical and chemical properties of the soil.
Therefore we broadly classified all sites as either upland moorland (UM), upland improved farmland (IF) or PAWS (P) based on the present land-use of the control sites or the land-use prior to afforestation for the clearfelled sites.
Both the control and the clearfelled sites were fenced to exclude stock.
Capreolus capreolus (roe deer) and Cervus elaphus (red deer) were present at the Clashindarroch and Lake District sites.
Only roe deer occurred in Bin forest.
Deer control was practiced by the Forestry Commission at all sites.
Sampling methods
Statistical analyses
Trees and shrubs
(i)
The effect of environmental characteristics (distance to seed source, % vascular plant cover, % woody debris, altitude and soil pH) on the tree regeneration densities were examined using Spearman rank correlation coefficients.
The analyses were carried out separately for the dominant species that were identified (birch, alder, rowan, willow and oak).
(ii)
To explore the influence of site type, regeneration densities on clearfelled upland moorland (UM) and clearfelled improved farmland (IF) were compared to control areas of unplanted UM and unplanted IF using a nested analysis of variance (ANOVA).
To avoid confounding the effects of site type, time since clearfelling and soil type this analysis was conducted on a subset of 4 clearfelled brown earth UM sites that were predicted to develop to NVC type W11 (U2L, U3L, U4L and U5) with similar times since clearfelling to our clearfelled IF sites (also brown earth sites predicted to develop to W11).
Our control sites were also all brown earth soils (UL and Ua; Fa, Fb and Fc).
A lack of Lake District IF sites meant that we were unable to account for the effect of site location as a covariate.
The data was transformed using logarithms and the Satterthwaite approximation used due to unequal sample sizes.
When the difference was found to be significant the means of the site types were compared by Tukey's honestly significant difference (HSD) test.
(iii)
Regeneration densities on Lake District brown earth sites (U2L, U3L, U4L and U7L) were compared with densities on Lake District peaty gley sites (U9L and U10L) using a nested ANOVA.
The data was transformed using logarithms and the Satterthwaite approximation used due to unequal sample sizes.
(iv)
The Clark-Evans nearest neighbour method (Blackith, 1958) was used to analyse the distribution pattern of regeneration for the animal-dispersed tree species of oak and rowan.
This method computes the ratio (R) of the mean distance between nearest neighbours and the expected distance in the case of random distribution dran (dran=1/2D, where the density D=number of stems/area).
For R=1 the population is randomly distributed, for R significantly less than 1 the population is clumped and for R significantly greater than 1 the population is evenly dispersed.
A t-test was used to determine whether R was significantly different from 1.
(v)
A paired t-test (data transformed by square root) was applied to examine differences in regeneration density between the windrows and interrows at sites U6a, F2 and F4.
A 2-proportion z-test was used to compare the proportion of regenerating trees that were rowan in windrows and interrows.
(vi)
Linear regression analysis was used to examine the change in height of birch with time since clearfelling.
Ground flora
Ground flora characteristics in each quadrat were analysed as: (i) Total number of species, S, (ii) % vascular plant cover of each species, and (iii) linear regression analysis was used to examine the difference in vascular plant coverage with time since clearfelling.
Results
Tree regeneration
A total of 14 tree and shrub species were found to be regenerating, of which 10 were species native to Great Britain.
The non-native species consisted of three conifers (Sitka spruce, Pinus contorta (lodgepole pine) and larch) and one broadleaved species (Alnus incana (grey alder)).
The native species were birch, oak, rowan, willow, common alder, Fraxinus excelsior (ash), holly, Fagus sylvatica (common beech), Corylus avellana (common hazel) and Juniperus communis (common juniper).
The mean density of regeneration of native species on clearfelled sites varied from 0stems/ha to >5000stems/ha (Table 2).
While the regeneration density of non-native tree species is shown in Table 2 it is important to note that in a number of study sites regenerating non-native conifers had been felled, making it difficult to draw any conclusions about the frequency of non-native regeneration.
The linear regression of time since clearfelling on regeneration density of native species was not found to be significant (r2=0.26, n.s.).
Table 3 shows the density of regeneration for native species and the fraction of clearfelled sites where each species was recorded.
Regeneration was dominated by birch and rowan.
Whilst the regeneration of holly and oak were recorded infrequently (<20% of sites), relatively high regeneration densities were recorded at specific sites for these species (for example, 723stems/ha in the case of oak).
The regeneration density of birch and alder was found to be negatively correlated with distance from seed source (see Table 4).
In the case of birch, for example, 63% of regeneration occurred within 20m of a seed source.
No significant relationship was found for rowan or oak.
No significant relationship between plant cover and regeneration density was seen for any species.
However, when the regenerating trees were divided into sapling (taller than 0.5m) or seedling (shorter than 0.5m) categories then a significant negative correlation was seen between birch seedling density and vascular plant cover.
Birch also showed a significant negative correlation with the percentage of brash (woody debris).
No such effects were noted for alder, willow, oak or rowan.
Regeneration density against distance from seed source is plotted in Fig. 2.
In general, birch showed a broad shoulder of dense regeneration close to source, followed by a very rapid decline and then a long tail consisting of a slow decline.
Linear regression found a logarithmic decline in birch density with increased distance to seed source (see Fig. 2).
No significant correlation between distance from seed source (for distances up to 100m from the source) and regeneration density was seen for animal-dispersed species (oak and rowan).
However, the regeneration of both rowan and oak were still strongly clumped (R=0.23 and 0.28 respectively, both p<0.0001).
We found significantly higher regeneration in interrows (mean (M)=2313, standard deviation (SD)=3463) than in windrows (M=522, SD=1113; t(66)=5.694, p=5×10-5).
We found no statistically significant difference between the proportion of trees that were rowans in windrows and interrows (z=-0.456, n.s.).
Table 5 shows that the regeneration density of different site types (upland improved farmland or upland moorland).
Site type (upland improved farmland or upland moorland) produced a significant variation in total regeneration densities (F(3,8.9)=4.1, p=0.03).
20% of the total observed variation was due to variation between the different site types.
The overall regeneration density on clearfelled upland moorland was significantly greater than on unplanted upland moorland (p<0.01).
However there was no significant difference between the regeneration density of clearfelled improved farmland and unplanted improved farmland (see Table 5).
No significant difference in regeneration densities was found between brown earth and peaty gley soils (F(1,3.95)=1.75, p=n.s.).
Mean birch height increased significantly with time after clearfelling from 19cm tall at 2years to 101cm tall 10years post felling (p=0.03).
Fig. 3 contrasts the height distributions of birch trees 4years post-felling (measured at U4L) and 10years post-felling (measured at U10L).
Four years post-felling the number of regenerating trees declines exponentially with tree height so that we see large numbers of seedlings and few saplings.
Ten years post-felling this has changed to a more Gaussian distribution of heights with fewer seedlings.
Ground flora
We recorded 70 species of vascular plants across the study locations (detailed in Supplementary Table 1).
The most frequent and abundant species was the perennial Deschampsia flexuousa (wavy hair-grass), being found on 78% of quadrats surveyed.
The similarity of upland clearfelled sites was noteworthy: 5 species (bilberry, Galium saxatile (heath bedstraw), ling heather, foxglove and Potentilla erecta (tormentil)) occurred in all upland sites and only 2 species occurred at a single site (Ajuga reptans (bugle) and Valeriana dioica (common valerian), both found at U10).
The predicted woodland type on clearfelled brown earth sites was W11 - upland oak - birch woodland with Hyacinthoides non-scripta (bluebell) (see Table 1).
However, on UM clearfelled sites desired invader species such as Oxalis acetosella (woodsorrel), Anemone nemorosa (wood anemone), Conopodium majus (pignut) and Primula vulgaris (primrose) were not found, while bluebell was seen on only 15 quadrats and Teucrium scorodonia (wood sage) on just 2.
The solitary PAWS site that was examined had a considerably richer ground flora with wood sorrel, wood sage and bluebell seen on 21%, 29% and 79% of quadrats respectively.
We found that the sites which had been clearfelled 10years ago had significantly greater vascular plant coverage (111%) compared to sites that had been clearfelled 2years ago (11.7%, p=0.001).
The % mean woody debris on spruce clearfell sites declined from 51% 2years after felling to 12.7% and 5.1% at 5 and 10years post-felling respectively.
Discussion and conclusion
We have explored the regeneration density of native broadleaved species on clearfelled conifer sites in upland Britain.
We compared regeneration on clearfelled sites to control sites that had neither been planted with conifers or clearfelled.
We restricted our analysis to a subset of sites with similar time since clearfelling and soil type.
Mean regeneration density on this subset of clearfelled upland moorland sites (3392individuals/ha) was significantly greater than on upland moorland (64individuals/ha) or improved farmland (14individuals/ha) sites.
Availability of data meant that in this analysis we combined sites across regions (Lake District and eastern Scotland) and were unable to account for site location as a covariate.
Regeneration density on all clearfelled upland moorland sites (3515individuals/ha) was at the lower end of that recorded by Harmer and Morgan (2009) (3000-11,000individuals/ha) in a storm damaged lowland conifer site in south-east England that had been allowed to naturally regenerate.
The regeneration density we recorded was lower than conifer regeneration within small windthrows (Jonásová et al., 2010) or clearfells (Modrý et al., 2004; Holgén and Hånell, 2000) where sapling densities as great as 160,000individuals/ha have been recorded (Modrý et al., 2004; Holgén and Hånell, 2000; Jonásová et al., 2010).
The high regeneration density in these studies was likely due to an ample seed source due to the surrounding woodland whereas in our study the seed source was limited to individual mature trees.
Nevertheless, the regeneration density on clearfelled upland moorland sites and a clearfelled PAWS site (5790stems/ha) exceeded the suggested sapling stocking densities for new native woodland in Britain of between 500 and 2000stems/ha (Forestry Commission, 2010).
The diversity of regenerating species was usually lower than that of the adjacent seed sources with regeneration dominated by birch on all but one clearfelled site, as has been found previously at storm damaged lowland sites in Britain (Harmer and Morgan, 2009; Harmer et al., 2011) and elsewhere in Europe (Degen et al., 2005).
Overall, birch accounted for 56% of regenerating saplings in our study.
The density of birch regeneration on clearfelled upland moorland on our study sites is similar to that recorded in a storm damaged lowland conifer site in Britain (Harmer and Morgan, 2009) and to clearfelled upland conifer sites in Scotland (Wallace, 1998).
Despite the presence of mature individuals of ash, beech, juniper and hazel adjacent to clearfelled sites only a handful of saplings of these species were noted.
Overall we found that pioneer, shade-intolerant species such as birch, rowan and willow regenerated more frequently than shade-tolerant species such as beech and holly (Brzeiziecki and Kienast, 1994).
We explored the role of distance from seed source on regeneration density for distances up to 100m from the source.
The regeneration of the small-seeded and wind-dispersed alder and birch species were found to be strongly dependent on the distance from parent trees.
The majority of the saplings were found within 20m of a parent tree, although for birch there was a long tail, limited in our study to the width of the clearfelled site.
The patchy distribution which results from this clumping around seed sources is not necessarily a disadvantage for establishment of natural woodland.
Rodwell and Patterson (1994) suggest that 20-50% of woodland sites should be retained as open ground to enhance structural diversity and wildlife value.
The fluctuations in sapling density may result in a more natural woodland structure to that produced through planting.
The shoulder of the regeneration curve at distances less than 10m from the woodland edge could be attributable to an edge effect - root competition or light and rain interception from the mature trees counteracting the increased regeneration caused by the rise in seed density as you approach the edge.
The seed dispersion curve for a point source (Harper, 1977; Nathan et al., 2001) is similarly shaped to the regeneration curves for solitary trees in having a peak in seed fall density a short distance from the parent tree.
Regeneration of oak and rowan was found to be significantly clumped although not significantly dependent on distance from the seed source.
Rowan is primarily dispersed through ingestion by birds, particularly various thrush species (Raspe et al., 2000), while oak relies on hoarding by both birds and mammals but especially Garrulus glandarius (jay) and Apodemus sylvaticus (wood mouse) (Forget et al., 2004), both of which occur at the study sites.
The distribution of regenerating saplings will therefore be partly controlled by the behaviour of the dispersing animal.
Previous work in central Europe has demonstrated that the majority of oak regeneration occurs within 100m of a seed source and declines rapidly at greater distances (Mirschel et al., 2011).
However, our findings are in contrast to previous work carried out in lowland sites in the UK that found positive relationships between the number of oak seedlings and distance to parent trees but no significant effect for birch seedlings (Harmer et al., 2005), possibly indicating differences between the shelterwood examined by Harmer et al. (2005) and the more extensive clearfells that we considered.
The determination of any relationship between vascular plant cover and regeneration density was complicated by the constantly changing nature of ground flora - the current vegetation structure does not necessarily reflect that present when the seedlings first started growing.
Indeed, the only significant correlation between regeneration density and vascular plant cover was the negative correlation found for birch seedlings (shorter than 0.5m).
The small size of a birch seed means that its food reserve is only sufficient to grow to 2cm in height (Miles and Kinnaird, 1979), before it must be able to support itself through photosynthesis.
This results in birch's difficulty in establishing itself in thick vegetation.
Scarification (exposure of mineral soil) can increase seedling density in birch spp.
(Kinnaird, 1974; Karlsson, 1996).
The ground disturbance and lack of ground vegetation after clear felling provides opportunities for seedlings to become established in bare ground before it is covered with vegetation.
In contrast, the lack of regeneration seen on the unplanted upland moorland and unplanted improved farmland sites is likely due to the dense flora coverage (120% and 142% respectively) in combination with the lack of any ground disturbance.
The rate of tree growth was slow, with regenerating trees achieving a median height of 104cm after 10years of growth post-felling.
These growth rates are markedly poorer than those recorded by Harmer and Morgan (2009) in lowland England or by Worrell et al. (2000) in upland NE Scotland.
We found that the height distribution of the regenerating trees changed with time since clearfelling (Fig. 3), with large numbers of small trees 4years post-felling changing to a more even distribution of heights 10years post-felling.
This indicates that the recruitment of new trees is most prolific in the first few years following felling, with fewer seedlings 10years post-felling indicating a slowdown in this process.
This decline is likely to be driven by the increase in herbaceous cover following clearfelling combined with the negative correlation between birch regeneration and herbaceous cover.
The weighting of seedling recruitment to the years immediately following clearfelling may also contribute to the observed site to site variability in regenerating tree number since any temporal fluctuations in the ability of trees to regenerate will have substantial effects on the resulting density.
Potential factors influencing interannual variability in seed dispersal and seedling germination include temporal variation in seed production (Harper, 1977) and climatic factors such as wind speed or precipitation (Nyland, 1996) and amount of snow cover (Greene and Johnsson, 1997; Forestry Commission, 2004).
We found that the dense layers of brash produced by windrowing significantly reduced the amount of natural regeneration.
Windrows could be up to a metre high and several metres wide, producing a physical barrier that prevented seedling establishment and creating regions with little or no regeneration.
While we might expect seedlings from larger seeded species like rowan (200,000 seeds weigh 1kg) to have an advantage over seedlings from smaller seeded species such as birch (5.9million seeds weigh 1kg) in growing through brash (Leishman and Westoby, 1994) we found no significant difference between the proportion of rowan in windrows and interrows.
Furthermore, previous studies have found that where grazing pressure is high, brash (Truscott et al., 2004) and coarse woody debris (Smit et al., 2012) can help protect seedlings from browsing.
However, it is difficult to draw any conclusions from our study as only a single site (U15) recorded significant browsing.
The low incidence of browsing at our study sites (grazing pressure was controlled) means that grazing is unlikely to limit regeneration (Palmer et al., 2004; Olesen and Madsen, 2008; Yamagawa et al., 2010).
Clearfelled sites undergo substantial ground disturbance resulting in a mean 19% ground flora coverage 2years post-felling.
On upland moorland sites, vegetation after clearfelling was largely comprised of ruderal species such as wavy hair-grass and Deschampsia cespitosa (tufted hair-grass) before being joined by species associated with open moorland like ling heather and G. saxatile (heath bedstraw).
Colonisation by woodland ground flora species was poor.
Many previous studies have focused on restoration of PAWS to semi-natural woodland with current advice advocating a gradual approach to restoration through thinning (Thompson et al., 2003; Woodland Trust, 2005).
In this study we explored the potential conversion of conifer plantations on upland moorland and improved farmland to semi-natural woodland through a process of clearfelling followed by natural regeneration.
There has been comparatively little work carried out on this despite the large area of uplands used for conifer plantations in Britain.
We found that where remnants of native woodland survive, clearfelling results in conditions favourable for natural regeneration and typically producing regeneration densities of native species equal to or greater than that recommended for planting.
Where forest managers aim to develop part of their forest estate as native woodland, we recommend sites be surveyed for native woodland remnants and adjacent conifers clearfelled to allow regeneration of native woodland.
Where seed sources of non-native conifer exist these species may also regenerate at high densities (Stokes et al., 2009; Stokes and Kerr, 2013) and further work is needed to explore to what extent this hinders the development of semi-natural woodlands.
Gradual thinning of the conifer crop may be less likely to produce ideal conditions for natural regeneration (disturbed soil and little ground vegetation) while extending the supply of non-native conifer seed sources (Stokes et al., 2009), although further work is required to compare these approaches.
Taking advantage of the natural regeneration process means that it may be possible to produce semi-natural woodland of a high ecological and landscape value at a substantially reduced cost (Jonásová et al., 2006).
However, where extensive thinning of non-native species would be required this would greatly increase costs (Stokes and Kerr, 2013).
We found natural regeneration was mostly of shade-intolerant pioneer species and was dominated by birch.
The lack of important timber producing species within the regeneration has been raised as a concern in lowland British sites (Harmer and Morgan, 2009) but is less likely to be a issue for upland sites where timber production may be a lower priority.
The dominance of birch within natural regeneration follows the expected pattern of natural succession and, given oak seed sources in the area, we might expect oak regeneration to follow in due course (Patterson, 1993).
Future work will quantify the rate at which oak seedlings establish and explore whether supplementary planting may be required.
Given that recent work (Harmer and Kiewitt, 2007; Harmer et al., 2011) has shown that a gradual conversion of lowland conifer PAWS may not always allow satisfactory regeneration of broadleaved tree seedlings, we feel that clearfelling of conifer plantations followed by natural regeneration as a method of establishing semi-natural woodlands warrants further research and consideration.
Acknowledgments
We acknowledge the Forestry Commission for permitting site access and providing maps.
The Friends of the Lake District and the Natural Environment Research Council (NE/G015015/1) provided funding for this study.
We acknowledge two anonymous reviewers whose comments greatly improved this manuscript.
Supplementary material
Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.foreco.2013.08.001.
Supplementary material
Supplementary data

Surface modification of Mg-doped spinel with different Li-containing manganese oxides
All the chemicals used here were purchased from Tianjin Chemical Reagent Co. Ltd. (AR, 99 %). Mn(CH3COO)2*4H2O, MgSO4*7H2O, Ni (CH3COO)2*4H2O, and LiOH*H2O were used as starting materials. At first, an stoichiometric amount of Mn(CH3COO)2*4H2O and MgSO4*7H2O (cationic ratio of Mn:Mg = 1.95:0.05, Mn-Mg solution) was dissolved completely in deionized water for Mg-doped spinel precursor; a mixture solution of Ni(CH3COO)2*4H2O with Mn(CH3COO)2*4H2O (cationic ratio of Ni:Mn = 0.5:1.5, Ni-Mn solution) was prepared for LiNi0.5Mn1.5O4 coating layer, and a saturated LiOH*H2O solution was also prepared as precipitant avoiding incorporating impurity in case other precipitant is used. Then the Mn-Mg solution was dropped into the LiOH*H2O solution in a flask under continuously stirring in a N2 atmosphere. For the LiNi0.5Mn1.5O4-coated spinel, after the coprecipitate Mg0.05Mn1.95(OH)4 formed, the Ni-Mn solution was then dropped into the above coprecipitate suspension to obtain Mg0.05Mn1.95(OH)4-Ni0.5Mn1.5(OH)4 coprecipitate, followed by the same treatment as that for the bare spinel. To obtain the Li2Mn4O9-coated and Li4Mn5O12-coated spinels, two different stoichiometric aqueous solutions of Mn(CH3COO)2*4H2O were dropped into the above-prepared coprecipitate Mg0.05Mn1.95(OH)4 suspensions to get Mg0.05Mn1.95(OH)4-Mn(OH)2 coprecipitate. After dropping was finished, the suspension with Mg0.05Mn1.95(OH)4 coprecipitate coated with and without three different layers formed and then aged for 1 day. These suspensions were then filtered to get the sediment followed by washing with deionized water until the pH value degraded to 7, indicating that there is no residual Li ion inside the precursor. The sediments were then heated at 500 degC for 5 h in air to form a composite oxide. After mixing with 3 % excessive in molar ratio of LiOH*H2O, the powder was calcined at 750 degC for 10 h to obtain the final bare spinel LiMg0.05Mn1.95O4, the Li2Mn4O9-coated, Li4Mn5O12-coated, and LiNi0.5Mn1.5O4-coated LiMg0.05Mn1.95O4 spinels. All the coated layers were in a ratio of 3 wt.%.Optimization of reaction parameters for the electrochemical oxidation of lidocaine with a Design of Experiments approach
Lidocaine (L7757), monoethylglycinexylidide (MEGX, SML0087, the N-dealkylated form of lidocaine) and ammonium hydroxide (NH4OH, 221228) were purchased from Sigma-Aldrich. Formic acid (HCOOH, 94318) and acetaminophen (00370) were purchased from Fluka and ultra-pure HPLC grade acetonitrile (ACN, 01203502) was purchased from Biosolve. Trifluoroacetic acid (TFA, 289084) was purchased from Thermo Fisher, and sulfuric acid (H2SO4, 100731100) from Merck Millipore. Ultrapure water was obtained from a Milli-Q Advantage A10 Water Purification system (Millipore Corp., Billerica, MA, USA).Enhanced electrochemical performance of Ti-doped Li1.2Mn0.54Co0.13Ni0.13O2 for lithium-ion batteries
Li-rich layered oxide materials Li1.2-xTixMn0.54Co0.13Ni0.13O2 (x = 0, 0.025, 0.05, 0.10, 0.20, 0.50, denoted as Pristine Ti, Ti-2.5, Ti-5, Ti-10, Ti-20, Ti-50 respectively) were synthesized by a modified Pechini method. Stoichiometric amounts of Co(CH3COO)2*4H2O, Ni(CH3COO)2*4H2O, Mn(CH3COO)2*4H2O, Ti(OCH(CH3)2)4 and excess (5%) LiCH3COO*H2O were dissolved in absolute alcohol together with citric acid and ethylene glycol (molar ratio = 1:4) to form a polymeric precursor. More details can be found in our previous paper [22]. The precursor was heated at 400 degC for 5 h before it was calcined at 800 degC for 12 h in air.Fabrication and characteristics of BaTi0.85Sn0.15O3 thin films on tin doped indium oxide/glass substrate

Raw materials of BaCO3, SnO2 and TiO2 were first weighed in accordance with the composition of BaTi0.85Sn0.15O3. After mixing and ball milling for 1 h, the mixture was dried for 24 h and then ground. Then, the calcining process with a temperature of 1100 degC for 2 h was conducted. Finally, the pressed target was sintered at 1370 degC in an air ambient for 2 h. The BTS films with various thicknesses were deposited on ITO/glass substrate with 200 nm ITO thickness and In/Sn = 90/10 using the rf magnetron sputtering at 600 degC substrate temperature through controlling the sputtering time. The target of BTS was placed about 5 cm away from the ITO/glass substrate. Thin films were deposited under the sputtering parameters such as the rf power of 160 W, chamber pressure of 1.3 Pa and oxygen concentration of 25%. The film deposition rate is estimated to be 3.6 nm/min.


Facile synthesis of hierarchical hollow MoS 2 nanotubes as anode materials for high-performance lithium-ion batteries


In a typical procedure, 1.165 g of Na2MoO4*2H2O, 0.312 g of MnCl2*4H2O and 1.37 g of (NH2)2CS were dissolved in 30 ml of deionized water and 10 ml of absolute ethanol. After stirring for 30 min, the solution was transferred into a 50 ml Teflon-lined stainless steel autoclave and sealed tightly and then heated at 210 degC for 16 h. After cooling naturally, the black precipitates were collected, washed with diluted hydrochloric acid and deionized water several times, and dried at 60 degC for 5 h in a vacuum oven. Finally, the hierarchical hollow MoS2 nanotubes were obtained. MoS2 nanoflowers were obtained when the reagents were dissolved in 30 ml of absolute ethanol and 10 ml of deionized water with the other reaction conditions left unchanged.


Uniform iron oxide hollow spheres for high-performance delivery of insoluble anticancer drugs
Ferric chloride hexahydrate (FeCl3*6H2O), anhydrous sodium acetate (NaOAc), polyethylene glycolethylene (PEG-2000), glycol (EG), ethanolamine (ETA), and ethanol were purchased from Beijing Chemicals. Camptothecin (CPT) was purchased from Aladdin Reagent. All chemical agents used in these experiments were of analytical grade and used directly without further purification.
The monodisperse Fe3O4 hollow spheres were synthesized using a one-pot solvothermal method. In detail, 1.5 g of FeCl3*6H2O was dissolved in 40 mL of solvent containing EG (30 mL) and ETA (10 mL) to form a stable orange solution. 4.0 g of NaAc and 1.0 g PEG-2000 were added into the above solution under vigorously magnetic stirring until completely dissolved. The homogeneous solution obtained was transferred to a Teflon-lined stainless-steel autoclave (50 mL), sealed and heated at 200 degC. After reacting for 8 h, the autoclave was cooled to ambient temperature naturally. The obtained magnetite particles were washed with ethanol and deionized water in order, and then dried under vacuum at 60 degC for 12 h. The solvent effect of ETA in the solvothermal process was investigated in detail with different amounts of EG and ETA.The Sleipner storage site: Capillary flow modeling of a layered CO2 plume requires fractured shale barriers within the Utsira Formation

Highlights
•
Flow model for Sleipner with a plausible layering of CO2 plume and mass balance.
•
CO2 mass balance analysis for poor temperature and layer thickness constraints.
•
An unusual capillary flow model, not Darcy flow, results in a successful 3D model.
•
Low shale barrier threshold pressures indicate pervasive micro-fracturing.
•
Pre-CO2 fracturing hypothesis based on rapid melting of Pleistocene ice sheets.
Abstract
To prevent ocean acidification and mitigate greenhouse gas emissions, it is necessary to capture and store carbon dioxide.
The Sleipner storage site, offshore Norway, is the world's first and largest engineered waste repository for a greenhouse gas.
CO2 is separated from the Sleipner gas condensate field and stored in the pore space of the Utsira Formation, a saline aquifer approximately 1km below the surface and 200km from the coast.
Statoil, the field operator, has injected almost 1Mt/yr of captured CO2 into the storage site since 1996.
The buoyant CO2 plume ascended rapidly through eight thin shale barriers within the aquifer to reach the top seal in less than three years.
The plume's progress has been monitored by eight seismic surveys, as well as gravimetric and electromagnetic monitoring, which record the spreading of nine thin CO2 layers.
This paper presents a capillary flow model using invasion percolation physics that accurately matches the plume's geometry.
The approach differs from standard Darcy flow simulations, which fail to match the plume geometry.
The calibrated capillary flow simulation indicates that a mass balance for the plume is likely, but can only replicate the plume geometry if the thin intra-formational shale barriers are fractured.
The model enables an estimate of the shale barrier behavior and caprock performance.
The fractures are very unlikely to have been caused by CO2 injection given the confining stress of the rock and weak overpressure of the plume, and so fracturing must pre-date injection.
A novel mechanism is suggested: the deglaciation of regional ice sheets that have rapidly and repeatedly unloaded approximately 1km of ice.
The induced transient pore pressures are sufficient to hydro-fracture thin shales.
The fractures enable fast CO2 ascent, resulting in a multi-layered plume.
Shallow CO2 storage sites in the Northern North Sea and other regions that have been loaded by Quaternary ice sheets are likely to behave in a similar manner.

Introduction
Climate change and ocean acidification are driven by high rates of CO2 emission to the atmosphere.
One contribution to the mitigation of CO2 emissions is carbon capture and storage, CCS (Lovell, 2011).
This is proposed by the G8 and the International Energy Agency as an essential technology for lowering greenhouse gas emissions.
CCS reduces the emission of CO2 at a power station, or other large industrial sources such as oil and gas fields.
This captured CO2 is compressed, transported by pipeline, and injected for storage into porous rock formations deep below the land or sea surface.
Large-scale sites have been demonstrating engineered CO2 storage technology for nearly two decades.
At the Sleipner field, CO2 is produced with natural gas and condensate, separated from the production stream, and compressed.
The supercritical CO2 is then injected into the Utsira Formation, offshore Norway.
This is the longest running carbon capture and storage experiment in the world, with more than 14 Mt of supercritical CO2 injected during the period 1996-2013.
The injection schedule is intended to remain at around 0.9Mt/yr until around 2020, separating and storing CO2 from the West Sleipner gas-condensate field to prevent climate change.
This is incentivized by the Norwegian state tax on offshore petroleum industry emissions, commencing in 1991 at NOK 210 and increasing to NOK 410 in 2013, equivalent to about $65 per tonne of CO2.
The storage site, a saline formation and sandstone aquifer, is 800m below sea level (mbsl).
This is considered to be a shallow depth setting for a storage environment (Chadwick et al., 2008) as the pressure and temperature conditions are likely to be close to the critical point above which CO2 becomes a more buoyant gas phase.
The overlying Nordland Group, which extends from the top of the Utsira Formation to the seafloor, is predominantly shale (Fig. 1), and is expected to provide a caprock with a high threshold pressure, sufficient to seal the site and prevent leakage of the buoyant CO2 fluid over several millennia, i.e. the timescale required to offset climate change (Lindeberg and Bergmo, 2003).
Saline formations worldwide are considered to be candidates for carbon sequestration because of their suitable depths, large storage volumes, and common occurrence.
However, one of the critical uncertainties for saline formation storage is the ability of the caprock (primary seal), and the overlying containment geology (secondary seals), to retain buoyant CO2 without leakage.
The fluid flow processes currently occurring in the Sleipner storage site provide a crucial exemplar for saline formation storage projects worldwide.
Published interpretations of a series of seismic reflection surveys show that, by 1999, the Sleipner CO2 plume had ascended more than 200m vertically within the Utsira Formation from the injection point (1012mbsl) to the caprock (800mbsl).
The plume has encountered and breached eight shale barriers within the storage site: seven thin shales that are approximately 1-2m thick, and an uppermost thick shale that is 6-7m thick and geologically similar to the primary seal (Gregersen and Johannessen, 2001; Zweigel et al., 2004).
These shale barriers result in 10-20m thick CO2 layers that are vertically stacked and extend laterally for hundreds of meters.
Biennial monitoring of the plume behavior and the resultant six seismic reflection surveys make this experiment ideally suited to numerical modeling studies (Hamborg et al., 2003; Zweigel et al., 2004; Hellevang, 2006).
The areal distribution of CO2 stored within the storage site has been precisely mapped from the seismic surveys (Chadwick et al., 2006; Bickle et al., 2007).
However, no published dynamic model to date has accurately replicated the layered morphology of the plume or flow behavior, and three significant aspects of the plume are poorly understood, as outlined below.
Fundamental aspects of the CO2 plume layering are examined by constructing a mass-balanced flow model to test the physical containment processes through iterative numerical simulation.
The simulations are calibrated to published observations.
Capillary flow, not Darcy flow, is applied to obtain a good match with observations.
It is essential for the calibration that the shale barriers within the Utsira Formation have high vertical permeabilities.
This implies that the shales may be fractured.
A novel mechanism is proposed to account for fracturing prior to CO2 injection: transient fluid overpressure during rapid deglaciation.
Unexplained aspects of the plume
Firstly, there is a significant uncertainty concerning mass-balance estimates for the plume.
To illustrate this, consider estimates for the uppermost layer alone, circa July 2002, equivalent to 5Mt injected, bearing in mind that the uppermost layer is the best constrained portion of the plume with respect to observational data (Chadwick et al., 2008).
The CO2 volume in this layer is estimated to be approximately 110-165 thousand cubic meters (Chadwick et al., 2009).
Assuming an upper layer density of 426kg/m3 (after Bickle et al., 2007), this is equivalent to a mass of 0.05-0.07Mt.
Although Chadwick and co-workers (2009) favor the upper end of this range, 0.07Mt is considerably less than estimates of 0.11Mt (Bickle et al., 2007) and 0.16Mt (Singh et al., 2010).
This wide disparity in outcomes stems partly from (a) uncertainty concerning the plume temperature profile (Fig. 2a), with Singh and co-workers (2010) assuming a much colder plume with an uppermost layer density of 760kg/m3; and (b) poorly constrained assumptions regarding layer thickness (Fig. 2b), which are unresolvable at less than 15m thick given the seismic resolution (Arts et al., 2004).
This uncertainty is further compounded by poorly constrained gas saturations for the plume layers.
All three aspects are discussed further below.
Secondly, despite the layer thickness uncertainty, the plume morphology indicates that column heights for trapped CO2 are unexpectedly low.
The only published laboratory measurements for the caprock threshold pressure (Springer and Lindgren, 2006; Harrington et al., 2009), from a cored production well, 15/9A-11, close to the storage site, suggest a range of 1.6-1.9MPa, i.e. the fluid pressure necessary for CO2 to break through a low permeability rock.
If this range is applicable to the shale barriers within the Utsira Formation, such a scenario would result in CO2 column heights of hundreds of meters (Springer and Lindgren, 2006).
However, estimated column heights for the layered plume consistently fall within the range 7-14m (Chadwick et al., 2005, 2006; Bickle et al., 2007).
These thin layers occur beneath all three-barrier types: (a) the 1-2m thin shales within the formation; (b) the uppermost 7m thick shale barrier; and (c) the much thicker Nordland Group shale caprock that forms the primary seal.
Thirdly, the plume flow behavior is not indicative of sealing shale barriers punctuated by faults, holes or penetrated by a high permeability chimney or sand injectite (Zweigel et al., 2004), and the means of CO2 ascent is poorly understood.
The plume initially ascended 200m vertically through the eight shale barriers in less than three years (Chadwick et al., 2006), resulting in a 'pancake stack' of layers.
If the laterally extensive shales had acted as seals, preventing the vertical migration of CO2, the plume would have taken much longer to breakthrough, and its behavior would have been more akin to a 'zig-zag' distribution with lateral offsets, resulting from the CO2 tracking along the base of a barrier until encountering a hole through which to escape up to the next barrier, and then repeating this behavior.
Mass balance estimates
Addressing the mass balance question first, available areal mapping of the plume layers derived from time-lapse seismic reflection survey images (Bickle et al., 2007) are combined with layer-thickness estimates (Chadwick et al., 2009) to build a three-dimensional representation of the CO2 plume (Fig. 1).
The approach assumes a flat gas-water contact for the plume layers, which implies that each layer has backfilled the trap as a gravitationally ponding fluid.
The model provides an insight into the volumetric relationship between layer thickness and mass balance (Fig. 2).
Recent published estimates of volume and mass balance appear to have been primarily hampered by uncertainty in layer thickness and plume density (Fig. 2).
For the uppermost layer, circa 2002, thickness estimates vary from 4 to 7m (Chadwick et al., 2005; Bickle et al., 2007).
The CO2 layers circa 2006 have recently been estimated as 10m thick (Lippard et al., 2008; Chadwick et al., 2009).
Both these estimates are thinner than the limit of phenomena resolvable by conventional seismic reflection surveys at this depth (Arts et al., 2004; Chadwick et al., 2006).
A case has been made for amplitude tuning of the Sleipner seismic to produce greater precision (Chadwick et al., 2006); however, the sensitivity of this seismic interpretation technique is such that the amplitude does not discriminate between layers more than 4m thick and less than 15m thick.
Furthermore, the thin shale barriers within the aquifer are unresolvable on the baseline 1994 seismic (Arts et al., 2004) making tuning even more difficult.
Published estimates of the CO2 volume in the top layer (110,000m3 from amplitude analysis, and 165,000m3 from structure) differ by 150% for the same seismic survey (Chadwick et al., 2009).
Although structure estimates are considered to be more reliable than amplitude (Chadwick et al., 2008), there is no method to discriminate between them.
An additional uncertainty arises from saturation profiling, which discerns between pores filled with brine and pores filled with CO2 above a poorly constrained level of saturation (Arts et al., 2004).
It follows from the commonly used Gassmann equation (Gassmann, 1951) that: (a) very low saturations of less than a few percent are undetectable; (b) a strong correlation emerges as the gas saturation increases from a few percent to around thirty percent; and (c) for saturations much above thirty percent, it is difficult to distinguish between moderate and high saturations.
It follows that the 80% gas saturation that is commonly assumed for the Sleipner plume, while reasonable (Chadwick et al., 2005; Bickle et al., 2007), remains uncertain (Lumley, 2008).
If the 80% assumption represents a reasonable upper limit to the mean saturation for the plume, the lower limit could be as low as 40%, halving mass balance estimates premised on the widely assumed high-saturation value.
Finally, the liminal pressure and temperature conditions of the plume with respect to the critical point of CO2 (31°C and 7.4MPa) compound the uncertainty.
The ambient temperature at the injection depth is reasonably well constrained at 41±1°C (Bickle et al., 2007); however, the temperature at the top of the plume is much more poorly constrained at around 34±3°C (Singh et al., 2010).
This results in a potential density range for CO2 in the uppermost layer of 300-700kg/m3; a mid-range scenario is assumed, with a temperature of 35°C at the caprock and a linear geothermal gradient to the injection depth at 41°C for the flow simulations that follow (orange diamonds, Fig. 2a).
However, a brief consideration of different thickness and density scenarios (assuming 80% saturation) with respect to the solution space for mass balance (gray area, Fig. 2b) suggests that a cold high-density plume requires an upper layer that is at least 8-9m thick, circa July 2002 and 5Mt injected; while a hot low-density plume exceeds a mass balance with layers that are greater than 13m thick.
The uppermost layer thickness necessary for a mass balance, circa July 2002 and 5Mt injected, lies in the range 8-13m.
Consequently, these compounded uncertainties in mass balance (layer thickness, gas saturation and gas density) are highly sensitive to small changes in assumptions that are unresolvable by remote geophysical monitoring given the broadly constrained fields of pressure, temperature and saturation.
Although an accurate mass balance estimate would be desirable to confirm site integrity with respect to the present-day caprock behavior, this aspect of Sleipner is likely to remain elusive due to the uncertainties inherent in remote geophysical monitoring of gas plumes.
Modeling methodology
To better understand the mass balance, spatial distribution and flow behavior of the plume, a numerical flow model of the storage site is constructed.
This three-dimensional model consists of a geological framework of nine alternating intervals of sandstone and shale that extends from the base of the Utsira Formation below the injection point to the primary seal above the storage site.
Published morphology maps for each CO2 layer (Bickle et al., 2007) are used as the topography for the base of each shale barrier, assuming a flat gas-water contact for the layers.
This implies that the CO2 is ponding, and the layers are close to equilibrium with respect to any Darcy flow dynamics (Cavanagh, 2013).
The flow of CO2 within the storage site is then simulated with an invasion percolation simulator (Permedia, 2012), assuming percolation as a function of CO2 capillary pressure and shale threshold pressure.
This approach examines capillary flow in a deep saline formation that is segmented by thin laterally extensive horizontal shale layers.
Scenarios are tested for resemblance to the plume morphology and then quantified for CO2 in place to arrive at the best mass balance scenario for the simulated plume with respect to the known injected volume.
Conceptual model
The conceptual model for the simulation is a plume of vertically stacked buoyant CO2 layers that are trapped in the sandstone intervals as thin but highly saturated layers, breaking through the shale barriers as a result of percolation.
Percolation occurs when the threshold pressure of the shale barrier is exceeded by the buoyant gas capillary pressure of the trapped CO2 layer (Boettcher et al., 2002; Carruthers, 2003).
The 3D model is calibrated by adjusting the breakthrough threshold pressures of the shales to match published estimates of layer thickness (column heights underlying shale barriers) and the areal distribution for trapped CO2 derived from 4D seismic.
The Sleipner plume is likely to ascend as a result of gravity segregation (Singh et al., 2010), given the strong density contrast between the brine and CO2, and the high permeability of the Utsira Formation sandstones.
However, the crucial distinction between our model and other approaches is our assumption concerning the dominant physics of the flow process.
The buoyant separate phase behavior of CO2 in the formation not only promotes gravity segregation, but also results in strong capillary forces at the interface between the ascending gas (non-wetting fluid) and the brine (wetting fluid) in the pore throats of the geological media, which manifest as threshold pressures for the sandstones and shales.
This is quite distinct from other modeling approaches that assume a Darcy flow regime, where the plume behavior is primarily a function of the interplay between viscosity, pressure gradients, and permeability.
If capillary forces dominate over viscous forces, the plume will ascend and backfill beneath flow barriers, manifesting a sensitivity to the topography at the base of each shale barrier (Cavanagh, 2013).
Given the known correlation in shape between the upper layers and the basal topography of the caprock and underlying thick shale, it is proposed that the flow behavior may be best characterized as a gravity-dominated percolation phenomenon, as explained below.
Choice of simulator
Previous attempts to model the distribution of CO2 within the Utsira Formation have used fluid flow software based on Darcy flow physics.
To obtain multiple layers of CO2, these have either imposed a discrete vertical pathway to by-pass the thin shales within the Formation (Chadwick et al., 2006; Hermanrud et al., 2009, 2010) or assumed a convenient vertical juxtaposition of pre-existing holes (erosional or otherwise) for the eight intra-formational shale barriers.
These simulations fail to quantitatively or qualitatively reproduce the multi-layered CO2 plume.
To quote Chadwick et al. (2006) 'observed fluxes derived from the seismic data, do not match the flow simulation'.
In this approach, a flow simulator based on a different flow physics is used, and a different workflow undertaken to achieve a very good match and calibrated mass balance.
Permedia Migration is a multi-phase invasion percolation simulator.
The simulator uses capillary threshold pressures and fluid density descriptions for oil, gas and CO2.
Assuming a flow domain dominated by capillary and gravity forces, invasion percolation describes the behavior of an immiscible fluid (CO2) that is migrating, or percolating into, a porous medium (sandstones and shales).
Commonly known as drainage within petroleum systems modeling, as distinct from imbibition, the buoyant phase displaces the original pore fluid (brine).
Invasion percolation is commonly applied to slow moving immiscible fluids in geological flow systems.
Below a critical flux velocity threshold, such flow systems tend to self-organize into discrete migration pathways and pooling traps (de Gennes et al., 2004).
Percolation theory has been successfully applied to the invasion of non-wetting fluids such as oil and gas into water-wet porous geological media when studying regional hydrocarbon migration and local hydrocarbon field charge processes (Carruthers and Ringrose, 1998; Boettcher et al., 2002; Glass and Yarrington, 2003).
The approach is valid at low flux rates, as the viscosity contribution to flow is negligible and the viscous-dominated Darcy flow approximation breaks down.
Under capillary-dominated conditions the immiscible fluid is either migrating along a pathway at a low critical saturation or, is backfilling and flooding a trap at a higher saturation not exceeding the maximum gas saturation for the porous medium (Cavanagh and Rostron, 2013).
The model presented here differs significantly from earlier conceptual models of the storage site (e.g. Hellevang, 2006; Chadwick et al., 2008; Hermanrud et al., 2009, 2010) where researchers have assumed Darcy flow along a pressure gradient via holes in the shale barriers.
This study assumes that the thin shales represent laterally continuous barriers to flow.
While a Darcy flow model requires a rising cascade of fluid through the sandstone and around the shales, via holes or breaks in the shale succession, the approach presented here suggests a simpler hypothesis: the vertical stacking of kilometer-wide and meter-thick CO2 layers that are the result of buoyant gas percolating through the vertical heterogeneity of sandstones and shales found in the Utsira Formation, in a similar manner to hydrocarbon migration and trap charge in petroleum systems, as appropriate for two-phase flow with a low capillary number (England et al., 1987).
The capillary flow and ponding hypothesis does not require the unlikely geological coincidence of nine vertically aligned holes in the shales through which fluid cascades within the storage site.
Sleipner capillary number
An invasion percolation simulation assumes a dominance of capillary and gravity forces over viscous forces.
The interplay of forces (viscous, capillary and gravity) in reservoir models is commonly defined using scaling-group theory (Ringrose et al., 1993; Li and Lake, 1995), with the commonly accepted limiting condition for invasion percolation being that the capillary force must exceed the viscous force by a ratio of ten thousand-to-one.
In other words, the capillary number, Ca, is less than 10-4:(1)Capillary number,Ca=μqγwhere, Ca, the capillary number for a given flow regime; μ, the viscosity of the more viscous fluid in a two-phase system; q, the volumetric flux; γ, the interfacial tension between the invading phase and resident phase.
Under these conditions, invasion percolation is a reasonable approximation for the flow physics.
For example hydrocarbon migration typically has a capillary number of 10-10-10-14 (England et al., 1987).
If the capillary number is less than 10-4, flow modeling lends itself to fast invasion percolation simulations which yield high-resolution numerical solutions (Carruthers, 2003).
With respect to the Sleipner CO2 plume, the viscosity of the brine (0.7-0.8mPas) and interfacial tension between the brine and CO2 (25-27mN/m) have been estimated (Singh et al., 2010).
Therefore, the limiting migration rate (Ca∼10-4) for the storage site can be calculated as 3-4mm/s.
This fits well with observations, as the Sleipner CO2 plume flux is reasonably well constrained both vertically and horizontally: (a) the plume reached the caprock in 1999, approximately 3 years after injection began from the well 210m below - an observed ascent rate of about 70m/yr or 2μm/s; (b) the plume has spread laterally beneath the caprock, and is about 0.5km wide (E-W) and 3km long (N-S) after a decade - an observed lateral flux rate of less than 300m/yr or 10μm/s.
It follows that the capillary number for the plume (Ca<10-7) is much lower than the necessary limit for a viscous-dominated Darcy flow simulation.
The CO2 ascent and lateral spreading is very likely to be dominated by buoyancy and capillary forces, and therefore highly sensitive to contrasts in formation and shale threshold pressure (Table 1).
Given these flow rate indications, it makes sense, a priori, to model the plume distribution with an invasion percolation simulator.
The Young-Laplace equation describes the governing physics: a capillary pressure occurs at an interface between two immiscible fluids such as CO2 and brine.
This equation, named after the Enlightenment scientists who discovered the principles of capillary action (Young, 1805; Laplace, 1806), is commonly used for multi-phase migration models.
The pressure is a result of the tension of the surface that forms at the interface between the two fluids.
A popular expression of the Young-Laplace equation (Eq.
(2a)) relates the gravitationally stable column height of an oil or gas trap, and related capillary pressure, to the threshold pressure of the rock at which breakthrough occurs (Hobson, 1954).
By exchanging the capillary pressure term with the density difference between the two fluids, the column height of the buoyant fluid can be determined (Eqs.
(2b) and (2c)):(2a)Threshold pressure, Pth=2γ cos θr(2b)Capillary pressure, Pc=Δρgh(2c)Breakthrough condition, Δρgh=2γ cos θrWhere Pth is the threshold pressure of the rock at which breakthrough occurs; γ, the interfacial tension between the invading phase and resident phase; θ, the wetting angle between the invading phase and the rock (a low angle results in a high threshold pressure as the cosine value approaches unity); r, the representative pore throat radius for the rock.
Pc, the capillary pressure of the invading phase at the interface; Δρ, the density difference between the two phases; g, standard gravity; h, the column height of the invading phase.
If the capillary pressure does not exceed the threshold pressure, the fluid will pool beneath the shale and back-fill the shale topography until a spill-point is reached.
The CO2 will then migrate laterally until trapped again.
However, if the capillary pressure at the top of the CO2 pool does exceed the shale threshold pressure, the pool will breach the seal and migrate vertically until the next shale is reached and trapping occurs again, resulting in a vertical stack of pools.
Hence, the conceptual model is that this pattern of pooling, breaching and lateral spill will match the observed plume distribution.
Plume model
The properties of the shale and sandstone intervals are constrained by the geological framework of the Utsira Formation and present-day observations from the Sleipner storage site.
The Utsira Formation is internally segmented by laterally extensive horizontal shales, approximately 1-2m thick, acting as barriers (Isaksen and Tonstad, 1989).
Approximately 20m beneath the caprock of the Lower Seal, the shallowest shale barrier (7m thick shale) is thought to be lithologically identical to the caprock (Gregersen and Johannessen, 2001).
Reviews of the reservoir geology indicate that this 7m thick shale merges laterally with the Lower Seal, which is considered to be an effective primary seal (Arts et al., 2000; Chadwick et al., 2004).
As such, above the storage site and inter-layering with it as the 7m thick shale, are the Lower Seal shales of the Nordland Group.
This is overlain by the Middle Seal, bounded at the top and base by regional unconformities (Gregersen and Johannessen, 2001; Gregersen and Johannessen, 2007).
The upper unconformity provides important evidence of ice sheet erosion and exhumation during the Pleistocene.
The shallower Pleistocene shales are referred to as the Upper Seal (Fig. 1).
The primary seal and overburden stratigraphy have considerably more complexity and heterogeneity when considered in detail (Nicoll et al., 2012), with numerous shallow sands, chimney structures and sub-glacial relict features.
However, a simple first approximation is suitable for this study.
Geocellular model design
The 3D profile of the plume (Fig. 1) is used to build a three-dimensional model of the storage site.
The geometry of each CO2 layer and the associated shale barrier topographies are derived from published images of the CO2 layering (Bickle et al., 2007; Chadwick et al., 2008).
The nine layers define the cross-sectional relief of the shale barriers above the CO2.
The thin shale barriers that trap the first seven layers are assumed to be 1-2m thick; the thick shale overlying layer 8 is assumed to be 7m thick.
The modeled shales are assumed to be petrologically similar to the Lower Seal shale, conforming to petrophysical data from the sampled well, 15/9A-11 (Gregersen and Johannessen, 2001).
The model Utsira sandstone conforms to petrophysical data, i.e. clean, well-sorted and poorly cemented (Hellevang, 2006).
Numerical simulation
The simulation releases CO2 at the known perforation interval of the injection well, close to the base of the model, and calculates the percolation pathway and resultant accumulations for the ascent of a discrete supercritical phase of CO2.
The simulation assumes a buoyant gravity-driven flow dominated by capillary forces.
The depth range of the storage site is close to the phase boundary conditions for supercritical CO2.
This results in significant fluid density changes with ascent (Bickle et al., 2007).
These density variations are estimated from a commonly used equation of state for CO2 (Duan and Sun, 2003) assuming hydrostatic pressure and geothermal temperature profiles for the storage site (Table 2).
The output of the model is a plume distribution of CO2 with respect to the geological framework (Fig. 3) and a numerical estimate of the associated masses for each CO2 layer and threshold pressures for each shale barrier (Table 2).
While the layer distributions are known and stable in the model, the multiple migration pathways and breakthrough points for vertical migration (Fig. 3) only represent equivalent simulation pathways and potential breach locations for a given barrier at its shallowest expression.
These are not expected to be better than an accurate approximation of the breach locations, as ascending migration paths in the actual storage site are likely to be sensitive to local heterogeneities in the shale barriers and sandstone formation.
However, multiple realisations of the simulation confirm the robust and stable outcome of stacked CO2 layers that insensitive to variations in the precise position of CO2 breakthrough.
Results for two simulation scenarios
Two scenarios with identical geometry were tested by varying the threshold pressure of the shales within Utsira Formation: the first, a base case, assumed that the threshold pressure of approximately 1.6-1.9MPa measured in core recovered from Lower Seal shales (Springer and Lindgren, 2006) is applicable to all shale barriers within the model.
The threshold pressures are assigned to the shales as a global mean value i.e. a single shale barrier has a uniform mean threshold pressure of 1.75MPa (three-sigma standard deviation of ±0.15MPa, equivalent to approximately ±8.5%, assuming a normal distribution).
This base case model failed to match the observed CO2 distribution, as the high threshold pressures for the shales resulted in a 'zig-zag' pattern of predominantly laterally spilling migration.
The simulation ultimately failed to reach the caprock, as the CO2 backfilled beneath the 7m thick shale without breaching, resulting in a final column height of hundreds of meters and total saturation of the reservoir.
This scenario is totally unlike the Sleipner plume seismic observations, and is rejected.
The second scenario is unchanged from the base case with the exception of reduced threshold pressures for the shales within the storage site.
The threshold pressure was gradually reduced by iterative experimentation until the simulation exhibited thin CO2 layers similar to those observed in the seismic monitoring.
The normal distribution range for the lower threshold pressures remained as ±8.5% of the mean assigned to a given shale barrier.
This second scenario provides an excellent match with the observed phenomena i.e. a vertical 'pancake stack' of layers approximately 7-11m thick (Figs.
1 and 3).
The barrier geometry is prescribed by the observed geometry of the CO2 layers, and so the morphology of the simulated layers is forced, given the derivation of shale layer topography from the 4D seismic.
However, the significance of the result lies in the replication of the stacked plume layering, and an estimate of CO2 layer thicknesses that results in a reasonable match to observations and a plausible mass balance (Fig. 3 and Table 2).
It is notable that the model breakthrough pressure is surprisingly low in the calibrated match to the observed Sleipner plume.
The simulation enables the inference of permeability and threshold pressures for the shales at the kilometer scale of the entire Sleipner storage site which are difficult or impossible to measure directly.
The inferred threshold pressures required to achieve a thinly layered plume trapped by intra-formational shales are two orders of magnitude lower than the laboratory values measured on the caprock at well 15/9A-11.
This is an important outcome of the model.
The mode of CO2 ascent
Given the model results, the mode of CO2 ascent within the storage site deserves further scrutiny.
The favored scenario of thin, and stacked, CO2 layering with low intra-formational shale threshold pressures raises an obvious concern: if the assumptions hold regarding the similarity of the 7m thick shale to the Lower Seal, and the unbroken lateral continuity of the shale barriers, then the breakthrough condition for CO2 at the caprock may be as low as 50kPa (Table 2).
Such an extremely low threshold pressure for shales can only occur in fractured rocks.
It is therefore inferred that a network of micro-fractures span the shale barriers within the Utsira Formation, allowing the plume to ascend vertically, leaving a stack of thin CO2 layers.
The significance of this finding with respect to the integrity of the caprock and Nordland Group shales is discussed below.
Inferred fracture networks for shale barriers
The width of the inferred micro-fractures can be characterized to a first approximation by a modified form of the Young-Laplace equation (Pankow and Cherry, 1996).
In this modification for a fractured shale, the fracture half-width substitutes for pore throat radius, and the seal is breached when the capillary pressure of the CO2 layer, Pc (Eq.
(2b)), exceeds the threshold pressure of a fracture, Pf, equivalent to the product of the interfacial tension, γ, and cosine of the wetting angle, θ, divided by the fracture half-width, f (Eq.
(3)).
The calibrated invasion percolation simulation (Fig. 3) enables the fracture width to be estimated as a function of the height of column retained beneath the shales.
This indicates a fracture width of approximately 2μm (Table 2).
Micron-thick fractures commonly occur in shales, and are physically observed in physical shale specimens as pale gossamer-like threads that appear on the faces of rock samples indicating cement-filled fractures, or salts drying out from fluids in open fractures.(3)Fracture threshold pressure, Pf=2γ cos θfwhere, Pf, threshold pressure of the fracture; γ, interfacial tension; θ, wetting angle; f, the half-width of the representative fracture aperture.
The micro-fracture explanation for low threshold pressures raises a number of questions: what is the vertical extent of the inferred fractures? And, do these connect to form fracture networks that span the shale barriers? What are the origins of the fractures? And, is the integrity of the caprock compromised? Firstly, it is apparent that the 7m thick shale within the Utsira Formation transmits CO2.
It follows that this span is the minimum limit for a connected network of percolating fractures within the storage site.
It is unclear if such a percolation bridge might affect the much thicker Lower Seal.
No observational evidence to date indicates that CO2 has penetrated the caprock.
Given the very thin layers of CO2 (10-15m column heights with correspondingly low capillary pressures), a caprock breach would require a pre-existing fracture network.
The evidence from a neighboring shale caprock, albeit older, more compacted and buried to 4km (the seal for the nearby Miller oilfield), shows that CO2 penetration of an unfractured caprock is very slow, taking approximately 70Ma to diffuse 10m into the caprock (Lu et al., 2009).
However, the maximum limit of fracture connectivity at the Sleipner caprock, or through the Lower Seal remains unknown.
In addition to potential fracture networks, paleo-gas chimneys and glacial tunnel valleys with the Nordland Group shales may provide physical routes for the rapid vertical escape of CO2, though these are not thought to intersect the present CO2 plume footprint or expected future distribution beneath the caprock (Zweigel et al., 2004; Nicoll et al., 2012).
Concerning the origin of the micro-fractures, CO2 injection is an unlikely fracture mechanism for the breaching of thick or thin shales.
A low capillary pressure of around 50kPa, associated with the buoyancy of a 10-15m CO2 column, may be high enough to allow CO2 to percolate through an existing fracture network for 1-7m thick shale barriers.
However, the fracturing of shales as a consequence of such small pressure changes within the Utsira Formation is highly unlikely.
By analogy, micro-fractures in overpressured hydrocarbon fields occur when the pore pressures exceed around 80% of the lithostatic pressure, typically equivalent to multi-MPa changes (Aplin et al., 2000).
The Sleipner storage site is not significantly stressed or overpressured at the present day (Wei et al., 1990); the pore fluids are effectively at hydrostatic pressure; and so for Sleipner, the fluid pressure increase necessary to fracture an intact seal (about 10MPa) is highly unlikely.
It is therefore inferred that the shales have been fractured prior to CO2 injection.
Ice sheet unloading as a mechanism for fracturing
Previous studies have put forward a number of ideas to explain the rapid vertical migration of CO2 within the Utsira Formation, as neatly summarized by Chadwick et al. (2009).
The gamut ranges from small faults and natural holes in the intra-formational shale barriers, to geomechanically induced chimneys, sand injectites and changes in the wetting characteristics of shales exposed to CO2.
However, these scenarios are hypothetical and frequently driven by model design, in terms of creating suitable pathways for simulating flow.
The lack of sampling and direct observations from within the Utsira Formation has done little to narrow the field of possibilities.
The modeling presented in this paper implies fracturing prior to CO2 injection, as outlined above.
As a consequence, a plausible pre-injection fracturing mechanism is sought, given the geological history and context of the storage site.
The following hypothesis further adds to the various competing ideas concerning the flow properties of the Utsira Formation, and the shale barriers in particular; and in common with those ideas, shares a lack of direct evidence from the storage site itself.
However, there are a number of supporting regional indications, presented below, and the simplicity of the model, in itself, is encouraging.
The Sleipner storage site location lies within the Pleistocene ice sheet domain of NW Europe (Boulton and Hagdorn, 2006).
An erosional unconformity at the Pliocene/Pleistocene boundary between the Middle Seal and Lower Seal of the Nordland Group shales indicates that ice was in contact with the seabed, scouring and eroding the underlying sediments.
Additional unconformities within the Middle Seal and Upper Seal indicate several similar events (Nicoll, 2012).
The youngest ice sheet (Late Devensian) has left well-preserved sediments to record its position and demise; reconstructions of ice sheet thickness over the British Isles and Southern Norway during this final stage indicate that the ice was around 1000m thick and grounded over the Sleipner area of the Northern North Sea (Boulton and Hagdorn, 2006).
Similar grounded ice sheets probably occurred during several glaciations over the last 1Ma from marine isotope stage MIS 12 onwards.
Considering the effect of ice on fluid pressure and stress on the underlying shale, the following scenario is hypothesized:
The gradual increase of ice sheet thickness during prograde glaciation produces a slow transient additional overburden load.
This load is transferred to the sediments below the ice sheet if it is grounded rather than floating.
The youngest grounded ice sheet (Late Devensian, MIS 2) can be precisely mapped; the MIS 2 ice sheet formed as a confluence of the British and Fennoscandian ice sheets, flowing to the north-west, and ultimately reaching the continental margin (Bradwell et al., 2008; Davison, 2012).
The preservation of relict landform features indicates that, during deglaciation, the ice melted very rapidly, possibly catastrophically, within hundreds of years at 24ka, coincident with a global sea-level change (Bradwell et al., 2008).
It is possible that the sea level rise transgressed beneath the grounded ice, causing it to float off the seabed; at which point the overburden pressure is removed instantaneously (Davidson, 2005; Bradwell et al., 2008).
The significance of this deglaciation event is the possibility of unusually rapid and transient pore pressure changes within shales that may have exceeded the geomechanical limit, fracturing the shales (Cavanagh et al., 2006).
Pressure fluctuations associated with deglaciation are distinct from more commonly encountered subsurface overpressure regimes in that both the hydrostatic and lithostatic baseline pressures are offset to higher values by loading from a grounded ice sheet (Fig. 4).
Gradients remain the same but a change in the baseline pressure occurs which is dependent on ice sheet thickness.
Hypothetically, the loading of the Nordland Group shales by a 1000m thick ice sheet (gray upper layer, Fig. 4) would cause both hydrostatic and lithostatic values to increase by about 10MPa during a glacial stage lasting several tens of millennia (Fig. 4).
This affects a pore pressure increase within the Utsira Formation from 7MPa (700m depth) to a new equilibrium beneath the ice of 16MPa (1700m depth, including ice sheet).
The new hydrostatic pressure gradient (gray lines, Fig. 4) starts near the top of the ice sheet; the new lithostatic gradient runs from the base of the ice sheet.
The maximum ice load does not need to be located directly above the Utsira Formation as, for a hydraulically connected aquifer, a lateral transmission of pressure will occur for tens of kilometers.
During deglaciation the ice melts rapidly, and both the lithostatic and hydrostatic equilibrium lines return geologically instantaneously to their normal positions (black lines, Fig. 4).
Since pore fluids in the deep subsurface cannot flow instantaneously to re-equilibrate the rock, this rapid deglaciation leaves the pore fluids isolated at, or above the re-established normal fracture gradient.
While the highly permeable Utsira Formation is free to flow and equilibrate rapidly, the much lower permeability shales are not.
The shale pore pressure collides with the fracture gradient, at about 80% of lithostatic pressure, creating a zone of hydraulic fracturing potential (red triangle, Fig. 4).
The thickness and strength of the overburden for any given stage is highly uncertain, hence the dashed line (fracture gradient, Fig. 4), which is conservatively estimated from present day leak-off test data (Nicoll, 2012).
This mechanism suggests that, within this zone of fracturing potential, the shales of the Utsira Formation pervasively micro-fractured perpendicular to geological bedding, enabling rapid porewater escape during deglaciation.
This fracturing is now responsible for the rapid migration of CO2 within the storage site.
Five independent lines of evidence are considered to support this hypothesis:
Firstly, reservoirs in the much deeper Sleipner hydrocarbon fields, both East and West, appear to be overpressured by about 10MPa (Wei et al., 1990; NPD, 2013).
Secondly, a phenomenon known as the 'Oligocene-Miocene bump' appears to record a regional occurrence of 5-10MPa overpressure at about 1.3-1.5km depth in the Northern North Sea (Vik et al., 1998).
There is no diagenetic reaction or hydrocarbon charge effect to explain such an overpressure.
This suggests that both pressure phenomena are relicts from regional deglaciation events.
Thirdly, seismic data for the Sleipner storage site locale display a number of discrete anomalies (pock marks, soft kicks, small chimney features) where sediment layering is disrupted, indicative of vertical gas migration in the Nordland Group shales (Zweigel, 2000; Nicoll et al., 2012).
It is proposed that these shallow gas indications are relicts of natural gas leakage from the Utsira Formation during depressurization.
Fourthly, observations of thirty nine deeper hydrocarbon fields across the North Sea show that thermogenic gas and condensates have leaked through caprocks to form diffuse zones hundreds of meters thick above the hydrocarbon reservoirs (Aplin et al., 2006).
These seismic chimneys suggest that the leakage may be a regional manifestation of vertical micro-fractures created by the same transient depressurization phenomenon associated with ice sheet unloading.
Finally, tectonophysical modeling of lithospheric flexure in response to the elastic deformation of the crust during glaciation and deglaciation (Grollimund and Zoback, 2000) indicates that the Sleipner region is currently in a state of horizontal extensional stress (5-15MPa at 2-3km depth) as a consequence of ice sheet unloading.
Grollimund and Zoback suggest an explanation that is sympathetic to the hypothesis proposed here, i.e. the present-day unstressed hydrostatic pore pressure regime results from fracturing and pore fluid leakage: 'The stress decrease due to deglaciation might have brought horizontal compressional stress down to the existing pore pressure at the time when the stress decrease took place and parts of the fluids leaked away, leading to a pore pressure reduction' (Grollimund and Zoback, 2000).
A recent, more detailed, analysis by Grollimund and Zoback (2003) includes the influence of ice sheet loading from 1Ma onwards, and a history of recent ice sheet melting events, with similar results.
They determine that the common and widely observed indications of vertical fluid leakage from oil and gas fields within the region via faults or fractures is unlikely during glacial stages, but much more likely during regional deglaciations, with particular emphasis on events at 60ka and 15ka.
It seems reasonable that these regional findings for paleo-stress, pore pressure, and fluid flow also apply to the Sleipner storage site.
This combination of present-day overpressure observations, local vertical natural gas migration features and regional indications of pore pressure fluctuations associated with ice sheet-related lithospheric stress changes affirm the possibility of our inferred process for fracturing the Utsira Formation shales, both with respect to mechanism and magnitude.
Discussion
Successive seismic reflection surveys of the Sleipner CO2 storage site have shown by observation that the plume of injected CO2 at the Sleipner storage site has a geometry which reveals partial trapping beneath thin shales within the Utsira Formation.
The flow modeling approach and analysis presented here suggest that the shale barriers have uncharacteristically low threshold pressures, enabling the rapid vertical migration of injected CO2 within the storage site.
The inferred threshold of around 50kPa is much lower than the observed value of approximately 1.75MPa from core measurements by Springer and Lindgren (2006) and Harrington et al. (2009).
However, it should be noted that standard methodologies for core sampling and threshold pressure measurements specifically avoid fractured samples, assuming such fractures to be indicative of drilling damage.
As such, the influence of natural fracture networks on bulk shale threshold pressures is likely to be under-represented, if considered at all.
The model results indicate that a definitive mass balance for the plume is difficult to ascertain given the inherent uncertainties in the data.
However, a mid-range temperature profile and reasonable layer thickness estimate indicates that a mass balance for the plume is certainly possible.
Two rival processes are acting on today's CO2 storage performance at this site.
The first process, favoring retention, is the spreading and dispersal of the injected CO2.
For this process, weak shale barriers within the storage formation are beneficial.
These encourage the development of many thin layers of injected CO2, rather than one thick plume.
This maximizes the surface area contact of CO2 to porewater within the formation while decreasing the column height acting on the caprock and minimizing the lateral spread of CO2.
An increased surface area promotes the dissolution of CO2 in brine (Gilfillan et al., 2009), which results in permanent sequestration, as the slightly increased brine density causes the dissolved CO2 to descend away from the upper barrier surface (Arts et al., 2008).
Solubility trapping is desirable as it eliminates the need to physically retain a lower density buoyant fluid; sequestration of CO2 by growth into new minerals is likely to be too slow in nature to effect trapping over the required timescale of hundreds-to-thousands of years (Wilkinson et al., 2011).
The second process, opposing retention, is the vertical flow of CO2 which may be mediated by existing micro-fracture networks that span the shale barriers.
This may be disadvantageous to CO2 storage for two reasons: firstly, because the CO2 is only partially spread laterally beneath the shale barriers, potentially resulting in a focused vertical flow path; secondly, because CO2 could potentially break through the caprock seal.
Evidence from Sleipner indicates that the fracturing affects all the internal shale barriers including the uppermost 7m thick shale.
This thick shale is geologically considered to be part of the overlying Lower Seal (Gregersen and Johannessen, 2001; Eidvin et al., 2013).
Fracturing of the Lower Seal would be of particular concern, as the caprock acts as the primary seal for storage site.
The question arises, has the caprock been fractured by the same deglaciation mechanism? And if so, do the observed chimney-like structures in the secondary seal within the vicinity of the storage site (Nicoll et al., 2012) contain vertical fracture networks that compromise the seal integrity? As yet, a percolation network of connected fractures through the much thicker shale sequences of the overburden has not been proven.
Evidence against such a fracture network is the continued lateral migration of the uppermost CO2 plume at about 1m per day to the north (Chadwick et al., 2008), and the lack of seismic velocity changes observed in the caprock above the storage site, and within the Middle Seal, on successive 4D seismic surveys (StatoilHydro, 2009; Chadwick et al., 2009).
This is encouraging, as even small amounts of CO2 gas added to the Lower Seal would result in a strong seismic reflection signal, given the strong density contrast between brine and gas for the pressure and temperature conditions of the overburden.
These are reasonable grounds to assert that the caprock is a functioning seal that is significantly thicker than the underlying shales.
If CO2 were to seep into the Lower Seal, then predictions of observable phenomena to be expected would include:(a)
Stabilization or shrinkage of the upper plume layer on a timescale of years rather than decades, due to the seepage of CO2 via fractures rather than dissolution into the underlying formation.
(b)
Observable brightening and extending of seismic anomalies for the existing bright spots of natural gas pockets above and within the Lower Seal; 'lighting up' of existing seismic chimneys.
(c)
New 'soft kick' seismic anomalies appearing in the overburden as a consequence of gaseous CO2 ascending through micro-fractures and charging shallow sand bodies within the near future.
(d)
Ultimately, some form of surface expression at the sea floor, including pockmarking, seawater geochemical changes, bubble trains, and possibly behavioral changes in bottom-dwelling fauna.
While none of these are apparent, or inferred at present (Pedersen et al., 2012), future monitoring of the Sleipner storage site could be directed toward testing such possibilities.
Note that a significant increase in fluid pressure for aquifers and sand bodies above Utsira, a commonly cited detection criteria, is not expected due to the buoyant capillary-dominated nature of the flow and associated weak pressure differentials.
Processes occurring in this storage site are relevant for all areas of the North Sea affected by regional ice sheets.
The hydraulic fracturing of thin, shallow shale intervals during deglaciation could be a widespread feature beneath continental ice sheets.
Similar processes may be expected to affect all northern hemisphere regions exposed to thick regional ice sheets in the past million years.
Consequently, shales shallower than about 1.5km may be less effective primary seals than anticipated, but vertical stacking of multiple thin CO2 layers within storage formations is advantageous, as is the distribution of further CO2 layers beneath secondary seals within the storage complex, increasing the contact area with porewater and enhancing dissolution trapping.
Dissolution uses more of the reservoir pore volume efficiently, and potentially increases site capacity relative to regional estimates of storage efficiency.
However, a lack of understanding with respect to flow processes in the overburden will likely lead to conservative estimates of the secondary sealing potential and an underestimation of the important contribution of the overlying stratigraphy to the performance of a storage site.
At the present time, in our opinion, the evidence from the very extensively analyzed geophysical data for Sleipner is compatible with retention of CO2.
Conclusions
The Sleipner CO2 storage site is an iconic carbon sequestration project and extraordinary fluid flow experiment in terms of the setting, nature and volume of fluid injected.
This paper has documented a numerical modeling approach that results in a plausible CO2 distribution and mass balance estimate.
Data acquisition and monitoring at Sleipner has been via remote geophysical sensing, primarily 4D seismic.
As a consequence, a number of quite large uncertainties remain with respect to the observed plume distribution.
The paper demonstrates the sensitivity of mass balance estimates associated with a poorly constrained temperature profile close to the critical point of CO2, and poorly constrained thickness estimates of the multiple thin layers that characterize the plume.
With respect to simulations, it is noted that previous attempts to numerically model the plume have used software governed by Darcy flow physics, and achieved poor results.
This paper presents an alternative approach that leads to a successful 3D simulation of the injected plume distribution using a model based on capillary flow and percolation physics.
The paper carefully documents the modeling approach and justification for this unusual but effective method.
The modeling indicates that the multiple layers of thin CO2 are likely a consequence of unexpectedly low threshold pressures for vertical migration through the shale barriers within the Utsira Formation.
The estimated threshold pressures for CO2, at around 50kPa, are approximately 35-fold less than that measured on a sample of the caprock shale from well 15/9A-11, close to the site, at approximately 1.75MPa.
This difference is most plausibly explained by microfracturing of the shale barriers, but CO2 injection at Sleipner is not considered to be a likely cause of fracturing.
It is postulated that the fracturing occurred long before CO2 injection commenced, as a result of rapid pore pressure fluctuations associated with the collapse of thick ice sheets during multiple episodes of deglaciation in the region over the last million years.
A number of independent observations are considered to support this hypothesis.
Concerning seal integrity, the caprock and overlying Nordland Group shales of the Lower Seal may have also been fractured during deglaciation.
However, it is noted that there is no evidence of CO2 leaking, possibly as a result of a different caprock response to CO2 retention, which is notably thicker than the thin shale barriers within the storage site.
It is inferred that similar fracture networks within the primary seal are potentially only proximal to the formation, and may have limited percolation connectivity, preventing vertical migration through the overburden.
Acknowledgements
AJC was funded by the Scottish Funding Council.
RSH was funded by the UK Energy Research Center, NERC, Scottish Power, and EPSRC (EP/F034520/1 and EP/K000446/1).
We wish to thank Permedia for a license to use their software.
We are also grateful to the Norwegian Petroleum Directorate archives for open-access.
Much thanks to Grant Nicoll and Mark Wilkinson, University of Edinburgh, for helpful discussions.
We sincerely appreciate the editorial work of Stefan Bachu, and constructive comments from the two anonymous reviewers.
Surface datum is mean sea level.
All measurements are in SI units unless stated otherwise.
Geothermal gradient assumed linear between 41°C at 1012m (Bickle et al., 2007) and 35°C at 800m (Singh et al., 2010).
Hydrostatic gradient, Pz=Pa+ρgz, the standard formulation (atmospheric pressure, 101.3kPa; gravitational constant, 9.81kg/m3).
Pore water density, 1020kg/m3, assumed constant.
CO2 interfacial tension, 27mN/m and CO2-pore water contact angle, 140°, assumed constant given reservoir temperature and pressure gradients, after published data (Shah et al., 2008; Bachu and Bennion, 2009).
CO2 density 377-625kg/m3, from crest to injection point, calculated from Permedia PVT solver, based on a published equation of state (Huang et al., 1985).
Sandstone porosity, 0.36±0.04 below the caprock.
Sandstone porosity, 0.34±0.04at the base of the formation.
Sandstone permeability assumed to be 1-5 darcy, after published data (Singh et al., 2010).
Sandstone threshold pressure ∼10-20kPa (Sorkhabi and Tsuji, 2005).
Sandstone CO2 saturation 0.80 (after a published Scw value of 0.2 (Bickle et al., 2007).
Shale porosity, 0.30±0.02 assumed given depth.

1. A lithium-transition metal composite particles A, characterized in, including:
A lithium transition metal oxide particles;
Metal-doped layer, to form the lithium transition metal oxide particles doped; and
LiF, metal-doped layer comprises a lithium transition metal oxide formed in or on the particles, wherein the metal-doped layer of the metal is Zr.2. A lithium transition metal composite particles according to claim 1, characterized in, relative to 100 weight percent of the lithium transition metal composite particles, 0.01% by weight to 3% by weight to the range of the metal-doped layer contains.3. A lithium-transition metal composite particles according to claim 1, characterized in, from the group consisting of lithium transition metal oxide particles of the metal has a gradually decreasing concentration gradient of the surface to the inside.4. A lithium-transition metal composite particles according to claim 3, characterized in, metal-doped layer of the composite particles comprising the following chemical formula 1:
Formula 1><
LiaM1-bZrbO2,
In a chemical formula,
M=NixMnyCoz, wherein 0.3 ≤ x ≤ 0.9,0 ≤ y ≤ 0.6 and a 0 ≤ z ≤ 0.6,
0.9 ≤ a ≤ 1.3,
0<b ≤ 0.02.5. A lithium-transition metal composite particles according to claim 1, characterized in, metal-doped layer further comprises an oxide, the oxide comprises a metal.6. A lithium transition metal composite particles according to claim 1, characterized in, and the amount of a metal is more than 0 µM % 2 µM % or less.7. A lithium-transition metal composite particles according to claim 1, characterized in, LiF-based polymer is a fluorinated lithium transition metal oxide particles contained in the at least one of a partial response of the modified lithium of the impurities.8. A lithium transition metal composite particles according to claim 1, characterized in, with respect to the total weight of the lithium transition metal complex particles, 0.1 to 0.5% by weight to weight percent LiF comprising.9. A lithium-transition metal composite particles according to claim 7, characterized in, LiOH impurity is a group consisting of lithium, Li2CO3or a mixture thereof.10. A lithium-transition metal composite particles according to claim 7, characterized in, with respect to the total weight of the lithium-transition metal composite particles, an amount of less than 0.3 weight percent lithium as the impurity.11. A lithium-transition metal composite particles according to claim 7, characterized in, fluoride-based polymer is a polyvinylidene fluoride, polyvinylidene fluoride - hexafluoropropylene copolymer or a mixture thereof.12. A lithium transition metal composite particles according to claim 1, characterized in, lithium cobalt-based lithium-transition metal oxide particles are selected from oxides, lithium manganese oxide, lithium nickel manganese oxide, lithium manganese cobalt nickel-manganese-cobalt oxide and a lithium-based oxide of one or a group consisting of a mixture of two or more of them.13. A lithium-transition metal composite particles according to claim 12, characterized in, lithium transition metal oxide is selected from the group consisting of one of or a combination of two or more of a mixture of:
LiCoO2,
LiNiO2,
LiMnO2,
LiMn2O4,
Li (NiaCobMnc) O2, wherein 0<a<1, 0<b<1, 0<c<1, a + b + c=1,
LiNi1-YCoYO2, LiCo1-YMnYO2, LiNi1-YMnYO2, 0 ≤ Y<1 wherein,
Li (NiaCobMnc) O4, wherein 0<a<2, 0<b<2, 0<c<2, a + b + c=2, LiMn and the2-zNizO4, LiMn2-zCozO4, wherein 0<Z<2.14. The preparation of the lithium-transition metal composite particles one method, characterized in, including:
The mixed transition metal precursor, a lithium compound and a metal oxide are mixed and calcined, to obtain a metal-doped layer comprises a lithium transition metal oxide particles of the step; and
A lithium transition metal oxide particles including a metal-doped layer is mixed with the surface modification agent and performing a heat treatment,
Wherein the metal oxide is ZrO2.15. The preparation of the lithium-transition metal composite particles one method, characterized in, including:
The doping of the metal-mixed transition metal precursor and a lithium compound are mixed and calcined, metal-doped layer comprises a lithium-transition metal oxide obtained by steps of; and
A lithium-transition metal-doped layer including the metal oxide particles with a surface modifier are mixed with and of heat-treating step,
Wherein the metal is Zr.16. A lithium-transition metal composite the production of particles method according to claim 14 or 15, characterized in, firing is performed at a temperature 800 °C to 1000 °C.17. The production of particles of lithium-transition metal composite method according to claim 14 or 15, characterized in, performed at a temperature 300 °C to heat treatment at 500 °C.18. A lithium-transition metal composite particles for producing method according to claim 14 or 15, characterized in, or mixed transition metal precursor to have M MOOH2consisting of a compound, wherein, M=NixMnyCoz, 0.3≤ x ≤ 0.9,0 ≤ y ≤ 0.6 and a 0 ≤ z ≤ 0.6, x + y + z=1.19. A lithium-transition metal composite particles for producing method according to claim 14, characterized in, with respect to the total weight of the lithium-transition metal composite particles, 0.1 weight percent to 1 percent by weight to the amount of use of a metal oxide.20. A lithium-transition metal composite the production of particles method according to claim 14 or 15, characterized in, surface modification agent is a polyvinylidene fluoride, polyvinylidene fluoride hexafluoropropylene copolymer or a mixture thereof is -.21. The production of particles of lithium-transition metal composite method according to claim 14 or 15, characterized in, with respect to the total weight of the lithium-transition metal composite particles, 0.5 weight percent to 0.2 weight percent to an amount of the surface modification agent.22. A positive electrode active material, a lithium-transition metal composite particles contain claim 1.23. One anode, anode active material comprising claim 22.24. A lithium secondary battery, comprising anode, cathode, cathode and separator interposed between the anode, characterized in, claim 23 as the positive electrode the positive electrode.1. Positive active material for a lithium secondary battery, characterized in, with a hexagonal crystal structure containing transition metal composite oxide of lithium, transition metal Me comprises Ni or, Co and the Mn, Li-transition metal composite oxide, the molar ratio of Ni in the transition metal is 0.5 ≤ Ni/Me ≤ 0.9 to Me, 0.1 ≤ Co/Me ≤ 0.3 molar ratio of Co, Mn 0.03 ≤ Mn/Me ≤ 0.3 molar ratio of, with respect to Li/Li+F (003) /F (104) obtained by dividing to 4.3V with respect to the potential of the leaders at half maximum of Li/Li+leaders at half maximum of F (003) /F (104) to 2.0V 0.9-1.1 when a value of between.2. A positive electrode active material for a lithium secondary battery according to claim 1, characterized in, Li-transition metal composite oxide by the composition formula Li<sub>1+x</sub> (Ni<sub>a</sub> Co<sub>b</sub> Mn<sub>c</sub>) <sub>1-x</sub> O<sub>2</sub> representation, wherein, -0.1<x<0.1,0.5 ≤ a ≤ 0.9,0.1 ≤ b ≤ 0.3,0.03 ≤ c ≤ 0.3, a + b + c=1.3. A positive electrode active material for a lithium secondary battery according to claim 1 or 2, characterized in, lithium transition metal composite oxide has a particle size distribution of not more than a maximum value of 2.4. The positive electrode for a lithium secondary battery manufacturing method of an active material, characterized in, are made to contain a lithium-transition metal composite has a hexagonal crystal structure oxide cathode active material for a lithium secondary battery according to method, Me-transition metal is Ni containing, Co and the Mn,
Containing Ni in a solution, Mn and a compound of Co in the step of making the co-precipitation of the precursor, is dropped at the same time respectively With a solution of compounds containing Ni or Co and the Mn a solution of compounds, Me-transition metal is Ni-making in a molar ratio of 0.5 ≤ Ni/Me ≤ 0.9, 0.1 ≤ Co/Me ≤ 0.3 molar ratio of Co, Mn 0.03 ≤ Mn/Me ≤ 0.3 molar ratio of a transition metal composite oxide containing precursor.5. The positive electrode for lithium secondary battery active material according to claim 4 in which a manufacturing method, characterized in, of a tap density of 1.4 g/cc or more precursors.6. An electrode for a lithium secondary battery of a type, comprising the lithium secondary battery according to any one of claim 1-3 positive electrode active material.7. One of a lithium secondary battery, comprising an electrode for a lithium secondary battery claim 6.8. One power storage device, wherein the secondary battery is configured to set a plurality of Li claim 7.
1. A method for preparing a transition-metal composite oxide, the method comprising:
a first step of preparing a first-interior forming aqueous metal-salt solution and a second-interior forming aqueous metal-salt solution that are different each other in concentration of nickel, cobalt, and manganese;
a second step of supplying a chelating agent and a basic solution into a reactor;
a third step of growing a particle including a first interior, which is uniform in concentration of nickel, cobalt, and manganese and has a radius of r1(0.2 μm≦r1≦5 μm), while the first-interior forming aqueous metal-salt solution, the chelating agent, and the basic solution are continuously supplied into the reactor; and
a fourth step of mixing a chelating agent and a basic solution in a reactor, at the same time of mixedly supplying the first-interior forming aqueous metal-salt solution and the second-interior forming aqueous metal-salt solution until a mixing ratio gradually goes to 0 v %:100 v % from 100 v %:0 v %, and forming a particle including a second interior, which has a radius of r<b>2</b> (r<b>2</b><10 μm), on the contour of the first interior,
wherein the second step adjusts concentration of the basic solution to 0.25 g/L through 0.5 g/L in the reactant.2. The method of claim 1, wherein the second step adjusts pH of a solution to be 11.8 through 12.3 in the reactor.3. The method of claim 1, wherein nickel concentration of the first-interior forming aqueous metal-salt solution is adjusted in 0.8 through 1 mol %.4. The method of claim 1, wherein, through the first step to fourth step, D50 becomes equal to or less than 4 μm in a size distribution of particles formed after mixedly reacting the first-interior forming aqueous metal-salt solution, the chelating agent, and the basic solution in the reactor for 30 minutes.5. The method of claim 1, further comprising a fifth step of drying or thermally treating a transition-metal composite oxide that is obtained from the first step through the fourth step, wherein an average particle diameter of the transition-metal composite oxide prepared by the fifth step is 5 to 10 μm.6. A transition-metal composite oxide prepared by the method of claim 1.7. The method of claim 1, further comprising a sixth step of mixing a lithium salt with the transition-metal composite oxide and thermally treating the mixture.8. A lithium composite oxide prepared by claim 7.9. A lithium composite oxide prepared by claim 8 and given in Formula 1 that is LiaaNixaCoyaMnzaO2+δ(in Formula 1, 0.5≦xa≦1.0).10. A method for preparing a transition-metal composite oxide, the method comprising:
forming a first aqueous metal-salt solution with nickel, manganese, and cobalt;
forming a second aqueous metal-salt solution with nickel, manganese, and cobalt;
mixing a basic solution and an ammonia solution in a reactor to adjust pH of a reactant to 11.8 through 12.3; and
supplying a first mixed aqueous metal-salt solution, which is formed of the first aqueous metal-salt solution and the second aqueous metal-salt solution, and ammonia and basic solutions into the reactor,
wherein a mixing ratio of the first aqueous metal-salt solution and the second aqueous metal-salt solution in the first mixed aqueous metal-salt solution is equal to or higher than 0 v % and equal to or higher than 100 v %.11. The method of claim 10, satisfying at least one of x1≠x2, y1≠y2, and z1≠z2under x1+y1+z1=1 and x2+y2+z2=1,
wherein nickel content is x1, manganese content is y1, and cobalt content is z1in the first aqueous metal-salt solution, and wherein nickel content is x2, manganese content is y2, and cobalt content is z3in the second aqueous metal-salt solution.12. The method of claim 10, wherein the x1is equal to or higher than 0.8 and equal to or lower than 1.0.13. The method of claim 10, wherein the x2is equal to or lower than 0.8.14. The method of claim 10, wherein the supplying of the first mixed aqueous metal-salt solution comprises:
gradually changing the mixing ratio of the first aqueous metal-salt solution and the second aqueous metal-salt solution toward 1 v %:100 v % from 100 v %:0 v %.15. The method of claim 10, further comprising:
forming a third aqueous metal-salt solution with nickel, manganese, and cobalt; and
supplying a second mixed aqueous metal-salt solution, which is formed of the first mixed aqueous metal-salt solution and the third aqueous metal-salt solution, and ammonia and basic solutions into a reactor,
wherein a mixing ratio of the first mixed aqueous metal-salt solution and the third aqueous metal-salt solution in the second mixed aqueous metal-salt solution is higher than 0 v % and equal to or lower than 100 v %.16. The method of claim 15, satisfying at least one of x3≠x4, y3≠y4, and z3≠z4under x3+y3+z3=1 and x4+y4+z4=1,
wherein a nickel content is x3, a manganese content is y3, and a cobalt content is z3in the first mixed aqueous metal-salt solution, and wherein a nickel content is x4, a manganese content is y4, and a cobalt content is z4in the third aqueous metal-salt solution.17. The method of claim 15, wherein the supplying of the second mixed aqueous metal-salt solution comprises:
gradually changing the mixing ratio of the first mixed aqueous metal-salt solution and the third aqueous metal-salt solution toward 1 v %:100 v % from 100 v %:0 v %.Carbon microspheres via microwave-assisted synthesis as counter electrodes of dye-sensitized solar cells
CSs were prepared using the method reported in our previous works [36]. In briefly, 6.846 g sucrose and 2 g concentrated sulfuric acid were dissolved in 100 ml mixing solvent of water and ethylene glycol (6:4 v/v). Subsequently, 20 ml of the solution was heated at 160 degC with a microwave irradiation power of 100 W for 10 min using a microwave system (Explorer-48, CEM Co.). After being centrifuged and washed with deionized water, the resultant precipitate was dried in a vacuum oven at 80 degC for 24 h and finally thermally treated at different temperatures for 2 h in nitrogen. In this work, CSs with different thermal temperatures of 600, 800 and 1000 degC are named as CS600, CS800 and CS1000, respectively. The as-synthesized CSs were deposited on F-doped SnO2 (FTO) (resistivity: 14 Ω/#, Nippon Sheet Glass, Japan) glass by screen printing. The as-prepared counter electrodes were sintered at 400 degC for 30 min. The conventional Pt film was also used as counter electrodes for comparison.
Leaf-templated synthesis of 3D hierarchical porous cobalt oxide nanostructure as direct electrochemical biosensing interface with enhanced electrocatalysis
Cobalt acetate (Co(COOH)2*4H2O), sodium hydroxide (NaOH) and d-glucose were purchased from Sinopharm Chemical Reagent Corporation (Shanghai, China). Nafion(r) perfluorinated resin solution (5 wt% in mixture of lower aliphatic alcohols and water) were purchased from Sigma-Aldrich. All other reagents are of analytical grade and were used without purification. All aqueous solutions were prepared with Milli-Q water (18.2 MΩ cm). The mature gingko leaves were collected from the local park. H2O2 and glucose solutions with different concentrations were diluted from their respective stock solution with 0.1 M NaOH solution.
The mature gingko leaves were treated with 2 M hydrochloric acid solution for 3 h. After rinsing thoroughly with water and subsequent drying at 60 degC for 3 h, the leaves were subsequently immersed in 0.1 M Co(CH3COO)2 solution at room temperature overnight. After drying at 60 degC, the leaves were calcined in air at 400 degC for 3 h and then naturally cooled to room temperature. The remaining powder was washed several times with water and collected by centrifugation. After drying at 60 degC overnight, the black Co3O4 was obtained. The powder was finely ground in an agate mortar and stored at room temperature. For the control experiment, non-templated Co3O4 was prepared from Co(CH3COO)2 solution by the same method except that the leaf template had not been used.What is claimed is:
1. A composite transition metal oxide-based precursor represented by the following Chemical Formula 1:
<in-line-formulae>Ni<sub>a</sub>Co<sub>b</sub>M′<sub>c</sub>O<sub>x </sub>(1<<i>x≤</i>1.5)  [Chemical Formula 1]</in-line-formulae>
in the formula,
M′ is one or more selected from the group consisting of an alkali metal, an alkaline earth metal, a Group XIII element, a Group XIV element, a Group XV element, a Group XVI element, a Group XVII element, a transition metal, and a rare earth element, and
<in-line-formulae>0.6≤<i>a<</i>1.0, 0≤<i>b≤</i>0.4, 0≤<i>c≤</i>0.4, <i>a+b+c=</i>1.</in-line-formulae>8. A method for preparing the composite transition metal oxide-based precursor of claim 1, the method comprising a step of oxidizing a composite transition metal hydroxide-based precursor represented by the following Chemical Formula 2:
NiaCobM′c(OH)2[Chemical Formula 2]
in the formula,
M′ is one or more selected from the group consisting of an alkali metal, an alkaline earth metal, a Group XIII element, a Group XIV element, a Group XV element, a Group XVI element, a Group XVII element, a transition metal, and a rare earth element, and
<in-line-formulae>0.6≤<i>a<</i>1.0, 0≤<i>b≤</i>0.4, 0≤<i>c≤</i>0.4, <i>a+b+c=</i>1.</in-line-formulae>9. The method of claim 8, wherein in the step of oxidizing,
(i) a heat treatment is performed under an oxygen atmosphere,
(ii) an oxidizing agent is used, or
(iii) both (i) and (ii) are applied.10. The method of claim 8, wherein in the oxidizing, a heat treatment is performed in a range of 200 to 1,000° C. under an oxygen atmosphere at an oxygen concentration of 80% or more for 1 to 12 hours.11. The method of claim 9, wherein the oxidizing agent is one or more selected from the group consisting of KMnO4, H2O2, Na2O2, FeCl3, CuSO4, CuO, PbO2, MnO2, HNO3, KNO3, K2Cr2O7, CrO3, P2O5, H2SO4, K2S2O8, a halogen, and C6H5NO2.
What is claimed is:
1. A precursor of a positive electrode active material for a secondary battery, comprising:
a secondary particle having a single layer structure, wherein the single layer structure is an aggregate of pillar-shaped primary particles radially oriented in a surface direction from the particle center of the secondary particle,
wherein
the primary particle includes a composite metal hydroxide of Ni—Co—Mn of the following Chemical Formula 1:
Ni1−(x+y+z)CoxMyMnz(OH)2[Chemical Formula 1]
wherein, in Chemical Formula 1,
M includes any one, or two or more elements selected from the group consisting of Al, Zr, Mg, Zn, Y, Fe and Ti; and
x, y and z are each 0<x<1, 0≤y<1, 0<z<1 and 0<x+y+z<1, wherein the precursor is prepared by a method comprising:
introducing an ammonium cation-containing complex forming agent and a basic compound to a metal-containing solution to form a reaction solution, wherein a metal-containing solution includes a nickel raw material, a cobalt raw material and a manganese raw material; and
co-precipitation reacting the reaction solution under a pH of 10.50 to 12.00 and a temperature of 50° C. to 70° C.,
wherein the ammonium cation-containing complex forming agent is introduced at a rate of 0.5 times to 1.5 times with respect to an introduction rate of the metal-containing solution.5. The precursor of a positive electrode active material for a secondary battery of claim 1, wherein the precursor has an average particle diameter (D50) of 7 μm to 20 μm and a BET specific surface area of 5.0 m2/g to 30.0 m2/g.6. The method for preparing the precursor of a positive electrode active material for a secondary battery of claim 1, wherein the ammonium cation-containing complex forming agent and the basic compound are used in a molar ratio of 1:10 to 1:2.7. A positive electrode active material for a secondary battery, comprising:
a secondary particle having a single layer structure,
wherein the single layer structure is an aggregate of pillar-shaped primary particles radially oriented in a surface direction from the particle center of the secondary particle,
wherein
the primary particle includes a lithium composite metal oxide of Ni—Co—Mn of the following Chemical Formula 2 and exhibits mono-modal-type particle distribution:
Liα[Ni1-(x+y+z)COxMyMnz]O2[Chemical Formula 2]
wherein, in Chemical Formula 2,
M includes any one, or two or more elements selected from the group consisting of Al, Zr, Mg, Zn, Y, Fe and Ti;
x, y and z are each 0<x<1, 0≤y<1, 0<z<1 and 0<x+y+z<1; and
a is 1.0≤a≤1.5.8. The positive electrode active material for a secondary battery of claim 7, wherein the positive electrode active material has an average particle diameter of 7 μm to 15 μm and a BET specific surface area of 0.1 m2/g to 1.0 m2/g.9. The positive electrode active material for a secondary battery of claim 7, which has tap density of 1.7 g/cc to 3.0 g/cc.10. A positive electrode for a secondary battery comprising the positive electrode active material of claim 7.11. A lithium secondary battery comprising the positive electrode of claim 10.Marine and terrestrial environmental changes in NW Europe preceding carbon release at the Paleocene-Eocene transition

Abstract
Environmental changes associated with the Paleocene-Eocene thermal maximum (PETM, ∼56Ma) have not yet been documented in detail from the North Sea Basin.
Located within proximity to the North Atlantic igneous province (NAIP), the Kilda Basin, and the northern rain belt (paleolatitude 54°N) during the PETM, this is a critical region for testing proposed triggers of atmospheric carbon release that may have caused the global negative carbon isotope excursion (CIE) in marine and terrestrial environments.
The CIE onset is identified from organic matter δ13C in exceptional detail within a highly expanded sedimentary sequence.
Pollen and spore assemblages analysed in the same samples for the first time allow a reconstruction of possible changes to vegetation on the surrounding landmass.
Multiproxy palynological, geochemical, and sedimentologic records demonstrate enhanced halocline stratification and terrigenous deposition well before (103yrs) the CIE, interpreted as due to either tectonic uplift possibly from a nearby magmatic intrusion, or increased precipitation and fluvial runoff possibly from an enhanced hydrologic cycle.
Stratification and terrigenous deposition increased further at the onset and within the earliest CIE which, coupled with evidence for sea level rise, may be interpreted as resulting from an increase in precipitation over NW Europe consistent with an enhanced hydrologic cycle in response to global warming during the PETM.
Palynological evidence indicates a flora dominated by pollen from coastal swamp conifers before the CIE was abruptly replaced with a more diverse assemblage of generalist species including pollen similar to modern alder, fern, and fungal spores.
This may have resulted from flooding of coastal areas due to relative sea level rise, and/or ecologic changes forced by climate.
A shift towards more diverse angiosperm and pteridophyte vegetation within the early CIE, including pollen similar to modern hickory, documents a long term change to regional vegetation.
Highlights
► We identify the carbon isotope excursion from the North Sea in exceptional detail.
► We document changes to ocean stratification well before atmospheric carbon release.
► Precursor stratification may be associated with the trigger for carbon release.
► Increased precipitation and runoff likely occurred along with carbon release.
► We document a rapid change to regional vegetation during carbon release.

Introduction
The PETM was a period of geologically-rapid global warming that punctuated a warming Eocene climate 55.8Ma ago (Charles et al., 2011), and saw sea surface temperatures rise by 5-8°C from background levels (Zachos et al., 2005; Sluijs et al., 2007).
It was associated with a substantial injection of δ13C-depleted carbon into the ocean-atmosphere system (see Pagani et al., 2006a) over <20ka (Cui et al., 2011), causing a negative carbon isotope excursion (CIE) between -2 and -7‰ in marine and terrestrial sediments (see overview in Schouten et al., 2007) lasting 170ka (Röhl et al., 2007), and a prominent dissolution horizon in the deep sea signifying deep ocean acidification (Kennett and Stott 1991; Zachos et al., 2005).
The source and rate of released carbon are still under debate (Pagani et al., 2006a; Zeebe et al., 2009; Cui et al., 2011), but may have been linked to the dissociation of marine hydrates containing biogenic methane (δ13C of<-60‰) (Dickens et al., 1995), thermogenic methane from marine sediments around the Norwegian Sea (Svensen et al., 2004), or dissolved methane from a silled Kilda Basin between Greenland and Norway (Nisbet et al., 2009).
The PETM may be a good analogue to test modelling studies that suggest current global warming trends may result in an enhanced hydrologic cycle (Seager et al., 2010), whereby increased precipitation in temperate rain belts is coupled with increased evaporation in lower latitudes.
Modelling studies of the PETM have further indicated the potential importance of an increased hydrologic cycle (Lunt et al., 2010; Bice and Marotzke 2002), which could have altered ocean circulation causing methane hydrate reservoirs to destabilise, triggering massive carbon release (Bice and Marotzke, 2002).
Palynological evidence from Arctic Spitsbergen (Harding et al., 2011) and New Zealand (Crouch et al., 2003a) suggests increased terrestrial runoff occurred at the onset of the CIE which may be related to hydrologic changes, and massive Pyrenees conglomerate deposits have been interpreted as the result of an abrupt increase in extreme precipitation within the early CIE (Schmitz and Pujalte, 2007).
In addition, hydrogen and carbon isotope measurements of terrestrial plant and aquatic-derived n-alkanes from the central Arctic Ocean indicate that the core of the PETM was associated with increased precipitation and hence hydrologic cycle (Pagani et al., 2006b), although the onset of PETM warming was not recovered in the sediment core.
Despite numerous additional evidence for changes in terrestrial runoff and potentially hydrology during the PETM (see overview in McInterney and Wing, 2011), there is a lack of clear evidence for hydrologic changes from high resolution sections able to resolve important lead/lag relationships, and therefore there remains a need for studies of hydrologic changes in sensitive locations over the onset of the CIE in order to understand the relationship between precipitation and global carbon release.
Biome changes in response to modern global warming have been observed, but approaches to predict the vulnerability of ecosystems to future changes are still in development (Gonzalez et al., 2010).
Vegetation shifts during the rapid warming associated with the PETM may provide a useful analogue to future biome responses.
Whilst neotropical vegetation in Central America appears to have responded to warming during the PETM with increased diversity and origination rates (Jaramillo et al., 2010), central North America experienced a rapid migration of plant communities associated with lower precipitation at the onset of the CIE (Wing et al., 2005), and southern England may have experienced a major change in plant composition due to changes in local fire-regime (Collinson et al., 2009).
To better understand biome responses to climatic change during the PETM, further high resolution vegetation cover studies are needed, specifically from temperate and boreal forests which may be amongst the most vulnerable ecosystems to global warming (Gonzalez et al., 2010).
In this study we focus on paleoenvironmental signals from a high resolution marine core collected from the central North Sea (Fig. 1), in order to understand changes to precipitation, ocean stratification, productivity and vegetation over the onset of the PETM.
This core is located in a critical region proximal to the NAIP, as there are currently no high resolution records of environmental change during the PETM from the North Sea Basin.
Furthermore, as overturning of a stratified Kilda Basin at the CIE onset is hypothesised as a possible trigger for dissolved methane release to the atmosphere (Nisbet et al., 2009), analysing the stratification history of the nearby North Sea is a possible way to test this hypothesis.
Regional setting
During the late Paleocene-early Eocene the North Sea was a restricted marine basin, characterised by siliciclastic sedimentation and high terrigenous input, principally from the Scotland-Faeroe-Shetland landmass (Knox 1998, Fig.
S1).
Core 22/10a-4 is located in the central part of the basin (Figs.
1 and S1) and is therefore disconnected from many marginal marine processes that could mask oceanographic signals (e.g. tidal or storm-induced erosion and slumping).
Paleobathymetry estimates in the North Sea during the Paleocene and Eocene are difficult to constrain accurately, as the extant benthic foraminifera present in the Paleogene are found today living between 200 and >1000m water depth (Gradstein et al., 1992), and are controlled predominantly by substrate and bottom water properties.
However using a number of paleoecologic micropaleontology methods together (Gillmore et al., 2001), along with 2D structural restoration (Kjennerud and Sylta, 2001), broad agreement was found and central parts of the northern North Sea appear to have had paleodepths of >0.5km in the earliest Eocene near 22/10a-4 (Kjennerud and Gillmore, 2003, Fig. S1).
As 22/10a-4 is in the deep (>0.5km) central part of the basin, it acted as a depocentre and exhibits a Paleocene-Eocene transition sequence that is not only expanded but is also close to being stratigraphically complete.
The only evidence for breaks in the succession is minor erosion at the base of thin turbidite sandstones (typically <10cm).
Because these sandstones may contain reworked material, they were not sampled in this study.
During the late Paleocene, the basin became restricted following a fall in in the order of 100m that resulted from regional uplift associated with the proto-Iceland mantle plume in the North Atlantic (see Knox, 1996).
This event is evident in 22/10a-4 as a lithologic change from unbedded to bedded mudstone (the Lista and Sele Formation boundary, Fig. 2).
Restriction of the basin also led to the establishment of poorly oxygenated bottom waters, as is evident by a shift in the benthic foraminiferal assemblages towards low diversity low oxygen-tolerant agglutinated species (Knox, 1996).
The CIE at the Paleocene-Eocene boundary was accompanied by a relative sea level rise, as documented in southeast England (Powell et al., 1996) and Spitsbergen (Harding et al., 2011), due to the thermal expansion of sea water and possible melting of ice caps, although the North Sea basin remained restricted as evidenced by the persistence of low oxygen facies in 22/10a-4.
The North Sea had a widespread freshwater catchment area, and a halocline was in place from the late Paleocene to early Eocene (Zacke et al., 2009).
Therefore, surface water salinity changes in the North Sea Basin provide a sensitive gauge for stratification forced by changes in tectonics and the hydrologic cycle.
Methods and materials
Sedimentology
Borehole 22/10a-4 (57°44'8.47"N; 1°50'26.59"E) provides a continuous core through the Forties Sandstone Member and into the Lista Formation (Fig. 2).
The core consists of variably fissile claystone with interbedded fine to coarse grained sandstone layers interpreted as turbidites, with occasional mm-thick ash layers (Fig. 3).
All samples in this study were taken from claystone horizons to avoid sampling substantial quantities of reworked material.
The section of 22/10a-4 analysed in this study is from 2605m to 2634m (core depth), chosen because this part of the core is predominantly in claystone facies and provides a greatly expanded section over the onset of the CIE (Figs.
2 and 3).
At 2609-2613m the claystone becomes finely laminated with alternately pale and dark laminae couplets ranging from 1 to 25permm (Fig.
S2).
The pale laminae consist of clay and silt, and the dark laminae are rich in organic carbon and pyrite inclusions.
Laminae were counted at 26 horizons throughout the core and approximately 13 pairspermm.
Micropaleontology
A total of 71 palynology samples were prepared at the British Geological Survey using standard preparation procedures (Moore et al., 1991).
Samples were demineralised with hydrochloric (HCl) and hydrofluoric (HF) acids, and residual mineral grains removed using heavy liquid (zinc bromide) separation.
Elvacite was used to mount slides.
The palynomorphs were analysed using a Nikon transmitted light microscope, counting the total number of palynomorphs on a strew slide (Table S1).
Each slide was produced from 1/100th of the total material processed, where the initial weight of material was 5g of dried sediment.
Thus, the palynology counts represent the total number of specimens per 0.05g of dried sediment.
Statistical analysis was carried out using the software of Hammer et al. (2005).
The %wood/plant tissue was determined by palynological investigation, and is the sum of '%wood plant tissue' and '%various (non-woody) plant tissue' in Table S1.
Organic material for δ13CAOM analysis was collected from the same palynology samples, and the remaining processed material separated into size fractions.
The >250μm fractions, found through light microscope analysis to be dominated (>90%) by amorphous organic matter (AOM), were also analysed for δ13C.
Foraminifera samples between 20 and 60g of dried sediment were processed by washing through a 63μm sieve with water.
All specimens were counted and converted to foraminifera/g (Table S2).
All species exhibited agglutinated (non calcareous) test walls.
Geochemistry
All analyses were carried out at the NERC Isotope Geosciences Laboratory.
C and N analyses (from which we present weight % C/N) were performed on 225 samples by combustion in a Costech ECS4010 Elemental Analyser (EA) calibrated against an acetanilide standard (Table S3).
C/N atomic ratios were calculated by multiplying by 1.167.
Replicate analysis of well-mixed samples indicated a precision of ±<0.1.
Carbon isotope analysis was carried out on 289 bulk rock samples (Table S4) after removing migrated hydrocarbons (Stephenson et al., 2005).
The hydrocarbons were removed by crushing the rock fragments using a ball mill, and the soluble organic matter from all rock samples was extracted using a Soxhlet extractor.
The samples were refluxed for 24h in an azeotropic mixture of dichloromethane and methanol (93:7, v/v).
All materials (cellulose Soxhlet thimbles, silica wool, vials) were cleaned with analytical grade organic solvents prior to use.
Any remaining solvent was then removed by evaporation and the dried sediments were transferred to vials.
Any calcites (shelly fragments) were removed by placing the samples in 5% HCl overnight before rinsing and drying down.
Carbon isotope analysis was also carried out on palynology residues of the >250μm size fractions dominated by AOM.
13C/12C analyses were performed on 35 samples by combustion in a Costech Elemental Analyser (EA) online to a VG TripleTrap and Optima dual-inlet mass spectrometer, with δ13C values calculated to the VPDB scale using a within-run laboratory standards calibrated against NBS-18, NBS-19, and NBS-22.
Replicate 13C/12C analyses were carried out on the section, and the mean standard deviation on the replicate analyses is 0.4‰.
Results and discussion
Statistical analysis
Correspondence analysis (CA) and statistical diversity analysis were carried out on the palynological dataset (total counts per gram) to confirm assemblage designations (Figs.
4 and 5), to identify any disturbance to the core prior to interpretation, and to estimate diversity (Fig. 6).
Dinoflagellate cyst assemblages (DA1-DA5) and pollen assemblages (PA1-PA4) were defined by visually comparing changes in the species dominance (Figs.
7 and 8), and confirmed by CA (Fig. 4) using the first three axes (describing the highest percentages of variance).
Five samples from below the CIE at 2619.60, 2617.35, 2617.44, 2614.73, and 2614.71m (indicated in Fig. 5) contain Apectodinium, in contrast to the other samples below the CIE (Figs.
3 and 7).
Some of these samples (2619.60, 2614.73, and 2614.71m) also contain negative δ13CTOC, indicative of the CIE (Fig. 3).
To test if coincident pollen and spore changes also occur in these samples, we used CA on the pollen and spore data only (Fig. 5a).
PA1-PA4 (symbols) plot in clusters, signifying their palynological similarity.
The species most associated with an assemblage are clustered with the samples from that assemblage.
For example, Inaperturopollenites hiatus and bisaccate pollen (highly abundant before the CIE, Fig. 8) are high on axis 1 where the earlier samples from PA1 and PA2 occur, and Caryapollenites spp.
and fungal spores (abundant after the CIE, Fig. 8) are low on axis 1 near the younger samples from PA4.
The two samples 2619.60 and 2614.71 have a spore and pollen palynological signature similar to samples from PA3/4 during the CIE (plot lower on axis 1) and are either not in the correct location (it cannot be discounted that these samples represent tectonically emplaced younger material (Payne et al., 2005), or were misplaced during drilling operations core handling), or represent very short episodes of both marine and terrestrial ecologic change to CIE-type conditions.
The rapid and transient nature of these two shifts appears to suggest that the latter explanation may be unlikely, and we have therefore shaded samples from these two depths in Figs.
3, 6-9.
The majority of the morphospecies in our study represent taxonomic groupings of terrestrial plant species, from generic to higher level groupings, such that any palynology diversity measure will underestimate vegetation diversity.
However, although subtle changes may not be resolved, large changes to the diversity of regional vegetation are likely to be reflected in palynology assemblages.
Palynological data have therefore previously been used to estimate plant diversity in the geologic record (e.g. Ogaard, 2001; Harrington, 2004; Jaramillo et al., 2010).
The number of pollen and spore species in each sample (Fig. 6d) fluctuates, trending towards increasing number over the CIE onset (from PA2 to PA3).
To account for the changing number of specimens counted in each sample (more species will be encountered with higher counts), two statistical indices were used (Fig. 6e, f).
Both confirm the significant increase in pollen and spore diversity from before to after the CIE onset.
We do not use the range-through method, as we are interested in changes to local vegetation habitats.
The increase in the number of pollen and spore species appears to be largely driven by pteridophytes (largely ferns, Fig. 6g), although angiosperm diversity also increases over the CIE onset (Fig. 6h) and fungal spores become more prevalent (Fig. 8m).
Gymnosperms are only represented by two morphospecies and so diversity was not calculated for this group.
Isotope changes and age model
The Lista-Sele Formation boundary (Fig. 2) occurs in the lower part of magnetochron 24r in the Faeroe-Shetland Basin (Mudge and Bujak, 2001), giving a date of >56.6Ma for the base of the studied section (Gradstein and Ogg, 2005).
The top of the studied section is within nannofossil zone NP9 (Knox, 1996), dated as <55.7Ma (Gradstein and Ogg, 2005).
The Paleocene-Eocene marker event Apectodinium augustum (Bujak and Brinkhuis, 1998) occurs in 22/10a-4 at 2617.35m (Fig. 3), approximately at the CIE onset (as was found at 30/14-1, Fig. 1, Sluijs et al., 2007).
The global CIE is determined by the negative shift in both marine and terrestrial δ13C between 2‰ and 7‰ (see Schouten et al., 2007) and has recently been radioisotopically dated in Spitsbergen to 55.8Ma (Charles et al., 2011).
The global CIE onset is present in 22/10a-4 within the interval from 2614 to 2612m (Fig. 3), where a negative δ13C shift of 5‰ occurs along with a peak in Apectodinium.
The palynological results (from samples containing sufficient material for analysis; red symbols in Fig. 3) show that %wood/plant tissue varies from 10% to 88% throughout this interval (Fig.
S3).
We found that the samples containing >30% wood/plant tissue (open red symbols in Fig. 3) in the palynological residues have consistently heavier δ13CTOC values than those with less wood/plant tissues (solid red symbols).
The presence of transported Corg thus precludes an unambiguous interpretation of the rate of atmospheric carbon release from the shape of the CIE onset at 22/10a-4, but the initial negative δ13C shift between 2614.3 and 2613.5m (shaded box, Fig. 3) can be taken as marking the earliest evidence of δ13C-depleted atmospheric carbon release and the onset of the global CIE.
This positioning of the CIE onset is supported by δ13C analysis of isolated AOM (Fig. 3), likely of marine origin (see Supplementary Material) and less likely to be composed of mixtures of different sources of Corg (although it is still susceptible to reworking).
As we observe only a 1‰ 'recovery' in the CIE, the age of the top of the studied section appears to be no more than 75ka from the CIE onset by comparison with the cyclostratigraphically correlated Longyearbyen section in Spitsbergen (Charles et al., 2011), giving an average sedimentation rate of ≥8cm/ka after the CIE onset (if sand horizons (Fig. 3) are removed, assuming rapid turbidite deposition).
The long term 0.7‰ positive shift in δ13C at 2609m (Fig. 3) may correlate to a similar shift in the Longyearbyen section (Charles et al., 2011), which is cyclostratigraphically correlated to 45ka after the CIE onset (Charles et al., 2011), and would give a sedimentation rate of on average 8cm/ka for this section.
The finely laminated part of the core may also provide temporal insight.
There is no direct evidence for the period of deposition of each lamination couplet, but as modern marine lamination-forming basins produce annual laminae pairs (e.g. the Black Sea: Arthur et al., 1994; Carioaco Basin: Tedesco and Thunell, 2003; Santa Barbara Basin: Thunell et al., 1995), the 40,000 estimate of the number of laminae pairs present between 2613 and 2609m (see Section 3.1) may represent approximately 40ka, and a sedimentation rate of 7.5cm/ka.
Dinoflagellate cysts and surface water changes
Dinoflagellate cysts have been used extensively for reconstructing paleoenvironments in the Paleogene (see overview in Sluijs et al., 2005), as they are particularly sensitive to changes in salinity, temperature, and nutrient levels (Powell et al., 1992; Pross and Brinkhuis 2005; Sluijs et al., 2005).
We calculate "%low salinity dinoflagellate cysts" (Figs.
7-9) by grouping cysts of similar inferred ecologic preferences (see Fig. 7, and discussion in the Supplementary Material) to provide an indication of environmental change, and by excluding species of uncertain affinity such as Apectodinium.
Samples with fewer than 20 specimens were also excluded.
Despite the limitations of this method, the large variation in the %low salinity dinoflagellate cysts (ranging from 0% to 80%) clearly indicates that significant environmental changes in surface water conditions occurred during the CIE onset in the central North Sea, and is supported by coeval changes in the sedimentary carbon/nitrogen (C/N) ratio (Fig. 10) which reflects changes in the proportion of terrestrial/marine organic material deposited in the North Sea Basin due to terrestrial runoff and productivity (see Section 4.4).
Dinoflagellate cyst assemblage 1 (DA1, from 2632 to 2618m, Fig. 7p) contains high proportions of typically open marine and hence normal marine salinity associated Achomosphaera/Spiniferites spp., undifferentiated chorate cysts and Areoligera/Glaphyrocysta spp.
DA1 also contains on average 5% peridinoid cysts including Deflandrea spp.
regarded as a coastal/neritic taxon indicating high productivity and nutrient availability (Brinkhuis, 1994; Pross and Brinkhuis, 2005).
These characteristics indicate that a somewhat restricted but fully marine shelf environment was present before the onset of the CIE in the central North Sea, with availability of nutrients indicated by the presence of Deflandrea spp.
DA2 (- 2614m) contains a similar abundance of Achomosphaera/Spiniferites spp.
and chorate cysts to DA1, but with a marked increase in the abundance of low salinity tolerate Cerodinium depressum and Senegalinium spp., which may also have been a heterotrophic genus indicative of elevated nutrient levels (Sluijs and Brinkhuis, 2009).
Undifferentiated peridinoid cysts also peak in abundance, and may also be indicative of elevated nutrient and reduced salinity conditions (although they are not included in the %low salinity dinoflagellate cysts, Fig. 7c).
There is also an increase in the abundance of Areoligera/Glaphyrocysta spp., thought to indicate unrestricted neritic environments of more typical marine salinity.
In summary, the higher proportion of low salinity tolerant dinoflagellate cysts in DA2 (Fig. 7c) appears to indicate continuous or episodic freshening surface waters below typical marine salinities (<31‰ in the modern ocean).
In addition, DA2 is characterised by elevated numbers of dinoflagellate cysts per gram (70per 0.05g in DA1; 110per 0.05g in DA2) indicating a possible increase in cyst production which would be consistent with increased fluvial runoff carrying nutrients from nearby landmasses.
DA2 also contains an abundance maximum of Apectodinium, an extinct genus with a somewhat uncertain ecologic affinity (see Crouch et al., 2003b), although likely reflecting relatively warm and eutrophic conditions (Crouch et al., 2003b; Sluijs et al., 2007; see Supplementary Material).
The transient appearance of Apectodinium occurs with a reduced abundance of open marine dinoflagellates, and a peak in Deflandrea indicating possibly lower salinity and higher nutrient availability.
DA3 (2614-2612m) is characterised by Apectodinium making up an average 40% of the assemblage, and a reduction in the abundance of all other species apart from Deflandrea and Senegalinium.
DA3 may therefore indicate a continuation of high nutrient surface water with an elevated freshwater input.
In this respect, it is perhaps similar to DA2, although the low abundances of dinoflagellates with known ecologic affinities make interpretations more tentative.
The appearance of bottom water anoxia (laminations) and sporadic high accumulation of marine AOM (see Supplementary Material) is consistent with highly productive surface waters.
DA4 (2612-2609m) shows an increase in the number of low salinity/high nutrient species Cerodinium depressum and Senegalinium spp., with a continued low abundance of normal marine salinity Achomosphaera/Spiniferites spp., such that the proportion of low salinity tolerant dinoflagellate cysts is at a maximum (Fig. 7c) indicating the presence of a pronounced halocline.
The lack of freshwater fern and algal spores Azolla, Pediastrum and Botryococcus (Table S1) indicates that surface water salinities were probably not below 5‰ (the limit for Azolla, Brinkhuis et al., 2006), and may have been greater than 20‰ as Botryococcus was found living in salinities as high as 20‰ in modern Australian lakes (de Deckker, 1988).
This minimum salinity value is consistent with Zacke et al. (2009), who found continuous occurrences of shark teeth in shallow marine North Sea facies throughout the late Paleocene/early Eocene, and noted that sharks do not live in salinities below 20‰ in the modern ocean.
Highly productive surface waters are indicated by dinoflagellate cysts (Fig. 7c), and consistent with bottom water anoxia (laminations and disappearance of benthic foraminifera) and very high accumulation of marine AOM.
DA5 (2609-2607m) shows a decrease in low salinity tolerant dinoflagellate cysts and an increase in unrestricted (normal marine salinity) Areoligera/Glaphyrocysta spp.
and other chorate cysts.
The reduction in AOM together with the loss of lamination (possibly end of bottom water anoxia) indicates a return towards marine salinities that existed before the onset of the CIE (represented by DA1), although the persistence of Apectodinium may indicate a long term change in marine ecology.
Enhanced terrigenous deposition
The increase in abundance of low salinity tolerant Cerodinium depressum and Senegalinium spp.
before and after the CIE onset (DA2-DA4, Fig. 7) most likely indicates a reduction in surface water salinity and elevated nutrient levels (see Section 4.3).
This appears to have been associated with increased terrigenous deposition, as evidenced by a concomitant increase in the C/N ratio of 22/10a-4 (Figs.
9c and S4), and an elevated kaolinite contribution to the clay assemblage (Fig. 9e).
Sedimentary atomic C/N ratios can be used to differentiate the origin of organic matter (Meyers, 1997; Storme et al., 2012), with values in 22/10a-4 averaging 10-15 (Fig. 10) indicating a mix of terrestrial land plant-derived and marine/lacustrine algal-derived carbon (Meyers, 1997).
C/N values broadly follow %low salinity dinoflagellate cysts (Fig. 9), with an increase at the Lista/Sele Formation boundary, 4m before the onset of the CIE and within the early CIE.
As dinoflagellate cyst assemblages suggest higher productivity (%low salinity dinoflagellate cysts, Apectodinium), the C/N ratio increase likely indicates an increased flux of terrigenous material to 22/10a-4, rather than a lower marine carbon flux.
Kaolinite also increases before and after the onset of the CIE in 22/10a-4 (Fig. 9e).
Kaolinite forms as a result of intense chemical weathering that typically develops on well-drained surfaces receiving high precipitation (Robert and Kennett, 1994).
Increased kaolinite at the PETM had been regarded as indicative of an increase in chemical weathering and hence humidity in the source region (e.g. Robert and Kennett, 1994; Knox, 1996), but the long formation time of thick soil kaolinite (>1myr) suggests these increased proportions probably resulted from erosion of previously formed kaolinite (Thiry and Dupuis, 2000; Schmitz et al., 2001).
Pollen, spores and vegetation shifts
The pollen and spore assemblages (PA) that characterise the pre-CIE interval in 22/10a-4 (PA1 and PA2, Fig. 8n) are dominated by I. hiatus and bisaccate pollen.
Both taxa are produced in abundance by a variety of coniferous plants, and are typical in the Paleogene of the northern UK and Greenland region (Boulter and Manum 1989; Jolley and Whitham 2004; Jolley and Morton 2007), as well as mid-latitude North America (Smith et al., 2007) and Arctic Canada (Greenwood and Basinger, 1993).
I. hiatus is a member of the Cupressaceae family (coniferous trees) and most likely represents Metasequoia and/or Glyptostrobus swamp conifers (Greenwood and Basinger, 1993; Jolley and Morton, 2007; Jolley et al., 2009).
In terrestrial Paleocene deposits from western Scotland, abundant I. hiatus was recorded in association with Momipites, Cupuliferoipollenites, Platycaryapollenites, Plicapollis pseudoexcelsus, and Alnipollenites interpreted as derived from a channel-margin bog community on a wet substrate (Jolley et al., 2009).
Bisaccate pollen (family Pinaceae) was likely derived from temperate coniferous trees possibly on drier substrates, with an elevation from several metres within swamps (Greenwood and Basinger, 1993) to possibly much higher altitude (Jolley and Whitham 2004).
Both are relative overproducers of pollen (Smith et al., 2007).
Lowland swamp vegetation appears to have been present before the CIE on the Scotland-Faeroe-Shetland platform (this study; Jolley and Morton 2007), the catchment area for 22/10a-4 (Fig.
S1, Knox 1996), with the relative increase of I. hiatus over bisaccate pollen from PA1 to PA2 possibly due to sea level fall at the Lista/Sele boundary (Knox, 1996) allowing lowland swamp conifers to expand and/or come into closer proximity to 22/10a-4.
The most significant floral change at 22/10a-4 occurs at the onset of the CIE (PA2-PA3), with a large drop in the proportion of I. hiatus, increasing diversity (Fig. 6), higher proportions of angiosperms (Alnipollenites and P. pseudoexcelsus), and fern and fungal spores (Fig. 8).
Alnipollenites was probably from the birch Alnus (alder), and in this setting represents generalist vegetation as it occurred in all the terrestrial Paleocene communities from western Scotland (Jolley et al., 2009).
Alnus and associated ferns were most common where Metasequoia (possible source of I. hiatus) was rare in Arctic Canada, where exceptionally well-preserved Paleocene-Eocene Metasequoia and Glyptostrobus swamp deposits are prevalent, and interpreted as Alnus-fern bogs (Greenwood and Basinger, 1993).
Alnus is a known nitrogen-fixing pioneer species in nutrient-depleted soils (Hobbie et al., 1998).
P. pseudoexcelsus is similar to modern Juglans (walnut) pollen sometimes associated with wetland plants (Jolley and Whitham, 2004).
The fern spores Cicatricosisporites and Laevigatosporites (family Schizaeaceae) were generalists, associated with all environments from the terrestrial Paleocene communities analysed in western Scotland (Jolley et al., 2009).
Since dinoflagellate cysts indicate increased halocline stratification of the North Sea at the CIE (Fig. 9d), and C/N ratios indicate a greater proportion of terrestrial carbon (Fig. 9c), it seems likely that the large reduction in I. hiatus in 22/10a-4 reflects a change in vegetation cover in the source region rather than simply a general reduction in the supply of pollen due to proximity of the coast.
Replacement of I. hiatus-dominated swamp communities (PA2) with generalist taxa (PA3) indicative of Alnus-fern bogs (Greenwood and Basinger, 1993) would be consistent with local sea level fall (partially draining coastal plains) or sea level rise (flooding established coastal swamp plains).
There is evidence for sea level rise at the onset of the CIE in southeast England (Powell et al., 1996) and in Spitsbergen (Harding et al., 2011), although any uplift associated with the proximal NAIP could have caused local sea level fall near the Faeroe-Shetland platform.
Alternatively, the reduction in gymnosperm swamp conifers and pines at the expense of angiosperms may have been a climatic response to the PETM, as a similar floristic change was also recorded during the PETM in the Bighorn Basin, Wyoming (Smith et al., 2007), and the Lomonosov Ridge (Sluijs et al., 2006).
The early ephemeral peak in Alnipollenites and P. pseudoexcelsus and drop in coastal swamp I. hiatus at 2617.35m, associated with the initial peak in Apectodinium (Fig. 8), may indicate a brief episode of coastal flooding from increased precipitation, in association with lower surface water salinity (peak in Deflandrea, Fig. 7i).
A further ecologic shift occurs within the core of the CIE (PA4) as Caryapollenites (Fig. 8h) begins to dominate, moss (Stereisporites) and fungal spores increase in relative abundance, bisaccate pollen and Alnipollenites decreases (Fig. 8), and diversity remains relatively high (Fig. 6).
Caryapollenites, similar to modern Carya (hickory) pollen, is common in terrestrial Paleocene deposits from western Scotland interpreted as bogs (Jolley et al., 2009), and many may have been adapted as primary colonisers in wet substrates (Jolley and Whitham 2004).
Its association with moss and fern spores in PA4 indicate a possible predominance of bog environments, consistent with high regional precipitation and poorly drained acidic bedrock.
As bogs receive water and nutrients directly from precipitation (Price and Waddington, 2000), the change from PA3 to PA4 appears to be the result of climatic change (e.g. precipitation and temperature) rather than sea level.
Other changes include a further increase in the proportion of angiosperm over gymnosperm pollen (largely a reduction in pine), consistent with previous observations from the Lomonosov Ridge (Sluijs et al., 2006) and Bighorn Basin, Wyoming (Smith et al., 2007).
Environmental and climatic changes before the CIE
The North Sea Basin became restricted before the PETM, as indicated by a lithologic change throughout the North Sea Basin (the Lista/Sele boundary, Figs.
2 and 9) and the presence of low oxygen benthic foraminifera in 22/10a-4 (Knox, 1996).
This relative sea level fall was recognised as a lithologic change from marine to lagoonal/shallow marine facies in southeast England (Knox et al., 1994) and considered to have been largely the result of major regional uplift possibly in the order of 100m (Knox, 1996).
The documented major tectonic uplift at the Lista/Sele boundary best explains the concomitant increase in terrigenous input to 22/10a-4 (increase C/N ratio, Fig. 9c), the domination of lowland swamp vegetation (swamp conifers, Fig. 8), and the lowering of surface water salinity (dinoflagellate cysts, Fig. 9d) as the basin became more restricted.
The gradual lowering of surface water salinity and increase in the C/N ratio from 4m before the CIE onset at 22/10a-4 (above 2618m, Fig. 9), therefore, may also be the result of further uplift and restriction of the North Sea Basin, bringing the coastline and transported terrestrial material into closer proximity to the centrally-located 22/10a-4, although there is no documented sea level fall in the North Sea preceding the CIE (Knox, 1996; Powell et al., 1996).
The increase in kaolinite in this scenario would be coincidental, perhaps having been formed after (>1myr, Thiry and Dupuis, 2000) the uplift at the List/Sele boundary.
As the North Sea Basin was proximal to the NAIP during the Paleocene/Eocene (Figs.
1 and S1), regional uplift before the CIE would most likely be related to intrusive activity west of the North Sea Basin, restricting deep-water connections between the North Sea and Atlantic Ocean.
Uplift and restriction is therefore consistent with the hypothesis that a mantle-derived magmatic intrusion of organic-rich sediments occurred in the NE Atlantic before the CIE, triggering atmospheric methane release (Svensen et al., 2004).
Alternatively, an increase in regional precipitation could have caused elevated terrestrial runoff (C/N ratios, kaolinite) and lower surface water salinity above 2618m before the CIE onset (Fig. 9).
The North Sea Basin surrounding landmasses were within the northern rain belt (the southern boundary today is 40°N), which would have experienced elevated precipitation if the global hydrologic cycle became enhanced (Pagani et al., 2006b; Schmitz et al., 2001).
This scenario would be consistent with a gradual increase in the global hydrologic cycle before the CIE, perhaps from gradual warming, which was hypothesised to have triggered ocean circulation changes, methane hydrate destabilisation, and global carbon release at the CIE (Bice and Marotzke, 2002).
We note however that there is currently no evidence for an enhanced hydrologic cycle well before the CIE in other regions.
Our results provide the first evidence that the North Sea became stratified from 103 yrs before the CIE onset (above 2618m, Fig. 9).
This is significant as Nisbet et al. (2009) hypothesised that the proximal Kilda Basin became stratified and anoxic before the CIE, allowing significant build-up of methane and CO2 at depth.
They proposed that overturning of this basin could have released greenhouse gases and triggered the CIE, although there is currently no direct evidence as marine records from the Kilda Basin remain rare (Nisbet et al., 2009).
Our North Sea records likely indicate enhanced stratification also of the proximal and linked Kilda Basin before the CIE (Fig. 1).
Evidence for the linkage of the North Sea, Kilda and Arctic Basins comes from the coincident onset of A. augustum and laminated sediments at the CIE onset in sections from the North Sea (this study), Spitsbergen (Harding et al., 2011), and Lomonosov Ridge (Sluijs et al., 2006).
Although our results evidence a probable stratified Kilda Basin before the CIE, proxies for overturning are now needed to further test the Kilda basin hypothesis.
The brief peak in Apectodinium, AOM and low salinity dinoflagellate cysts (Deflandrea) at 2617.4m (Fig. 7) indicate a sporadic episode of surface water freshening/eutrophication before the CIE, which is best explained by an increase in regional precipitation due to its rapid nature.
A coincident reduction in I. hiatus swamp conifers indicates possible disturbance of nearby coastal environments possibly from flooding (see Section 4.5).
An associated reduction in δ13C may have been caused by stratification of the North Sea from an enhanced halocline, trapping 12C-enriched organic carbon at depth.
This scenario may also explain the other peaks in Apectodinium at 2619.6 and 2614.7m (although see Section 4.1).
Environmental and climatic changes within the CIE
The influx of Apectodinium (indicating the onset of the CIE) at the Paleocene-Eocene boundary in southeast England occurred in characteristically marine assemblages following non-marine deposition, and was interpreted as an indication of relative sea level rise associated with a transgression immediately preceding a maximum flooding surface within the early PETM (Powell et al., 1996).
Sea level rise at the CIE onset was also recorded in Arctic Spitsbergen (Harding et al., 2011).
The large reduction in lowland swamp pollen at the CIE onset of 22/10a-4 indicates a possible change in regional sea level.
As there is no direct evidence for regional uplift causing further restriction of the North Sea Basin associated with the CIE onset, increased regional precipitation may have been the major cause of increased C/N ratios (elevated fluvial runoff) and dinoflagellate cyst changes (lower surface water salinity) at and following the CIE onset in 22/10a-4 (Fig. 9).
Although our records cannot distinguish changes in seasonality, which may have increased in the Pyrenees (Schmitz et al., 2001; Schmitz and Pujalte, 2007) and mid-latitude North America (Wing et al., 2005; Kraus & Riggins 2007), they are consistent with an overall increase in precipitation over NW Europe associated with the CIE.
Our records therefore provide evidence consistent with the hypothesis that a northward migration of storm tracks occurred from an intensified hydrologic cycle as a result of global warming at the PETM, proposed by Pagani et al. (2006b) to explain elevated Arctic runoff during the PETM.
As Pagani et al. (2006b) could not fully resolve the CIE onset due to incomplete core recovery, our records provide evidence that an enhanced hydrologic cycle may have occurred in approximate concert with global carbon release at the CIE onset.
Alternatively, uplift of the NAIP could have caused further restriction and stratification of the North Sea.
The reduction in %low salinity dinoflagellate cysts and the C/N ratio above 2609m (Fig. 9) may indicate a reduction in precipitation over NW Europe, or more likely tectonic subsidence causing the basin to become less restricted.
Conclusions
A negative carbon isotope excursion of 5‰ has been identified from δ13CTOC and δ13CAOM in an expanded Paleocene-Eocene boundary section from the central North Sea Basin.
Palynological (dinoflagellate cyst, pollen, and spore assemblages) and sedimentologic (C/N ratios and kaolinite distribution) evidence indicate major changes occurred to marine and terrestrial environments in NW Europe both preceding and over the CIE.
Enhanced halocline stratification and terrigenous input from 4m before the CIE may indicate tectonic uplift and oceanic restriction of the North Sea, supporting hypotheses for NAIP volcanism as a trigger for the CIE (Svensen et al., 2004), and/or increased terrigenous runoff and regional precipitation, supporting hypotheses of an enhanced hydrologic cycle triggering carbon release (Bice and Marotzke, 2002).
A peak in Apectodinium before the CIE is interpreted as an ephemeral increase in terrestrial runoff causing local eutrophication.
Further enhanced halocline stratification and terrigenous input at and immediately after the CIE onset, coupled with evidence for sea level rise in coastal areas, indicate possible increased regional precipitation over NW Europe.
At this location (paleolatitude 54oN) increased precipitation would support the hypothesis that a poleward migration of storm tracks from an enhanced hydrologic cycle resulted from global warming during the PETM (Pagani et al., 2006b).
Palynological spore and pollen assemblages from 22/10a-4 record a rapid major shift in vegetation at the CIE onset, with dominant swamp conifer communities and pines largely replaced by generalist taxa including fern and fungal spores and various angiosperms.
A change to a dominance of angiosperm over gymnosperm pollen at the CIE onset has previously been recorded in the Arctic (Sluijs et al., 2006) and Wyoming (Smith et al., 2007).
The rapid reduction in lowland gymnosperm swamp pollen at the CIE onset may indicate a change in lowland topography from sea level alteration, and/or it may indicate ecologic changes driven by climate (e.g. precipitation and temperature).
This floral shift occurs simultaneously with the first persistent appearance of Apectodinium in 22/10a-4, indicating that precursor CIE ecologic changes identified in NW Atlantic and North Sea marine records (Sluijs et al., 2007) had a terrestrial counterpart in the North Sea region.
Longer term vegetation changes after the CIE onset indicate a move towards more diverse generalist angiosperm and pteridophyte communities (dominance of Caryapollenites, fern, and fungal spores).
The pollen and spore assemblages therefore indicate that long term ecologic change occurred in NW Europe probably in response to temperature and hydrologic changes during the PETM, but that the most dramatic changes recorded in 22/10a-4 occurred abruptly at the onset of the CIE.
Acknowledgements
We thank B.G.
Group for making core material available for analysis.
We gratefully acknowledge two anonymous reviewers for enhancing the paper, and Appy Sluijs for constructively reviewing a previous version of the paper.
The work is published with the approval of the Executive Director of the British Geological Survey (NERC).
Supplementary information
Supplementary data associated with this article can be found in the online version at http://dx.doi.org/10.1016/j.epsl.2012.08.011.
Supplementary information
Supplementary materials
Supplementary materials
Supplementary materials

Marine and terrestrial environmental changes in NW Europe preceding carbon release at the Paleocene-Eocene transition

Abstract
Environmental changes associated with the Paleocene-Eocene thermal maximum (PETM, ∼56Ma) have not yet been documented in detail from the North Sea Basin.
Located within proximity to the North Atlantic igneous province (NAIP), the Kilda Basin, and the northern rain belt (paleolatitude 54°N) during the PETM, this is a critical region for testing proposed triggers of atmospheric carbon release that may have caused the global negative carbon isotope excursion (CIE) in marine and terrestrial environments.
The CIE onset is identified from organic matter δ13C in exceptional detail within a highly expanded sedimentary sequence.
Pollen and spore assemblages analysed in the same samples for the first time allow a reconstruction of possible changes to vegetation on the surrounding landmass.
Multiproxy palynological, geochemical, and sedimentologic records demonstrate enhanced halocline stratification and terrigenous deposition well before (103yrs) the CIE, interpreted as due to either tectonic uplift possibly from a nearby magmatic intrusion, or increased precipitation and fluvial runoff possibly from an enhanced hydrologic cycle.
Stratification and terrigenous deposition increased further at the onset and within the earliest CIE which, coupled with evidence for sea level rise, may be interpreted as resulting from an increase in precipitation over NW Europe consistent with an enhanced hydrologic cycle in response to global warming during the PETM.
Palynological evidence indicates a flora dominated by pollen from coastal swamp conifers before the CIE was abruptly replaced with a more diverse assemblage of generalist species including pollen similar to modern alder, fern, and fungal spores.
This may have resulted from flooding of coastal areas due to relative sea level rise, and/or ecologic changes forced by climate.
A shift towards more diverse angiosperm and pteridophyte vegetation within the early CIE, including pollen similar to modern hickory, documents a long term change to regional vegetation.
Highlights
► We identify the carbon isotope excursion from the North Sea in exceptional detail.
► We document changes to ocean stratification well before atmospheric carbon release.
► Precursor stratification may be associated with the trigger for carbon release.
► Increased precipitation and runoff likely occurred along with carbon release.
► We document a rapid change to regional vegetation during carbon release.

Introduction
The PETM was a period of geologically-rapid global warming that punctuated a warming Eocene climate 55.8Ma ago (Charles et al., 2011), and saw sea surface temperatures rise by 5-8°C from background levels (Zachos et al., 2005; Sluijs et al., 2007).
It was associated with a substantial injection of δ13C-depleted carbon into the ocean-atmosphere system (see Pagani et al., 2006a) over <20ka (Cui et al., 2011), causing a negative carbon isotope excursion (CIE) between -2 and -7‰ in marine and terrestrial sediments (see overview in Schouten et al., 2007) lasting 170ka (Röhl et al., 2007), and a prominent dissolution horizon in the deep sea signifying deep ocean acidification (Kennett and Stott 1991; Zachos et al., 2005).
The source and rate of released carbon are still under debate (Pagani et al., 2006a; Zeebe et al., 2009; Cui et al., 2011), but may have been linked to the dissociation of marine hydrates containing biogenic methane (δ13C of<-60‰) (Dickens et al., 1995), thermogenic methane from marine sediments around the Norwegian Sea (Svensen et al., 2004), or dissolved methane from a silled Kilda Basin between Greenland and Norway (Nisbet et al., 2009).
The PETM may be a good analogue to test modelling studies that suggest current global warming trends may result in an enhanced hydrologic cycle (Seager et al., 2010), whereby increased precipitation in temperate rain belts is coupled with increased evaporation in lower latitudes.
Modelling studies of the PETM have further indicated the potential importance of an increased hydrologic cycle (Lunt et al., 2010; Bice and Marotzke 2002), which could have altered ocean circulation causing methane hydrate reservoirs to destabilise, triggering massive carbon release (Bice and Marotzke, 2002).
Palynological evidence from Arctic Spitsbergen (Harding et al., 2011) and New Zealand (Crouch et al., 2003a) suggests increased terrestrial runoff occurred at the onset of the CIE which may be related to hydrologic changes, and massive Pyrenees conglomerate deposits have been interpreted as the result of an abrupt increase in extreme precipitation within the early CIE (Schmitz and Pujalte, 2007).
In addition, hydrogen and carbon isotope measurements of terrestrial plant and aquatic-derived n-alkanes from the central Arctic Ocean indicate that the core of the PETM was associated with increased precipitation and hence hydrologic cycle (Pagani et al., 2006b), although the onset of PETM warming was not recovered in the sediment core.
Despite numerous additional evidence for changes in terrestrial runoff and potentially hydrology during the PETM (see overview in McInterney and Wing, 2011), there is a lack of clear evidence for hydrologic changes from high resolution sections able to resolve important lead/lag relationships, and therefore there remains a need for studies of hydrologic changes in sensitive locations over the onset of the CIE in order to understand the relationship between precipitation and global carbon release.
Biome changes in response to modern global warming have been observed, but approaches to predict the vulnerability of ecosystems to future changes are still in development (Gonzalez et al., 2010).
Vegetation shifts during the rapid warming associated with the PETM may provide a useful analogue to future biome responses.
Whilst neotropical vegetation in Central America appears to have responded to warming during the PETM with increased diversity and origination rates (Jaramillo et al., 2010), central North America experienced a rapid migration of plant communities associated with lower precipitation at the onset of the CIE (Wing et al., 2005), and southern England may have experienced a major change in plant composition due to changes in local fire-regime (Collinson et al., 2009).
To better understand biome responses to climatic change during the PETM, further high resolution vegetation cover studies are needed, specifically from temperate and boreal forests which may be amongst the most vulnerable ecosystems to global warming (Gonzalez et al., 2010).
In this study we focus on paleoenvironmental signals from a high resolution marine core collected from the central North Sea (Fig. 1), in order to understand changes to precipitation, ocean stratification, productivity and vegetation over the onset of the PETM.
This core is located in a critical region proximal to the NAIP, as there are currently no high resolution records of environmental change during the PETM from the North Sea Basin.
Furthermore, as overturning of a stratified Kilda Basin at the CIE onset is hypothesised as a possible trigger for dissolved methane release to the atmosphere (Nisbet et al., 2009), analysing the stratification history of the nearby North Sea is a possible way to test this hypothesis.
Regional setting
During the late Paleocene-early Eocene the North Sea was a restricted marine basin, characterised by siliciclastic sedimentation and high terrigenous input, principally from the Scotland-Faeroe-Shetland landmass (Knox 1998, Fig.
S1).
Core 22/10a-4 is located in the central part of the basin (Figs.
1 and S1) and is therefore disconnected from many marginal marine processes that could mask oceanographic signals (e.g. tidal or storm-induced erosion and slumping).
Paleobathymetry estimates in the North Sea during the Paleocene and Eocene are difficult to constrain accurately, as the extant benthic foraminifera present in the Paleogene are found today living between 200 and >1000m water depth (Gradstein et al., 1992), and are controlled predominantly by substrate and bottom water properties.
However using a number of paleoecologic micropaleontology methods together (Gillmore et al., 2001), along with 2D structural restoration (Kjennerud and Sylta, 2001), broad agreement was found and central parts of the northern North Sea appear to have had paleodepths of >0.5km in the earliest Eocene near 22/10a-4 (Kjennerud and Gillmore, 2003, Fig. S1).
As 22/10a-4 is in the deep (>0.5km) central part of the basin, it acted as a depocentre and exhibits a Paleocene-Eocene transition sequence that is not only expanded but is also close to being stratigraphically complete.
The only evidence for breaks in the succession is minor erosion at the base of thin turbidite sandstones (typically <10cm).
Because these sandstones may contain reworked material, they were not sampled in this study.
During the late Paleocene, the basin became restricted following a fall in in the order of 100m that resulted from regional uplift associated with the proto-Iceland mantle plume in the North Atlantic (see Knox, 1996).
This event is evident in 22/10a-4 as a lithologic change from unbedded to bedded mudstone (the Lista and Sele Formation boundary, Fig. 2).
Restriction of the basin also led to the establishment of poorly oxygenated bottom waters, as is evident by a shift in the benthic foraminiferal assemblages towards low diversity low oxygen-tolerant agglutinated species (Knox, 1996).
The CIE at the Paleocene-Eocene boundary was accompanied by a relative sea level rise, as documented in southeast England (Powell et al., 1996) and Spitsbergen (Harding et al., 2011), due to the thermal expansion of sea water and possible melting of ice caps, although the North Sea basin remained restricted as evidenced by the persistence of low oxygen facies in 22/10a-4.
The North Sea had a widespread freshwater catchment area, and a halocline was in place from the late Paleocene to early Eocene (Zacke et al., 2009).
Therefore, surface water salinity changes in the North Sea Basin provide a sensitive gauge for stratification forced by changes in tectonics and the hydrologic cycle.
Methods and materials
Sedimentology
Borehole 22/10a-4 (57°44'8.47"N; 1°50'26.59"E) provides a continuous core through the Forties Sandstone Member and into the Lista Formation (Fig. 2).
The core consists of variably fissile claystone with interbedded fine to coarse grained sandstone layers interpreted as turbidites, with occasional mm-thick ash layers (Fig. 3).
All samples in this study were taken from claystone horizons to avoid sampling substantial quantities of reworked material.
The section of 22/10a-4 analysed in this study is from 2605m to 2634m (core depth), chosen because this part of the core is predominantly in claystone facies and provides a greatly expanded section over the onset of the CIE (Figs.
2 and 3).
At 2609-2613m the claystone becomes finely laminated with alternately pale and dark laminae couplets ranging from 1 to 25permm (Fig.
S2).
The pale laminae consist of clay and silt, and the dark laminae are rich in organic carbon and pyrite inclusions.
Laminae were counted at 26 horizons throughout the core and approximately 13 pairspermm.
Micropaleontology
A total of 71 palynology samples were prepared at the British Geological Survey using standard preparation procedures (Moore et al., 1991).
Samples were demineralised with hydrochloric (HCl) and hydrofluoric (HF) acids, and residual mineral grains removed using heavy liquid (zinc bromide) separation.
Elvacite was used to mount slides.
The palynomorphs were analysed using a Nikon transmitted light microscope, counting the total number of palynomorphs on a strew slide (Table S1).
Each slide was produced from 1/100th of the total material processed, where the initial weight of material was 5g of dried sediment.
Thus, the palynology counts represent the total number of specimens per 0.05g of dried sediment.
Statistical analysis was carried out using the software of Hammer et al. (2005).
The %wood/plant tissue was determined by palynological investigation, and is the sum of '%wood plant tissue' and '%various (non-woody) plant tissue' in Table S1.
Organic material for δ13CAOM analysis was collected from the same palynology samples, and the remaining processed material separated into size fractions.
The >250μm fractions, found through light microscope analysis to be dominated (>90%) by amorphous organic matter (AOM), were also analysed for δ13C.
Foraminifera samples between 20 and 60g of dried sediment were processed by washing through a 63μm sieve with water.
All specimens were counted and converted to foraminifera/g (Table S2).
All species exhibited agglutinated (non calcareous) test walls.
Geochemistry
All analyses were carried out at the NERC Isotope Geosciences Laboratory.
C and N analyses (from which we present weight % C/N) were performed on 225 samples by combustion in a Costech ECS4010 Elemental Analyser (EA) calibrated against an acetanilide standard (Table S3).
C/N atomic ratios were calculated by multiplying by 1.167.
Replicate analysis of well-mixed samples indicated a precision of ±<0.1.
Carbon isotope analysis was carried out on 289 bulk rock samples (Table S4) after removing migrated hydrocarbons (Stephenson et al., 2005).
The hydrocarbons were removed by crushing the rock fragments using a ball mill, and the soluble organic matter from all rock samples was extracted using a Soxhlet extractor.
The samples were refluxed for 24h in an azeotropic mixture of dichloromethane and methanol (93:7, v/v).
All materials (cellulose Soxhlet thimbles, silica wool, vials) were cleaned with analytical grade organic solvents prior to use.
Any remaining solvent was then removed by evaporation and the dried sediments were transferred to vials.
Any calcites (shelly fragments) were removed by placing the samples in 5% HCl overnight before rinsing and drying down.
Carbon isotope analysis was also carried out on palynology residues of the >250μm size fractions dominated by AOM.
13C/12C analyses were performed on 35 samples by combustion in a Costech Elemental Analyser (EA) online to a VG TripleTrap and Optima dual-inlet mass spectrometer, with δ13C values calculated to the VPDB scale using a within-run laboratory standards calibrated against NBS-18, NBS-19, and NBS-22.
Replicate 13C/12C analyses were carried out on the section, and the mean standard deviation on the replicate analyses is 0.4‰.
Results and discussion
Statistical analysis
Correspondence analysis (CA) and statistical diversity analysis were carried out on the palynological dataset (total counts per gram) to confirm assemblage designations (Figs.
4 and 5), to identify any disturbance to the core prior to interpretation, and to estimate diversity (Fig. 6).
Dinoflagellate cyst assemblages (DA1-DA5) and pollen assemblages (PA1-PA4) were defined by visually comparing changes in the species dominance (Figs.
7 and 8), and confirmed by CA (Fig. 4) using the first three axes (describing the highest percentages of variance).
Five samples from below the CIE at 2619.60, 2617.35, 2617.44, 2614.73, and 2614.71m (indicated in Fig. 5) contain Apectodinium, in contrast to the other samples below the CIE (Figs.
3 and 7).
Some of these samples (2619.60, 2614.73, and 2614.71m) also contain negative δ13CTOC, indicative of the CIE (Fig. 3).
To test if coincident pollen and spore changes also occur in these samples, we used CA on the pollen and spore data only (Fig. 5a).
PA1-PA4 (symbols) plot in clusters, signifying their palynological similarity.
The species most associated with an assemblage are clustered with the samples from that assemblage.
For example, Inaperturopollenites hiatus and bisaccate pollen (highly abundant before the CIE, Fig. 8) are high on axis 1 where the earlier samples from PA1 and PA2 occur, and Caryapollenites spp.
and fungal spores (abundant after the CIE, Fig. 8) are low on axis 1 near the younger samples from PA4.
The two samples 2619.60 and 2614.71 have a spore and pollen palynological signature similar to samples from PA3/4 during the CIE (plot lower on axis 1) and are either not in the correct location (it cannot be discounted that these samples represent tectonically emplaced younger material (Payne et al., 2005), or were misplaced during drilling operations core handling), or represent very short episodes of both marine and terrestrial ecologic change to CIE-type conditions.
The rapid and transient nature of these two shifts appears to suggest that the latter explanation may be unlikely, and we have therefore shaded samples from these two depths in Figs.
3, 6-9.
The majority of the morphospecies in our study represent taxonomic groupings of terrestrial plant species, from generic to higher level groupings, such that any palynology diversity measure will underestimate vegetation diversity.
However, although subtle changes may not be resolved, large changes to the diversity of regional vegetation are likely to be reflected in palynology assemblages.
Palynological data have therefore previously been used to estimate plant diversity in the geologic record (e.g. Ogaard, 2001; Harrington, 2004; Jaramillo et al., 2010).
The number of pollen and spore species in each sample (Fig. 6d) fluctuates, trending towards increasing number over the CIE onset (from PA2 to PA3).
To account for the changing number of specimens counted in each sample (more species will be encountered with higher counts), two statistical indices were used (Fig. 6e, f).
Both confirm the significant increase in pollen and spore diversity from before to after the CIE onset.
We do not use the range-through method, as we are interested in changes to local vegetation habitats.
The increase in the number of pollen and spore species appears to be largely driven by pteridophytes (largely ferns, Fig. 6g), although angiosperm diversity also increases over the CIE onset (Fig. 6h) and fungal spores become more prevalent (Fig. 8m).
Gymnosperms are only represented by two morphospecies and so diversity was not calculated for this group.
Isotope changes and age model
The Lista-Sele Formation boundary (Fig. 2) occurs in the lower part of magnetochron 24r in the Faeroe-Shetland Basin (Mudge and Bujak, 2001), giving a date of >56.6Ma for the base of the studied section (Gradstein and Ogg, 2005).
The top of the studied section is within nannofossil zone NP9 (Knox, 1996), dated as <55.7Ma (Gradstein and Ogg, 2005).
The Paleocene-Eocene marker event Apectodinium augustum (Bujak and Brinkhuis, 1998) occurs in 22/10a-4 at 2617.35m (Fig. 3), approximately at the CIE onset (as was found at 30/14-1, Fig. 1, Sluijs et al., 2007).
The global CIE is determined by the negative shift in both marine and terrestrial δ13C between 2‰ and 7‰ (see Schouten et al., 2007) and has recently been radioisotopically dated in Spitsbergen to 55.8Ma (Charles et al., 2011).
The global CIE onset is present in 22/10a-4 within the interval from 2614 to 2612m (Fig. 3), where a negative δ13C shift of 5‰ occurs along with a peak in Apectodinium.
The palynological results (from samples containing sufficient material for analysis; red symbols in Fig. 3) show that %wood/plant tissue varies from 10% to 88% throughout this interval (Fig.
S3).
We found that the samples containing >30% wood/plant tissue (open red symbols in Fig. 3) in the palynological residues have consistently heavier δ13CTOC values than those with less wood/plant tissues (solid red symbols).
The presence of transported Corg thus precludes an unambiguous interpretation of the rate of atmospheric carbon release from the shape of the CIE onset at 22/10a-4, but the initial negative δ13C shift between 2614.3 and 2613.5m (shaded box, Fig. 3) can be taken as marking the earliest evidence of δ13C-depleted atmospheric carbon release and the onset of the global CIE.
This positioning of the CIE onset is supported by δ13C analysis of isolated AOM (Fig. 3), likely of marine origin (see Supplementary Material) and less likely to be composed of mixtures of different sources of Corg (although it is still susceptible to reworking).
As we observe only a 1‰ 'recovery' in the CIE, the age of the top of the studied section appears to be no more than 75ka from the CIE onset by comparison with the cyclostratigraphically correlated Longyearbyen section in Spitsbergen (Charles et al., 2011), giving an average sedimentation rate of ≥8cm/ka after the CIE onset (if sand horizons (Fig. 3) are removed, assuming rapid turbidite deposition).
The long term 0.7‰ positive shift in δ13C at 2609m (Fig. 3) may correlate to a similar shift in the Longyearbyen section (Charles et al., 2011), which is cyclostratigraphically correlated to 45ka after the CIE onset (Charles et al., 2011), and would give a sedimentation rate of on average 8cm/ka for this section.
The finely laminated part of the core may also provide temporal insight.
There is no direct evidence for the period of deposition of each lamination couplet, but as modern marine lamination-forming basins produce annual laminae pairs (e.g. the Black Sea: Arthur et al., 1994; Carioaco Basin: Tedesco and Thunell, 2003; Santa Barbara Basin: Thunell et al., 1995), the 40,000 estimate of the number of laminae pairs present between 2613 and 2609m (see Section 3.1) may represent approximately 40ka, and a sedimentation rate of 7.5cm/ka.
Dinoflagellate cysts and surface water changes
Dinoflagellate cyst assemblage 1 (DA1, from 2632 to 2618m, Fig. 7p) contains high proportions of typically open marine and hence normal marine salinity associated Achomosphaera/Spiniferites spp., undifferentiated chorate cysts and Areoligera/Glaphyrocysta spp.
DA1 also contains on average 5% peridinoid cysts including Deflandrea spp.
regarded as a coastal/neritic taxon indicating high productivity and nutrient availability (Brinkhuis, 1994; Pross and Brinkhuis, 2005).
These characteristics indicate that a somewhat restricted but fully marine shelf environment was present before the onset of the CIE in the central North Sea, with availability of nutrients indicated by the presence of Deflandrea spp.
DA2 (- 2614m) contains a similar abundance of Achomosphaera/Spiniferites spp.
and chorate cysts to DA1, but with a marked increase in the abundance of low salinity tolerate Cerodinium depressum and Senegalinium spp., which may also have been a heterotrophic genus indicative of elevated nutrient levels (Sluijs and Brinkhuis, 2009).
Undifferentiated peridinoid cysts also peak in abundance, and may also be indicative of elevated nutrient and reduced salinity conditions (although they are not included in the %low salinity dinoflagellate cysts, Fig. 7c).
There is also an increase in the abundance of Areoligera/Glaphyrocysta spp., thought to indicate unrestricted neritic environments of more typical marine salinity.
In summary, the higher proportion of low salinity tolerant dinoflagellate cysts in DA2 (Fig. 7c) appears to indicate continuous or episodic freshening surface waters below typical marine salinities (<31‰ in the modern ocean).
In addition, DA2 is characterised by elevated numbers of dinoflagellate cysts per gram (70per 0.05g in DA1; 110per 0.05g in DA2) indicating a possible increase in cyst production which would be consistent with increased fluvial runoff carrying nutrients from nearby landmasses.
DA2 also contains an abundance maximum of Apectodinium, an extinct genus with a somewhat uncertain ecologic affinity (see Crouch et al., 2003b), although likely reflecting relatively warm and eutrophic conditions (Crouch et al., 2003b; Sluijs et al., 2007; see Supplementary Material).
The transient appearance of Apectodinium occurs with a reduced abundance of open marine dinoflagellates, and a peak in Deflandrea indicating possibly lower salinity and higher nutrient availability.
DA3 (2614-2612m) is characterised by Apectodinium making up an average 40% of the assemblage, and a reduction in the abundance of all other species apart from Deflandrea and Senegalinium.
DA3 may therefore indicate a continuation of high nutrient surface water with an elevated freshwater input.
In this respect, it is perhaps similar to DA2, although the low abundances of dinoflagellates with known ecologic affinities make interpretations more tentative.
The appearance of bottom water anoxia (laminations) and sporadic high accumulation of marine AOM (see Supplementary Material) is consistent with highly productive surface waters.
DA4 (2612-2609m) shows an increase in the number of low salinity/high nutrient species Cerodinium depressum and Senegalinium spp., with a continued low abundance of normal marine salinity Achomosphaera/Spiniferites spp., such that the proportion of low salinity tolerant dinoflagellate cysts is at a maximum (Fig. 7c) indicating the presence of a pronounced halocline.
The lack of freshwater fern and algal spores Azolla, Pediastrum and Botryococcus (Table S1) indicates that surface water salinities were probably not below 5‰ (the limit for Azolla, Brinkhuis et al., 2006), and may have been greater than 20‰ as Botryococcus was found living in salinities as high as 20‰ in modern Australian lakes (de Deckker, 1988).
This minimum salinity value is consistent with Zacke et al. (2009), who found continuous occurrences of shark teeth in shallow marine North Sea facies throughout the late Paleocene/early Eocene, and noted that sharks do not live in salinities below 20‰ in the modern ocean.
Highly productive surface waters are indicated by dinoflagellate cysts (Fig. 7c), and consistent with bottom water anoxia (laminations and disappearance of benthic foraminifera) and very high accumulation of marine AOM.
DA5 (2609-2607m) shows a decrease in low salinity tolerant dinoflagellate cysts and an increase in unrestricted (normal marine salinity) Areoligera/Glaphyrocysta spp.
and other chorate cysts.
The reduction in AOM together with the loss of lamination (possibly end of bottom water anoxia) indicates a return towards marine salinities that existed before the onset of the CIE (represented by DA1), although the persistence of Apectodinium may indicate a long term change in marine ecology.
Enhanced terrigenous deposition
The increase in abundance of low salinity tolerant Cerodinium depressum and Senegalinium spp.
before and after the CIE onset (DA2-DA4, Fig. 7) most likely indicates a reduction in surface water salinity and elevated nutrient levels (see Section 4.3).
This appears to have been associated with increased terrigenous deposition, as evidenced by a concomitant increase in the C/N ratio of 22/10a-4 (Figs.
9c and S4), and an elevated kaolinite contribution to the clay assemblage (Fig. 9e).
Sedimentary atomic C/N ratios can be used to differentiate the origin of organic matter (Meyers, 1997; Storme et al., 2012), with values in 22/10a-4 averaging 10-15 (Fig. 10) indicating a mix of terrestrial land plant-derived and marine/lacustrine algal-derived carbon (Meyers, 1997).
C/N values broadly follow %low salinity dinoflagellate cysts (Fig. 9), with an increase at the Lista/Sele Formation boundary, 4m before the onset of the CIE and within the early CIE.
As dinoflagellate cyst assemblages suggest higher productivity (%low salinity dinoflagellate cysts, Apectodinium), the C/N ratio increase likely indicates an increased flux of terrigenous material to 22/10a-4, rather than a lower marine carbon flux.
Kaolinite also increases before and after the onset of the CIE in 22/10a-4 (Fig. 9e).
Kaolinite forms as a result of intense chemical weathering that typically develops on well-drained surfaces receiving high precipitation (Robert and Kennett, 1994).
Increased kaolinite at the PETM had been regarded as indicative of an increase in chemical weathering and hence humidity in the source region (e.g. Robert and Kennett, 1994; Knox, 1996), but the long formation time of thick soil kaolinite (>1myr) suggests these increased proportions probably resulted from erosion of previously formed kaolinite (Thiry and Dupuis, 2000; Schmitz et al., 2001).
Pollen, spores and vegetation shifts
The pollen and spore assemblages (PA) that characterise the pre-CIE interval in 22/10a-4 (PA1 and PA2, Fig. 8n) are dominated by I. hiatus and bisaccate pollen.
Both taxa are produced in abundance by a variety of coniferous plants, and are typical in the Paleogene of the northern UK and Greenland region (Boulter and Manum 1989; Jolley and Whitham 2004; Jolley and Morton 2007), as well as mid-latitude North America (Smith et al., 2007) and Arctic Canada (Greenwood and Basinger, 1993).
I. hiatus is a member of the Cupressaceae family (coniferous trees) and most likely represents Metasequoia and/or Glyptostrobus swamp conifers (Greenwood and Basinger, 1993; Jolley and Morton, 2007; Jolley et al., 2009).
In terrestrial Paleocene deposits from western Scotland, abundant I. hiatus was recorded in association with Momipites, Cupuliferoipollenites, Platycaryapollenites, Plicapollis pseudoexcelsus, and Alnipollenites interpreted as derived from a channel-margin bog community on a wet substrate (Jolley et al., 2009).
Bisaccate pollen (family Pinaceae) was likely derived from temperate coniferous trees possibly on drier substrates, with an elevation from several metres within swamps (Greenwood and Basinger, 1993) to possibly much higher altitude (Jolley and Whitham 2004).
Both are relative overproducers of pollen (Smith et al., 2007).
Lowland swamp vegetation appears to have been present before the CIE on the Scotland-Faeroe-Shetland platform (this study; Jolley and Morton 2007), the catchment area for 22/10a-4 (Fig.
S1, Knox 1996), with the relative increase of I. hiatus over bisaccate pollen from PA1 to PA2 possibly due to sea level fall at the Lista/Sele boundary (Knox, 1996) allowing lowland swamp conifers to expand and/or come into closer proximity to 22/10a-4.
The most significant floral change at 22/10a-4 occurs at the onset of the CIE (PA2-PA3), with a large drop in the proportion of I. hiatus, increasing diversity (Fig. 6), higher proportions of angiosperms (Alnipollenites and P. pseudoexcelsus), and fern and fungal spores (Fig. 8).
Alnipollenites was probably from the birch Alnus (alder), and in this setting represents generalist vegetation as it occurred in all the terrestrial Paleocene communities from western Scotland (Jolley et al., 2009).
Alnus and associated ferns were most common where Metasequoia (possible source of I. hiatus) was rare in Arctic Canada, where exceptionally well-preserved Paleocene-Eocene Metasequoia and Glyptostrobus swamp deposits are prevalent, and interpreted as Alnus-fern bogs (Greenwood and Basinger, 1993).
Alnus is a known nitrogen-fixing pioneer species in nutrient-depleted soils (Hobbie et al., 1998).
P. pseudoexcelsus is similar to modern Juglans (walnut) pollen sometimes associated with wetland plants (Jolley and Whitham, 2004).
The fern spores Cicatricosisporites and Laevigatosporites (family Schizaeaceae) were generalists, associated with all environments from the terrestrial Paleocene communities analysed in western Scotland (Jolley et al., 2009).
Since dinoflagellate cysts indicate increased halocline stratification of the North Sea at the CIE (Fig. 9d), and C/N ratios indicate a greater proportion of terrestrial carbon (Fig. 9c), it seems likely that the large reduction in I. hiatus in 22/10a-4 reflects a change in vegetation cover in the source region rather than simply a general reduction in the supply of pollen due to proximity of the coast.
Replacement of I. hiatus-dominated swamp communities (PA2) with generalist taxa (PA3) indicative of Alnus-fern bogs (Greenwood and Basinger, 1993) would be consistent with local sea level fall (partially draining coastal plains) or sea level rise (flooding established coastal swamp plains).
There is evidence for sea level rise at the onset of the CIE in southeast England (Powell et al., 1996) and in Spitsbergen (Harding et al., 2011), although any uplift associated with the proximal NAIP could have caused local sea level fall near the Faeroe-Shetland platform.
Alternatively, the reduction in gymnosperm swamp conifers and pines at the expense of angiosperms may have been a climatic response to the PETM, as a similar floristic change was also recorded during the PETM in the Bighorn Basin, Wyoming (Smith et al., 2007), and the Lomonosov Ridge (Sluijs et al., 2006).
The early ephemeral peak in Alnipollenites and P. pseudoexcelsus and drop in coastal swamp I. hiatus at 2617.35m, associated with the initial peak in Apectodinium (Fig. 8), may indicate a brief episode of coastal flooding from increased precipitation, in association with lower surface water salinity (peak in Deflandrea, Fig. 7i).
A further ecologic shift occurs within the core of the CIE (PA4) as Caryapollenites (Fig. 8h) begins to dominate, moss (Stereisporites) and fungal spores increase in relative abundance, bisaccate pollen and Alnipollenites decreases (Fig. 8), and diversity remains relatively high (Fig. 6).
Caryapollenites, similar to modern Carya (hickory) pollen, is common in terrestrial Paleocene deposits from western Scotland interpreted as bogs (Jolley et al., 2009), and many may have been adapted as primary colonisers in wet substrates (Jolley and Whitham 2004).
Its association with moss and fern spores in PA4 indicate a possible predominance of bog environments, consistent with high regional precipitation and poorly drained acidic bedrock.
As bogs receive water and nutrients directly from precipitation (Price and Waddington, 2000), the change from PA3 to PA4 appears to be the result of climatic change (e.g. precipitation and temperature) rather than sea level.
Other changes include a further increase in the proportion of angiosperm over gymnosperm pollen (largely a reduction in pine), consistent with previous observations from the Lomonosov Ridge (Sluijs et al., 2006) and Bighorn Basin, Wyoming (Smith et al., 2007).
Environmental and climatic changes before the CIE
The North Sea Basin became restricted before the PETM, as indicated by a lithologic change throughout the North Sea Basin (the Lista/Sele boundary, Figs.
2 and 9) and the presence of low oxygen benthic foraminifera in 22/10a-4 (Knox, 1996).
This relative sea level fall was recognised as a lithologic change from marine to lagoonal/shallow marine facies in southeast England (Knox et al., 1994) and considered to have been largely the result of major regional uplift possibly in the order of 100m (Knox, 1996).
The documented major tectonic uplift at the Lista/Sele boundary best explains the concomitant increase in terrigenous input to 22/10a-4 (increase C/N ratio, Fig. 9c), the domination of lowland swamp vegetation (swamp conifers, Fig. 8), and the lowering of surface water salinity (dinoflagellate cysts, Fig. 9d) as the basin became more restricted.
The gradual lowering of surface water salinity and increase in the C/N ratio from 4m before the CIE onset at 22/10a-4 (above 2618m, Fig. 9), therefore, may also be the result of further uplift and restriction of the North Sea Basin, bringing the coastline and transported terrestrial material into closer proximity to the centrally-located 22/10a-4, although there is no documented sea level fall in the North Sea preceding the CIE (Knox, 1996; Powell et al., 1996).
The increase in kaolinite in this scenario would be coincidental, perhaps having been formed after (>1myr, Thiry and Dupuis, 2000) the uplift at the List/Sele boundary.
As the North Sea Basin was proximal to the NAIP during the Paleocene/Eocene (Figs.
1 and S1), regional uplift before the CIE would most likely be related to intrusive activity west of the North Sea Basin, restricting deep-water connections between the North Sea and Atlantic Ocean.
Uplift and restriction is therefore consistent with the hypothesis that a mantle-derived magmatic intrusion of organic-rich sediments occurred in the NE Atlantic before the CIE, triggering atmospheric methane release (Svensen et al., 2004).
Alternatively, an increase in regional precipitation could have caused elevated terrestrial runoff (C/N ratios, kaolinite) and lower surface water salinity above 2618m before the CIE onset (Fig. 9).
The North Sea Basin surrounding landmasses were within the northern rain belt (the southern boundary today is 40°N), which would have experienced elevated precipitation if the global hydrologic cycle became enhanced (Pagani et al., 2006b; Schmitz et al., 2001).
This scenario would be consistent with a gradual increase in the global hydrologic cycle before the CIE, perhaps from gradual warming, which was hypothesised to have triggered ocean circulation changes, methane hydrate destabilisation, and global carbon release at the CIE (Bice and Marotzke, 2002).
We note however that there is currently no evidence for an enhanced hydrologic cycle well before the CIE in other regions.
Our results provide the first evidence that the North Sea became stratified from 103 yrs before the CIE onset (above 2618m, Fig. 9).
This is significant as Nisbet et al. (2009) hypothesised that the proximal Kilda Basin became stratified and anoxic before the CIE, allowing significant build-up of methane and CO2 at depth.
They proposed that overturning of this basin could have released greenhouse gases and triggered the CIE, although there is currently no direct evidence as marine records from the Kilda Basin remain rare (Nisbet et al., 2009).
Our North Sea records likely indicate enhanced stratification also of the proximal and linked Kilda Basin before the CIE (Fig. 1).
Evidence for the linkage of the North Sea, Kilda and Arctic Basins comes from the coincident onset of A. augustum and laminated sediments at the CIE onset in sections from the North Sea (this study), Spitsbergen (Harding et al., 2011), and Lomonosov Ridge (Sluijs et al., 2006).
Although our results evidence a probable stratified Kilda Basin before the CIE, proxies for overturning are now needed to further test the Kilda basin hypothesis.
The brief peak in Apectodinium, AOM and low salinity dinoflagellate cysts (Deflandrea) at 2617.4m (Fig. 7) indicate a sporadic episode of surface water freshening/eutrophication before the CIE, which is best explained by an increase in regional precipitation due to its rapid nature.
A coincident reduction in I. hiatus swamp conifers indicates possible disturbance of nearby coastal environments possibly from flooding (see Section 4.5).
An associated reduction in δ13C may have been caused by stratification of the North Sea from an enhanced halocline, trapping 12C-enriched organic carbon at depth.
This scenario may also explain the other peaks in Apectodinium at 2619.6 and 2614.7m (although see Section 4.1).
Environmental and climatic changes within the CIE
The influx of Apectodinium (indicating the onset of the CIE) at the Paleocene-Eocene boundary in southeast England occurred in characteristically marine assemblages following non-marine deposition, and was interpreted as an indication of relative sea level rise associated with a transgression immediately preceding a maximum flooding surface within the early PETM (Powell et al., 1996).
Sea level rise at the CIE onset was also recorded in Arctic Spitsbergen (Harding et al., 2011).
The large reduction in lowland swamp pollen at the CIE onset of 22/10a-4 indicates a possible change in regional sea level.
As there is no direct evidence for regional uplift causing further restriction of the North Sea Basin associated with the CIE onset, increased regional precipitation may have been the major cause of increased C/N ratios (elevated fluvial runoff) and dinoflagellate cyst changes (lower surface water salinity) at and following the CIE onset in 22/10a-4 (Fig. 9).
Although our records cannot distinguish changes in seasonality, which may have increased in the Pyrenees (Schmitz et al., 2001; Schmitz and Pujalte, 2007) and mid-latitude North America (Wing et al., 2005; Kraus & Riggins 2007), they are consistent with an overall increase in precipitation over NW Europe associated with the CIE.
Our records therefore provide evidence consistent with the hypothesis that a northward migration of storm tracks occurred from an intensified hydrologic cycle as a result of global warming at the PETM, proposed by Pagani et al. (2006b) to explain elevated Arctic runoff during the PETM.
As Pagani et al. (2006b) could not fully resolve the CIE onset due to incomplete core recovery, our records provide evidence that an enhanced hydrologic cycle may have occurred in approximate concert with global carbon release at the CIE onset.
Alternatively, uplift of the NAIP could have caused further restriction and stratification of the North Sea.
The reduction in %low salinity dinoflagellate cysts and the C/N ratio above 2609m (Fig. 9) may indicate a reduction in precipitation over NW Europe, or more likely tectonic subsidence causing the basin to become less restricted.
Conclusions
A negative carbon isotope excursion of 5‰ has been identified from δ13CTOC and δ13CAOM in an expanded Paleocene-Eocene boundary section from the central North Sea Basin.
Palynological (dinoflagellate cyst, pollen, and spore assemblages) and sedimentologic (C/N ratios and kaolinite distribution) evidence indicate major changes occurred to marine and terrestrial environments in NW Europe both preceding and over the CIE.
Enhanced halocline stratification and terrigenous input from 4m before the CIE may indicate tectonic uplift and oceanic restriction of the North Sea, supporting hypotheses for NAIP volcanism as a trigger for the CIE (Svensen et al., 2004), and/or increased terrigenous runoff and regional precipitation, supporting hypotheses of an enhanced hydrologic cycle triggering carbon release (Bice and Marotzke, 2002).
A peak in Apectodinium before the CIE is interpreted as an ephemeral increase in terrestrial runoff causing local eutrophication.
Further enhanced halocline stratification and terrigenous input at and immediately after the CIE onset, coupled with evidence for sea level rise in coastal areas, indicate possible increased regional precipitation over NW Europe.
At this location (paleolatitude 54oN) increased precipitation would support the hypothesis that a poleward migration of storm tracks from an enhanced hydrologic cycle resulted from global warming during the PETM (Pagani et al., 2006b).
Palynological spore and pollen assemblages from 22/10a-4 record a rapid major shift in vegetation at the CIE onset, with dominant swamp conifer communities and pines largely replaced by generalist taxa including fern and fungal spores and various angiosperms.
A change to a dominance of angiosperm over gymnosperm pollen at the CIE onset has previously been recorded in the Arctic (Sluijs et al., 2006) and Wyoming (Smith et al., 2007).
The rapid reduction in lowland gymnosperm swamp pollen at the CIE onset may indicate a change in lowland topography from sea level alteration, and/or it may indicate ecologic changes driven by climate (e.g. precipitation and temperature).
This floral shift occurs simultaneously with the first persistent appearance of Apectodinium in 22/10a-4, indicating that precursor CIE ecologic changes identified in NW Atlantic and North Sea marine records (Sluijs et al., 2007) had a terrestrial counterpart in the North Sea region.
Longer term vegetation changes after the CIE onset indicate a move towards more diverse generalist angiosperm and pteridophyte communities (dominance of Caryapollenites, fern, and fungal spores).
The pollen and spore assemblages therefore indicate that long term ecologic change occurred in NW Europe probably in response to temperature and hydrologic changes during the PETM, but that the most dramatic changes recorded in 22/10a-4 occurred abruptly at the onset of the CIE.
Acknowledgements
We thank B.G.
Group for making core material available for analysis.
We gratefully acknowledge two anonymous reviewers for enhancing the paper, and Appy Sluijs for constructively reviewing a previous version of the paper.
The work is published with the approval of the Executive Director of the British Geological Survey (NERC).
Supplementary information
Supplementary data associated with this article can be found in the online version at http://dx.doi.org/10.1016/j.epsl.2012.08.011.
Supplementary information
Supplementary materials
Supplementary materials
Supplementary materials

1. Transition metal oxide-based precursor of a composite, by the following chemical formula 1 shown:
[Formula 1]
NiaCobM'cOx, wherein, 1<x ≤ 1.5,
Wherein,
M'is selected from the alkali metal, alkaline earth metals, Group 13 elements, Group 14, Group 15 elements, Group 16 element, a Group 17 element, a transition metal and one or more of the group consisting of rare earth elements,
0.6 ≤ a<1.0,0 ≤ b ≤ 0.4,0 ≤ c ≤ 0.4,a + b + c=1.2. Composite transition metal oxide-based precursor according to claim 1, characterized in, M'is selected from the group consisting of Y and the one of Al,Mn,Zr,W,Ti,Mg,Sr,Ba,Ce,Hf,F,P,S,La or more.3. Composite transition metal oxide-based precursor according to claim 1, characterized in, one or more of the precursor is a secondary particle formed by aggregation of primary particles of the secondary particles.4. Composite transition metal oxide-based precursor according to claim 3, characterized in,
Average particle diameter of primary particles of the platy or acicular form 0.01-0.8 µm range, exists within a lot of pores on the surface or structure,
The secondary particles have an average particle size D50 to 3-30 μm of the range.5. Composite transition metal oxide-based precursor according to claim 1, characterized in, or more of the tap density of 2.0 g/cc precursor.6. Composite transition metal oxide-based precursor according to claim 1, characterized in, according to the BET nitrogen adsorption of the precursor of the specific surface area of 5-80 m measured2/gof the range.7. Composite transition metal oxide-based precursor according to claim 1, characterized in, precursor, based on the weight of the particles in the range of 5 nm-50 nm 1x10 pores volumes-3-1x10-2cm3/g·nm of the range.8. An anode active material, wherein the transition metal composite oxide according to any one of claim 1-7 use-based precursor and a lithium precursor is manufactured.9. The anode active material according to claim 8, characterized in, of all transition metals in the group consisting of nickel (Ni) content of not less than 60%.10. Claim 1-7 system as claimed in any one of the precursor of the transition metal oxide composite manufacturing method, comprising the following Chemical Formula 2 are a transition metal hydroxide represented by the step of oxidizing the precursor composite,
[Formula 2]
NiaCobM'c(OH)2
Wherein,
M'is selected from the alkali metal, alkaline earth metals, Group 13 elements, Group 14, Group 15 elements, Group 16 element, a Group 17 element, a transition metal and one or more of the group consisting of rare earth elements,
0.6 ≤ a<1.0,0 ≤ b ≤ 0.4,0 ≤ c ≤ 0.4,a + b + c=1.11. Method for producing a composite transition metal oxide-based precursor according to claim 10, characterized in, oxidation step is performed by:
(I) performing a heat treatment in an oxygen atmosphere;
(Ii) using an oxidizing agent; or
(Iii) the application (i) and a (ii) both.12. Method for producing a transition metal oxide-based precursor composite according to claim 10, characterized in, an oxidizing step, an oxygen concentration of not less than 80% of the oxygen atmosphere is in the range of 200-1000 C 1-12 hours heat treatment.13. A transition metal oxide precursor for producing a composite method according to claim 11, characterized in, KMnO oxidizing agent is selected from the group4, H2O2, Na2O2, FeCl3, CuSO4, CuO,PbO2, MnO2, HNO3, KNO3, K2Cr2O7, CrO3, P2O5, H2SO4, K2S2O8, halogen and C6H5NO2one or more of the group consisting of.
Human RPE Stem Cells Grown into Polarized RPE Monolayers on a Polyester Matrix Are Maintained after Grafting into Rabbit Subretinal Space

Summary
Transplantation of the retinal pigment epithelium (RPE) is being developed as a cell-replacement therapy for age-related macular degeneration.
Human embryonic stem cell (hESC) and induced pluripotent stem cell (iPSC)-derived RPE are currently translating toward clinic.
We introduce the adult human RPE stem cell (hRPESC) as an alternative RPE source.
Polarized monolayers of adult hRPESC-derived RPE grown on polyester (PET) membranes had near-native characteristics.
Trephined pieces of RPE monolayers on PET were transplanted subretinally in the rabbit, a large-eyed animal model.
After 4 days, retinal edema was observed above the implant, detected by spectral domain optical coherence tomography (SD-OCT) and fundoscopy.
At 1 week, retinal atrophy overlying the fetal or adult transplant was observed, remaining stable thereafter.
Histology obtained 4 weeks after implantation confirmed a continuous polarized human RPE monolayer on PET.
Taken together, the xeno-RPE survived with retained characteristics in the subretinal space.
These experiments support that adult hRPESC-derived RPE are a potential source for transplantation therapies.
Graphical Abstract
Highlights
•
Adult hRPESC-derived RPE had comparable in vitro characteristics to fetal hRPE
•
hRPE monolayers survived 4 weeks on PET carriers under the rabbit retina
•
Better xenograft survival may be due to the maintained hRPE cell polarity
•
Atrophy of the retina overlaying the hRPE xenograft remains a future challenge
Stanzel et al. have transplanted polarized monolayers of human retinal pigment epithelium (hRPE) grown on polyester membranes derived from adult hRPE stem cells or fetal tissue under the rabbit retina.
The xeno-RPE survived 4 weeks with retained cell polarity characteristics in the subretinal space.
These experiments support that adult hRPESC-derived RPE are a potential source for transplantation therapies.

Introduction
The retinal pigment epithelium (RPE) is a cellular monolayer between the retina and the underlying choroidal vasculature.
The RPE participates actively in the visual process, notably by supporting the diurnal replenishment of the photoreceptors (Strauss, 2005).
RPE dysfunction significantly contributes to the pathophysiology of age-related macular degeneration (AMD), a leading cause of blindness (Lim et al., 2012).
There are currently no disease-altering therapies available for the vast majority (over 85%) of AMD patients that suffer from the dry form of the disease, which is characterized by extracellular deposits termed drusen beneath the RPE and subsequent RPE atrophy in the macula.
The remaining approximately 15% of patients have wet AMD, in which neovascularization invades from the choroid; for these patients, repeated intravitreal injections with antiangiogenic drugs offer a highly effective, albeit palliative, treatment (Singer et al., 2012).
Replacement of dysfunctional submacular RPE with a cell-based therapeutic agent represents a potentially curative treatment strategy (Binder et al., 2007).
Some previous attempts in patients have been shown to improve vision, but most were limited by immune reactions, surgical complications, late-stage disease, or lack of an adequate RPE cell source (Stanzel and Holz, 2012).
Translocation of an autologous patch of RPE/choroid remains clinically the most popular approach, because some patients benefit from the procedure, despite its high complication rates (van Zeeburg et al., 2012).
With the development of RPE differentiation protocols from human embryonic stem cells (hESCs) and induced pluripotent stem cells (iPSCs) (Hirami et al., 2009; Klimanskaya et al., 2004), RPE transplantation has experienced a powerful renaissance, as scientists and clinicians envision an unlimited supply of RPE for transplantation.
However, much is still not understood with regard to the physiology of stem-cell-derived RPE (Liao et al., 2010) and transplantation into patients is in the early stages.
Pilot data from a phase I/II trial (NCT01226628 and NCT01344993) with a suspension of hESC-derived RPE injected in patients with dry AMD or Stargardt's disease suggest a favorable safety profile and some limited improvement in vision (Schwartz et al., 2012); further dose-escalation in this multicenter study is on-going.
This is encouraging, given that prior studies using RPE cell suspensions showed they failed to survive or function on aged submacular Bruch's membrane (Sugino et al., 2011) and are more likely to be rejected than are RPE monolayers (Diniz et al., 2013).
A cultured human RPE monolayer that exhibits the physiology of its native counterpart could be a valuable alternative to an RPE-cell suspension.
This type of culture has been readily attained using fetal- or pluripotent-stem-cell-derived RPE.
However, establishing such cultures from adult RPE has proven difficult and inconsistent, due to its propensity to undergo epithelial-mesenchymal transition (reviewed in Burke, 2008).
We have optimized culture conditions that robustly activate a subpopulation of adult human RPE stem cells (RPESC), expand, and then differentiate them into highly pure RPE monolayers that exhibit physiological features of native RPE (Blenkinsop et al., 2013; Salero et al., 2012).
This protocol allows us to explore the potential of adult RPESC-derived RPE for cell-replacement therapy.
To date, we do not know which cell source will turn out to be therapeutically successful, and therefore, testing all potential candidates is important.
Using a cell source derived from the adult human RPE may possess several potential advantages, such as fewer ethical concerns compared to hESC and fetal human RPE (hRPE), the possibility of routine histocompatibility leukocyte antigen matching or even autologous transplantation (using a patient's own remaining healthy RPESCs) to minimize immunosuppression, reduced proliferative potential than hESCs or human iPSCs and therefore reduced tumorigenesis risk, and reduced threat of generating abnormal cell types.
RPE monolayers grown on cell carriers would facilitate surgical handling and long-term functionality by substituting some or all of the functions of the aged Bruch's membrane (Binder et al., 2007).
Coimplantation of differentiated RPE monolayers on a substrate has been attempted in animal models only in a few instances and with limited success (Bhatt et al., 1994; Diniz et al., 2013; Nicolini et al., 2000).
Improvements would involve employing a biocompatible matrix that exhibits minimal deformation after transplantation, longer-term assessment postsurgery, and use of a large-eyed animal model for better assessment of surgical technique.
We have previously reported on a method and instrumentation to deliver ultrathin rigid-elastic cell carriers (polyester [PET]) into the subretinal space (SRS) of rabbits (Stanzel et al., 2012).
Here, we demonstrate this technology can be used to deliver monolayers of human RPE on permeable polyester carriers into the SRS of the rabbit.
Notably, we find RPE isolated from adult cadaver donors can expand 20-fold and survive as a polarized RPE monolayer for 1 month after transplantation, therefore representing a clinically relevant RPE cell source.
Transplants were followed with state-of-the-art ophthalmic imaging technology, including spectral domain optical coherence tomography (SD-OCT), confocal scanning-laser ophthalmoscopy (cSLO), color funduscopic photography, and histology.
In addition, the influence of local and systemic immunosuppression on retinal tissue alterations following xenografting was evaluated.
Results
Preparation and Characterization of the RPE-Carrier Monolayers
Adult human RPESC (hRPESC)-derived RPE and fetal hRPE monolayers were compared for their growth characteristics on the carrier PET material.
Fetal hRPE cells seeded at 2 × 105 cells/cm2 on PET inserts formed uniform, hexagonal monolayers by 2 weeks postconfluence, which repigmented by 6-8 weeks (Figure 1A).
Cultures were used for transplantation at 2 to 3 months postconfluence, when transepithelial electrical resistance (TER) values ranged from 737-1415 Ω∗cm2, mean 1244 ± 161 Ω∗cm2 (n = 60), and 514-776 Ω∗cm2, mean 657 ± 60 Ω∗cm2 (n = 60), from two different donor cultures (Figure 1C).
Fetal RPE cultured in this manner shows morphology and gene/protein expression similar to native RPE, thereby serving as a reference for adult-stem-cell-derived RPE (Liao et al., 2010).
About 5 × 106 adult hRPEs were typically isolated per donor.
During the first month, the cultures went through 2-4 population doublings, thus expanding to approximately 2.5 × 107.
We then seeded adult hRPE onto PET inserts at 1 × 105 cells/cm2 until they formed uniform, hexagonal monolayers 4 weeks postconfluence, expanding another 2-4 more times during this passage.
As a result, after 3 months, the adult RPESCs and their progeny had expanded 20-fold, and from one donor, we obtained approximately 1 × 108 RPEs (Figure 1B).
We monitored adult hRPE characteristics during the culture period by measuring TER, gene expression, and polarized protein localization immunohistochemically.
After 2 months on the PET substrate, when the adult hRPE monolayers exhibited a uniform, polygonal shape, TER was measured regularly to confirm development of tight junctions (Figure 1 and Figure S1 available online) and were found to range between 210-339 Ω∗cm2 at 8 weeks after plating, with a mean of 308 ± 18.7 Ω∗cm2 (n = 12) and mean 240 ± 24.9 Ω∗cm2 (n = 12) from respective donor cultures.
Gene-expression profiling was conducted using RNA extracted from adult hRPE tissue at the time of dissection (referred to as native) versus RNA extracted from adult hRPE after 2 months on PET (Figure 1E; n = 5 donors).
Cultured adult hRPE exhibited largely similar but sometimes higher expression of RPE markers compared to the native tissue.
To determine purity and polarization of the adult hRPE cultures, we performed immunostaining using characteristic RPE markers, assessed by confocal imaging (Figure 1F).
Claudin-19 was present along the apical-lateral membrane along with the tight-junction complex protein ZO-1, indicating the existence of a functional epithelial barrier (Peng et al., 2011).
Ezrin, a membrane-associated protein involved in cytoskeletal organization, was found preferentially in RPE microvilli (Bonilha et al., 1999).
The visual cycle proteins cellular retinaldehyde-binding protein (CRALBP) and RPE65 (Bunt-Milam and Saari, 1983; Redmond et al., 1998) were localized in the cytoplasm, as expected.
Monocarboxylate transporter 1 (MCT1) was present apically (Philp et al., 1998).
The appropriate localization of these proteins, combined with the gene-expression pattern and TER measurements, were similar to native RPE, demonstrating appropriate physiology of the adult hRPE monolayers to be used for transplantation.
Cultured hRPE Xenografted into Rabbit SRS
hRPE cultures grown on PET membranes were trephined to generate bullet-shaped implants approximately 1.1 mm × 2.2 mm (∼6-8000 cells), as described previously (Stanzel et al., 2012).
Subretinal implantation of 45 such constructs was evaluated, which included 40 fetal and five adult human RPE monolayer implants (Table 1).
The RPE monolayer transplant was placed cell-carrier down on intact host RPE within the SRS (Figures 2 and 3; Movie S1).
In addition, controls with bleb retinal detachments alone (bRD) (n = 19) and PET carrier-only implants (n = 7) were performed to differentiate surgical trauma from biological effects (Figure S2; Movie S2).
On the first SD-OCT follow-up (at 3-5 days post-OP), the neural retina above the fetal and adult hRPE implant showed an increased overall thickness, with loss of typical retinal reflectance layering, as well as slowed clearance of subretinal fluid from the bRD (Figure 2, B1 and D1, respectively).
Xenografted adult hRPE on PET followed similar, albeit slightly delayed, patterns as seen with the fetal transplants.
Notably, however, the retina was typically adherent to both hRPE implant variants on the apical surface, and the implant plus carrier appeared apposed to the host RPE layer on the basal surface.
The choroidal vascular layer underneath both hRPE implant types appeared to have less reflectivity and slightly increased volume compared to adjacent regions on SD-OCT at all time points.
After 1 week, an apparent retinal tissue loss over the implant center was discernible on SD-OCT (Figure 2).
The findings were well correlated with color fundus photography.
Following the initial retinal thinning seen after the first week, the retina remained stable for the duration of the follow-up examinations (Figure 2; Movies S3 and S4).
On infrared cSLO, a distinct reflectivity halo appeared to surround the PET alone as well as the hRPE implants, which correlated with a loss of outer retinal SD-OCT bands (Figure 2A).
The halo diameter was smaller than the original size of the bRD.
PET-only implants produced a smaller halo than hRPE xenografts.
We asked whether encapsulating the graft in additional materials would help the surgical delivery and potentially reduce the retinal thinning.
Temporary graft encapsulation with thermosensitive gelatin and/or plasmin-assisted vitrectomy, however, was found to cause more aggressive retinal destruction and choroidal engorgement on SD-OCT compared to unaided implantation of fetal hRPE/PET implants (Figure S2).
In contrast, we found that all these negative effects were ameliorated through 1 to 2 mg intravitreal triamcinolone (TCA) injection at the end of the surgery.
This long-acting synthetic corticosteroid is routinely given intraocularly to reduce immune responses.
Ophthalmic and systemic complications related to the implantation procedure are summarized in Table 2.
Characterization of hRPE Xenografts by Immunolabeling and Transmission Electron Microscopy
Animals were sacrificed and perfusion-fixed at 4 weeks posttransplantation and the recovered grafts then sectioned.
Microscopic inspection revealed that the grafts had been maintained as a largely intact and continuous cell monolayer.
A positive pan-cytokeratin (pCK) signal, an established RPE marker, was confirmed for fetal and adult hRPE monolayers (Figures 3A and 3C).
Additional pCK reactivity was seen "underneath" the cell carrier, likely from host RPE.
Moreover, fetal and adult hRPE on PET carriers stained positively for the human-specific marker SC121, confirming survival of human RPE for 1 month as a monolayer (Figure 3).
Costaining of SC121 with an antibody to MCT1 and Ezrin, both apical membrane markers, further confirmed that the RPE were (still) polarized (Figure 3).
SC121+ RPE transplants were negative for the expression of the cell-cycle marker ki67, the proliferation marker phosphohistone H3, and for the apoptotic marker caspase-3 (Figures 3F-3H), indicating absence of proliferation and apoptosis.
We estimated the total human RPE cell survival to be approximately 95% after 1 month by using the SC121 positivity; we measured the total length of the carrier and the length of SC121 stain and calculated the percent coverage of SC121 over the total carrier length.
On transmission electron micrography (TEM), polarized RPE cells were observed on the PET carriers from both fetal and adult transplants (Figures 3I and 3J).
These results confirm survival of polarized human RPE from fetal and adult donors xenografted into rabbit SRS over 4 weeks.
Effect of Systemic Immunosuppression on Fetal RPE Transplant and Retinal Integrity
We conducted a consecutive series of transplants with preoperative, systemically immunosuppressed animals, aiming to improve preservation of the neural retina above the fetal RPE monolayer xenograft.
Systemic dexamethasone (DXP) immunosuppression (2.5-3 mg/kg/day intramuscularly) in 14 rabbits resulted in inconsistent maintenance of the inner retinal layers (3 of 14), yet always leading to an atrophy of the outer photoreceptor cell layer above the hRPE implant (Figure 4; Movie S5).
The surviving RPE cells appeared pleiomorphic and scattered into patches on the PET carrier.
By contrast, animals that did not receive systemic immunosuppression, but were treated with TCA injection, had a complete disarray and/or absence of all retinal layers above the implant, but the hRPE monolayer was much better preserved.
Remarkably, on SD-OCT, near-normal choroidal reflectivity patterns were seen underneath the fetal hRPE implant as early as 1 week postimplantation in most (9 of 14) DXP-suppressed animals (Figure 4G), similar to areas with normal retina or PET carrier-only implants.
Nonimmunosuppressed rabbits showed a distinctly diminished reflectivity and locally increased volume within the choroid underneath the fetal hRPE grafts, with no apparent histomorphologic correlate at 4 weeks postimplantation for these choroidal changes on SD-OCT.
Perhaps mediated through suppression of all peripheral blood leukocytes (Jeklova et al., 2008), systemic DXP seemed to effectively dampen choroidal immune reactions underneath fetal hRPE grafts.
See Table 2 for DXP-related complications.
In summary, systemic immunosuppression with DXP resulted in poor survival of fetal hRPE transplants, whereas local administration of TCA intravitreally improved surgical outcome.
Discussion
The advances in hESC, iPSC, and RPESC technology have opened new opportunities to test whether transplantation of RPE into the SRS of patients with RPE-related diseases might restore some of the lost RPE function, ultimately leading to preservation or restoration of vision.
One major unknown is whether these RPE cells can survive as a monolayer in the SRS and maintain their identity.
Here, we demonstrate that polarized human fetal and adult RPE monolayers cultured on a polyester matrix can survive grafting into the rabbit SRS, thus resolving many of the current roadblocks concerning clinical RPE monolayer transplantation.
RPESCs isolated from elderly donor eyes were expanded and successfully differentiated to generate approximately 1 × 108 RPE per donor.
Considering 5 × 104 or fewer RPE cells are required to cover the macula, using adult hRPE from one donor has the potential to treat many hundreds of patients.
While promising, the expansion potential of adult hRPE is limited and the generation of multiple banks would be required to enable treatment of the millions of patients suffering from eye diseases such as AMD.
One potential alternative to adult hRPE thought to have an unlimited expansion potential is hESC- and iPSC-derived RPE.
However, a recent study shows changes in the physiology of iPSC-RPE after serial passaging (Singh et al., 2013); therefore, the difference between these lines and adult hRPE with regard to their expansion capabilities may not be significant.
The adult RPESC-derived RPE cultures were shipped live from the US to the German surgical team.
Despite the transportation stress, the cultures soon exhibited morphology and TER values comparable to preshipment levels.
Successful shipment of cultured implants between clinical-grade cell production to distant surgical sites should therefore be possible, hinting at the potential rapid spread of therapy adoption.
Very little is known regarding optimal conditions for RPE implant shipment, and our studies have established conditions for successful cultured RPE monolayer transport.
Here, we show that, similar to fetal hRPE (Liao et al., 2010), adult hRPE cultures can exhibit characteristics similar to native adult human RPE.
Cultured adult hRPE had TER measurements that were actually closer to native adult RPE than cultured fetal hRPE.
These two RPE culture systems were developed independently in separate laboratories and as such used slightly different culture methods.
More detailed comparison is warranted between human native fetal and adult tissues and RPE cultures made from these cells in order to determine the optimal cells for a particular target patient population.
After implantation into the rabbit eye, both fetal and adult RPE grown on a biocompatible, permeable polyester matrix survived and maintained key properties.
Moreover, we observed no occurrence of scarring on the retinal surface (due to proliferative vitreoretinopathy) or graft proliferation.
The fact that both fetal and adult hRPE behaved similarly suggests these two distinct RPE populations share comparable features with regard to transplantation.
We tested whether gelatin and plasmin could facilitate implant placement in the SRS but found survival of the RPE cells adversely affected.
Gelatin and plasmin have been reported to be proinflammatory (Huang et al., 1998; Lai, 2009; Verstraeten et al., 1993), which may explain the reduced and patchy RPE coverage on PET carriers we observed in these cases at 4 weeks postimplantation (data not shown).
Remarkably, fetal and adult hRPE monolayers survived without systemic immunosuppression, which agrees with a previous fetal hRPE carrier transplant study (Bhatt et al., 1994).
Our study now extends these findings to adult hRPE.
Importantly, we demonstrate that adult hRPESC-derived RPE monolayers maintained cell-polarity markers up to 4 weeks postgrafting.
A significant body of evidence now suggests that the RPE actively modulates the immune system via cell-surface and secretory mechanisms; that is, it has qualities of an immune-privileged tissue (Sugita, 2009).
Like other epithelial cells, RPEs in single-cell suspension lose their polarized cytoskeletal and membrane-protein distribution, likely becoming more susceptible to immune rejection.
Consistent with this, single-cell suspensions of RPE do not survive when transplanted underneath the kidney capsule, whereas intact RPE sheets do (Wenkel and Streilein, 2000).
Suspensions of human fetal RPE transplanted into the rabbit SRS showed cellular infiltration with subsequent destruction (Gabrielian et al., 1999).
In contrast, unsupported, cultured patches of fetal human RPE transplanted into albino rabbits seemed to survive 1 month, after which they showed signs of rejection (Sheng et al., 1995).
Hence, it appears that maintaining RPE intercellular connectivity, i.e., as monolayers, is beneficial to prevent rejection.
We therefore speculate that survival of these fetal and adult RPE xenografts in rabbits is promoted by their maintenance as a polarized monolayer.
It is possible that xenograft survival in our model was also facilitated due to implant placement above an intact host RPE, because the SRS itself has also been considered an immune privileged site (Jiang et al., 1994).
Destruction of the RPE by sodium iodate abolishes such immune privilege (Wenkel and Streilein, 1998).
However, we note that, in the study by Bhatt et al. (1994) in which fetal hRPE cell monolayers survived 6 weeks, they removed the host RPE prior to transplantation, indicating that graft survival was not due to the host RPE layer creating an immune- privileged SRS.
We found that systemic DXP immunosuppression was less successful than local administration of TCA but view the translational value of these findings cautiously.
Little is known about differences in immune rejection between RPE xenografts, as done here, and allografts as contemplated in patients.
To our knowledge, there have been no direct comparisons of these two scenarios, and we note that species and animal strain differences may also be relevant.
Attempts to compare the immune responses in the SRS with other xenograft models may not be appropriate because of the unique immunologic environment underneath the retina.
In RPE allografts, major histocompatibility complex II expression by the RPE (and its subsequent recognition by T cells; Zhang and Bok, 1998), along with cytokine production (interleukin [IL]-6 and interferon γ) have been implicated in rejection (Enzmann et al., 2000).
In contrast, RPE xenografts in rat or rabbit elicit macrophage infiltration and subsequent destruction (Carr et al., 2009; Gabrielian et al., 1999).
Cytokines (IL-1 and IL-6) have also been implicated in such a reaction (Abe et al., 1999).
(Thymic) lymphocyte infiltration has not been demonstrated in more than 20 published studies on RPE xenografts, yet both cyclosporine and tacrolimus, which are thought to suppress T-cell-mediated rejection, showed some efficacy in delaying xenograft rejection in rabbits (Lai et al., 2000; Wang et al., 2002).
Cyclosporine has erratic plasma levels (thus requiring frequent serum measurements) and does not prevent allo- or xenograft rejection of an RPE (suspension) in rats or rabbits (Carr et al., 2009; Lai et al., 2000), thus questioning its usefulness.
Tacrolimus has similar mechanisms of action but greater likelihood of toxic side effects.
In an attempt to ameliorate negative retinal changes, we chose to administer DXP, as it predictably suppresses all peripheral leukocyte lineages in the rabbit (Jeklova et al., 2008).
The adverse effect of DXP that we observed on xenograft survival was unexpected but perhaps mediated through the RPE glucocorticoid receptor, which promotes RPE proliferation (He et al., 1994).
Taken together, no ideal, evidence-based immunosuppression protocol exists at present for hRPE xenograft models, but our data indicate that intravitreal TCA allows hRPE survival for at least 1 month, indicating that it could be valuable for longer-term studies.
Carrier implantation into the SRS with or without hRPE resulted in a degenerated retina immediately above the implant.
The cause is not clear and might be multifactorial.
Despite reports to the contrary (Szurman et al., 2006), we believe that the bRD is unlikely to cause retinal degeneration, because we found that producing a similar bleb without subsequent insertion of a carrier results in minimal damage to photoreceptors and negligible retinal degeneration, a finding supported by others (Ivert et al., 2002).
The rabbit retina is merangiotic, meaning only part of the inner retina is supplied by retinal vessels and is therefore more dependent on the choriocapillaris for all metabolic needs compared to holangiotic species that possess a retinal vasculature that penetrates throughout the inner retina, i.e., humans and rats.
Because we placed a 10-μm-thick device between the photoreceptors and the RPE, we disrupted this normally intimidate intercellular relationship with a barrier, which likely led secondarily to the observed outer retinal degeneration.
Similar observations have been made using retinal chip implants, where impermeable variants of the implants result in atrophy of photoreceptor cell layers (Montezuma et al., 2006).
Porosity of cell carriers seems to play a crucial role in maintaining neural retinal health and layering, as implantation of an acellular PET carrier with 3.0 μm diameter pores results in significantly better preservation of outer retinal layers compared to a PET carrier with smaller, 0.4 μm diameter pores (B.V.S., Z.L., R.B., and F.G.H., unpublished data).
However, other than a small study by Lu et al. (2012), carrier membranes with optimal porosity for retinal preservation and permissive to RPE culture were not systematically evaluated to our knowledge.
hRPE on PET carriers xenografted into the SRS resulted in initial retinal edema followed by a dramatic atrophy of all overlying retinal layers within 1 week.
Such a pattern was clearly different from carrier-only controls, where only the outer retinal layers degenerated.
RPE-transplant-induced retinal "melting" (Del Priore et al., 2001; Rezai et al., 2000) or photoreceptor destruction (Diniz et al., 2013; Zhang and Bok, 1998) has been observed by others and remains an unsolved problem in RPE xenotransplantation (da Cruz et al., 2007).
Our work indicates that, rather than surgical trauma, it is the configuration of the subretinal implants themselves which cause the observed retinal degeneration.
Optimization of RPE cell carriers to better mimic physiologic Bruch's membrane is an important subject and is currently being explored by us and many others (Liu et al., 2013; Hynes and Lavik, 2010; Kearns et al., 2012; Subrizi et al., 2012).
Reducing such retinal degeneration after subretinal hRPE engraftment in rabbit SRS will improve the use of the rabbit as a cost-effective, large-eyed animal model.
Future work will focus on identifying an appropriate cell carrier that more closely mimics the precious relationship between the outer retinal layer and the choriocapillaris.
Experimental Procedures
RPE Cultures
Fetal RPE
Permission to work with human RPE was obtained from the Ethics Committee of the University of Bonn.
Two pairs of fetal human eyes at 19th and 20th week of gestation were obtained from Advanced Bioscience Resources.
The tissues were transported in CO2-independent media (Invitrogen) supplemented with 5% normal calf serum and 100 IU penicillin/100 μg streptomycin on ice and were processed within 48 hr postenucleation.
RPE cells were isolated, expanded in low calcium media, and subsequently differentiated on uncoated 10-μm-thick polyester Transwell (PET) inserts (Corning Life Sciences catalog number 3470) according to a prior protocol (Hu and Bok, 2001).
Adult RPE
The method for adult RPE culture followed our prior publication (Blenkinsop et al., 2013).
Briefly, cadaver donor globes within 36 hr postmortem were dissected, the vitreous and retina removed, and the posterior eye-cup rinsed with calcium- and magnesium-free PBS, and then incubated in 1% dispase (2.4 IU/ml; Sigma-Aldrich catalog number D4818) for 45 min at 37°C.
Sheets of RPE were gently scraped off Bruch's membrane and then layered onto Dulbecco's modified Eagle's medium (DMEM)/F12 supplemented with 10% fetal bovine serum (FBS) and with 10% sucrose.
After 10 min, the lower fraction containing RPE sheets was collected, centrifuged at 285 g for 5 min, resuspended in modified DMEM/F12 (see Table S1) supplemented with 15% FBS, and plated to cover roughly 50% of the well coated with placental extracellular matrix (BD Biosciences).
The culture medium was changed every 2 to 3 days, with a gradual decrease in FBS from 15% down to 2% by 2 weeks.
RPE sheets attached and grew to confluence within 2 weeks and were then passaged, plated into coated PET inserts (described above), and cultured for another 2 months, changing the medium two to three times weekly.
Transepithelial Electrical Resistance Measurements
Followed standardized methods described in the Supplemental Experimental Procedures.
Quantitative PCR
Followed standardized methods described in the Supplemental Experimental Procedures.
Preparation of Implants
Adult hRPE cells isolated from two donors (72-year-old and 71-year-old females) cultured on PET inserts were shipped on wet ice with a commercial courier (http://www.biocair.com) from Rensselaer, NY to Bonn, Germany within 24 hr.
The cultures were left to recover in the incubator at 37°C, 5% CO2 for 1 week until transplantation.
The TER values immediately prior to implantation were around 220 and 315 Ω∗cm2 for the respective donor cultures.
Acellular and RPE-seeded implants were trephined from above PET membranes using a custom-made, bullet-shaped trephine, as described previously (Stanzel et al., 2012).
RPE on the cut carriers were rinsed three times with calcium- and magnesium-containing Hank's balanced salt solution (HBSS) immediately prior to implantation.
The implant dimensions were approximately 2.2 × 1.1 mm, accommodating approximately 6-8000 cells.
Prior to implantation, some membranes with and without cultured RPE were encapsulated in sterile filtered 15% porcine gelatin, bloom index 100 (Sigma-Aldrich catalog number G6144).
Rabbit Transplantation
Sixty female chinchilla bastard rabbits weighing 2-2.5 kg (Charles River Laboratories) were utilized in experiments, which are summarized in Table 1.
All procedures were approved by the state regulatory authorities of North Rhine-Westphalia (LANUV 84-02.04.2011.A130).
Animals were kept in a specialized facility with temperatures between 20 and 25°C and exposure to regular daylight, in standardized individual cages with free access to food and water.
The surgical technique was further refined from our previously published protocol (Stanzel et al., 2012; Movie S1).
In brief, rabbits were anesthetized by intramuscular injection of 65 mg/kg ketamine and 5 mg/kg xylazine and pupils dilated with 2.5% phenylephrine and 1% tropicamide eye drops.
Eleven of 60 rabbits received an intravitreal injection of 1 U homologous rabbit plasmin in HBSS 1 hr prior to starting the procedure.
Following partial surgical removal of the vitreous (two-port core-vitrectomy), a small bRD was gently raised with 25-30 μl HBSS via a 41G Teflon cannula (DORC catalog number 1270.EXT) and Hamilton syringe, thereby expanding the SRS for surgical maneuvers.
Intraocular pressure was set for 30 mmHg and was consistently associated with a facile bRD induction, as compared to 20 or >50 mmHg.
The implant was then passed with the custom delivery instrument from the vitreous cavity through an enlarged incision in the retina into the SRS.
RPE monolayer transplants were placed cell-carrier-side down on intact host RPE, so that the xenografted RPE faced the photoreceptors (Movie S1).
Only the right eye was used for experimentation, and one implant was placed per eye.
Immunosuppression
Local
Most rabbits received an intravitreal injection of 1 to 2 mg preservative-free TCA at the end of the surgical procedure (see Table 1).
To control wound healing at the ocular surface, dexamethasone 1 mg/g, neomycin sulfate 3,500 IU/g, polymyxin B sulfate 6,000 IU/g ointment (Isoptomax, Alcon Pharma) was applied twice daily for 1 week postoperative onto the ocular surface.
Systemic
Immunosuppression was induced in 14 rabbits with intramuscular injection of DXP (Dexa-ratiopharm, Ratiopharm) over 2 days, three times/day, every 6 hr, 2.5-3 mg/kg, modified from a prior study (Jeklova et al., 2008).
On the day of surgery, the animals were injected twice, 12 hr apart, with 2.5-3 mg/kg DXP, followed by a once daily maintenance dose of 2.5-3 mg/kg DXP in the morning until sacrifice.
Animals were weighed regularly postimplantation and inspected by a veterinarian when necessary.
In Vivo Follow-Up
Rabbits had repetitive noninvasive retinal imaging performed at post-OP days 4, 7, 14, and 28.
Anesthesia and pupil dilation were performed as described above.
The cornea was frequently lubricated with artificial tears (Optive, Allergan) to maintain its optical clarity throughout imaging.
Spectral Domain Optical Coherence Tomography and Confocal Scanning Laser Ophthalmoscopy
The Spectralis HRA/OCT device (Heidelberg Engineering) was used to acquire laser-interferometric reflectance OCT images of the retina and choroid, with resolutions comparable to a light microscopic section.
Longitudinal and transversal line OCT scans through the center of the implant were taken with the 30-degrees-of-visual-field setting of the Spectralis.
Volume scans were obtained with 60 μm distance between each scan with the device set to 20 × 20 degrees of visual field centered on the implant.
To approximate human optical parameters to rabbit eyes, the Spectralis' corneal curvature settings were set by default to 4.2 mm.
Red-free and infrared cSLO images were taken at 30 degrees of visual field in the HRA mode of the device.
Color Fundus Photographs
A Zeiss FF 450IR camera set to 30 degrees of visual field was used to obtain color photographs to document funduscopic changes around the implant site.
Histology
Perfusion Fixation and Sectioning
Rabbits were sacrificed 4 weeks after implantation in deep intramuscular anesthesia with an intracardial injection of T61.
The animals were desanguinated and then perfusion-fixed with 2% glutaraldehyde (GA) or 4% formaldehyde (FA) both diluted in 0.1 M phosphate buffer.
The enucleated eyes were then immersion-fixed overnight in the same fixative.
Anterior segments were cut away with Vannas scissors and the eyecups photographed under a binocular microscope (Zeiss OPMI 1) with a 5-megapixel smartphone digital camera (iPhone 4, Apple).
Full-thickness samples (retina-sclera) of the implantation site were cut with a surgical blade (Feather No.
22).
GA probes were embedded in Spurr's resin (Sigma, EM0300-1KT) and 1 to 2 μm semithin sections cut and then stained with toluidine blue.
FA-fixed materials in paraffin were cut into 5 μm sections and stained with hematoxylin/eosin or Mayer's hematoxylin alone (if combined with immunohistochemistry, see below).
Light micrographs were taken on an Olympus BX50 microscope equipped with a Nikon DS-Vi1 digital camera.
Transmission Electron Microscopy
Ultrathin sections from both implant types (Spurr-embedded, see above) were analyzed using a Philips CM 10 electron microscope (Philips).
Images were taken with Megaview3 CCD digital camera and coupled with digital image software analysis (Olympus).
Immunolabeling
Transwell Membranes
Transwell membranes were processed for immunocytochemistry against Claudin-19 (1:100; R&D Systems), ZO-1 (1:100; Invitrogen), Ezrin (1:100; Cell Signaling Technology), MCT-1 (1:100; Sigma), CRALBP (1:100; Abcam), RPE65 (1:100; a gift from Dr. T.
Michael Redmond, National Institutes of Health [NIH]), and nuclear stain DAPI (1:10,000; Invitrogen).
Secondary antibodies used at 1:1,000 dilutions were Alexa Fluor 488-goat anti-mouse IgG2A, Alexa Fluor 546-goat anti-rabbit immunoglobulin G (IgG) (H+L), Alexa Fluor 488-goat anti-mouse IgG1, and Alexa Fluor 546-goat anti-mouse IgG (H+L) (Invitrogen).
Transwell membranes were fixed with ice-cold 4% FA solution for 15 min and then blocked and permeabilized with a solution containing 0.1% saponin, 5% normal goat serum, and 1% bovine serum albumin in phosphate-buffered saline.
Controls included incubations of secondary antibodies in the absence of primary antibodies and block solutions in the absence of primary/secondary antibodies.
Fluorescent images were collected using a Leica TCS SP5 confocal microscope.
Paraffin Sections
Some paraffin sections obtained from implant regions were processed for immunocytochemistry against pan-cytokeratin (1:100; catalog number ab11213, Abcam), ezrin (1:50; CST), MCT-1 (1:50; LifeSpan Biosciences), Ki67 (1:50; Sigma), phosphohistone H3 (1:50; Millipore), caspase-3 (1:50, Promega), and human-specific antibody SC121 (1:50; Stem Cells) to verify the presence of human RPE cells on the subretinally implanted cell carriers.
Pan-cytokeratin was visualized with the Biotin-ExtrAvidin-AEC chromogen system.
All other primary antibodies were visualized using Alexa Fluor secondary antibodies as mentioned above.
Acknowledgments
This work was supported by BONFOR/Gerok Scholarships O-137.0015 (to B.V.S.), Ruediger Foundation grants (to B.V.S.), State Scholarship Fund/Chinese Scholarship Council 2008627116 (to Z.L.), NIH - National Eye Institute 5R01EY022079-02 (to S.T.), and NYSDOH contract C028504 (to S.T. and J.H.S.) supported by the Empire State Stem Cell Fund.
Opinions expressed here are solely those of the authors and do not necessarily reflect those of the National Institutes of Health, Empire State Stem Cell Board, the New York State Department of Health, or the state of New York.
A European patent application on the shooter instrument was submitted by B.V.S., Z.L., R.B., N.E. and F.G.H.
(PCT/EP2012/058083).
S.T. and J.H.S. have a granted US patent on aRPESC (application number 12/428,456).
This paper was presented in part at the 20th ISER meeting in July 2012 and the 3rd TERMIS World congress in September 2012.
C.
Strack assisted with histologic processing, and J. Bedorf at the Institute of Pathology, University of Bonn was very helpful in obtaining transmission electron micrographs.
Dr.
Andrea Lohmer of HET facility, University Clinics Bonn, Bonn, Germany is gratefully acknowledged for her veterinarian services.
N.
Braun of Geuder AG Heidelberg, Germany provided support for instrumentation used in rabbits.
We thank Carol Charniga for invaluable help with RPE dissections and culture maintenance.
Supplemental Information
Supplemental Information includes Supplemental Experimental Procedures, three figures, one table, and five movies and can be found with this article online at http://dx.doi.org/10.1016/j.stemcr.2013.11.005.
Supplemental Information
Document S1.
Supplemental Experimental Procedures, Figures S1-S3, and Table S1Movie S1.
Full Implantation Movie with Fetal hRPE/PET Implant
(1) An enlarged opening in the retina (retinotomy) is created with vertical scissors.
(2) The implant is inserted with a custom-made instrument from epiretinal underneath the retina.
(3) The implant is pushed further into the subretinal space with half-closed branches of 23G scissors.
(4) Final positioning of the graft is achieved with a 41G Teflon-cannula.
Movie S2.
SD-OCT Volume Scan and Three-Dimensional Topographic Retinal Surface View of PET-Only Implant from Figure S1Movie S3.
SD-OCT Volume Scan and Three-Dimensional Topographic Retinal Surface View of Representative Fetal hRPE Implant from Figure 2Movie S4.
SD-OCT Volume Scan and Three-Dimensional Topographic Retinal Surface View of Representative Adult hRPE Implant from Figure 2Movie S5.
SD-OCT Volume Scan and Three-Dimensional Topographic Retinal Surface View of Representative Fetal hRPE Implant in DXP-Immunosuppressed Rabbit from Figure 4

Performance improvement of lithium ion batteries using magnetite-graphene nanocomposite anode materials synthesized by a microwave-assisted method

Graphene oxide (GO) was prepared from graphite powder by the Staudenmaier method [24]. First, sulfuric acid and nitric acid were mixed well by stirring 15 min in an ice bath, and then graphite powder was dispersed into the solution. After 15 min, potassium chlorate was added into the system - very slowly to prevent strong reaction during the oxidation process. After reacting for 96 h, the mixture was diluted by de-ionized (DI) water and then filtered. The GO was next washed by hydrochloric acid to remove sulfate ions from the solution, which could be detected by adding BaCl2 in solution. After that, the mixture was repeatedly rinsed with DI water until its pH value became neutral. Finally, before the GO was ground into a powder in an agate mortar, the GO solution was filtered and dried in an oven at 80 degC overnight.

The GO powder was dispersed in DI water (1.5 mg/ml) by ultrasonication. The solution was mixed with Fe(NO3)3, urea, and ethylene glycol (EG) to form a homogeneous suspension. Next, the mixture was heated with different microwave powers - 200, 600, and 1000 W - under a reflux condition for 30 min. Before being dried overnight in the oven, the products were purified by washing repeatedly with DI water. Finally, Fe3O4/graphene nanocomposites were obtained by thermal reduction in an Ar atmosphere at 873 K for 4 h.
Vertical MgZnO Schottky ultraviolet photodetector with Al doped MgZnO transparent electrode
The MgZnO:Al thin film was grown on a commercial c-plane sapphire substrate by using radio frequency (RF) magnetron sputtering technique. A ceramic target with 45 wt.% MgO, 54.81 wt.% ZnO and 0.19 wt.% Al2O3 was used. The ceramic target was made by the Northeast Normal University. The sputtering rate is about 5 nm/min and the chamber pressure was maintained at 1.8 Pa. Oxygen with 20 sccm flow rate and argon with 40 sccm flow rate were used as the sputtering gas. Growth temperature was 400 degC and the RF power was 90 W. The thickness of the as-grown film was 300 nm. Then the as-grown MgZnO:Al films were vacuum annealed for 1 h in different temperatures to reduce the resistivity of the MgZnO:Al thin film, in order to meet the requirement of the device.
Structural manipulation and tailoring of dielectric properties in SrTi1-xFexTaxO3 perovskites: Design of new lead free relaxors

Nominal compositions with stoichiometry SrTi1-2xFexTaxO3 (0.0 <= x <= 0.5) were prepared by solid state reaction of appropriate amounts of SrCO3, Fe2O3, Ta2O5 and TiO2. Homogenous mixtures of reactants were heated at 1173 K for 30 h in static air followed by regrinding and heating at 1473 K for 30 h in pellet form. The products were reground and pressed into pellets (10 mm diameter and 2 mm thickness) and sintered at 1623 K for 30 h. 
The Cassini Enceladus encounters 2005-2010 in the view of energetic electron measurements

Highlights
► Ramp-like features in electron intensities in the low-energy channels.
► High-energy depletions upstream for electrons with energies above the resonant energy.
► "Dust"-related peaks in the electron data correlated with data from other instrument onboard Cassini.
Abstract
The moon Enceladus, embedded in Saturn's radiation belts, is the main internal source of neutral and charged particles in the Kronian magnetosphere.
A plume of water ice molecules and dust released through geysers on the south polar region provides enough material to feed the E-ring and also the neutral torus of Saturn and the entire magnetosphere.
In the time period 2005-2010 the Cassini spacecraft flew close by the moon 14 times, sometimes as low as 25km above the surface and directly through the plume.
For the very first time measurements of plasma and energetic particles inside the plume and its immediate vicinity could be obtained.
In this work we summarize the results of energetic electron measurements in the energy range 27keV to 21MeV taken by the Low Energy Magnetospheric Measurement System (LEMMS), part of the Magnetospheric Imaging Instrument (MIMI) onboard Cassini in the vicinity of the moon in combination with measurements of the magnetometer instrument MAG and the Electron Spectrometer ELS of the plasma instrument CAPS onboard the spacecraft.
Features in the data can be interpreted as that the spacecraft was connected to the plume material along field lines well before entering the high density region of the plume.
Sharp absorption signatures as the result of losses of energetic electrons bouncing along those field lines, through the emitted gas and dust clouds, clearly depend on flyby geometry as well as on measured pitch angle/look direction of the instrument.
We found that the depletion signatures during some of the flybys show "ramp-like" features where only a partial depletion has been observed further away from the moon followed by nearly full absorption of electrons closer in.
We interpret this as partially/fully connected to the flux tube connecting the moon with Cassini.
During at least two of the flybys (with some evidence of one additional encounter) MIMI/LEMMS data are consistent with the presence of dust in energetic electron data when Cassini flew directly through the south polar plume.
In addition we found gradients in the magnetic field components which are frequently found to be associated with changes in the MIMI/LEMMS particles intensities.
This indicates that complex electron drifts in the vicinity of Enceladus could form forbidden regions for electrons which may appear as intensity drop-outs.

Introduction
Enceladus, one out of currently 62 satellites of Saturn, orbits the planet at a distance of 3.95 Saturn radii RS (1RS=60,268km) and is embedded in the radiation belts of Saturn's inner magnetosphere.
Even with a small radius REnc of only 252km and an apparent similarity to other icy moons, this moon is by far the most important internal source of dust, neutral gas and plasma in the saturnian system and especially in the magnetosphere.
As already inferred from data of the first flybys of the Pioneer 11 (1979), Voyager 1 (1980) and Voyager 2 (1981) spacecraft, respectively (Smith et al., 1981; Krimigis et al., 1982), this moon plays the same role Io does for the jovian system.
The Cassini spacecraft, in orbit around Saturn since July 2004, has flown by Enceladus 14 times between 2005 and 2010.
Data from Cassini instrument teams identified a plume of water geysers above the so called "tigerstripes" on the south pole of the moon (Dougherty et al., 2006; Porco et al., 2006).
This discovery is one of the most important findings to better understand the saturnian magnetosphere.
The material released in the form of water gas, ice molecules and dust (Waite et al., 2006; Spahn et al., 2006) from those geysers is the main source of the E-ring and the neutral torus and is the major plasma source of water group ions in the magnetosphere.
A summary of Enceladus findings from Cassini measurements is very well presented by Spencer et al. (2009).
The interaction between energetic electrons and the moon essentially is given by the individual motion of the particles.
Energetic charged particles bounce along the magnetic field lines between the north and south pole of Saturn and drift azimuthally at the same time.
A moon or any other object (ring material or ring arcs, plume material, neutral gas, dust, etc.) blocks and absorbs some of those bouncing particles, appearing as an absorption signature in the differential intensities of the measured particles in a given energy range.
The gaps in the distribution due to those losses are categorized as macrosignatures and microsignatures (Selesnick, 1993).
While the macrosignatures are seen at all local times and latitudes the microsignatures are highly dependent on the longitudinal distance between the observer (spacecraft) and the absorbing body (for a mathematical description see van Allen et al. (1980), Thomsen and van Allen (1980) and also Roussos et al. (2007)).
Particles are lost in that region of the magnetosphere when encountering the moon but refilled in time by particle sources or diffusion processes.
The depths of the absorption signature in the particle intensities can be used to determine the amount of material causing the loss of energetic particles as a function of longitudinal distance with respect to Enceladus if the loss processes are known and one of them involves interaction with gas or dust.
In addition to the bounce motion up and down along the field lines the charged particles magnetically drift perpendicular to the B→×∇B→ direction.
Since both B→ and ∇B→ are disturbed around Enceladus, the electron motion will also be disturbed, modifying to some extent the flux distribution in the moon's vicinity.
Therefore, particle fluxes may reflect integrated effects of the disturbed field environment around a moon, seen in the particles that drift across it.
Ions and electrons drift in opposite directions around the planet.
Cassini data analysis provided the opportunity to confirm earlier findings from previous missions.
Effects in the local particle distributions have been studied in the literature.
Paranicas et al. (2005) studied the first encounter E0 of the Cassini spacecraft with Enceladus.
The authors made it clear that upstream of the moon it is only expected to measure an absorption signature of MeV electrons.
Nothing should be seen in energy channels below the "resonance" energy (Thomsen and van Allen, 1980).
The presence of microsignatures in MeV electrons upstream of the moon therefore are a strong indicator of a fresh absorption caused by the moon itself.
Jones et al. (2006) investigated the effect of the plume on energetic particles in the magnetosphere and analyzed the response of charged particle measurements from the Low Energy Magnetospheric Measurement System (LEMMS) onboard Cassini.
Khurana et al. (2007) studied the magnetic measurements from the flybys E0-E2 in terms of mass-loading while Tokar et al. (2009) found in CAPS plasma data freshly-produced water-group ions and heavier water dimer ions HxO2+ during encounters E3 and E5.
Kempf et al. (2008) studied the dust particles in the E-ring close to Enceladus and found its vertical distribution to be 4300km or 17 REnc thick.
Jones et al. (2009) found that negatively and positively charged particles of nanometer scales are released through the vents in the south polar region.
Farrell et al. (2009) reported electron density dropouts near Enceladus during flyby E3 most probably caused by absorption by sub-millimeter size water ice grains in the plume.
Coates et al. (2010) reported negative ions in plume measurements of the Electron Spectrometer CAPS/ELS and Teolis et al. (2010) investigated the density and structure of Enceladus' south polar plume during E3 and E5 by using data of the Cassini ion neutral mass spectrometer INMS and indicated that 100kg of plume water vapor escapes every second and spreads out uniformly.
In addition they detected fine-grained ice concentrated in the jets of the multiple local sources along the tigerstripes.
Farrell et al. (2010) studied the modified plasma of Saturn's magnetosphere in the vicinity of Enceladus.
They pointed out that a cloud of dust exists around the moon extending at least 20 REnc both north and south of the equator, while Shafiq et al. (2011) reported dusty plasma measurements during encounter E3.
Besides the data analysis theoretical work, analytic modelling, and especially numerical simulations have been performed recently (Jia et al., 2010b,; Tenishev et al., 2010; Cassidy and Johnson, 2010; Kriegel et al., 2009; Omidi et al., 2010; Simon et al., 2011).
In those studies the environment of Enceladus has been simulated and parameters like the local gas distribution, the importance of dust near the moon and the implications for the magnetosphere have been determined.
Tseng and Ip (2011) predicted Enceladus to be an important source of Saturn's ring atmosphere and ionosphere theoretically, and suggested additional plasma and particle data to compare with their predictions.
Recently evidence of an Enceladus footprint in Saturn's auroral zone has been reported by Pryor et al. (2011).
Cravens et al. (2011) studied the Enceladus torus theoretically based on CAPS measurements and found two populations: a denser part near 4 REnc and a less dense and more radially extended part.
Finally Hartogh et al. (2011) inferred that the activity on Enceladus is even likely the source of water in the upper atmosphere of Saturn, underlining the importance of that moon for the saturnian system as a whole.
All these investigations show the importance of Enceladus studies to better explain the source- and transport processes in Saturn's magnetosphere.
In this paper we present an overview of energetic electron measurements (27keV to 21MeV) obtained by the Low Energy Magnetospheric Measurement System (LEMMS) onboard the Cassini spacecraft during the first 14 close encounters with Enceladus (E0-E13) between 2005 and 2010.
Instrumentation and data analysis
We use data taken by the Low Energy Magnetospheric Measurement System (LEMMS), part of the Magnetospheric Imaging Instrument (MIMI) onboard Cassini.
MIMI/LEMMS is able to measure the intensities, energy spectra, and pitch angle distributions of energetic charged ions and electrons separately in the energy range between about 20keV and several tens of MeV.
Particles are measured simultaneously from two opposite directions (low-energy telescope (LE) and high-energy telescope (HE)).
Low-energy electrons and ions measured in LE are separated by a strong internal magnet.
Electrons are bent towards the two different detectors E and F while ions are detected in detectors A and B.
The HE consists of a stack of five detectors.
The species and energy separation is performed by coincidence measurements between these detectors.
Data are recorded in rate channels.
The instrument is mounted on top of a movable turntable which rotates about the -y-axis of the spacecraft within 86s, nominally.
The -y-axis points in the direction of the remote sensing instruments (often pointed towards the planet or another object).
Under this configuration MIMI/LEMMS is therefore scanning in the x-z-plane of the spacecraft coordinate system allowing very good pitch angle coverage.
Unfortunately the turntable stopped rotating in the beginning of 2005 for unknown reasons after the release of the Huygens probe.
Therefore during all the Enceladus encounters MIMI/LEMMS was not rotating anymore.
Since then the low-energy telescope points at an angle of 77.45° away from the -z direction towards the -x-direction (see Fig. 1).
The advantage of that non-rotating mode is the time resolution of the instrument which is 16 times better than in the rotating mode (5.65s) for the rate channels or 0.66s for the priority channels instead of 86s for a given pointing/pitch angle).
This enables us to measure small-scale, short-lived features in the data set.
Incoming ions and electrons are separated through a strong internal magnet.
Semiconductor detector sets are positioned in different places inside the instrument enabling us to measure ions and electrons in a variety of energy channels.
The angular resolution is restricted by the field-of-view of the instrument's apertures and look direction (limitation in measured pitch angle) of the MIMI/LEMMS sensors.
In the calculation of the pitch angle it is assumed that all particles enter through the central entrance of the instrument's apertures with opening angles of 15° and 30° for low-energy and high-energy end, respectively.
This value has to be subtracted from 180° to get the measured particle pitch angle for the high-energy end.
The apertures have 7 (LE-end) and 19 (HE-end) entrance holes allowing particles to enter the instrument with a variety of angles relative to the central hole.
Good pitch angle coverage can only be achieved if the spacecraft itself is rotating.
This is often the case during the downlink periods when the spacecraft antenna of Cassini points towards Earth.
However, during the close moon encounters the spacecraft is not rotating and is often reoriented before and after closest approach so that the remote-sensing instruments (camera and spectrometers) have the moon in their field-of-view.
As a consequence the measurable pitch angle of the incoming charged particles also changes which makes the data more difficult to interpret.
MIMI/LEMMS also performs a Pulse Height Analysis (PHA) for the low-energy end separately for the detectors E and F (for electrons with energies of 6.7-2300keV) and for ion detector A (ions with energies of 12-833keV) enabling a good energy resolution of up to 256 channels.
As pointed out by Paranicas et al. (2005) and Roussos et al. (2007) each energy channel of the MIMI/LEMMS instrument measures the sum of a real signal and a background noise signal.
The most relevant background for this study is penetrating radiation affecting primarily the low-energy electron channels (C-channels).
To identify periods where the signal/noise-ratio is good enough checks are performed with data of the omni-directional channel G1 which is primarily counting MeV electrons only.
Furthermore if the time vs. intensity curves of electrons measured in the C-channels follow the curves of channel G1 then we do not consider the data in the C-channels to be real.
If the energy of the measured electrons is lower than the resonant energy for a given orbital distance from the planet an absorption feature should be observed only downstream of the moon as described above while the bite-out in the high-energy channels should only be observed upstream of the moon.
Therefore assuming to a first order that electrons are not deflected from magnetic field gradients and/or flow perturbations and the absorption signature appears on the "wrong" side of the moon simultaneous to the high-energy channels it could be an indication of "bad" data.
A complete description of the MIMI instrument can be found in Krimigis et al. (2004).
The latest MIMI/LEMMS energy channel passbands can be found in Krupp et al. (2009).
In this paper we used measurements only from a representative subset of available energy channels and concentrated on electrons only.
In order to avoid confusion between flyby number and channel number we put the flyby numbers in italic fonts.
For the interpretation of the energetic electron data we use measurements of the magnetometer MAG onboard the spacecraft (Dougherty et al., 2004) and we correlate the MIMI/LEMMS with plasma electron measurements of the Electron Spectrometer ELS, part of the Cassini Plasma Spectrometer Investigation CAPS (Young et al., 2004).
Observations
The times of all Enceladus encounters between 2005 and 2010, closest approach distances and type of encounter are summarized in Table 1.
We subdivided the encounters into flybys parallel to the equatorial plane ("eq north" and "eq south") and high latitude flybys ("hl").
As pointed out above, high-energy ions are absent from regions magnetically connected to the orbit of the moon around the planet (Enceladus macrosignature).
Therefore we concentrate on MIMI/LEMMS measurements of electrons with different energies and with two different pitch angles 180° apart from each other given by the look directions of the low-energy- and high-energy end of the MIMI/LEMMS instrument relative to the local magnetic field.
In addition we derive the dust ram angle of incoming dust particles on Keplerian orbits relative to the MIMI/LEMMS look direction where the motion of the spacecraft is taken into account (0° means that dust is moving coaligned to the instrument's symmetry axis).
Finally these overview plots include the radial, azimuthal, and north-south components of the measured magnetic field from the MAG instrument.
Energetic electrons observed by the MIMI/LEMMS sensor bounce along the field lines and drift perpendicular to B→×∇B→, which, away from Enceladus, is along Enceladus' orbit.
If they encounter a moon or any other dense material like a plume, they get absorbed (whole or partially) and leave a dropout signature in the electron intensities (except at the previously mentioned resonance energy where the drift motion of the electron and the orbital motion of the moon are the same and no absorption is expected).
At the orbit of Enceladus the resonance energy is at about 1MeV for 90° equatorial pitch angle and about 1.3MeV for electrons with 30° pitch angle (Roussos et al., 2007) assuming full corotation at the distance of the moon.
For 80% of corotation, as published by Wilson et al. (2009, 2010), the resonant energies change to 0.69MeV and 0.84MeV for 90° and 30° pitch angle.
It is worth noting that flow stagnation or slow down close to Enceladus will further change the resonant energy.
Upstream of the moon, the flow perturbation changes gradually (see more quantitative details in the discussion section of that paper).
Electrons with energies greater than the resonance energy drift opposite to the direction of the plasma motion (corotation), relative to the moon.
Therefore any absorption signature related to the moon upstream is caused by high-energy electrons.
Vice versa downstream of the moon: only electrons with energies less than the resonance energy can be absorbed.
In the following we distinguish between electrons measured in the low-energy end (channels C1-C7 and BE) and measured in the high-energy end (E0, E4 and E6) plus G1 as an omnidirectional channel serving as a proxy for penetrating radiation.
Since those penetrators are electrons only it is also a proxy for omnidirectional flux profiles at the distance of Enceladus.
Equatorial north flybys E0, 12, and 13
An overview of the electron measurements from the MIMI/LEMMS instrument in the vicinity of Enceladus for the flybys E0, E12, and E13 is shown in Fig. 2.
Those encounters were slightly north of the equatorial plane.
The time period selected is ±10min around closest approach which is marked by a solid line.
Absorption signatures are clearly visible in most of the displayed electron channels C1, C3, E0, E6, BE, and G1 (same is true for those not displayed here).
The nominal energy passbands are given on the right-hand side of Fig. 2.
Only the measurements in channel C1 during flybys E0 and E13 show no absorption signatures (off scale for E13).
As mentioned above the region in the vicinity of a moon where electrons are absorbed (upstream or downstream) depends on their energy.
Therefore it is important to know the flyby geometry during each encounter.
Fig. 3 shows the trajectory of Cassini during the flybys E0, E12, and E13 close to the moon inside of 10 Enceladus radii REnc.
We show the spacecraft trajectories projected into the xy-, xz-, and yz-planes of a coordinate system where the moon is in the center (x in plasma flow direction, y towards the planet and z northward).
This frame is often referred to as the Enceladus Interaction System (ENIS).
The individual flybys are labelled with the flyby numbers E0-E13, respectively.
The measured differential intensity of electrons (56-100keV) as measured in MIMI/LEMMS channel C3 is plotted in a color-code along the trajectory.
Blue means low intensities, red1
For interpretation of color in Figs.
2-10, the reader is referred to the web version of this article.
1 and white high intensities.
Clearly visible is a lack of electrons in this channel in the vicinity of the moon.
The highest count rates further away from the moon are more typical of a magnetospheric distribution in that region of Saturn's magnetosphere.
The variation in the measured intensities are a combination of dynamic changes in the magnetosphere and from the fact that MIMI/LEMMS sampled different pitch angles during the individual flybys.
Flyby E0 was upstream of the moon north of the equator (left panel of Fig. 2).
The absorption signatures are found approximately 8 REnc distance from the moon and at about 5 REnc above the equatorial plane.
Absorption signatures "drift" relative to the background particle flow.
North/south depletions exist because of the fast bounce motion.
The bite-out signature is seen in all channels upstream of the moon indicating that high-energy electrons with energies greater than the resonance energy are lost at the moon while drifting upstream.
Consequently this means that the responses in channel C3 as well as in channel E0 designed to measure electrons with energies of several tens to hundreds of keV are contaminated and dominated by MeV electrons for that particular flyby.
This does not mean that at all times those channels are contaminated by high-energy penetrators.
Therefore this flyby is a good example to demonstrate the simple scenario: high-energy depletions upstream only, no low-energy features unless the penetrators are significant.
The dropout size is comparable to the diameter of Enceladus which means that the absorption takes place at the surface of the moon.
For both encounters the differential intensity drops to background levels between the two red lines as indicated in Fig. 4A when Cassini is directly north of the moon (see Fig. 4B) and the spacecraft is directly connected to the Enceladus flux tube, where all electrons are absorbed.
This is exactly the region where the -∇xBϕ- drops, indicating weaker field-aligned currents.
In addition we could identify an "interaction" region during both flybys (marked in yellow) for electrons with MeV energies above the resonant energy.
The differential intensities drop to background levels inside that region.
For E13 this region is completely upstream while for E12 the region is up- and downstream.
In addition a sharp drop-out spike is observed during the E12 flyby after closest approach downstream in channels BE and E4 coinciding with sharp gradients in Bϕ.
Finally we checked the azimuthal component of the measured magnetic field (lowest panel of Fig. 4A).
This blue-shaded region marks the time period (or region around Enceladus) of changes in the azimuthal magnetic field component.
We observed large negative values in Bϕ (in anti-corotation direction) which is consistent with an Alfvén-wing type of interaction (Neubauer, 1980).
Equatorial south flybys E1, 7, 8, 9, 10, and 11
The responses of energetic electrons inside the MIMI/LEMMS instrument during the equatorial south flybys are summarized in Fig. 5.
Flyby geometries are shown in Fig. 6.
The shape and the width of the dropout signatures vary with energy and from flyby to flyby partially caused by the differences in the flyby geometry, and partially by changes in the magnetosphere.
Depletion signatures are present in all electron channels shown, except C1 and C3 during flyby E10.
The flyby distances varied between 99km and 2550km.
Flybys E8 and E11 were further south with respect to the moon's orbital plane than all the others.
Flybys E1 and E10 were upstream of the moon with respect to the corotational plasma flow direction, E8 was downstream (but far south).
Drop-out signatures in all the selected channels indicate that the bite-out is caused by high-energy electrons lost at the moon drifting upstream.
Field-aligned particles (0 and 180° pitch angle) only could be observed during encounter E1.
Flyby E11 was a downstream flyby far south of the moon outside the plotting range of Fig. 6.
The drop-out features during flyby E11 in the low-energy channels C1, C3, and E0 are comparable to the flyby E1 measurements.
The intensity profile is asymmetric at least in high-energy channel BE.
During the flybys E7 and E9 Cassini crossed the south polar region within the densest part of the plume.
Fig. 7 shows more details of the energetic electron responses during encounters E7, E8, and E9.
"Ramp"-like signatures are marked by green and red-colored areas and arrows.
The region where background values in channel C3 have been observed are marked by the red solid lines (for E7 and E9 only).
In yellow the high-energy interaction region measured in channels BE, E4, and G1 is marked again, as for the north equatorial flybys.
During E7 inbound a gradual decrease from magnetospheric levels followed by a sharp drop to background levels is observed.
The high-energy interaction area is obviously not identical for the selected channels and is wider for higher energies.
It is worth noting that we observed an additional spike during flyby E7 in higher-energy channels similar to the spike observed during northern equatorial flyby E12 again associated with large Bϕ gradients.
The Bϕ-component gradually changed to negative values, remained nearly unchanged when background levels in the low-energy channels have been observed (between the two red lines).
During flyby E8 the intensities of all channels increased caused by the reorientation of Cassini during the flyby and therefore a change in pitch angle.
Sharp "ramps" are seen on both sides of the drop-out signature.
The high-energy interaction region is quite small as a result of the flyby distance combined with the small overlap with the Enceladus flux tube and/or the small overlap with the Alfvén-wing (Simon et al., 2011) together with pitch angle changes during the encounter.
The Bϕ-component changed from negative to positive and back to negative values during the encounter.
During encounter E9 a sharp decrease upstream and a step-like "ramp" downstream has been observed in the low-energy channel C3.
A very clear and sharp signature in the high-energy channels (marked in yellow) could be observed slightly shifted upstream compared to the low-energy feature.
The azimuthal magnetic field component Bϕ turned negative and remained nearly unchanged during the time period when background level measurements in low-energy electrons have been observed, very similar to encounter E7.
High-latitude flybys E2, 3, 4, 5, and 6
The energetic electron intensities measured during the high-latitude flybys at Enceladus are summarized in Fig. 8.
The flyby trajectories of those 5 high-latitude encounters are shown in Fig. 9 again in an Enceladus-centered coordinate system (ENIS).
The electron responses during the encounters in the various MIMI/LEMMS channels are quite different from each other, partially because of the differences in the trajectories and also because of the spacecraft orientation (look direction of the detector telescopes).
Important for the interpretation of the high-latitude flybys is also the pointing of the detector heads relative to both the magnetic field lines and to the direction of the incoming dust particles (dust ram angle).
The flyby geometries of flyby E4, E5, and E6 are nearly identical while the pitch angle and dust ram angle coverage are in one nearly identical orientation for flyby E3 and E5, and in a different almost identical orientation for E4 and E6.
The trajectory of flyby E3 projected into the x-z-plane is slightly different compared to E4, E5, and E6.
Encounter E2 happened upstream of the moon, all the other flybys were downstream and north of moon on the inbound path and upstream south outbound.
In this paper we will not go into any details for flyby E2 (see Jones et al. (2006)) and concentrate on the others.
To study the remaining high-latitude flybys in more detail we show the electron intensities for low-energy channel C1 and for high-energy channel E6 in Fig. 10 as a function of z.
The flyby trajectories are color-coded with the differential intensity as measured in channel C3.
The intensities of low-energy electrons measured during flyby E5 is higher by a factor of about 2 compared to the measurements taken during E4 and E6, most probably caused by differences in the magnetosphere during those different time periods.
The values from flyby E3 are intermediate.
Those fluctuations are common throughout the magnetosphere of Saturn and are not special to flybys nor to Enceladus.
The most obvious features in the electron data for flybys E3-E6 are:•
"Ramp" signatures: During flybyE3 the "ramp" was observed at about 4 REnc above the equatorial plane and 2 REnc downstream in low-energy electron intensities.
In contrast a similar feature was observed as far north as 20 REnc and 12 REnc downstream during E5.
There is also some evidence of ramps in the high-energy channel E6 at about 16 REnc north (low-energies) and at about 6 REnc south (high-energies).
•
Large-scale high-energy drop-outs upstream and south of the moon: During flybys E3 and E5 the intensities of high-energy electrons drop to background levels during the crossing of the plume region, recovered gradually and reached magnetospheric values 25 REnc below the equatorial plane and up to 10 REnc upstream.
During flybys E4 and E6 this interaction region is much smaller.
The magnetospheric values were reached between 5 and 10 REnc south and about 5 REnc upstream.
•
Intensity peaks related to dust: We observed additional peaks in a variety of channels supposed to measure low-energy electrons.
During flybys E4, E5 and E6 the peaks are observed shortly after closest approach when Cassini went inside the densest part of the south polar plume.
The locations of the peaks in various MIMI/LEMMS channels correlate well with peaks in the dust measurements of the Cosmic Dust Analyzer CDA onboard the spacecraft (Kempf, private communications) and with the reported negatively charged dust grain signatures in plasma measurements of the CAPS/ELS instrument (Jones et al., 2009) onboard the spacecraft.
The largest signature in MIMI/LEMMS data was observed during flyby E5 illustrated in Fig. 11 where electron energy spectrograms from the low-energy end of MIMI/LEMMS (upper panel) and from CAPS/ELS (lower panel) are plotted together (only data from anode 5 is plotted which points approximately in the same direction as the low-energy head of MIMI/LEMMS).
Closest approach is marked by a white line.
During that flyby MIMI/LEMMS pointed nearly into the dust ram direction with very high relative velocity of 17.7km/s.
The peaks in both data sets correlate very well.
The particles measured in the high-energy telescope of MIMI/LEMMS are likely responsible for the high-energy magnetospheric background of CAPS/ELS measurements.
Therefore the blue-colored areas in Fig. 11 mean that the penetration radiation background is substantially removed during those periods.
The absorption signature above the resonant energy of about 700keV is clearly visible in the MIMI/LEMMS energy spectrogram.
These are electrons that cross the downstream interaction region before they reach Cassini.
In this region electric and magnetic fields are significantly disturbed resulting in more complex drifts, especially for electrons with higher energies.
This can potentially be an explanation why a "broadening" of the drop-out signature is observed.
Summary and discussion
The 14 flybys of Cassini at the moon Enceladus shed new light on the interaction of material released from the moon and the magnetosphere.
The variety of the flyby trajectories offered the opportunity to study the environment north and south of the equatorial plane as well as the plume directly southward of the moon during the high-latitude flybys.
The electron measurements obtained by the MIMI/LEMMS in the energy range between 15keV and several MeV onboard Cassini showed that the drift and bounce motion of the electrons perpendicular and along the B→×∇B→lines cause drop-out signatures in the distribution of electrons by the moon itself and by any material in the environment around the moon.
It was found that there are regions along the spacecraft trajectories where the electron intensity drops partially in the so called "ramp-like" depletion features followed by full intensity drop-outs in the low-energy electron measurements.
One possible interpretation is that full drop-outs occurs when a magnetic flux tube fully connects the moon with the spacecraft.
All electrons within that flux tube are absorbed by the moon.
They cannot escape absorption due to their gyro- or bounce motion since the associated length scales are much smaller than the diameter of Enceladus.
Partial absorption could be caused by the presence of an extended plume cloud.
For a finite energy passband channel, if all of the particles lose just a little energy in the gas/grain interaction, some will slip out of the passband giving a partial decrease in flux.
However, Kollmann et al. (2011) showed that the intensity of electrons in the E-ring could also increase during the interaction between neutral gas and dust grains.
Inside the plume the conditions are different and therefore it is unclear how the intensity is changed by the interaction.
The ramp-like signatures were observed during flybys E3, 5, 6(?), 7, 8, 9, 12, 13 as summarized in Table 2.
The questions of why this "boundary" most of the time is very sharp and abrupt remains.
From the existence of disturbances in the electromagnetic fields in the presence of a electrically conducting moon it is expected that the drifts of energetic particles are also very disturbed.
Energetic electrons are strongly affected and forbidden depletion region around the moon are created.
Such "ramp"-like signatures in low-energy electron data are not unique to Enceladus and have also been observed near other moons in the Solar System, e.g.
in the vicinity of the jovian moon Europa during encounter E4 of the Galileo spacecraft in 1996 (Paranicas et al., 2005).
As described by Schulz and Eviatar (1977) the absorption near an electrically conducting moon is depending on species and energy.
The drift of energetic electrons in the equatorial plane of Enceladus's interaction region has several components:•
An energy-independent (ExB)-component from corotation, along the +x-direction (Vcorot).
•
A gradient and curvature drift component towards -x resulting from gradient and curvature of B from Saturn's dipole field.
The intensity of this drift (Vdipole) increases with energy and tends to cancel Vcorot at the Keplerian resonant energy Erk), as discussed earlier.
This is typically around 700keV to 1MeV for equatorially mirroring particles, far from Enceladus.
•
An energy-independent (ExB)-component from velocity perturbations near Enceladus (Vfdisturb).
At the equatorial plane these perturbations can lead to non-zero velocities along the y-direction.
The x-velocity components of this perturbation tend to reduce Vcorot upstream of the moon.
Overall, Vfdisturb tends to cause a flow deflection around the conducting obstacle (Fig. 12, top row panels).
The model we use here by Ip (1982) describes this flow disturbance by two parameters: the parameter "a", which indicates which fraction of corotation speed achieved at the surface of the moon's effective flux tube, and the parameter "R", which describes the radius of the effective flux tube.
Here we used a=0.1 and R=1REnc.
Upstream of the moon, the flow perturbation reaches gradually from a=1 to a=0.1 (and the opposite downstream from the moon).
Obviously, since upstream Vcorot+Vfdisturb≈a*Vcorot<Vcorot, the resonant energy where the total (ExB) x-component drift is cancelled occurs at lower values as we get closer to the moon, and can reach values as low as 100keV.
At the locations where energy resonance is achieved, the y-component of Vfdisturb starts to become dominant.
This causes a significant distortion visible in the streamlines near Enceladus.
The distortion maximizes around the resonant energy (Fig. 12, third row from the top).
High energy electrons that come initially from upstream (initial energy lower than Erk) and achieve the resonant energy as they progressively move closer to Enceladus, will reverse their motion and never encounter the moon (third row from the top-right).
On the other hand, high energy electrons that come initially from downstream (initial energy greater than Erk) will tend to be focused on the moon.
This is due to the fact that the initial drift velocity (Vcorot+Vdipole) is towards -x.
Adding a -x velocity component to a perturbation that tends to deflect the flow around the obstacle, gives a total drift vector pointing towards the moon.
Focusing of streamlines of these energetic electrons may then lead to a wider depletion for these energies, upstream of the moon (Fig. 12, bottom panels, left).
At energies much higher than Erk, Vdipole dominates and drifts tend to follow lines of equal B, which in this calculation are parallel to the x-axis.
•
Another drift component would arise from magnetic field perturbations near the moon.
Such disturbances may become significant at high energies, where gradient and curvature drifts start to dominate.
These are not included in this simplified model here, since we do not have a magnetic field model coupled with the flow perturbation given in Ip (1982).
Gradient-curvature drifts in the interaction region may lead to significant drift perturbations along the x and y-directions.
We are currently working on tracing electrons in simulated interaction regions from hybrid, MHD or other analytical approaches (Kriegel et al., 2009; Saur et al., 2007).
Results will be shown in future studies.
At this stage, drift paths drawn in Fig. 12 are more appropriate to illustrate how changes in the E, B environment near Enceladus may affect electron distributions and lead to the appearance of forbidden regions.
These should not be considered for direct comparison with LEMMS observations at this stage, although at low energies (e.g. ≈20keV), where gradient and curvature drifts are much less significant, our simulated streamlines may be closer to reality (e.g. Fig. 7, middle panels, top).
What these simple calculations in principle also show is that streamlines become increasingly complex and the forbidden regions become larger, as we approach the resonant energy.
This is something that we see sometimes in MIMI/LEMMS data (flybys E3, E5).
As we go to higher energies, our approach shows that forbidden regions tend to disappear.
However, at E≫Erk, gradient/curvature drifts from field perturbations near Enceladus become very important.
Since these drifts are not modelled here, we cannot be conclusive at this stage about how forbidden regions would behave.
We also note that our calculations are only done for equatorially mirroring particles (90° pitch angle).
Both equatorial flybys E7 and E8 (observing at approximately 90deg around closest approach) reveal "ramps" in the lowest energy channels of MIMI/LEMMS, on both sides of Enceladus.
We believe that observation of forbidden (or partial) access for keV electrons is indeed visible during flybys E7 and E8.
While in principle the same drift physics discussed before apply for non-equatorially mirroring particles, we expect that drift perturbation effects would be reduced to some extent, especially for the highest energies measured with MIMI/LEMMS.
The reason is that magnetic field perturbations are more intense (compared to the background field parameters) near Enceladus than at high latitudes: non-equatorially mirroring particles are bouncing along the field lines and spend less time near Enceladus during one bounce period.
This may explain for instance why the electron depletion during flybys E0 and E2 are equal to 1 moon diameter, but may not explain why depletions at flybys E3 and E5 become so broad at high energies, although MIMI/LEMMS is oriented far from 90° pitch angle.
Variability of the interaction region between different flybys, may play a role.
We believe answers for these discrepancies may be given by tracing energetic electrons in simulated interaction regions and by also understanding better the exact responses of the MIMI/LEMMS channels.
In addition to the absorption features in the electron data there is also evidence that MIMI/LEMMS detected additional signals correlated with dust measurements from the CDA detector and CAPS spectrometer.
There are three possibilities to explain those additional peaks: (1) directly measured dust, or (2) directly measured electrons, or (3) instrumental effects.
Directly measured charged dust would require that the dust grains entering the low-energy aperture of MIMI/LEMMS are bent in the internal magnetic field towards the electron detector E. Assuming a bend radius of about 1cm with a 800 G internal magnetic field the mass of a singly charged dust grain cannot be higher than 4.3amu which is much lighter than those dust particles measured by the dust instrument CDA and therefore cannot account for the correlation between the peaks in MIMI/LEMMS and CDA.
Another consideration could be that a dust grain hitting the E and F low-energy electron detectors triggers the 15keV threshold.
Having the corresponding velocity of the incoming dust (17.7km/s) would mean that the dust grain has to have a mass of about 9300amu.
However, this corresponds to a gyroradius of about 20m, too big to reach directly detector E or F.
From this it follows that incoming dust particles could only hit detector A directly.
Finally the fact that we observed some of those peaks in the high-energy aperture HE of MIMI/LEMMS makes it difficult to correlate it with direct dust hits.
As described in the instrumentation section HE consists of a set of five detectors where multiple coincidences between the detectors are used to determine ions or electrons and their energies.
HE electron channels like channel E4 are double-coincidence measurements.
This would require that the dust particle entering HE penetrates through the front Aluminum foil, the first detector (150μm thick), the second detector (700μm thick) and gets stuck in detector 3.
This is very unlikely.
Directly measured electrons in the corresponding energy ranges cannot be fully ruled out.
However, the fact that those peaks are highly correlated to the CDA dust measurements would mean that within the high-density plume of gas and dust grains some of the energetic electrons survive and are not lost.
We consider this possibility also not very likely.
Since the MIMI/LEMMS instrument was not built to measure dust we cannot rule out instrumental issues.
If dust reaches one of the detectors inside the low-energy aperture a cloud of plasma could be produced creating electrons during the impact.
However, their energies should only extend up to several hundreds of eV to keV range, still too low to be measured by the dedicated electron detectors with a threshold energy of 15keV.
Another possibility could be that the power supplies of the various detectors "react" after a massive dust particle hit the detectors, creating a false signal at a huge count rate.
Therefore we think that the "dust"-signals in MIMI/LEMMS is a false signal and cannot be used to determine further dust parameters, although they might be caused by the incoming dust grains.
Future investigations
Cassini will continue to perform relatively close encounters with Enceladus, as summarized in Table 3.
Each of the additional encounters will shed further light on the interaction of the material released from the plume and the corotating particles in the saturnian magnetosphere.
In addition the outcome of the studies related to Enceladus can also be helpful in better understanding the environment of other "active" moons in our Solar System, e.g.
the jovian moon Europa where unusual magnetic field disturbances measured during one of the Galileo flybys have been reported by Kivelson et al. (2009), which could be related to dynamic effects in the jovian magnetosphere or by (less effective as compared to Enceladus) plume activity on Europa.
Future missions to Jupiter will be able to study the activity and dynamics of icy satellites in more detail.
Acknowledgments
The German contribution of the MIMI/LEMMS Instrument was in part financed by the German BMWi through the German Space Agency DLR under Contracts 50 OH 0103, 50 OH 0801, 50 OH 0802, 50 OH 1101 and by the Max Planck Society.
G.H.J. and C.S.A. were supported by UK Science and Technology Facilities Council Advanced and Postdoctoral Fellowships, respectively.
We thank Andreas Lagg (MPS) for extensive software support, Martha Kusterer and Jon Vandegriff (both JHUAPL) for reducing the MIMI data, and Getyn Lewis and Lin Gilbert (both MSSL/UCL) for reducing the CAPS/ELS data.

The Cassini Enceladus encounters 2005-2010 in the view of energetic electron measurements

Highlights
► Ramp-like features in electron intensities in the low-energy channels.
► High-energy depletions upstream for electrons with energies above the resonant energy.
► "Dust"-related peaks in the electron data correlated with data from other instrument onboard Cassini.
Abstract
The moon Enceladus, embedded in Saturn's radiation belts, is the main internal source of neutral and charged particles in the Kronian magnetosphere.
A plume of water ice molecules and dust released through geysers on the south polar region provides enough material to feed the E-ring and also the neutral torus of Saturn and the entire magnetosphere.
In the time period 2005-2010 the Cassini spacecraft flew close by the moon 14 times, sometimes as low as 25km above the surface and directly through the plume.
For the very first time measurements of plasma and energetic particles inside the plume and its immediate vicinity could be obtained.
In this work we summarize the results of energetic electron measurements in the energy range 27keV to 21MeV taken by the Low Energy Magnetospheric Measurement System (LEMMS), part of the Magnetospheric Imaging Instrument (MIMI) onboard Cassini in the vicinity of the moon in combination with measurements of the magnetometer instrument MAG and the Electron Spectrometer ELS of the plasma instrument CAPS onboard the spacecraft.
Features in the data can be interpreted as that the spacecraft was connected to the plume material along field lines well before entering the high density region of the plume.
Sharp absorption signatures as the result of losses of energetic electrons bouncing along those field lines, through the emitted gas and dust clouds, clearly depend on flyby geometry as well as on measured pitch angle/look direction of the instrument.
We found that the depletion signatures during some of the flybys show "ramp-like" features where only a partial depletion has been observed further away from the moon followed by nearly full absorption of electrons closer in.
We interpret this as partially/fully connected to the flux tube connecting the moon with Cassini.
During at least two of the flybys (with some evidence of one additional encounter) MIMI/LEMMS data are consistent with the presence of dust in energetic electron data when Cassini flew directly through the south polar plume.
In addition we found gradients in the magnetic field components which are frequently found to be associated with changes in the MIMI/LEMMS particles intensities.
This indicates that complex electron drifts in the vicinity of Enceladus could form forbidden regions for electrons which may appear as intensity drop-outs.

Introduction
Enceladus, one out of currently 62 satellites of Saturn, orbits the planet at a distance of 3.95 Saturn radii RS (1RS=60,268km) and is embedded in the radiation belts of Saturn's inner magnetosphere.
Even with a small radius REnc of only 252km and an apparent similarity to other icy moons, this moon is by far the most important internal source of dust, neutral gas and plasma in the saturnian system and especially in the magnetosphere.
As already inferred from data of the first flybys of the Pioneer 11 (1979), Voyager 1 (1980) and Voyager 2 (1981) spacecraft, respectively (Smith et al., 1981; Krimigis et al., 1982), this moon plays the same role Io does for the jovian system.
The Cassini spacecraft, in orbit around Saturn since July 2004, has flown by Enceladus 14 times between 2005 and 2010.
Data from Cassini instrument teams identified a plume of water geysers above the so called "tigerstripes" on the south pole of the moon (Dougherty et al., 2006; Porco et al., 2006).
This discovery is one of the most important findings to better understand the saturnian magnetosphere.
The material released in the form of water gas, ice molecules and dust (Waite et al., 2006; Spahn et al., 2006) from those geysers is the main source of the E-ring and the neutral torus and is the major plasma source of water group ions in the magnetosphere.
A summary of Enceladus findings from Cassini measurements is very well presented by Spencer et al. (2009).
The interaction between energetic electrons and the moon essentially is given by the individual motion of the particles.
Energetic charged particles bounce along the magnetic field lines between the north and south pole of Saturn and drift azimuthally at the same time.
A moon or any other object (ring material or ring arcs, plume material, neutral gas, dust, etc.) blocks and absorbs some of those bouncing particles, appearing as an absorption signature in the differential intensities of the measured particles in a given energy range.
The gaps in the distribution due to those losses are categorized as macrosignatures and microsignatures (Selesnick, 1993).
While the macrosignatures are seen at all local times and latitudes the microsignatures are highly dependent on the longitudinal distance between the observer (spacecraft) and the absorbing body (for a mathematical description see van Allen et al. (1980), Thomsen and van Allen (1980) and also Roussos et al. (2007)).
Particles are lost in that region of the magnetosphere when encountering the moon but refilled in time by particle sources or diffusion processes.
The depths of the absorption signature in the particle intensities can be used to determine the amount of material causing the loss of energetic particles as a function of longitudinal distance with respect to Enceladus if the loss processes are known and one of them involves interaction with gas or dust.
In addition to the bounce motion up and down along the field lines the charged particles magnetically drift perpendicular to the B→×∇B→ direction.
Since both B→ and ∇B→ are disturbed around Enceladus, the electron motion will also be disturbed, modifying to some extent the flux distribution in the moon's vicinity.
Therefore, particle fluxes may reflect integrated effects of the disturbed field environment around a moon, seen in the particles that drift across it.
Ions and electrons drift in opposite directions around the planet.
Cassini data analysis provided the opportunity to confirm earlier findings from previous missions.
Effects in the local particle distributions have been studied in the literature.
Paranicas et al. (2005) studied the first encounter E0 of the Cassini spacecraft with Enceladus.
The authors made it clear that upstream of the moon it is only expected to measure an absorption signature of MeV electrons.
Nothing should be seen in energy channels below the "resonance" energy (Thomsen and van Allen, 1980).
The presence of microsignatures in MeV electrons upstream of the moon therefore are a strong indicator of a fresh absorption caused by the moon itself.
Jones et al. (2006) investigated the effect of the plume on energetic particles in the magnetosphere and analyzed the response of charged particle measurements from the Low Energy Magnetospheric Measurement System (LEMMS) onboard Cassini.
Khurana et al. (2007) studied the magnetic measurements from the flybys E0-E2 in terms of mass-loading while Tokar et al. (2009) found in CAPS plasma data freshly-produced water-group ions and heavier water dimer ions HxO2+ during encounters E3 and E5.
Kempf et al. (2008) studied the dust particles in the E-ring close to Enceladus and found its vertical distribution to be 4300km or 17 REnc thick.
Jones et al. (2009) found that negatively and positively charged particles of nanometer scales are released through the vents in the south polar region.
Farrell et al. (2009) reported electron density dropouts near Enceladus during flyby E3 most probably caused by absorption by sub-millimeter size water ice grains in the plume.
Coates et al. (2010) reported negative ions in plume measurements of the Electron Spectrometer CAPS/ELS and Teolis et al. (2010) investigated the density and structure of Enceladus' south polar plume during E3 and E5 by using data of the Cassini ion neutral mass spectrometer INMS and indicated that 100kg of plume water vapor escapes every second and spreads out uniformly.
In addition they detected fine-grained ice concentrated in the jets of the multiple local sources along the tigerstripes.
Farrell et al. (2010) studied the modified plasma of Saturn's magnetosphere in the vicinity of Enceladus.
They pointed out that a cloud of dust exists around the moon extending at least 20 REnc both north and south of the equator, while Shafiq et al. (2011) reported dusty plasma measurements during encounter E3.
Besides the data analysis theoretical work, analytic modelling, and especially numerical simulations have been performed recently (Jia et al., 2010b,; Tenishev et al., 2010; Cassidy and Johnson, 2010; Kriegel et al., 2009; Omidi et al., 2010; Simon et al., 2011).
In those studies the environment of Enceladus has been simulated and parameters like the local gas distribution, the importance of dust near the moon and the implications for the magnetosphere have been determined.
Tseng and Ip (2011) predicted Enceladus to be an important source of Saturn's ring atmosphere and ionosphere theoretically, and suggested additional plasma and particle data to compare with their predictions.
Recently evidence of an Enceladus footprint in Saturn's auroral zone has been reported by Pryor et al. (2011).
Cravens et al. (2011) studied the Enceladus torus theoretically based on CAPS measurements and found two populations: a denser part near 4 REnc and a less dense and more radially extended part.
Finally Hartogh et al. (2011) inferred that the activity on Enceladus is even likely the source of water in the upper atmosphere of Saturn, underlining the importance of that moon for the saturnian system as a whole.
All these investigations show the importance of Enceladus studies to better explain the source- and transport processes in Saturn's magnetosphere.
In this paper we present an overview of energetic electron measurements (27keV to 21MeV) obtained by the Low Energy Magnetospheric Measurement System (LEMMS) onboard the Cassini spacecraft during the first 14 close encounters with Enceladus (E0-E13) between 2005 and 2010.
Instrumentation and data analysis
Incoming ions and electrons are separated through a strong internal magnet.
Semiconductor detector sets are positioned in different places inside the instrument enabling us to measure ions and electrons in a variety of energy channels.
The angular resolution is restricted by the field-of-view of the instrument's apertures and look direction (limitation in measured pitch angle) of the MIMI/LEMMS sensors.
In the calculation of the pitch angle it is assumed that all particles enter through the central entrance of the instrument's apertures with opening angles of 15° and 30° for low-energy and high-energy end, respectively.
This value has to be subtracted from 180° to get the measured particle pitch angle for the high-energy end.
The apertures have 7 (LE-end) and 19 (HE-end) entrance holes allowing particles to enter the instrument with a variety of angles relative to the central hole.
Good pitch angle coverage can only be achieved if the spacecraft itself is rotating.
This is often the case during the downlink periods when the spacecraft antenna of Cassini points towards Earth.
However, during the close moon encounters the spacecraft is not rotating and is often reoriented before and after closest approach so that the remote-sensing instruments (camera and spectrometers) have the moon in their field-of-view.
As a consequence the measurable pitch angle of the incoming charged particles also changes which makes the data more difficult to interpret.
MIMI/LEMMS also performs a Pulse Height Analysis (PHA) for the low-energy end separately for the detectors E and F (for electrons with energies of 6.7-2300keV) and for ion detector A (ions with energies of 12-833keV) enabling a good energy resolution of up to 256 channels.
As pointed out by Paranicas et al. (2005) and Roussos et al. (2007) each energy channel of the MIMI/LEMMS instrument measures the sum of a real signal and a background noise signal.
The most relevant background for this study is penetrating radiation affecting primarily the low-energy electron channels (C-channels).
To identify periods where the signal/noise-ratio is good enough checks are performed with data of the omni-directional channel G1 which is primarily counting MeV electrons only.
Furthermore if the time vs. intensity curves of electrons measured in the C-channels follow the curves of channel G1 then we do not consider the data in the C-channels to be real.
If the energy of the measured electrons is lower than the resonant energy for a given orbital distance from the planet an absorption feature should be observed only downstream of the moon as described above while the bite-out in the high-energy channels should only be observed upstream of the moon.
Therefore assuming to a first order that electrons are not deflected from magnetic field gradients and/or flow perturbations and the absorption signature appears on the "wrong" side of the moon simultaneous to the high-energy channels it could be an indication of "bad" data.
A complete description of the MIMI instrument can be found in Krimigis et al. (2004).
The latest MIMI/LEMMS energy channel passbands can be found in Krupp et al. (2009).
In this paper we used measurements only from a representative subset of available energy channels and concentrated on electrons only.
In order to avoid confusion between flyby number and channel number we put the flyby numbers in italic fonts.
For the interpretation of the energetic electron data we use measurements of the magnetometer MAG onboard the spacecraft (Dougherty et al., 2004) and we correlate the MIMI/LEMMS with plasma electron measurements of the Electron Spectrometer ELS, part of the Cassini Plasma Spectrometer Investigation CAPS (Young et al., 2004).
Observations
The times of all Enceladus encounters between 2005 and 2010, closest approach distances and type of encounter are summarized in Table 1.
We subdivided the encounters into flybys parallel to the equatorial plane ("eq north" and "eq south") and high latitude flybys ("hl").
As pointed out above, high-energy ions are absent from regions magnetically connected to the orbit of the moon around the planet (Enceladus macrosignature).
Therefore we concentrate on MIMI/LEMMS measurements of electrons with different energies and with two different pitch angles 180° apart from each other given by the look directions of the low-energy- and high-energy end of the MIMI/LEMMS instrument relative to the local magnetic field.
In addition we derive the dust ram angle of incoming dust particles on Keplerian orbits relative to the MIMI/LEMMS look direction where the motion of the spacecraft is taken into account (0° means that dust is moving coaligned to the instrument's symmetry axis).
Finally these overview plots include the radial, azimuthal, and north-south components of the measured magnetic field from the MAG instrument.
Energetic electrons observed by the MIMI/LEMMS sensor bounce along the field lines and drift perpendicular to B→×∇B→, which, away from Enceladus, is along Enceladus' orbit.
If they encounter a moon or any other dense material like a plume, they get absorbed (whole or partially) and leave a dropout signature in the electron intensities (except at the previously mentioned resonance energy where the drift motion of the electron and the orbital motion of the moon are the same and no absorption is expected).
At the orbit of Enceladus the resonance energy is at about 1MeV for 90° equatorial pitch angle and about 1.3MeV for electrons with 30° pitch angle (Roussos et al., 2007) assuming full corotation at the distance of the moon.
For 80% of corotation, as published by Wilson et al. (2009, 2010), the resonant energies change to 0.69MeV and 0.84MeV for 90° and 30° pitch angle.
It is worth noting that flow stagnation or slow down close to Enceladus will further change the resonant energy.
Upstream of the moon, the flow perturbation changes gradually (see more quantitative details in the discussion section of that paper).
Electrons with energies greater than the resonance energy drift opposite to the direction of the plasma motion (corotation), relative to the moon.
Therefore any absorption signature related to the moon upstream is caused by high-energy electrons.
Vice versa downstream of the moon: only electrons with energies less than the resonance energy can be absorbed.
In the following we distinguish between electrons measured in the low-energy end (channels C1-C7 and BE) and measured in the high-energy end (E0, E4 and E6) plus G1 as an omnidirectional channel serving as a proxy for penetrating radiation.
Since those penetrators are electrons only it is also a proxy for omnidirectional flux profiles at the distance of Enceladus.
Equatorial north flybys E0, 12, and 13
An overview of the electron measurements from the MIMI/LEMMS instrument in the vicinity of Enceladus for the flybys E0, E12, and E13 is shown in Fig. 2.
Those encounters were slightly north of the equatorial plane.
The time period selected is ±10min around closest approach which is marked by a solid line.
Absorption signatures are clearly visible in most of the displayed electron channels C1, C3, E0, E6, BE, and G1 (same is true for those not displayed here).
The nominal energy passbands are given on the right-hand side of Fig. 2.
Only the measurements in channel C1 during flybys E0 and E13 show no absorption signatures (off scale for E13).
As mentioned above the region in the vicinity of a moon where electrons are absorbed (upstream or downstream) depends on their energy.
Therefore it is important to know the flyby geometry during each encounter.
Fig. 3 shows the trajectory of Cassini during the flybys E0, E12, and E13 close to the moon inside of 10 Enceladus radii REnc.
We show the spacecraft trajectories projected into the xy-, xz-, and yz-planes of a coordinate system where the moon is in the center (x in plasma flow direction, y towards the planet and z northward).
This frame is often referred to as the Enceladus Interaction System (ENIS).
The individual flybys are labelled with the flyby numbers E0-E13, respectively.
The measured differential intensity of electrons (56-100keV) as measured in MIMI/LEMMS channel C3 is plotted in a color-code along the trajectory.
Blue means low intensities, red1
For interpretation of color in Figs.
2-10, the reader is referred to the web version of this article.
1 and white high intensities.
Clearly visible is a lack of electrons in this channel in the vicinity of the moon.
The highest count rates further away from the moon are more typical of a magnetospheric distribution in that region of Saturn's magnetosphere.
The variation in the measured intensities are a combination of dynamic changes in the magnetosphere and from the fact that MIMI/LEMMS sampled different pitch angles during the individual flybys.
Flyby E0 was upstream of the moon north of the equator (left panel of Fig. 2).
The absorption signatures are found approximately 8 REnc distance from the moon and at about 5 REnc above the equatorial plane.
Absorption signatures "drift" relative to the background particle flow.
North/south depletions exist because of the fast bounce motion.
The bite-out signature is seen in all channels upstream of the moon indicating that high-energy electrons with energies greater than the resonance energy are lost at the moon while drifting upstream.
Consequently this means that the responses in channel C3 as well as in channel E0 designed to measure electrons with energies of several tens to hundreds of keV are contaminated and dominated by MeV electrons for that particular flyby.
This does not mean that at all times those channels are contaminated by high-energy penetrators.
Therefore this flyby is a good example to demonstrate the simple scenario: high-energy depletions upstream only, no low-energy features unless the penetrators are significant.
The dropout size is comparable to the diameter of Enceladus which means that the absorption takes place at the surface of the moon.
For both encounters the differential intensity drops to background levels between the two red lines as indicated in Fig. 4A when Cassini is directly north of the moon (see Fig. 4B) and the spacecraft is directly connected to the Enceladus flux tube, where all electrons are absorbed.
This is exactly the region where the -∇xBϕ- drops, indicating weaker field-aligned currents.
In addition we could identify an "interaction" region during both flybys (marked in yellow) for electrons with MeV energies above the resonant energy.
The differential intensities drop to background levels inside that region.
For E13 this region is completely upstream while for E12 the region is up- and downstream.
In addition a sharp drop-out spike is observed during the E12 flyby after closest approach downstream in channels BE and E4 coinciding with sharp gradients in Bϕ.
Finally we checked the azimuthal component of the measured magnetic field (lowest panel of Fig. 4A).
This blue-shaded region marks the time period (or region around Enceladus) of changes in the azimuthal magnetic field component.
We observed large negative values in Bϕ (in anti-corotation direction) which is consistent with an Alfvén-wing type of interaction (Neubauer, 1980).
Equatorial south flybys E1, 7, 8, 9, 10, and 11
The responses of energetic electrons inside the MIMI/LEMMS instrument during the equatorial south flybys are summarized in Fig. 5.
Flyby geometries are shown in Fig. 6.
The shape and the width of the dropout signatures vary with energy and from flyby to flyby partially caused by the differences in the flyby geometry, and partially by changes in the magnetosphere.
Depletion signatures are present in all electron channels shown, except C1 and C3 during flyby E10.
The flyby distances varied between 99km and 2550km.
Flybys E8 and E11 were further south with respect to the moon's orbital plane than all the others.
Flybys E1 and E10 were upstream of the moon with respect to the corotational plasma flow direction, E8 was downstream (but far south).
Drop-out signatures in all the selected channels indicate that the bite-out is caused by high-energy electrons lost at the moon drifting upstream.
Field-aligned particles (0 and 180° pitch angle) only could be observed during encounter E1.
Flyby E11 was a downstream flyby far south of the moon outside the plotting range of Fig. 6.
The drop-out features during flyby E11 in the low-energy channels C1, C3, and E0 are comparable to the flyby E1 measurements.
The intensity profile is asymmetric at least in high-energy channel BE.
During the flybys E7 and E9 Cassini crossed the south polar region within the densest part of the plume.
Fig. 7 shows more details of the energetic electron responses during encounters E7, E8, and E9.
"Ramp"-like signatures are marked by green and red-colored areas and arrows.
The region where background values in channel C3 have been observed are marked by the red solid lines (for E7 and E9 only).
In yellow the high-energy interaction region measured in channels BE, E4, and G1 is marked again, as for the north equatorial flybys.
During E7 inbound a gradual decrease from magnetospheric levels followed by a sharp drop to background levels is observed.
The high-energy interaction area is obviously not identical for the selected channels and is wider for higher energies.
It is worth noting that we observed an additional spike during flyby E7 in higher-energy channels similar to the spike observed during northern equatorial flyby E12 again associated with large Bϕ gradients.
The Bϕ-component gradually changed to negative values, remained nearly unchanged when background levels in the low-energy channels have been observed (between the two red lines).
During flyby E8 the intensities of all channels increased caused by the reorientation of Cassini during the flyby and therefore a change in pitch angle.
Sharp "ramps" are seen on both sides of the drop-out signature.
The high-energy interaction region is quite small as a result of the flyby distance combined with the small overlap with the Enceladus flux tube and/or the small overlap with the Alfvén-wing (Simon et al., 2011) together with pitch angle changes during the encounter.
The Bϕ-component changed from negative to positive and back to negative values during the encounter.
During encounter E9 a sharp decrease upstream and a step-like "ramp" downstream has been observed in the low-energy channel C3.
A very clear and sharp signature in the high-energy channels (marked in yellow) could be observed slightly shifted upstream compared to the low-energy feature.
The azimuthal magnetic field component Bϕ turned negative and remained nearly unchanged during the time period when background level measurements in low-energy electrons have been observed, very similar to encounter E7.
High-latitude flybys E2, 3, 4, 5, and 6
The energetic electron intensities measured during the high-latitude flybys at Enceladus are summarized in Fig. 8.
The flyby trajectories of those 5 high-latitude encounters are shown in Fig. 9 again in an Enceladus-centered coordinate system (ENIS).
The electron responses during the encounters in the various MIMI/LEMMS channels are quite different from each other, partially because of the differences in the trajectories and also because of the spacecraft orientation (look direction of the detector telescopes).
Important for the interpretation of the high-latitude flybys is also the pointing of the detector heads relative to both the magnetic field lines and to the direction of the incoming dust particles (dust ram angle).
The flyby geometries of flyby E4, E5, and E6 are nearly identical while the pitch angle and dust ram angle coverage are in one nearly identical orientation for flyby E3 and E5, and in a different almost identical orientation for E4 and E6.
The trajectory of flyby E3 projected into the x-z-plane is slightly different compared to E4, E5, and E6.
Encounter E2 happened upstream of the moon, all the other flybys were downstream and north of moon on the inbound path and upstream south outbound.
In this paper we will not go into any details for flyby E2 (see Jones et al. (2006)) and concentrate on the others.
To study the remaining high-latitude flybys in more detail we show the electron intensities for low-energy channel C1 and for high-energy channel E6 in Fig. 10 as a function of z.
The flyby trajectories are color-coded with the differential intensity as measured in channel C3.
The intensities of low-energy electrons measured during flyby E5 is higher by a factor of about 2 compared to the measurements taken during E4 and E6, most probably caused by differences in the magnetosphere during those different time periods.
The values from flyby E3 are intermediate.
Those fluctuations are common throughout the magnetosphere of Saturn and are not special to flybys nor to Enceladus.
The most obvious features in the electron data for flybys E3-E6 are:•
"Ramp" signatures: During flybyE3 the "ramp" was observed at about 4 REnc above the equatorial plane and 2 REnc downstream in low-energy electron intensities.
In contrast a similar feature was observed as far north as 20 REnc and 12 REnc downstream during E5.
There is also some evidence of ramps in the high-energy channel E6 at about 16 REnc north (low-energies) and at about 6 REnc south (high-energies).
•
Large-scale high-energy drop-outs upstream and south of the moon: During flybys E3 and E5 the intensities of high-energy electrons drop to background levels during the crossing of the plume region, recovered gradually and reached magnetospheric values 25 REnc below the equatorial plane and up to 10 REnc upstream.
During flybys E4 and E6 this interaction region is much smaller.
The magnetospheric values were reached between 5 and 10 REnc south and about 5 REnc upstream.
•
Intensity peaks related to dust: We observed additional peaks in a variety of channels supposed to measure low-energy electrons.
During flybys E4, E5 and E6 the peaks are observed shortly after closest approach when Cassini went inside the densest part of the south polar plume.
The locations of the peaks in various MIMI/LEMMS channels correlate well with peaks in the dust measurements of the Cosmic Dust Analyzer CDA onboard the spacecraft (Kempf, private communications) and with the reported negatively charged dust grain signatures in plasma measurements of the CAPS/ELS instrument (Jones et al., 2009) onboard the spacecraft.
The largest signature in MIMI/LEMMS data was observed during flyby E5 illustrated in Fig. 11 where electron energy spectrograms from the low-energy end of MIMI/LEMMS (upper panel) and from CAPS/ELS (lower panel) are plotted together (only data from anode 5 is plotted which points approximately in the same direction as the low-energy head of MIMI/LEMMS).
Closest approach is marked by a white line.
During that flyby MIMI/LEMMS pointed nearly into the dust ram direction with very high relative velocity of 17.7km/s.
The peaks in both data sets correlate very well.
The particles measured in the high-energy telescope of MIMI/LEMMS are likely responsible for the high-energy magnetospheric background of CAPS/ELS measurements.
Therefore the blue-colored areas in Fig. 11 mean that the penetration radiation background is substantially removed during those periods.
The absorption signature above the resonant energy of about 700keV is clearly visible in the MIMI/LEMMS energy spectrogram.
These are electrons that cross the downstream interaction region before they reach Cassini.
In this region electric and magnetic fields are significantly disturbed resulting in more complex drifts, especially for electrons with higher energies.
This can potentially be an explanation why a "broadening" of the drop-out signature is observed.
Summary and discussion
The 14 flybys of Cassini at the moon Enceladus shed new light on the interaction of material released from the moon and the magnetosphere.
The variety of the flyby trajectories offered the opportunity to study the environment north and south of the equatorial plane as well as the plume directly southward of the moon during the high-latitude flybys.
The electron measurements obtained by the MIMI/LEMMS in the energy range between 15keV and several MeV onboard Cassini showed that the drift and bounce motion of the electrons perpendicular and along the B→×∇B→lines cause drop-out signatures in the distribution of electrons by the moon itself and by any material in the environment around the moon.
It was found that there are regions along the spacecraft trajectories where the electron intensity drops partially in the so called "ramp-like" depletion features followed by full intensity drop-outs in the low-energy electron measurements.
One possible interpretation is that full drop-outs occurs when a magnetic flux tube fully connects the moon with the spacecraft.
All electrons within that flux tube are absorbed by the moon.
They cannot escape absorption due to their gyro- or bounce motion since the associated length scales are much smaller than the diameter of Enceladus.
Partial absorption could be caused by the presence of an extended plume cloud.
For a finite energy passband channel, if all of the particles lose just a little energy in the gas/grain interaction, some will slip out of the passband giving a partial decrease in flux.
However, Kollmann et al. (2011) showed that the intensity of electrons in the E-ring could also increase during the interaction between neutral gas and dust grains.
Inside the plume the conditions are different and therefore it is unclear how the intensity is changed by the interaction.
The ramp-like signatures were observed during flybys E3, 5, 6(?), 7, 8, 9, 12, 13 as summarized in Table 2.
The questions of why this "boundary" most of the time is very sharp and abrupt remains.
From the existence of disturbances in the electromagnetic fields in the presence of a electrically conducting moon it is expected that the drifts of energetic particles are also very disturbed.
Energetic electrons are strongly affected and forbidden depletion region around the moon are created.
Such "ramp"-like signatures in low-energy electron data are not unique to Enceladus and have also been observed near other moons in the Solar System, e.g.
in the vicinity of the jovian moon Europa during encounter E4 of the Galileo spacecraft in 1996 (Paranicas et al., 2005).
As described by Schulz and Eviatar (1977) the absorption near an electrically conducting moon is depending on species and energy.
The drift of energetic electrons in the equatorial plane of Enceladus's interaction region has several components:•
An energy-independent (ExB)-component from corotation, along the +x-direction (Vcorot).
•
A gradient and curvature drift component towards -x resulting from gradient and curvature of B from Saturn's dipole field.
The intensity of this drift (Vdipole) increases with energy and tends to cancel Vcorot at the Keplerian resonant energy Erk), as discussed earlier.
This is typically around 700keV to 1MeV for equatorially mirroring particles, far from Enceladus.
•
An energy-independent (ExB)-component from velocity perturbations near Enceladus (Vfdisturb).
At the equatorial plane these perturbations can lead to non-zero velocities along the y-direction.
The x-velocity components of this perturbation tend to reduce Vcorot upstream of the moon.
Overall, Vfdisturb tends to cause a flow deflection around the conducting obstacle (Fig. 12, top row panels).
The model we use here by Ip (1982) describes this flow disturbance by two parameters: the parameter "a", which indicates which fraction of corotation speed achieved at the surface of the moon's effective flux tube, and the parameter "R", which describes the radius of the effective flux tube.
Here we used a=0.1 and R=1REnc.
Upstream of the moon, the flow perturbation reaches gradually from a=1 to a=0.1 (and the opposite downstream from the moon).
Obviously, since upstream Vcorot+Vfdisturb≈a*Vcorot<Vcorot, the resonant energy where the total (ExB) x-component drift is cancelled occurs at lower values as we get closer to the moon, and can reach values as low as 100keV.
At the locations where energy resonance is achieved, the y-component of Vfdisturb starts to become dominant.
This causes a significant distortion visible in the streamlines near Enceladus.
The distortion maximizes around the resonant energy (Fig. 12, third row from the top).
High energy electrons that come initially from upstream (initial energy lower than Erk) and achieve the resonant energy as they progressively move closer to Enceladus, will reverse their motion and never encounter the moon (third row from the top-right).
On the other hand, high energy electrons that come initially from downstream (initial energy greater than Erk) will tend to be focused on the moon.
This is due to the fact that the initial drift velocity (Vcorot+Vdipole) is towards -x.
Adding a -x velocity component to a perturbation that tends to deflect the flow around the obstacle, gives a total drift vector pointing towards the moon.
Focusing of streamlines of these energetic electrons may then lead to a wider depletion for these energies, upstream of the moon (Fig. 12, bottom panels, left).
At energies much higher than Erk, Vdipole dominates and drifts tend to follow lines of equal B, which in this calculation are parallel to the x-axis.
•
Another drift component would arise from magnetic field perturbations near the moon.
Such disturbances may become significant at high energies, where gradient and curvature drifts start to dominate.
These are not included in this simplified model here, since we do not have a magnetic field model coupled with the flow perturbation given in Ip (1982).
Gradient-curvature drifts in the interaction region may lead to significant drift perturbations along the x and y-directions.
We are currently working on tracing electrons in simulated interaction regions from hybrid, MHD or other analytical approaches (Kriegel et al., 2009; Saur et al., 2007).
Results will be shown in future studies.
At this stage, drift paths drawn in Fig. 12 are more appropriate to illustrate how changes in the E, B environment near Enceladus may affect electron distributions and lead to the appearance of forbidden regions.
These should not be considered for direct comparison with LEMMS observations at this stage, although at low energies (e.g. ≈20keV), where gradient and curvature drifts are much less significant, our simulated streamlines may be closer to reality (e.g. Fig. 7, middle panels, top).
What these simple calculations in principle also show is that streamlines become increasingly complex and the forbidden regions become larger, as we approach the resonant energy.
This is something that we see sometimes in MIMI/LEMMS data (flybys E3, E5).
As we go to higher energies, our approach shows that forbidden regions tend to disappear.
However, at E≫Erk, gradient/curvature drifts from field perturbations near Enceladus become very important.
Since these drifts are not modelled here, we cannot be conclusive at this stage about how forbidden regions would behave.
We also note that our calculations are only done for equatorially mirroring particles (90° pitch angle).
Both equatorial flybys E7 and E8 (observing at approximately 90deg around closest approach) reveal "ramps" in the lowest energy channels of MIMI/LEMMS, on both sides of Enceladus.
We believe that observation of forbidden (or partial) access for keV electrons is indeed visible during flybys E7 and E8.
While in principle the same drift physics discussed before apply for non-equatorially mirroring particles, we expect that drift perturbation effects would be reduced to some extent, especially for the highest energies measured with MIMI/LEMMS.
The reason is that magnetic field perturbations are more intense (compared to the background field parameters) near Enceladus than at high latitudes: non-equatorially mirroring particles are bouncing along the field lines and spend less time near Enceladus during one bounce period.
This may explain for instance why the electron depletion during flybys E0 and E2 are equal to 1 moon diameter, but may not explain why depletions at flybys E3 and E5 become so broad at high energies, although MIMI/LEMMS is oriented far from 90° pitch angle.
Variability of the interaction region between different flybys, may play a role.
We believe answers for these discrepancies may be given by tracing energetic electrons in simulated interaction regions and by also understanding better the exact responses of the MIMI/LEMMS channels.
In addition to the absorption features in the electron data there is also evidence that MIMI/LEMMS detected additional signals correlated with dust measurements from the CDA detector and CAPS spectrometer.
There are three possibilities to explain those additional peaks: (1) directly measured dust, or (2) directly measured electrons, or (3) instrumental effects.
Directly measured charged dust would require that the dust grains entering the low-energy aperture of MIMI/LEMMS are bent in the internal magnetic field towards the electron detector E. Assuming a bend radius of about 1cm with a 800 G internal magnetic field the mass of a singly charged dust grain cannot be higher than 4.3amu which is much lighter than those dust particles measured by the dust instrument CDA and therefore cannot account for the correlation between the peaks in MIMI/LEMMS and CDA.
Another consideration could be that a dust grain hitting the E and F low-energy electron detectors triggers the 15keV threshold.
Having the corresponding velocity of the incoming dust (17.7km/s) would mean that the dust grain has to have a mass of about 9300amu.
However, this corresponds to a gyroradius of about 20m, too big to reach directly detector E or F.
From this it follows that incoming dust particles could only hit detector A directly.
Finally the fact that we observed some of those peaks in the high-energy aperture HE of MIMI/LEMMS makes it difficult to correlate it with direct dust hits.
As described in the instrumentation section HE consists of a set of five detectors where multiple coincidences between the detectors are used to determine ions or electrons and their energies.
HE electron channels like channel E4 are double-coincidence measurements.
This would require that the dust particle entering HE penetrates through the front Aluminum foil, the first detector (150μm thick), the second detector (700μm thick) and gets stuck in detector 3.
This is very unlikely.
Directly measured electrons in the corresponding energy ranges cannot be fully ruled out.
However, the fact that those peaks are highly correlated to the CDA dust measurements would mean that within the high-density plume of gas and dust grains some of the energetic electrons survive and are not lost.
We consider this possibility also not very likely.
Since the MIMI/LEMMS instrument was not built to measure dust we cannot rule out instrumental issues.
If dust reaches one of the detectors inside the low-energy aperture a cloud of plasma could be produced creating electrons during the impact.
However, their energies should only extend up to several hundreds of eV to keV range, still too low to be measured by the dedicated electron detectors with a threshold energy of 15keV.
Another possibility could be that the power supplies of the various detectors "react" after a massive dust particle hit the detectors, creating a false signal at a huge count rate.
Therefore we think that the "dust"-signals in MIMI/LEMMS is a false signal and cannot be used to determine further dust parameters, although they might be caused by the incoming dust grains.
Future investigations
Cassini will continue to perform relatively close encounters with Enceladus, as summarized in Table 3.
Each of the additional encounters will shed further light on the interaction of the material released from the plume and the corotating particles in the saturnian magnetosphere.
In addition the outcome of the studies related to Enceladus can also be helpful in better understanding the environment of other "active" moons in our Solar System, e.g.
the jovian moon Europa where unusual magnetic field disturbances measured during one of the Galileo flybys have been reported by Kivelson et al. (2009), which could be related to dynamic effects in the jovian magnetosphere or by (less effective as compared to Enceladus) plume activity on Europa.
Future missions to Jupiter will be able to study the activity and dynamics of icy satellites in more detail.
Acknowledgments
The German contribution of the MIMI/LEMMS Instrument was in part financed by the German BMWi through the German Space Agency DLR under Contracts 50 OH 0103, 50 OH 0801, 50 OH 0802, 50 OH 1101 and by the Max Planck Society.
G.H.J. and C.S.A. were supported by UK Science and Technology Facilities Council Advanced and Postdoctoral Fellowships, respectively.
We thank Andreas Lagg (MPS) for extensive software support, Martha Kusterer and Jon Vandegriff (both JHUAPL) for reducing the MIMI data, and Getyn Lewis and Lin Gilbert (both MSSL/UCL) for reducing the CAPS/ELS data.

1. A process for preparing a nickel composite hydroxide with a mean particle diameter in the range from 3 to 20 µm (d50) comprising the step(s) of combining
(a) an aqueous solution of water-soluble salts of nickel and of at least one of cobalt and manganese, and, optionally, at least one of Al, Mg, B, or transition metals other than nickel, cobalt, and manganese,
(b) with an aqueous solution of an alkali metal hydroxide and
(c) with an organic acid or alkali or ammonium salt of an organic acid or an anhydride of an organic acid whose nickel(+II) salt has a solubility of 1g/l or less in water at 20°C, and, optionally, with
(d) an aqueous solution of alkali metal aluminate.2. Process according to claim 1 wherein said nickel composite hydroxide contains a combination of transition metals and further metals according to general formula (I)           (NiaCobAlc)1-dMd(I)
a is in the range from 0.60 to 0.95,
b is in the range of from 0.025 to 0.2,
c is in the range of from 0.005 to 0.1, and
d is in the range of from zero to 0.05,
M is selected from Ti, Zr, W, Mo, Nb, and Mg, and combinations of at least two of the foregoing,
wherein a + b + c = 1.0.3. Process according to claim 1 or 2 wherein said process is carried out a pH value in the range of from 11 to 13.4. Process according to any of the preceding claims wherein said process is carried out in a batch mode.5. Process according to any of claims 1 to 3 wherein said process is carried out in a continuous mode.6. Process according to any of the preceding claims wherein said organic acid bears in the range of from one to seven carbon atoms per molecule.7. Process according to any of the preceding claims wherein said organic acid is selected from oxalic acid, formic acid and maleic acid.8. Process according to any of the preceding claims wherein M includes Mg in the range of from 0.1 to 2.5 mole-%, referring to the sum of metals in the respective nickel composite hydroxide.9. Process according to any of the preceding claims wherein water-soluble salts are selected from sulfates and nitrates.10. A nickel composite hydroxide in particulate form with a mean particle diameter in the range from 3 to 20 µm (d50), containing in the range of from 60 to 95 mole-% Ni and at least one transition metal selected from Co and Mn, and in the range of from 0.1 to 3.0 % by weight of carboxylate whose Ni(+II) salt has a solubility in water at 20°C of 1g/l or less, and the percentage referring to said nickel composite hydroxide.11. Nickel composite hydroxide according to claim 10 having a composition according to formula (II)           (NiaCobAlc)1-dMdOx(OH)y(AOA)t(II)  wherein:
a is in the range from 0.60 to 0.95,
b is in the range of from 0.025 to 0.2,
c is in the range of from 0.005 to 0.1, and
d is in the range of from zero to 0.05,
M is selected from Ti, Zr, W, Mo, Nb, and Mg, and combinations of at least two of the foregoing,
AOA is an anion of an organic acid whose nickel(+II) salt has a solubility of 1 g/l or less in water at 20°C,
wherein a + b + c = 1.0 and
0.001 ≤ t ≤ 0.05
0 ≤ x < 1,
1 < y ≤ 2.1.12. Nickel composite hydroxide according to claim 10 or 11, wherein the metal part has a composition according to general formula (III)           (NiaCobAlc)1-dMd(AOA)t(III)  wherein
a is in the range from 0.80 to 0.95,
b is in the range of from 0.025 to 0.195,
c is in the range of from 0.02 to 0.05, and
d is in the range of from zero to 0.035,
M is selected from one or more of Ti, Zr, W, Mo, Nb, and Mg, and combinations of Mg and at least one of Ti, Zr, and W,
0.001 ≤ t ≤ 0.05, and
b + c < 0.20.13. Nickel composite hydroxide according to any of claims 10 to 12 wherein said nickel composite hydroxide has a specific surface (BET) in the range of from 2 to 70 m2/g, determined according to.14. Nickel composite hydroxide according to any of claims 10 to 13 wherein said nickel composite hydroxide has a width of particle diameter distribution, calculated as [(d90-d10)/(d50)] of at least 0.5.15. Use of transition composite metal hydroxides according to any of claims 10 to 14 as precursors for the manufacture of cathode active materials for lithium ion batteries.Low pressure preparation of spherical Si@C@CNT@C anode material for lithium-ion batteries

Spherical Si@C@CNT@C multiple composite was prepared from Si powder. The detailed preparation process is described in Fig. 1. Firstly, 10 g Si powder was put in a chamber and vacuumed for 1 h. After that, 100 mL, 500 g L-1 glucose solution was added and the mixture was pyrolyzed at 650 degC for 5 h in an argon atmosphere. The obtained product was crushed to form Si@C composite and soaked in 0.5 mol L-1 Co(NO3)2 solution for 10 min. Then, the sample was filtrated and coated with CNT at 650 degC for 30 min by chemical vapor deposition (CVD) using acetylene as carbon source. The as-prepared Si@C@CNT was put in the vacuum chamber and vacuumed for 1 h. To form spherical Si@C@CNT@C multiple composite, 10 g pitch was acted as the carbon source for outer layer. The as-obtained sample was pyrolyzed at 800 degC for 5 h in an argon atmosphere and then crushed into smaller particles and granulated by ball-milling. The final product was obtained after a washing by water to remove the dust.
Ni-P and Ni-Cu-P modified carbon catalysts for methanol electro-oxidation in KOH solution
Electroless deposition solution was chosen to be as simple as possible. It consists of 26.27 g L-1 nickel sulphate as a source of nickel, 38.71 g L-1 sodium citrate, 27.2 g L-1 sodium acetate as a source of complexing agent to control the rate of the release of the free metal ions in the reduction solution, 21 g L-1 glycine and 38.12 g L-1 sodium hypophosphite as a source of reducing agent, which also constitutes the source of phosphorus in the deposit. 0.015 g L-1 copper sulphate was added to deposition baths for Ni-Cu-P/C coatings. In addition to other constituents, sodium hydroxide was added as a buffering agent to control the bath pH. The effect of temperature (70-90 degC), pH (7-12) and the deposition time (30-90 min) was investigated to achieve the optimum conditions for the electroless deposition process. All chemicals used for the present work were of analytical reagent grade and the solutions were prepared using double distilled water.Microwave-assisted hydrothermal synthesis of electrochemically active nano-sized Li2MnO3 dispersed on carbon nanotube network for lithium ion batteries
Electrochemically active Li2MnO3/CNT nanocomposite was successfully synthesized by MAH process. The detailed synthesis procedure of MnO2-coated CNT composite through redox reaction is described elsewhere [15]. A 0.1 M of KMnO4 solution and 1.0 g of CNT in double jacket vessel were heated to 70 degC using circulator (FP40, Julabo), and the mixture was maintained at same temperature for 12 h. The black suspension was filtered, washed, and then dried at 100 degC in oven. The LMO/CNT nanocomposite was prepared using MnO2-coated CNT composite with appropriate amount of different concentration of aqueous LiOH solution (0.1 M, 0.2 M, 0.5 M, and 1 M). These solution mixtures were rapidly heated to 200 degC in MAH reactor (MARS, CEM Corp.), maintained at 200 degC for (30 min), and cooled down to room temperature. The suspension was filtered, washed repeatedly with distilled water, and dried at 100 degC for 24 h in oven. The four different samples of LMO/CNT nanocomposites are named as shown in Table 1.

Flexible Fe3O4@Carbon Nanofibers Hierarchically Assembled with MnO2 Particles for High-Performance Supercapacitor Electrodes
Polyacrylonitrile (PAN, Mw = 90,000), polyvinyl alcohol (PVA, Mw = 86,000), Potassium hydroxide (KOH), Fe(acac)3, and KMnO4 were purchased from Aladdin Chemicals Co. Ltd., China. Dimethylformamide (DMF), and sulphuric acid (H2SO4), were provided by Shanghai Chemical Reagents Co., Ltd., China. All chemicals were used without further purification.
A homogenous solution comprising of Fe(acac)3 (1 wt.%) and PAN (8 wt.%) dissolved in DMF by magnetic stirring was electrospun following same conditions as reported in our earlier work18. Resultant precursor fibers were then vacuum dried and pre-oxidized at 280 degC for 2 h followed by carbonization at 800 degC for 2 h with a heating rate of 2 degC/min under nitrogen environment. Developed membrane showed ~50 um thickness after treatment.1. A cathode active material precursor including nickel (Ni) and cobalt (Co) and including an excess amount of Ni,
wherein A101/A001is 1 or more and I101/I001is 1 or more, and
A101is a peak area of (101) plane and A001is a peak area of (001) plane by an X-ray diffraction (XRD) analysis, and I101is a peak intensity of (101) plane and I001is a peak intensity of (001) plane by the XRD analysis.2. The cathode active material precursor according to claim 1, wherein A101/A001is in a range from 1 to 2.3. The cathode active material precursor according to claim 1, wherein I101/I001is in a range from 1 to 2.4. The cathode active material precursor according to claim 1, wherein the cathode active material precursor is represented by Chemical Formula 1:
Ni1−x−y−zCoxMnyMz(OH)2+a[Chemical Formula 1]
wherein, in Chemical Formula 1, 0.02≤x≤0.15, 0≤y≤0.15, 0≤z≤0.1, −0.5≤a≤0.1and M includes at least one of Mg, Sr, Ba, B, Al, Si, Mn, Ti, Zr or W.5. The cathode active material precursor according to claim 4, wherein a content ratio of Ni among Ni, Co and Mn is in a range from 0.75 to 0.95.6. The cathode active material precursor according to claim 5, wherein a content of Co is greater than a content of Mn.7. A cathode active material obtained from the cathode active material precursor according to claim 1.8. The cathode active material according to claim 7, wherein the cathode active material is represented by Chemical Formula 2:
Li1+bNi1−x−y−zCoxMnyMzO2+c[Chemical Formula 2]
wherein, in Chemical Formula 2, −0.05≤b≤0.15, 0.02≤x≤0.15, 0≤y≤0.15, 0≤z≤0.1, −0.5≤c≤0.1, and M includes at least one of Mg, Sr, Ba, B, Al, Si, Mn, Ti, Zr or W.9. The cathode active material according to claim 7, wherein a length of a major axis of a primary particle is 1.5 to 7 times a length of a minor axis of the primary particle10. A lithium secondary battery comprising:
a cathode including a cathode active material obtained from the cathode active material precursor according to claim 1;
an anode; and
a separation layer disposed between the cathode and the negative anode.Dual-chamber measurements of δ13C of soil-respired CO2 partitioned using a field-based three end-member model

Abstract
The contribution of old soil C (SOM) to total soil respiration (RS) in forest has been a crucial topic in global change research, but remains uncertain.
Isotopic methods, such as natural variations in carbon isotope composition (δ13C) of soil respiration, are more frequently being applied, and show promise in separating heterotrophic and autotrophic contributions to RS.
However, natural and artificial modification of δ13CRs can cause isotopic disequilibria in the soil-atmosphere system generating a mismatch between what is usually measured and what process-based models will predict.
Here we report the partitioning of the soil surface CO2 flux in a warm Mediterranean forest into components derived from root, litter/humus, and SOM sources using a new, three end-member mixing model, and compare this with the conventional partitioning into autotrophic and heterotrophic components.
The three end-member mixing model takes into account both the discrimination during CO2 respiration/decomposition of the three components, as well as the fractions of their CO2 fluxes integrated over the total soil profile mass.
In addition, we used a novel dual-chamber technique to ensure that δ13CRs was subjected to minimal artefacts during measurement.
We observed that by using measured soil surface CO2 concentrations as a baseline level for the dual-chamber operation, it was possible to achieve and monitor the necessary conservation of the soil CO2 steady-state diffusion conditions during the measurements, without using permanent collars inserted deeply into the soil.
When RS (8.64 g CO2 m2 d-1) was partitioned into two components, the mean autotrophic and heterotrophic respiration was 56 and 44%, respectively.
When RS was partitioned using the three-way model, however, roots, litter/humus, and SOM contributed 30, 33, and 37% of the total flux.
Our results confirm that to improve the estimates of the partitioning method, it is important to distinguish the fractional contribution of the long-term SOM-derived flux from younger and more labile sources.
Highlights
► Natural variations in carbon isotope composition (δ13C) of soil respiration.
► Dual-chamber technique to ensure that δ13CRs is subjected to minimal artefacts during measurement.
► Three end-member mixing model for partitioning the soil surface CO2 flux.
► Fractional contribution of the long-term SOM-derived flux from younger and more labile sources.

Introduction
Natural variations in carbon isotope composition (δ13C) provide a powerful tool for studying C dynamics in the soil-plant-atmosphere system (Hanson et al., 2000; Ehleringer et al., 2000; Bowling et al., 2008), including partitioning of the autotrophic and heterotrophic components of the surface flux, RS.
Reliably estimating the heterotrophic component of RS is crucial for the characterisation of an ecosystem's net C balance.
The difference between the C fluxes arising from soil heterotrophic respiration and net primary production defines an ecosystem's net exchange of C, which determines if the system is a net source of, or sink for, atmospheric CO2.
It is therefore crucial to estimate the contributions of individual components of RS, e.g., CO2 derived from autotrophic root respiration or from heterotrophic respiration of microbes decomposing recent litter, or old soil organic matter (SOM).
Normally, three-way partitioning can be achieved only using two isotopes.
Here we report a new approach that combines δ13C measurements of CO2 derived from SOM, roots and litter with the corresponding mass-dependent CO2 fluxes to estimate their relative and absolute contributions to RS.
Generally, chemical and biochemical processes accumulate the lighter isotope in the product, leaving the substrate enriched in the heavier isotope (Högberg and Ekblad, 1996).
This fractionation can differ among biological processes.
Recently, Werth and Kuzyakov (2010) reviewed 13C/12C fractionations by biotic processes occurring at the root-microbe-soil interface.
They reported that organic C in the root tissues of C3 vegetation are generally 13C-enriched by 1.2 ± 0.6‰ compared with the aboveground tissues, whereas the CO2 respired by roots is depleted by 2.1 ± 2.6‰, compared with root organic C.
Under C3 vegetation, microbial CO2 is enriched by 0.7 ± 2.8‰ in comparison to the C in SOM.
The causes of δ13C variation in respired CO2 can be summarised as: (i) isotopic effects during the metabolic synthesis of secondary compounds (e.g., lipids, lignin, cellulose); (ii) variation in photosynthetic discrimination against 13C combined with temporal lags in the movement of photosynthates through plant and soil pools; (iii) shifts in microbial δ13C due to anaplerotic CO2 fixation; (iv) preferential use of certain respiratory substrates by microbes; and (v) kinetic fractionation of 13C/12C during microbial respiration (Tu and Dawson, 2005; Bowling et al., 2008).
Most of the C in SOM derives from the microbial degradation of plant litter which consists of polysaccharides and more depleted lignin 13C.
Since easily decomposable substances like glucose or sucrose are preferentially decomposed by microbes, the balance between litter-derived compounds retained in the soil has a direct impact, via isotopic mass balance, on the net δ13C of SOM (Cotrufo et al., 2005).
However, the δ13C of SOM in forest ecosystems generally increases (i.e., becomes more negative) with soil depth by approximately 1-3‰ relative to that of the litter layer (Nadelhoffer and Fry, 1988).
The reasons for this are still unclear (Boström et al., 2007).
Possible explanations include the accumulated influence of isotopic discrimination during selective microbial decomposition of specific substrates within the SOM (Nadelhoffer and Fry, 1988; Buchmann et al., 1997; Lin et al., 1999), and more importantly by the increased contribution of microbially derived C to SOM with depth (Ehleringer et al., 2000; Högberg et al., 2005).
The use of δ13C for partitioning the contribution of different sources of RS has the advantage of permitting in situ measurements with minimal disturbance to soil and roots (Hanson et al., 2000; Paterson et al., 2009).
Its success relies on distinguishing small differences in δ13C of CO2 produced by autotrophic and heterotrophic respiration, and it is frequently applied in combination with incubation of excised roots, litter, and SOM samples.
Studies of variations in δ13C of RS commonly use mass balance equations to estimate the proportions of two (and rarely more than two) C sources that contribute to the soil surface CO2 flux (Lin et al., 1999; Ngao et al., 2005; Sakata et al., 2007; Millard et al., 2010).
Isotopic linear mixing models partition the contributions of n + 1 sources, when n isotopically distinct tracers (e.g., δ13C, δ15N, δ18O) are used as end-members, and assume that source signatures that fall closest to that of the mixture provide the greatest contribution.
Factors such as the seasonal variability of the photosynthetic discrimination, as well as environmental conditions such as soil temperature and soil moisture, can vary the differences in δ13C between CO2 derived from different C pools, and are therefore important constraints on the partitioning method (Phillips and Gregg, 2001; Trumbore, 2006).
The isotopic approach also needs to overcome uncertainties associated with the binary diffusion of 12CO2 and 13CO2 through the soil's gas-filled pore space and across the soil-air interface.
Generally, the slower molecular diffusivity of 13CO2 can induce CO2 in bulk soil to be up to 4.4‰ more enriched in 13C than that released at the soil surface δ13CRs (Amundson et al., 1998; Cerling et al., 1991).
Some complications in the interpretation of diffusive flux profiles were discussed by Koehler et al. (2010).
Transient conditions in diffusive flux profiles can be caused by time-varying respiratory CO2 production (Moyes et al., 2010).
In addition, changes in atmospheric pressure and wind effects (advection) can alter the diffusion of soil CO2 (Dudziak and Halas, 1996), From a diffusion experiment involving artificial soil and CO2 source, Kayler et al. (2008) concluded that non-steady-state effects must be considered in field investigations of δ13CRs in soils.
However, the correct interpretation of δ13CRs data is likely determined by the possibility of using measuring techniques which avoid perturbation of the steady-state diffusion conditions of respiration.
Soil surface chambers are the most commonly used technique to measure δ13CRs.
Such approaches include: (i) purging of atmospheric δ13C-CO2 in the chamber headspace (Flanagan et al., 1996; Buchmann and Ehleringer, 1998), (ii) long-term chamber deployment for δ13CRs equilibration (Mora and Raich, 2007), and (iii) the maintenance of chamber headspace at atmospheric CO2 concentration, [CO2] (Subke et al., 2004a; Bertolini et al., 2006; Midwood et al., 2008).
However, several decades of experience with chamber-based measurements have revealed numerous potential sources of errors in measuring soil respiration (Davidson et al., 2002).
Some recent studies on the use of closed- and open-chamber systems reported evidence of perturbation of the δ13C steady-state diffusion profile and 12CO2 and 13CO2 gradients (Ohlsson et al., 2005; Nickerson and Risk, 2009; Gamnitzer et al., 2011).
In addition, the installation of soil collars several centimetres into the soil, to minimize CO2 leakage in and out of the chamber (Hutchinson and Livingston, 2001), has raised concerns regarding the possibility of measuring δ13CRs beyond the collar insertion depth (Kayler et al., 2008), and their utility in soil respiration partitioning studies (Heinemeyer et al., 2011).
To address these concerns, we developed and used a dual-chamber technique to minimize the artefacts and biases in chamber-based measurements of δ13CRs.
In addition, we aimed to partition the soil surface CO2 flux into its components derived from root, litter/humus, and SOM sources and, alternatively, into autotrophic and heterotrophic components.
Materials and methods
Background and principles of the dual-chamber development
In general, to correctly measure δ13CRs it is essential to maintain unbiased the diffusion conditions of RS at soil surface.
The reason for this assumption is as follows.
The mass of 13C is larger than that of 12C and diffuses through the soil at a slower rate leading to a theoretical kinetic fractionation of 13C and 12C.
This means that for estimates of δ13CRs obtained using gas samples withdrawn from the soil profile, a correction of up to 4.4‰ is necessary to account for this fractionation (Amundson et al., 1998).
Although transient changes in soil diffusivity and CO2 concentration gradient might constantly alter the relative diffusion rates of 12CO2 and 13CO2 isotopologues, Cerling et al. (1991) demonstrated that if soil respiration is at a diffusive steady-state, the δ13C of soil surface flux should theoretically match the δ13C produced within the soil.
Thus the measurements made at the soil surface do not need to be corrected for fractionation due to diffusion.
This assumption means that a chamber technique used for δ13CRs measurements has to achieve and maintain over few hours after its deployment the equilibrium between the soil CO2 efflux and the RS captured inside the chamber headspace.
Depending on the type of soil surface chamber used, RS measurements are generally subject to disturbances which can have a large impact on the certainty of δ13CRs estimates and on subsequent source partitioning.
Some artefacts and biases in steady-state and non-steady-state respiration chambers measurements were discussed by Davidson et al. (2002).
For steady-state chambers, these can be summarized as: (i) lateral and vertical diffusion of CO2 produced below the chamber base due to the alteration of the diffusion gradient (Hutchinson and Livingston, 2001; Davidson et al., 2002; Nickerson and Risk, 2009); (ii) the risk of pressure differentials between the headspace volume and the surrounding atmosphere due to the improper control of gas flow rates (Hutchinson et al., 2000); (iii) over-pressurization of the headspace volume when pressurized CO2-free air is forced through to control the CO2 concentration inside the chambers (Lund et al., 1999); (iv) the exchange of gas through the vent in over-pressurized conditions which can dilute chamber air with ambient air (Longdoz et al., 2000); and (v) the alteration of concentration gradients at the soil surface due to excessive turbulence caused by fans inside the chambers used for air circulation (Le Dantec et al., 1999).
In general, it is difficult to make a clear distinction between the effects of chamber measurements and the transient changes of biological activities and gas transport on δ13CRs.
Nickerson and Risk (2009) predicted under lateral diffusion scenarios an error of estimation ranging around 4‰ for steady-state chambers, with a maximum of 15‰ for non-steady-state respiration.
However, the magnitude of the disturbance created by chambers depends on the soil CO2 production rate, and can vary with soil moisture conditions (Phillips et al., 2010).
For many surface chamber techniques, δ13CO2 measurements from dry soils are more biased towards enriched values than δ13CO2 measurements from wet soils.
In order to reduce the above artefacts we modified the steady-state chamber by introducing inside the headspace a horizontal CO2-permeable membrane to create a dual-chamber with two physically separate volumes.
The top-chamber (Ct; Fig. 1) has the characteristics and functions of a conventional steady-state chamber.
The CO2-permeable membrane effectively establishes a 'virtual' soil surface that mimics conditions at the real surface.
In the bottom-chamber (Cb), CO2 accumulates in response to a concentration gradient that matches that in undisturbed soil, so minimising many of the artefacts listed above.
This is achieved by continually adjusting the CO2 concentration in Ct to the CO2 concentration measured in a nearby open-topped reference chamber (Cr).
When the CO2 concentrations in Ct and Cr are matched, CO2 can be sampled from the bottom-chamber, Cb, for δ13C determination.
This system also eliminates the use of soil collars, permitting quicker equilibration of CO2 concentration for the δ13CRs measurements, and minimal severing of soil surface roots and hyphae.
Depending on the ecosystem type, the season, the time of the day, and the measuring technique used, the CO2 concentrations within a few centimetres of the soil surface are elevated above ambient atmospheric values reaching concentrations up to 10 times higher that in the atmosphere (Risk et al., 2002; Davidson et al., 2006; DeSutter et al., 2008; Albanito et al., 2009).
For this reason, when a background ambient concentration is used in both steady-state and non-steady-state chambers, the vertical gradient of CO2 concentration at soil surface must be carefully considered in order to avoid the alteration of the soil profile CO2 diffusion gradient (Davidson et al., 2002).
Following the method described by Albanito et al. (2009), in this study we used a soil surface diffusion chamber (here for simplicity named as reference chamber (Cr)), which permitted the estimation of surface CO2 concentrations at the locations where the chambers would successively be deployed.
Design and use of the dual-chamber
The chamber consisted of three parts: (i) polypropylene plain tubing of 12 cm inner diameter and 25.6 cm tall; (ii) a micro polyvinylidene difluoride (PVDF) flat membrane with a wall thickness of 0.06-0.08 mm and a nominal pore size of 0.2 μm (American Membrane Co., Ltd, Nantong City, Jiangsu, China); and (iii) a polypropylene double socket slip coupling (Fig. 1).
The tubing was divided horizontally into two sections, the PVDF membrane installed between them to physically separate the upper and lower sections and the slip coupling used to connect the sections.
The lower section was 15.2 cm tall and the upper was 10.4 cm tall.
The PVDF membrane had a CO2 diffusion coefficient varying from 10-9 to 10-7 m2 s-1, depending on pressure, CO2 concentrations, and temperature (Briscoe et al., 1998; Albanito et al., 2009).
The top section was closed with a lid provided with an open vent.
The bottom section had an open, bevelled base inserted into the ground.
Both chambers had a pair of nylon sampling tubes of 4 mm of inner diameter and 2 mm wall thickness attached on opposite sides to allow gas to be sampled continually for determination of CO2 concentrations and δ13C.
Finally, the dual-chambers were covered with reflective aluminium tape to minimize heating of the chamber headspace under sunny conditions.
Following the method described by Midwood et al. (2008), Ct was adjusted using an inflow to the top-chamber of CO2-free air (from a compressed-air cylinder or soda lime absorber), and an outflow from the chamber withdrawn by a diaphragm pump.
To prevent incursion of ambient air inside the top-chamber, the flow rate of the CO2-free air was maintained 10% higher than that from the chamber, ensuring a continuous outflow via an open vent positioned on the top of the chamber.
The bottom section of the dual-chamber was in constant contact with the soil surface.
Continuous measurements of 12C16O2 and 13C16O2 mixing ratios and CO2 concentrations were made by connecting the bottom-chamber via a closed sample loop to a cavity ring-down spectrometer (CRDS; G1111-i, Picarro, Inc., Sunnyvale, CA, USA).
The closed loop to/from the CRDS had a constant flow rate of 22 ml min-1 which maintained a constant volume of sampling air and avoided the suction of atmospheric air around the base of the bottom-chamber.
In addition, to minimize air pressure fluctuations within the bottom-chamber caused by the CRDS-controlled flow, a nylon gas sampling tube crossed the chamber.
This was 12 cm long, 0.5 cm inner diameter, and was perforated throughout its surface by 0.5 mm diameter holes.
Following the method described by Albanito et al. (2009), the reference chamber was cylindrical, 12 cm diameter and 20 cm tall.
The open lower end of the chamber was bevelled for insertion into the ground.
The upper end of the chamber was covered by a PVDF membrane.
The chamber had two nylon tubes, 0.4 cm inner diameter, on opposite sides and connected to an IRGA (EGM-4; PP Systems Hitchin, Hertfordshire, UK).
One tube drew air from the chamber and the other returned it after analysis, maintaining a constant sampling volume of air, and avoiding the suction of atmospheric air inside the bottom-chamber.
Measuring CO2 isotope composition and surface flux with the dual-chambers
In order to test the operation of the dual-chambers, measurements were made in a glasshouse using a mesocosm of 150 cm long, 100 cm wide, filled with a soil depth of 15 cm.
The mesocosm had been sown with Lolium perenne (perennial rye-grass) the previous year and kept in a glasshouse under optimal growing conditions for approximately four months.
Approximately 15 min before the beginning of the experiment, the aboveground vegetation was removed under the dual-chamber, as well as the reference chamber which was deployed at 30 cm distance from the dual-chamber.
The CRDS continuous measurements of δ13C-CO2, and CO2 concentration (Cb) from the bottom-chamber, started before deploying the dual-chamber on the ground.
Field measurements of RS (g CO2 m-2 h-1) were carried out from the 27th April to the 6th May 2011 in the San Rossore Regional Park, Tuscany, Italy (Longitude 10°17′13″ East and Latitude 43°43′47″ North).
The experimental area was situated in a semi-managed Mediterranean pine forest planted in 1964 at a distance of 800 m east from the seashore.
The site was characterized by the presence of 20 m tall mixed stands of Pinus pinaster, Pinus pinea, and some Quercus ilex.
The ground vegetation was sparse and consisted of Erica arborea, Phyllirea angustifolia, Rhamnus alaternus, and Myrtus communis.
During the measurements, the dual-chambers were deployed within an area of 4 m2 without altering the litter layer, and inserting the base of the dual-chamber into the soil to a depth of approximately 2-3 cm.
Four dual-chambers were connected to a multichannel gas sampling control system (details below) programmed to perform a 2 h routine of four sequential 30 min sub-cycles of measurements, one per dual-chamber.
Therefore, the sample airstream between the bottom-chambers and the CRDS was automatically switched between the four dual-chambers by the sub-routine program.
As with the bottom-chambers, the top-chambers were monitored every 30 min.
Isotopic equilibration is essential for the correct operation of the dual-chambers, and the subsequent measurement of δ13CRs from each location.
To achieve this, the dual-chambers were deployed on the soil for three 2 h routines (i.e., 6 h in total).
δ13CRs was calculated using data collected after two routines (4 h).
Generally, the CRDS required between 10 and 15 min to stabilize its reading, and to accurately measure δ13C from each dual-chamber.
Therefore, within the third routine, we discarded the initial 15 min of measurements, and we averaged the last 15 min of data in each sub-cycle.
Samples were collected and analysed throughout daylight hours, and separated into those collected in the mornings and afternoons to detect any short-term temporal changes in CO2 flux and δ13CRs.
The gas control system comprised twelve 2-way solenoid valves (ACL s.r.l, 20 040 Cavenago di Brianza, Milan, Italy), eight mass flow controllers (Omega Engineering Ltd, Manchester, UK), four diaphragm pumps (TD-3, Brailsford and Co.
Inc., Antrim, NH, USA), two analogue output modules (SDM-CV04) and one input/output module (SDM-CD 16AC) controlled by a data logger (CR 1000; Campbell Scientific Ltd., Loughborough, UK), and an infrared gas analyser (IRGA; Li840, LiCor Biosciences, Cambridge, UK).
One manifold of four 2-way solenoid valves was used for switching the outlet airstream from each of the top-chambers towards the IRGA.
Two manifolds of four 2-way solenoid valves controlled the inlet and outlet airstreams from the bottom-chambers.
The reference CO2 concentration (Cr) was measured within the 4 m2 of soil surface where the dual-chambers were deployed.
During the field measurements, Cr provided a starting point for achieving the CO2 efflux equilibration within the dual-chamber volume.
Depending on the Cr to be maintained in the top-chamber, the flow rate across the top-chamber ranged from 15 to 40 ml min-1.
This rate was kept similar for all the dual-chambers.
After initial care was taken to adjust Ct in all the dual-chambers to be similar to Cr, the continuous supply of CO2-free air into the top-chambers permitted the CO2 concentration to increase and decrease linearly and therefore to achieve equilibrium between the CO2 flux through the PVDF membrane and the soil efflux.
The CO2 diffusion coefficient across the dual chamber (Dc) was calculated from the following equation:(1)DC=RS×(L/(Cb-Ct))where L is the distance between the tubes inserted at the bottom and top-chambers (156 mm).
Finally, soil temperature (TS) and soil moisture (θ) were monitored continuously within 0.2 m of the dual-chamber locations at 5 cm depth using four thermistors (107, Campbell Scientific Ltd., Loughborough, UK), and four soil moisture sensors (ThetaProbe ML1, Delta-T, Cambridge, UK), respectively.
Measuring the isotopic signatures and CO2 respiration/mineralization rates of root, litter/humus, and SOM end-members
To obtain end-member δ13C values of CO2 to allow RS to be partitioned into its component fluxes, it was necessary to incubate root, litter, and SOM fractions sampled from were RS had been measured and to collect the CO2 produced by each fraction in order to measure their δ13C values.
Soil cores, 25 cm deep and 78.5 cm2 cross-sectional area, were extracted soon after the dual-chamber measurements were completed, and from midway between each pair of chambers.
Each core was divided horizontally into two layers distinguishable by their characteristic colour, dark-brown for the top 10 cm (litter and humus), and dark sandy colour for the remaining 15 cm (SOM).
The top layer of the core typically comprised a litter about 5 cm and characterized by dense fine roots and a sandy dark-brown horizon of humic substance.
The remaining compact sandy layer was more uniform in colour and texture with sparse roots.
These layers were easily separated by hand.
Separating roots from each layer was carried out by sieving samples through 10 and 4 mm sieves.
Roots were then brushed free of soil, and all the material collected was weighed.
A subsample of each of the three pools was quickly incubated in separate gas-tight Tedlar bags (Keika Ventures, Chapel Hill, NC, USA).
Air in the bags was evacuated, flushed three times with CO2-free air, and filled with a measured volume of CO2-free air before starting the incubation (Millard et al., 2010).
Bags containing litter/humus or SOM were filled with 2 L of CO2-free air, whereas for the smaller root samples 1 L was sufficient to achieve the necessary CO2 concentration increase.
To minimize the influence of disturbance on respiration rates and isotopic discrimination, the measurement of CO2 production began as soon as possible after separation of the litter and SOM.
The incubations of the excised roots started after 30 min as suggested by Subke et al. (2004b).
Preliminary field tests showed that 30 min of incubation time was the minimum required to achieve stable measurements of δ13C from the three end-members.
Sub-samples of incubated roots, litter/humus, and SOM were collected for measurements of the δ13C values of the solid matter by isotope ratio mass spectroscopy (Delta Plus XP, Thermo Finnigan, Hemel Hempstead, Hertfordshire, UK).
CO2 fluxes from roots, litter/humus, and SOM were calculated using the short-term respiration potential (r) (i.e., C respiration/mineralization rate (g CO2 g root-1 h-1, g CO2 g soil-1 h-1)) described by Robertson et al. (1999).
After the CO2 concentrations were converted from ppm to mass units (μg CO2 litre-1), r was calculated:(2)r=Crate×VWwhere Crate is the change in CO2 concentration over the initial 5 min of incubation (g CO2 litre-1 h-1), V is the headspace volume of the Tedlar bag (litre), and W is the mass (g) of incubated litter/humus, SOM or root.
Derivation of the three end-member mixing model
Assuming that roots, litter/humus, and SOM are the only significant contributors to the total soil surface flux of CO2, RS was partitioned among these three components using the δ13C values of RS (obtained with the dual-chamber), and the δ13C values of the CO2 produced from the three sources, as well as the mass-specific rate of CO2 production by each source (obtained from the incubations).
The three end-member mixing model was formulated from the following mass balance equation:(3)δ13CRS=δ13CR×fR+δ13CL×fL+δ13CSOM×fSOMwhere δ13CR, δ13CL and δ13CSOM are, respectively, the δ13C values (‰) of CO2 produced by roots, litter/humus, and SOM. fR, fL and fSOM are the fractional contributions of each source to RS.
Each fraction can be defined in terms of the absolute fluxes of CO2 and their corresponding δ13C values (Phillips and Gregg, 2001):(4)fR=(δCRS-δCSOM)(FL-FSOM)-(δCL-δCSOM)(RS-FSOM)(δCR-δCSOM)(FL-FSOM)-(δCL-δCSOM)(FR-FSOM)(5)fL=(δCRS-δCSOM)(δCL-δCSOM)-(δCR-δCSOM)(δCL-δCSOM)fR(6)fSOM=1-fR-fLwhere FR, FL, and FSOM (g CO2 h-1) are obtained using the incubation fluxes (r) from sample incubations scaled-up to the whole soil cores weight.
Here, the units of RS were converted from g CO2 m-2 h-1 to g CO2 h-1.
We assumed that: (i) during the first step of microbial decomposition, kinetic 13C/12C discrimination does not occur and, therefore, that CO2 respired during litter/humus, and SOM degradation had the same δ13C values as the respective source C; (ii) FR, FL, and FSOM estimated from the incubations are representative of the in situ respiration/mineralization CO2 fluxes.
The last assumption might include some potential uncertainties.
The incubated soil was disturbed by sieving, which might have released some labile carbon substrates for rhizosphere priming (RP) and microbial respiration during the incubations and potentially bias the proportional respiration of litter and SOM.
In order to minimize this possible disturbance effect in our calculations, we incubated the soil within a few minutes of extraction.
However, a previous study (Dijkstra and Cheng, 2007) showed that the RP effect of two tree species on decomposition of three different soils persisted throughout a 395-day experiment, until well after the initial disturbance-induced labile carbon substrates were depleted by respiration.
Therefore, it is unlikely that the disturbance during soil preparation significantly alter the respiration of RP (Zhu and Cheng, 2011).
Second, the possible change of the proportional microbial respiration of SOM due to the release of labile carbon substrates would possibly tend to overestimate our estimations.
But it should be the least concern for this study because, based on Eq.
(4), the overestimation of the SOM efflux during the incubation would compensate for the proportion of soil profile excluded in calculation of FSOM using Eq. (2).
For comparison with traditional two-source partitioning approaches, RS was also partitioned between autotrophic (Ra) and heterotrophic respiration (Rh) using a two end-member mixing model:(7)Ra=(δCRS-δCSOM)(δCR-δCSOM)RS(8)Rh=(1-Ra)RSwhere δ13CR and δ13CSOM (defined in Eq.
(3)) are the two isotopic end-members.
13C field gas analyses
δ13C values (‰) of CO2 samples analysed by the CRDS were calculated using the Pee Dee Belemnite (PDB) standard.
The CO2 molar absorptivity of the CRDS was factory-calibrated, but large changes in background CO2 concentration in the field can cause pressure-broadening effects on the CRDS analysis.
To account for this, CO2 standards with different δ13C values were analysed with the CRDS and compared with IRMS measurements.
This calibration comprised 36 gas samples with δ13C values ranging from -35.9 to -17.7‰ and CO2 concentrations from 348 to 3000 ppm.
Differences between the CRDS and the IRMS analyses ranged from +0.38 ± 0.75‰ (mean ± SD) for CO2 concentrations below 1000 ppm, -0.27 ± 0.27‰ between 1000 and 2000 ppm, and -0.28 ± 0.30‰ between 2000 and 3000 ppm.
The linearity of the CRDS was checked throughout the field campaign using two reference cylinders of CO2 balanced with N2 (500 ppm: δ13C = -32.9 ± 0.59‰; 1000 ppm: δ13C = -35.8 ± 0.57‰ (n = 5); BOC Ltd., Guildford, UK).
Statistical analysis
A one-way ANOVA followed by a Tukey post hoc test was used to identify differences between the measurements collected in the morning and in the afternoon.
Pearson correlation was employed to examine relationships between the dual-chamber measurements, as well as the incubation measurements, and environmental variables.
Significant effects were determined at p < 0.05.
The analysis was carried out using the SPSS 19 software package (Inc.
Chicago, IL, USA).
We used a first-order Taylor series approximation (Phillips and Gregg, 2001), to calculate the variance, standard errors (SE), and confidence intervals for sources proportions that account for variability in the isotopic signature of both the sources and the CO2 efflux (Taylor, 1982).
Results
RS and isotopic equilibration in the dual-chamber
Fig. 2 shows an experimental test of the operation of the dual-chamber using the grassland mesocosm.
Immediately after the deployment of the dual-chamber in the grassland mesocosm the accumulation of CO2 in the top-chamber was controlled using the inflow of CO2-free air to match the CO2 concentrations in the top-chamber to that measured in the nearby reference chamber (≈800 ppm).
CO2 concentration increased rapidly inside both sections of the dual-chamber (Fig. 2b).
The initially higher CO2 concentration in the top-chamber produced a downwards CO2 flux across the PVDF membrane which generated a negative RS (Fig. 2c).
This downwards CO2 concentration gradient between the two sections of the dual-chamber was probably due to the exchange of atmospheric CO2 between the closed loop with the CRDS and the inner volume of the bottom-chamber.
In addition, the initial adjustments of air flow rate occurring in the top-chamber forced the CO2 diffusion coefficient across the membrane (Dc) to fluctuate from negative to positive values until more stabilized conditions were reached.
However, following the gradual adjustment of the top-chamber CO2 concentration to the reference CO2 concentration, the RS and its δ13C stabilised after about 3 h to values for RS of 0.45 g CO2 m-2 h-1, δ13CRs of -28.5 ± 0.3‰ and Dc of 8.4 ± 0.4 mm2 s-1 (mean ± SD, n measurements = 60).
Fluxes and δ13C of CO2 derived from roots, litter/humus, and SOM during incubations
In general, the δ13C of CO2 respired from roots, litter/humus, and SOM during incubations of samples collected at San Rossore stabilized after about 10 min (Fig. 3a).
After stabilization, δ13CSOM was more 13C-enriched (-22.2 ± 0.5‰, mean ± SE) than δ13CL (-28.3 ± 1‰) and δ13CR (-30.3 ± 0.5‰) (Table 1).
Among the three end-members, only δ13CSOM showed a significant difference between morning and afternoon by about -1‰ (P = 0.02).
In addition, the δ13C of root tissues, litter/humus, and SOM collected at the field site were -27.9 ± 0.14‰, -27.3 ± 0.41‰ and -25.2 ± 0.73‰ respectively.
The evolution of CO2 concentrations during the incubations was strictly dependent on the weight of roots, litter/humus, and SOM incubated (Fig. 3b).
However, the mass-specific rates of CO2 production (r) from roots, calculated with Eq.
(2), were about 5-8 times greater than from litter/humus, and SOM (Table 1).
rroot was significantly faster in the afternoon than in the morning incubations by approximately one-third (P = 0.02).
Conversely, litter/humus, and SOM respiration showed only minor and insignificant differences between the afternoon and the morning incubations.
When the incubation fluxes were scaled-up to the whole soil core (F), however, about ten-times more CO2 was derived from humus/litter, and SOM combined than from roots.
Again, Froot was significantly greater (two-fold) in the afternoon than the morning (P = 0.02); no such difference was detected for the fluxes from the other components.
Contribution of roots, litter/humus, and SOM to RS
Throughout the measurement period at San Rossore, 56 separate sets of readings were made (Table 2).
TS and θ ranged from 13.7 to 17.4 °C (14.8 ± 1.2, mean ± SD), and from 7.9 to 20.7% (13.2 ± 2), respectively.
Ct measured in the top compartment of the dual-chambers, and, we assume, reflecting the CO2 concentration at the soil surface, varied from 368 to 1050 ppm (659.5 ± 103).
Cb ranged from 527.4 to 1423.4 ppm (922.7 ± 148).
The ability of the dual-chambers in replicating the soil CO2 efflux across the CO2-permeable membrane was also controlled by the values of Dc which varied from 2.8 to 8.3 mm2 s-1 (5.2 ± 1.8), while RS varied from 0.19 to 0.82 g CO2 m-2 h-1 (0.36 ± 0.02).
δ13CRs ranged from -22.6 to -29.6‰ (-26.6 ± 0.2), was significantly more depleted in the afternoon than in the morning by-1‰ (P = 0.026), and was better correlated with TS in the afternoon (R2 = 0.8, P < 0.001) than the morning (R2 = 0.1 P = 0.6).
In addition, changes in δ13CRs were significantly and positively correlated with the incubation results of δ13CR, δ13CL, and δ13CSOM in the morning (R2 = 0.5, 0.45, and 0.41, respectively), and only to δ13CR and δ13CSOM in the afternoon (R2 = 0.5 and 0.78).
The three end-member mixing model suggested that each component contributed about one-third of the total soil surface flux (Table 3).
Mean SOM-derived flux (fSOM), 0.13 g CO2 g soil-1 h-1, comprised 37% of the total soil surface flux, with the highest contribution.
fL was 33% with a mean contribution of 0.11 g CO2 g soil-1 h-1, and fR was 30% with a CO2 flux of 0.10 g CO2 g soil-1 h-1.
This partitioning can also be shown graphically by plotting mean δ13CRs within its confidence limits inside the area bounded by the three end-members (Fig. 4).
As for RS, fR, fL, and fSOM did not show a significant difference between the morning and afternoon measurements.
Overall, fL showed the highest variability, with a SE in the afternoon (15%, ±t0.05,dƒ = 14-75%), while fSOM showed the lowest SEs (5%, ±t0.05,dƒ = 27-48%).
The partitioning obtained using the two end-member mixing model suggested that the autotrophic flux of 0.18 g CO2 m-2 h-1contributed the larger fraction of RS, 56%, while the heterotrophic component flux of 0.15 g CO2 m-2 h-1accounted for the remaining 44% (Table 3).
As for the three end-member model, there was little evidence of large differences in these figures between morning and afternoon.
Discussion
This study has provided new information about the contributions of belowground components to the CO2 flux at the soil surface, RS, and has shown an alternative approach to partition RS components using their 13C signatures.
This was possible because of the development of the novel dual-chamber technique to ensure that δ13CRs was subjected to minimal artefacts during measurement, and the combination of δ13C and CO2 flux data to achieve a three-way partitioning of the soil surface flux, RS.
Partitioning of RS
CO2 emitted at the soil surface is derived from the microbial breakdown of various organic materials: root carbohydrates and exudates (both of which are derivatives of recent photosynthate), microbial biomass, mycorrhizal fungi, fresh litter, dead roots and SOM.
These materials have different chemical compositions, different decomposabilities and, therefore, make variable contributions to the total soil-derived CO2 flux.
Separating these distinct heterotrophic components is necessary for assessing how environmental changes may alter the carbon balance in belowground ecosystems.
As explained in the Introduction, it is the flux from SOM, usually the oldest and most durable C belowground C store that, with NPP, determines an ecosystem's net C balance.
For that reason, it is important to distinguish the SOM-derived flux from the others.
When RS was partitioned in two ways at San Rossore, the mean autotrophic and heterotrophic respiration was 56 and 44% of RS, respectively.
These results are similar to most reported values in temperate forests (Kelting et al., 1998; Hanson et al., 2000).
By contrast, when RS was partitioned three ways, roots, litter/humus, and SOM contributed 30, 33, and 37% of the total flux, respectively, which are similar to other studies in forest which partition RS among root, litter, and SOM (Tedeschi et al., 2006; Lin et al., 1999).
However, the mean heterotrophic flux obtained with the three- way partition was higher than those obtained from the two-way partition, 0.15 and 0.23 g CO2 m-2 h-1, respectively.
This means that during the spring, the litter and humus collectively represent the major component of RS in San Rossore.
Moreover RL had the highest variability among the three C pools, while RSOM was fairly constant throughout the study period (Table 3).
This may be explained by the mycorrhizal hyphal respiration which, being particularly sensitive to changes of soil surface moisture (Heinemeyer et al., 2007), could have contributed to increasing the variability of RL.
Researchers must always consider the uncertainty inherent in isotope measurements due to artefacts, spatial heterogeneity in the soil diffusive environment, possible advective processes which can cause transient changes in relative soil diffusion rates of 12CO2 and 13CO2 isotopologues, and time-varying 13C signals of different soil carbon pool.
We discuss here some of the uncertainties associated with the partitioning of RS obtained using our method.
First, the litter/humus layer is characterized by a dense proliferation of fine roots, mycorrhizal roots and extra-radical mycelium characterized by 13C discrimination in respired CO2 very similar to that of autotrophic respiration.
The similarity between δ13CR and δ13CL represents a limiting factor for the partitioning power of the three end-member mixing model.
In San Rossore, depending on the time of day, δ13CR and δ13CL were separated by 2-3‰.
However, CO2 emitted at the soil surface is a mixture of sources which depends not only on the sources involved, but also on the depth at which CO2 is produced from them and from which it is transported to the surface (Maseyk et al., 2009).
Hence, to correctly partition RS we need to understand the contribution of the depth-related enrichment of soil δ13C-CO2 to surface measurements of δ13CRs.
In that respect, the three end-member mixing model here reported takes into account the discriminations during CO2 respiration/decomposition of three bulk soil C pools, as well as the fractions of their CO2 flux integrated over their total mass.
The approximation of the litter/humus layer as a discrete component permitted a more accurate estimation of the flux from the most durable forms of C stored belowground.
Although further investigation is still required to identify the applicability of the model in different mineral and organic soils, the promising results obtained in the mineral soil of San Rossore were also obtained in a loamy skeleton podsol-brown earth soil of a spruce stand in Germany (data not shown).
However, in that case the model required an increase of the sampling depth to 40 cm, and the removal of the stone weight from the calculation of the mass-specific rate of CO2 production of the three end-members.
By contrast, in the two-way partitioning, the respiration from the contribution of the litter/humus layer was divided between the autotrophic and heterotrophic respiration.
When using that method, the soil depth from which the heterotrophic end-member is collected can usually be chosen arbitrarily assuming that unmeasured components or tracers have the same concentrations or do not contribute significantly to RS and δ13CRs.
This means that the estimation from the two-way partition could result in higher uncertainties due to a possible over- or under-estimation of the heterotrophic component.
Measuring δ13CR at the soil surface
Since Cerling et al. (1991) demonstrated that if soil respiration is at a diffusive steady-state, the δ13C of the soil surface flux should theoretically match the δ13C produced within the soil, much effort has been put into measuring the δ13C of soil surface CO2.
To date, several studies have been published using chamber techniques to measure δ13CRs (Subke et al., 2004a; Ohlsson et al., 2005; Bertolini et al., 2006; Mora and Raich, 2007; Midwood et al., 2008).
However, due to the conflicting results regarding the capability of the closed and open-chamber methods to provide unbiased measurements of δ13CRs, a degree of uncertainty still remains in the use of these techniques (Nickerson and Risk, 2009).
Approximately a decade ago, Davidson et al. (2002), summarizing a few decades of experience with chamber-based measurements, discussed several concerns about uncertainties associated with chamber-based measurements of CO2 emissions from soils.
Later, these concerns were reviewed by Nickerson and Risk (2009) who, for the first time, investigated the importance of the artefacts and biases of chamber-based measurements in δ13Cefflux estimates.
It is beyond the scope of this study to challenge the use of common techniques such as steady-state and non-steady-state for δ13Ceffux measurement, but our approach attempted to circumvent some of the major reported limitations of chamber-based measurements of δ13Cefflux.
The dual-chamber technique represents a novel approach for improving field measurements of δ13CRs.
Our results clearly demonstrate the ability of this technique to adequately measure δ13Ceffux, and it provides the possibility to ensure that conditions better approximate those that are ideally required for the accurate and unbiased measurement of soil surface respiration and δ13CRs.
Several features of our approach make this possible.
First, there is the possibility to maintain separate, within the dual-chamber volume, the top steady-state chamber headspace from the bottom-chamber volume where the δ13CORs is measured.
This should minimize all the artefacts and biases occurring in the top steady-state chamber.
In the top-chamber mass flow induced by pressure gradients and high air flow rates could result in isotopic fractionation of δ13CRs in the top-chamber.
However, the absence of fan, the presence of a vent, and the careful control of the flow rates in the top-chamber should be sufficient minimize the effects of advective gas transport across the membrane.
Whereas, the maintenance for few hours of sustained bulk flow in the top-chamber should stabilize the transport of 13CO2 and 12CO2 at the same rate across the PVDF membrane (Amundson et al., 1998).
In addition, the absence of bulk airflow in the bottom-chamber (calm conditions), should guarantee a gas movement exclusively by free air molecular diffusion along gradient of mole fraction, which maintained for few hours should be sufficient to reduce the differences between the measured δ13CRs and the soil δ13Cefflux (Bowling and Massman, 2011).
Arguably, the circulation of air in the bottom-chamber through the closed sampling loop, and the CRDS, could be subjected to a possible pump leakage.
Here the sampling loop was operated by the CRDS internal pump with at a flow rate of approximately 22 ml min-1.
Considering the volume of the bottom-chamber (≈1718 cm3), and the duration of the δ13C measurements (sub-routine with a duration time of 30 min), the volume circulated by the CRDS was approximately two-fifths of the entire volume of the bottom-chamber.
Therefore, any pump leakage was considered small enough to be ignored.
Second, the use of reference diffusion chambers, in combination with the dual-chambers, allowed near soil surface CO2 concentrations to be measured.
This allowed us to correctly set up the measuring conditions in the dual-chambers and to minimize the disturbance of the soil CO2 diffusion gradient during the dual-chamber deployment.
While the ideal methods to measure CO2 concentrations between the litter layer and a few centimetres above the soil surface are still to be developed, here we describe the first method which poses an alternative to the widespread use of an arbitrary near-constant CO2 concentration (ambient level) in chamber-based estimates of δ13CORs.
Moreover, the dual-chamber technique permits the constant estimation of the CO2 diffusion coefficient (Dc) occurring between the two sides of the dual-chambers.
Dc is a critical parameter to evaluate the diffusive conditions during the measurements of RS and δ13CORs.
Considering that the transport coefficient for soil gas exchange typically ranges from 1 to 10 mm2 s-1 (Kimball, 1983), our results showed that Dc was 5.2 ± 1.8 mm2 s-1 (mean ± SD).
This means that during our dual-chamber measurements Dc was always within the typical soil diffusivity range, and never reached values of CO2 diffusivity in free air (16.4 mm2 s-1).
Third, using the measured soil surface CO2 concentrations as the baseline level for the dual-chamber operation, it was possible to achieve the necessary CO2 efflux steady-state conditions without using permanent collars inserted in the soil.
In that respect, Heinemeyer et al. (2011) argued convincingly against the indiscriminate use of deep soil collars for successful CO2 efflux measurements.
They showed that in forests, chambers inserted on the shallowest collar were systematically unaffected by the mixing vortex of atmospheric CO2 in the upper soil profile.
Conclusion
We conclude that the biological components contributing to RS at the forest site of San Rossore were mostly from heterotrophic origin, and constrained within the top 20-30 cm of the soil profile.
Our results reflected the soil respiration processes which characterize a water- and nutrient-limited forest sites such as San Rossore (Rosenkranz et al., 2006).
In a recent study on the soil organic carbon of six Mediterranean forest sites in Italy, Chiti et al. (2010) reported that the pine forest of San Rossore had the lowest SOC accumulation within the top 20 cm of soil profile, and predicted that by the end of the second commitment period of the Kyoto protocol (2013-2017) would become a source of SOC.
Undoubtedly, the possibility of having the CO2 production constrained within the top 30 cm of the soil profile, as well as estimating δ13CRs on a highly porous sandy soil, represented a fundamental advantage for the application of the three end-member model in this study.
In a porous soil, the gradients in CO2 concentration and 13CO2 abundance adjust very quickly across depth.
Thus, the soil CO2 and δ13C profiles reflect the depth-integrated CO2 sources (biotic processes) rather than the local fluctuations in production rates due to transient changes in CO2 diffusivity (abiotic processes) (Maseyk et al., 2009).
Nevertheless, the results of this study demonstrate the importance of partitioning soil efflux considering both the 13C discrimination of respiratory components and their mass-specific rate of CO2 production.
Finally, the dual-chambers used in this study have the advantage of being inexpensive to construct and, once constructed, are robust and require little routine maintenance.
Acknowledgements
Funding was provided by the Natural Environment Research Council (NERC) [NE/G018944/1].
Pete Smith is a Royal Society-Wolfson Research Merit Award holder.
We thank Marco Matteucci for assistance in the field, Sergey Blagodatsky for valuable discussions about the three end-member mixing model, Andy Midwood, and the James Hutton Institute analytical group for their support.

1. A heterogeneous metal support material, which comprises a host material and a particulate dopant material, the body material is comprised of the secondary particles comprise primary particles are aggregated into,
Characterized in, uniformly distributing the dopant material of the host material in a granular secondary particles.2. A multi-phase metal support material according to claim 1, wherein the host material is a metal hydroxide, an oxyhydroxide, oxide, oxygen carbonate, one of the carbonate or an oxalate or a mixture thereof.3. A multi-phase metal support material according to claim 2, wherein a heterogeneous metal support material has the general formula: (the dopant material)a( host material)b,
Wherein a b is a weight fraction and the, 0 a 0.4, and wherein b=1-a.4. A multi-phase metal support material according to claim 3, wherein 0.001 a 0.02.5. As claim 1-3 metal support as claimed in any one of multi-phase material, wherein the dopant material is MgO, Cr2O3,ZrO2,Al2O3and the TiO2of the one or more, and the as the form of nanoparticles.6. A multi-phase a metal support material according to claim 3, wherein,
Dopant material is TiO2,
The host material is a NixMnyCozhydroxide, an oxyhydroxide and oxides of one or a mixture thereof, wherein x, y, z are atomic fractions, 0 ≤ x ≤ 1, 0 ≤ y ≤ 1, 0 ≤ z ≤ 1, and the x + y + z=1.7. As claim 1-3 as claimed in any one of a heterogeneous metal support material, wherein the secondary particles are spherical shaped multi-phase metal support material.8. As claim 1-3 material according to any one of a heterogeneous metal support, wherein the dopant material is a water-insoluble metal halide, and the dopant material is in the form of nanoparticles.9. A multi-phase metal support material according to claim 8, wherein the water insoluble metal halide is MgF2and the CaF2one.10. As claim 1-3 metal support as claimed in any one of multi-phase material, wherein the dopant material has a size in the range of 5-200 nm.11. A multi-phase metal support material according to claim 10, wherein the dopant material has a size in the range of 10-50 nm12. A multi-phase metal support material according to claim 6, wherein the host material containing a uniform distribution of Mg atoms.13. A particulate dopant material is uniformly distributed in a host material, so as to obtain a heterogeneous metal support method of a composite material, the body material is comprised of the secondary particles comprise primary particles are aggregated into, the method comprising the steps of:
-providing 1st fluid, which comprises a host material of the precursor solution;
-providing a 2nd fluid, comprising a precipitating agent;
-fluid provides 3rd, which comprises a complexing agent;
-fluid to the 1st, 3rd and the 2nd fluid in the fluid is provided in an amount of the one or more of an insoluble particulate dopant material, or 4th by the particulate dopant material made of a suspension of the insoluble particulate dopant material with an amount of fluid; and
-fluid of the 1st, 2nd and 3rd fluid and a fluid mixture fluid 4th that may be present, whereby the host material and a dopant precipitates.14. Method according to claim 13, wherein the precursor solution is an aqueous solution of metal salt, and the dopant material of the suspension is a suspension of water and a suspension stabilizer.15. 14 A method as claim 13 or, wherein the particulate dopant material made of metal or metal oxide nanoparticles stabilized, and the precursor is a metal nitrate, a chloride, sulfate powder and a halide of one or a mixture thereof.16. 14 A method according to claim 13 or, wherein the dopant material is MgO, Cr2O3,ZrO2,Al2O3and the TiO2of the one or more, and has a 5-200 nm range of sizes.17. Claim 1-12 The use according to any one of the materials, wherein the material with a lithium source is manufactured by sintering the cathode material for a secondary battery.18. Use according to claim 17, wherein the dopant material is MgO, Cr2O3,ZrO2,Al2O3and the TiO2one of the, cathode material is a lithium transition metal oxide.19. Use according to claim 18, wherein the dopant material is Al2O3, cathode material is LiNiO2.Preparation and characterization of a dual-layer carbon film on 6H-SiC wafer using carbide-derived carbon process with subsequent chemical vapor deposition

Abstract
It is reported that a dual-layer carbon film on SiC wafer is prepared using carbide-derived carbon (CDC) process with subsequent chemical vapor deposition (CVD).
The dual-layer film includes a sub-layer of CDC and a top layer of CVD, which are prepared by chlorination of SiC and pyrolysis of CCl4 at high temperature respectively.
The CDC and CVD layers are mainly amorphous.
And similar dispersion effects are observed in the Raman spectra, although the D-band position of the CVD layer shifts to higher wavenumber (~1354cm-1) than that of the CDC layer (~1337cm-1).
Surface chemistry analysis suggests that the unstable chemical bonds, mainly C-Cl, as well as dangling bonds in the CDC layer play an important role in promoting the nucleation of CVD carbon.
The surface morphology evolvement from SiC wafer to CDC layer and to dual-layer film is investigated by atomic force microscopy [AFM] and field emission scanning electronic microscopy [FESEM].
The nanoporous surface formed in the CDC process is favorable for capturing carbon species from the gas phase and can act as a "seedbed" for the nucleation and growth of CVD layer.
The primary tribological study indicates that the dual-layer film shows great advantages in friction reduction and wear resistance with comparison to SiC and CDC layer, suggesting its potential in lubrication for SiC-based moving components.
Graphical abstract
Highlights
•
The dual-layer carbon film is prepared using CDC process with subsequent CVD method.
•
The surface of the CDC layer is highly porous in structure and chemically active.
•
The CDC layer acts as a seedbed layer for the nucleation and growth of the CVD layer.
•
The dual-layer film is tested as an effective lubricating coating for SiC components.

Introduction
Because of their unique properties, such as high hardness, good corrosion resistance, and excellent chemical stability, SiC materials are widely used for tribological applications in extreme conditions [1].
However, the friction and wear coefficients of SiC materials are unacceptably high in unlubricated conditions [2].
To improve the tribological properties, carbon coatings such as diamond and diamond like carbon (DLC), are usually coated on such materials using chemical vapor deposition (CVD) process [3-5].
Although the CVD process is well established, the search for new methods for depositing carbon films continues.
A promising method for coating carbon films on SiC termed the "carbide-derived carbon" or CDC process is greatly attractive in recent years [6].
In this method, SiC is treated in a chlorine-containing gas mixture at high temperature, and the Si atoms preferentially react with Cl2 and leave the system in the form of SiCl4 [7].
The left carbon atoms rearrange themselves and form a smooth carbon film that is intimately bonded to the underlying SiC substrate.
Compared with CVD process, the CDC process can easily produce a thick carbon layer (above 100um) at a high rate [6,7].
The CDC with unique nanostructure possesses many attractive physical, mechanical, and tribological properties.
In particular, it is reported in recent studies that the CDC coating can afford very low friction and wear to improve the tribological performance of the carbides.
Specifically, this coating can provide a friction coefficient of 0.03-0.3 and wear rate of 10-9-10-7mm3/N·m depending on the chlorination process and friction test conditions [8-10].
Surprisingly, the CDC coating with high porosity (i.e.>57.2% for β-SiC [11]) shows excellent tribological properties under dry friction condition.
However, unpublished studies in our group confirm that the CDC coating is helpless in lubricating for SiC materials under some conditions, including high temperature (i.e. at 300°C), water or oil lubrication and high load.
The lubrication failure is attributed to the high porosity and the low strength of CDC coating.
Further surface modification to improve the density and strength of CDC coating is urgent for extensive tribology application.
Although there is little pertinent research on such work to date, some information can still be obtained in some literature.
Grannen and Chang [12] deposited diamond films from fluorocarbon gases in microwave plasma on SiC and WC substrates without any pretreatments.
The proposed growth mechanism suggests that the surface carbon layer formed by the etching of carbide substrates with fluorine atomic favors the nucleation of diamond.
It is reported that ultrananocrystalline diamond (UNCD) could also be deposited on the CDC films using microwave plasma CVD reactor and the tetrahedrally bonded carbon in the CDC films is suggested as a good seed for UNCD deposition [13].
Accordingly, we attempt to prepare a novel dual-layer carbon coating on 6H-SiC wafers using CDC technique with subsequent CVD process in the present work.
According to our considerations, the CDC layer which is formed by chlorination of SiC acts as a seed and intermediate layer for subsequent deposition.
The CVD process is operated in a same reactor using CCl4 as carbon source.
We used Raman spectroscopy, X-ray photoelectron spectroscopy (XPS), atomic force microscopy (AFM) and field emission scanning electron microscopy (SEM) to determine the microstructure and chemistry of CDC and CVD films.
And friction and wear properties of the CDC and dual-layer film are discussed.
It is expected that this dual-layer film can provide better lubrication for SiC materials in many applications.
Experimental section
Film preparation
The dual-layer film was synthesized by pre-chlorination of single crystalline 6H-SiC wafer with subsequent chemical vapor deposition (CVD) using CCl4 as precursor.
The n-type 6H-SiC wafer (10×10mm2) with polished Si-face was purchased from TianKe Blue Semiconducter Co.
Ltd (Beijing, China), with nominal cut-off angle of about 0-5°.
The as-received SiC wafers were etched in dilute HF acid (40% solution) for 5min to remove a SiO2 layer, and ultrasonically cleaned in acetone bath for treatment.
The schematic process to synthesize the dual-layer film was demonstrated in Fig. 1.
Both the CDC process and CVD deposition were carried out in a tube furnace in turn.
The CDC process was described in detail elsewhere [7].
In our experiments, the CDC layer was fabricated by chlorination of SiC in a gas mixture (Cl2 at a flow speed of 2mlmin-1+Ar at a flow speed of 98mlmin-1) at 1000°C for 10min.
The Si was preferred to react with Cl2 and removed away in the form of volatile SiCl4.
The surface of SiC was converted into CDC and the shape of the sample stayed unchanged, as demonstrated in Fig. 1.
Once the CDC process was finished, the Cl2 flow was stopped immediately.
Subsequently, the CCl4 was evaporated at 60°C [14] and introduced into reaction region with 50mlmin-1 Ar flow.
The carbon derived from pyrolyzing of CCl4 at 1000°C nucleated and grew on top of CDC layer to form the CVD layer.
The duration for CVD deposition was 2h.
The two principal chemical reactions taking place in the process included(1)SiC+2Cl2→SiCl4+Cwhich is thermodynamically allowed, with a Gibbs energy of ∆G=-434.1kJ/mol at 1000°C, and(2)CCl4→C+Cl2which is also thermodynamically allowed, with a Gibbs energy of ∆G=-36.672kJ/mol at 1000°C.
The carbon produced by reaction (1) formed the CDC layer, while reaction (2) formed the CVD layer.
As a result, the dual-layer carbon film composed of sub-layer of CDC and top layer of CVD carbon was synthesized.
Moreover, single CDC film was also synthesized for comparison.
The process parameters were the same to those in CDC process for dual-layer film preparation.
Characterization
Raman spectroscopy is a standard non-destructive analysis tool for characterization of various carbon materials.
The CDC and the dual-layer films were studied on a Horiba HR800 Raman system with an Ar laser excitation wavelength of 532nm.
This system is also equipped with a charge couple device (CCD) detector and an optical imaging for focusing the laser at a micro-region.
The Raman spectra were collected in the range between 600 and 2000cm-1 for 60s.
X-ray photoelectron spectroscopy (XPS) was used to analyze the surface composition and chemical nature of the CDC layer and the dual-layer film.
The XPS measurement was performed on PHI-5702 multifunctional photoelectron spectrometer, using Al-Ka X-ray (250W) as the excitation source.
No pre-treatment on the samples was conducted.
Survey spectra with energy step of 0.4eV and core level photoelectron lines C1s, O1s, Cl2p and Si2p with energy step of 0.125eV were recorded.
In order to investigate the surface topography evolvement, atomic force microscopy (AFM) was carried out with a Nanoscope IIIA Multimode apparatus (Veeco Instruments) under ambient conditions (relative humidity ~45%, temperature ~26-28°C).
AFM was performed in the tapping mode using rectangular silicon cantilevers with spring constant of ~40Nm-1 and typical resonance frequencies between 250 and 300kHz.
Field emission scanning electronic microscopy (FESEM, JSM-6701F) was also employed to observe the surface microstructure.
Friction test
Friction and wear behaviors of SiC, CDC layer and dual-layer film were contrastively studied on a UMT-2MT tribo-meter (CETR, USA) with a ball-on-disk configuration in ambient conditions (temperature of 27°C and relative humidity of 45%).
The Si3N4 and steel balls with diameter of 3mm made oscillating movement (5mm in amplitude) on the top of the samples for 30min.
The test conditions were 1N in load and 20mms-1 in sliding speed.
The friction coefficients were recorded by the tribo-meter.
Results and discussion
Raman spectrum
For the perfectly ordered graphite, the first-order Raman spectrum recorded in the near-infrared and visible light regimes shows only one peak around 1580cm-1, whereas disordered amorphous carbon generally exhibits two peaks: the so-called G-band at ~1580cm-1and the so-called D-band (disorder induced (D)) at ~1350cm-1, which is associated with the double-resonance Raman mechanism [15].
The positions of D- and G-bands, their relative intensity ratio (ID/IG), and their full widths at half maximum (FWHM) can be used to study the structural information of the carbon materials.
Fig. 2 presents the Raman spectra of SiC substrate, CDC layer, and dual-layer film to investigate the structure evolvement.
The Raman spectrum of SiC has several peaks in the range of 800-2000cm-1.
The strongest peak ~968cm-1 is the longitudinal (LO) optical phonon of SiC, while the peak ~782cm-1 is the transversal optical (TO) phonon.
The positions of D- and G-bands for CDC layer situate at ~1337cm-1 and ~1606cm-1, respectively, being different from the specific values for graphite (~1350cm-1 and ~1580cm-1).
Moreover, the D- and G-bands for CDC layer are greatly broadened and cannot be fitted with two Gaussians profiles.
As we know, the G-peak appears due to the in-plane bond-stretching motion of pairs of sp2 bonded C atoms in graphitic compounds whatever is their stacking order or their crystallite size.
The broadening and shifting to high wavenumbers of the G-band can be interpreted as an increase in bond angle disorder or the ultrasmall crystal size [16].
Alternatively, the internal stress induced by the nonuniformity of the crystalline structure and the mismatch between CDC and SiC substrate leads to peak broadening and up-shifting in Raman spectrum of CDC layer [17].
Unfortunately, the origin of the D-band shifting toward smaller wavenumbers is unclear.
However, the presence of broad D-band further demonstrates the disorder structure.
Its large FWHM implies a large distribution of sp2 bonded clusters with different ring sizes in the CDC layer [18].
2+2 Gaussian profiles can be used to fit the Raman spectrum [17] to calculate the integrated intensity ratio of D- and G-bands (ID/IG ≈1.70).
According to the Tuinstra and Koenig (TK) equation [19,20], the in-plane correlation length (La) or the average size of the graphite sheets in the CDC layer is estimated to be about 2.92nm.
XPS analysis
The XPS measurements were carried out aiming to determine the surface composition and chemical nature for SiC, CDC layer and dual-layer film, as demonstrated in Fig. 3.
It is mentioned that oxidizing of silicon carbide surface can easily take place even at room temperature [21].
The oxygen peak in the survey spectrum may correspond to the contaminated oxygen or the oxidation of SiC surface (Fig. 3a).
Surface analysis of the CDC layer reveals that it does not consist purely of carbon, but that it still contains O, Si and Cl components.
The oxygen is related to the oxidation of CDC as well as physical adsorption of O2 and H2O molecular due to the highly porous structure.
Surprisingly, the Si is identified within the CDC layer.
Zinovev et al. [13] reported that the silicon maybe exist in oxidized forms of silicon, i.e.
SiO2, which derives from the oxidation of the escaped SiCl4 gas with trace oxygen.
The surface atomic concentration of Cl in the CDC layer by XPS analysis is about 5.5at.%.
Since the CDC layer results from selective etching of Si atoms in SiC by Cl2, some of the Cl2 is likely retained in the CDC layer in atomic or bonded form [22].
For the survey spectrum of dual-layer film, the surface composition is characterized by mainly the presence of C and O components with minor contents of Si (6.5at.%) and Cl (1.8at.%).
The attenuation of Si and Cl signals does not indicate that they are removed in subsequent CVD process, but their signals are covered by the top layer with increasing thickness.
More detailed information about the surface chemistry can be obtained from the analysis of the core level C1s spectra.
Fig. 3b-d shows the C1s core levels of SiC, CDC layer and CVD layer and their line shape fits.
The C1s peak on SiC demonstrates that the composition is dominated by the binding energy of C-Si (282.7eV) with C═C (284.3eV), C-C (285.0eV), and C-O (286.1eV) at the high binding energy (Fig. 3b).
The C1s peak of CDC can be fitted by five components with binding energies of 284.4, 285.3, 286.2, 287.2, and 283.0eV.
The first component is attributed to the sp2 bonded carbon, whereas the second one is attributed to the sp3 coordinated bonds in amorphous carbon [23,24].
The component at 287.2 and 283.0eV is assigned to C═O bonded carbon [25] and C-Si bond, respectively.
The component with bonding energy of 286.2eV is attributed to C-Cl or C-O bond.
In Fig. 3d, the C1s peak of the dual-layer film is fitted by four components with the binding energy of 284.3, 285.1, 286.1, and 287.0eV.
Similar to that of the CDC layer, the two contributions at low binding energy are assigned to the sp2 and sp3 C.
The components at 286.1 and 287.0eV should be attributed to C-O and C═O bond, respectively.
From the XPS analysis, both the CDC and the dual-layer film mainly consist of sp2 bonded carbon with a small mixture of sp3 carbon, C-O, and C═O.
However, the effect of surface chemical nature of CDC layer under the top layer cannot be studied by XPS.
Indeed, there are many voids, defects, and active atomic and/or molecular sites, which may contain dangling or unsatisfied σ-bonds within the CDC film (especially near the top) [22].
Before the CVD process, such active sites are passivated by physical interaction (i.e. physical adsorption) and formation of metastable chemical bonds, e.g.
the C-Cl bond.
In other words, the surface of the CDC layer is highly active, which can act as a "seedbed" for nucleation and growth of the top layer.
Surface morphology
Fig. 4 shows the AFM images and corresponding 3D profiles of SiC wafer, CDC layer and dual-layer film, indicating surface morphology evolution in the process.
The as-received SiC surface in Fig. 4a and b is characterized by highly uniform and flat terraces.
The surface roughness RMS (root mean square roughness) is evaluated as 0.22nm.
The step direction and terrace width are determined by the incidental disorientation of the substrate surface with respect to the crystallographic (0001) plane [26].
The step height is smaller than that of the dimension of the 6H-SiC unit cell in the direction perpendicular to the surface (c axis), which may resulted from the silicon oxide layer.
The morphology of the CDC layer prepared by chlorination is shown in Fig. 4c and d.
The SiC surface undergoes significant modification: it is now covered with numerous nanopores up to 100nm in width, and the original are disappeared.
The formation of CDC is accompanied by substantial changes in the morphology, leading to a considerable increasing surface roughness of 1.46nm.
As a consequence of chlorination, the CDC layer possesses a nanoporous structure and an increasing specific surface area on the surface [6,27].
The nanopores can easily capture the carbon species from the gas phase for the subsequent CVD process, which benefits the nucleation and growth of the CVD layer.
After the CVD process, the surface morphology is characterized by spherical or ellipsoidal particles, as shown in Fig. 4e and f.
The size of carbon particles is up to approximately 3μm in diameter, and the surface roughness is substantially increased to 54.68nm in RMS.
The surface microstructures of the CDC layer and the dual-layer film are also investigated by FESEM as shown in Fig. 5.
As the CDC layer is formed by selective etching of Si from SiC, the surface is characterized of a highly porous structure mainly consisted of numerous nanoscale pores and a corresponding large specific area of the CDC layer (Fig. 5a).
Assuming all the Si atoms are removed from SiC and no volume change occurs, a large pore volume of 57.3% will be obtained.
Such a unique microstructure of nanopores as well as the surface chemical activity should promote the nucleation and growth of CVD layer.
Fig. 5b shows the microstructure of the CVD layer.
The whole surface is uniformly covered with quasi-spherical carbon particles with size in submicrometer scale, with accordance to the observation by AFM (Fig. 4c and d).
Tribological property
The tribological behaviors of SiC wafer, CDC, and dual-layer film are contrastively investigated to evaluate the potential lubrication.
Fig. 6a and b shows the friction coefficient as a function of sliding time of the samples sliding against Si3N4 and steel in room air, respectively.
After a run-in process of about 500s, the friction coefficient of SiC wafer against Si3N4 is steady at 0.80, which is the typical value of SiC ceramic in open air.
Meanwhile, the friction coefficient curve fluctuates rather heavily during testing.
The friction coefficient of the CDC layer greatly reduces to 0.25, showing an improved tribological behavior compared to SiC.
When sliding against Si3N4, the friction coefficient of the dual-layer film is further reduced to 0.15.
Differently, the friction coefficients of the CDC layer and the dual-layer film against steel ball are higher than Si3N4.
Specifically, the friction coefficient of the CDC layer drastically fluctuates in initial process and the average value is above 0.3, while that of the dual-layer film is very steady and slightly increases to 0.17.
From the tribological results, we can conclude that the Si3N4 is the preferred dual material over the steel whenever sliding against the CDC layer or the dual-layer film.
The steel is easily worn because of its much lower hardness than Si3N4.
The debris from the steel retained in the contact surface can lead to severe abrasive wear.
Moreover, chemical corrosion [28] and adhesive interaction prefer to occur in the contact surface to increase friction when sliding against steel.
Whereas, such negative effects are attenuated when against Si3N4 ball because of its high hardness and chemical inertness.
Additionally, the dual-layer film shows advantages over the CDC layer in friction reduction.
As aforementioned, the surface of the CDC layer is highly active with many dangling bonds, unsatisfied σ-bonds, and unstable chemical bonds (the C-Cl, for instance) [22].
The presence of such bonds at sliding contact interfaces is undesirable as they can easily cause adhesive interactions during sliding motion.
In addition, hard particles of SiO2 in the CDC layer can increase the friction in the form of abrasion.
For the dual-layer film, the active bonds are effectively eliminated by highly energetic carbon atoms generated in pyrolyzing of CCl4.
Moreover, the concentration of Cl atomics on the surface is greatly reduced, which can be seen from the XPS results.
As a result, the dual-layer film shows an improved tribological behavior compared with the CDC layer.
Conclusions
A dual-layer carbon film was successfully synthesized by carbide-derived carbon process and subsequent chemical vapor deposition.
The dual-layer film is composed of a CDC layer formed by chlorination of SiC and a CVD layer formed by pyrolyzing the CCl4 at high temperature.
Due to the selective removal of Si from SiC, the as-received CDC layer is highly porous in structure and contains many dangling bonds and unsatisfied σ-bonds on the surface.
By AFM and FESEM observation, the porous structure of CDC layer is characterized with numerous nanopores that can benefit the surface capturing carbon containing species from gas mixture.
On the other hands, the as-received CDC surface is chemically active, so the pyrolyzed carbon atoms are readily bonded to the surface.
In short, the CDC layer serves as a seed and intermediate layer for the subsequent CVD carbon deposition.
The dual-layer film shows a typical amorphous structure, exhibiting D- and G-bands in Raman spectrum at ~1354 and ~1605cm-1, respectively.
The preliminary tribological study demonstrates that the dual-layer film outperforms the single CDC layer in lubricating SiC substrate.
It is expected that this dual-layer film will find great potential in lubrication for SiC-based moving components.
In the future, it is necessary to further understand the growth mechanism, control the structure and improve the properties of this dual-layer film by optimizing the process parameters.

Coherent clusters in source code

Highlights
•
Introduction of efficient clustering algorithm.
•
Empirical analysis to assess the frequency and size of coherent clusters.
•
A series of case studies showing how clusters identify logical program structures.
•
A study on the relationship between coherent clusters and program faults.
•
A study on the relationship between coherent clusters and system evolution.
Abstract
This paper presents the results of a large scale empirical study of coherent dependence clusters.
All statements in a coherent dependence cluster depend upon the same set of statements and affect the same set of statements; a coherent cluster's statements have 'coherent' shared backward and forward dependence.
We introduce an approximation to efficiently locate coherent clusters and show that it has a minimum precision of 97.76%.
Our empirical study also finds that, despite their tight coherence constraints, coherent dependence clusters are in abundance: 23 of the 30 programs studied have coherent clusters that contain at least 10% of the whole program.
Studying patterns of clustering in these programs reveals that most programs contain multiple substantial coherent clusters.
A series of subsequent case studies uncover that all clusters of significant size map to a logical functionality and correspond to a program structure.
For example, we show that for the program acct, the top five coherent clusters all map to specific, yet otherwise non-obvious, functionality.
Cluster visualization also brings out subtle deficiencies in program structure and identifies potential refactoring candidates.
A study of inter-cluster dependence is used to highlight how coherent clusters are connected to each other, revealing higher-level structures, which can be used in reverse engineering.
Finally, studies are presented to illustrate how clusters are not correlated with program faults as they remain stable during most system evolution.

Introduction
Program dependence analysis is a foundation for many activities in software engineering such as testing, comprehension, and impact analysis (Binkley, 2007).
For example, it is essential to understand the relationships between different parts of a system when making changes and the impacts of these changes (Gallagher and Lyle, 1991).
This has led to both static (Yau and Collofello, 1985; Black, 2001) and blended (static and dynamic) (Ren et al., 2006, 2005) dependence analyses of the relationships between dependence and impact.
One important property of dependence is the way in which it may cluster.
This occurs when a set of statements all depend upon one another, forming a dependence cluster.
Within such a cluster, any change to an element potentially affects every other element of the cluster.
If such a dependence cluster is very large, then this mutual dependence clearly has implications related to the cost of maintaining the code.
More recently, our finding that large clusters are widespread in C systems has been replicated for other languages and systems by other authors, both in open source and in proprietary code (Acharya and Robinson, 2011; Beszédes et al., 2007; Szegedi et al., 2007).
Large dependence clusters were also found in Java systems (Beszédes et al., 2007; Savernik, 2007; Szegedi et al., 2007) and in legacy Cobol systems (Hajnal and Forgács, 2011).
There has been interesting work on the relationship between faults, program size, and dependence clusters (Black et al., 2006), and between impact analysis and dependence clusters (Acharya and Robinson, 2011; Harman et al., 2009).
Large dependence clusters can be thought of as dependence 'anti-patterns' because of the high impact that a change anywhere in the cluster has.
For example, it may lead to problems for on-going software maintenance and evolution (Acharya and Robinson, 2011; Binkley et al., 2008; Savernik, 2007).
As a result, refactoring has been proposed as a technique for breaking larger clusters of dependence into smaller clusters (Binkley and Harman, 2005; Black et al., 2009).
Dependence cluster analysis is complicated by the fact that inter-procedural program dependence is non-transitive, which means that the statements in a traditional dependence cluster, though they all depend on each other, may not each depend on the same set of statements, nor need they necessarily affect the same set of statements external to the cluster.
This paper introduces and empirically studies11
Preliminary results were presented at PASTE (Islam et al., 2010b).
 coherent dependence clusters.
In a coherent dependence cluster all statements share identical intra-cluster and extra-cluster dependence.
A coherent dependence cluster is thus more constrained than a general dependence cluster.
A coherent dependence cluster retains the essential property that all statements within the cluster are mutually dependent, but adds the constraint that all incoming dependence must be identical and all outgoing dependence must also be identical.
That is, all statements within a coherent cluster depend upon the same set of statements outside the cluster and all statements within a coherent cluster affect the same set of statements outside the cluster.
This means that, when studying a coherent cluster, we need to understand only a single external dependence context in order to understand the behavior of the entire cluster.
For a dependence cluster that fails to meet the external constraint, statements of the cluster may have a different external dependence context.
This is possible because inter-procedural dependence is non-transitive.
It might be thought that very few sets of statements would meet these additional coherence constraints, or that, where such sets of statements do meet the constraints, there would be relatively few statements in the coherent cluster so-formed.
Our empirical findings provide evidence that this is not the case: coherent dependence clusters are common and they can be very large.
This paper is part of a series of work that we have conducted in the area of dependence clusters.
The overarching motivation for this work is to gain a better understanding of the dependence clusters found in programs.
Although this paper is a continuation of our previous work on dependence clusters, we present the work in a completely new light.
In this paper we show that the specialized version of dependence clusters, coherent clusters are found in abundance in programs and need not be regarded as problems.
We rather show that these clusters map to logical program structures which will aid developers in program comprehension and understanding.
Furthermore, this paper extends the current knowledge in the area and motivates future work by presenting initial results of inter-cluster dependence which can be used as a foundation for reverse engineering.
We answer several representative open questions such as whether clusters are related to program faults and how clusters change over time during system evolution.
The primary contributions of the paper are as follows:1
An Empirical analysis of thirty programs assesses the frequency and size of coherent dependence clusters.
The results demonstrate that large coherent clusters are common, validating their further study.
2
Two further empirical validation studies consider the impact of data-flow analysis precision and the precision of the approximation used to efficiently identify coherent clusters.
3
A series of four case studies shows how coherent clusters map to logical program structures.
4
A study of inter-cluster dependence highlights how coherent clusters form the building blocks of larger dependence structures where identification can support, as an example, reverse engineering.
5
A study of bug fixes finds no relationship between program faults and coherent clusters implying that dependence clusters are not responsible for program faults.
6
A longitudinal study of system evolution shows that coherent clusters remain stable during evolution thus depicting the core architecture of systems.
The remainder of this paper is organized as follows: Section 2 provides background on coherent clusters and their visualization.
Section 3 provides details on the subject programs, the validation of the slice approximation used, and the experimental setup.
This is followed by quantitative and qualitative studies into the existence and impact of coherent dependence clusters and the inter-cluster dependence study.
It also includes studies on program faults and system evolution and their relationship to coherent clusters.
Section 4 considers related work and finally, Section 5 summarizes the work presented.
Background
This section provides background on dependence clusters.
It first presents a sequence of definitions that culminate in the definition for a coherent dependence cluster.
Previous work (Binkley and Harman, 2005; Harman et al., 2009) has used the term dependence cluster for a particular kind of cluster, termed a mutually-dependent cluster herein to emphasize that such clusters consider only mutual dependence internal to the cluster.
This distinction allows the definition to be extended to incorporate external dependence.
The section also reviews the current graph-based visualizations for dependence clusters.
Dependence clusters
Informally, mutually-dependent clusters are maximal sets of program statements that mutually depend upon one another (Harman et al., 2009).
They are formalized in terms of mutually dependent sets in the following definition.
Definition 2.1
Mutually-dependent set and cluster (Harman et al., 2009)
A mutually-dependent set (MDS) is a set of statements, S, such that
∀x, y∈S:x depends on y.
A mutually-dependent cluster is a maximal MDS; thus, it is an MDS not properly contained within another MDS.
The definition of an MDS is parameterized by an underlying depends-on relation.
Ideally, such a relation would precisely capture the impact, influence, and dependence between statements.
Unfortunately, such a relation is not computable (Weiser, 1984).
A well known approximation is based on Weiser's program slice (Weiser, 1984): a slice is the set of program statements that affect the values computed at a particular statement of interest (referred to as a slicing criterion).
While its computation is undecidable, a minimal (or precise) slice includes exactly those program elements that affect the criterion and thus can be used to define an MDS in which t depends on s iff s is in the minimal slice taken with respect to slicing criterion t.
The slice-based definition is useful because algorithms to compute approximations to minimal slices can be used to define and compute approximations to mutually-dependent clusters.
One such algorithm computes a slice as the solution to a reachability problem over a program's System Dependence Graph (SDG) (Horwitz et al., 1990).
An SDG is comprised of vertices, which essentially represent the statements of the program and two kinds of edges: data dependence edges and control dependence edges.
A data dependence connects a definition of a variable with each use of the variable reached by the definition (Ferrante et al., 1987).
Control dependence connects a predicate p to a vertex v when p has at least two control-flow-graph successors, one of which can lead to the exit vertex without encountering v and the other always leads eventually to v (Ferrante et al., 1987).
Thus p controls the possible future execution of v.
For structured code, control dependence reflects the nesting structure of the program.
When slicing an SDG, a slicing criterion is a vertex from the SDG.
A naïve definition of a dependence cluster would be based on the transitive closure of the dependence relation and thus would define a cluster to be a strongly connected component.
Unfortunately, for certain language features, dependence is non-transitive.
Examples of such features include procedures (Horwitz et al., 1990) and threads (Krinke, 1998).
Thus, in the presence of these features, strongly connected components overstate the size and number of dependence clusters.
Fortunately, context-sensitive slicing captures the necessary context information (Binkley and Harman, 2005, 2003; Horwitz et al., 1990; Krinke, 2002, 2003).
Two kinds of SDG slices are used in this paper: backward slices and forward slices (Horwitz et al., 1990; Ottenstein and Ottenstein, 1984).
The backward slice taken with respect to vertex v, denoted BSlice(v), is the set of vertices reaching v via a path of control and data dependence edges where this path respects context.
The forward slice, taken with respect to vertex v, denoted FSlice(v), is the set of vertices reachable from v via a path of control and data dependence edges where this path respects context.
The program P shown in Fig. 1 illustrates the non-transitivity of slice inclusion.
The program has six assignment statements (assigning the variables a, b, c, d, e and f) whose dependencies are shown in columns 1-6 as backward slice inclusion.
Backward slice inclusion contains statements that affect the slicing criterion through data and control dependence.
The dependence relationship between these statements is also extracted and shown in Fig. 2 using a directed graph where the nodes of the graph represent the assignment statements and the edges represent the backward slice inclusion relationship from Fig. 1.
The table on the right in Fig. 2 also gives the forward slice inclusions for the statements.
All other statements in P, which do not define a variable, are ignored.
In the diagram, x depends on y (y ∈ BSlice(x)) is represented by y → x.
The diagram shows two instances of dependence intransitivity in P. Although b depends on nodes a, c, and d, node f, which depends on b, does not depend on a, c, or d.
Similarly, d depends on e but a, b, and c, which depend on d do not depend on e.
Slice-based clusters
A slice-based cluster is a maximal set of vertices included in each other's slice.
The following definition essentially instantiates Definition 2.1 using BSlice.
Because x∈BSlice(y)⇔y∈FSlice(x) the dual of this definition using FSlice is equivalent.
Where such a duality does not hold, both definitions are given.
When it is important to differentiate between the two, the terms backward and forward will be added to the definition's name as is done in this section.
Definition 2.2
Backward-slice MDS and cluster (Harman et al., 2009)
A backward-slice MDS is a set of SDG vertices, V, such that
∀x, y∈V:x∈BSlice(y).
A backward-slice cluster is a backward-slice MDS contained within no other backward-slice MDS.
Note that as x and y are interchangeable, this is equivalent to ∀x, y∈V:x∈BSlice(y)∧y∈BSlice(x).
Thus, any unordered pair (x, y) with x∈BSlice(y)∧y∈BSlice(x) creates an edge (x, y) in an undirected graph in which a complete subgraph is equivalent to a backward-slice MDS and a backward-slice cluster is equivalent to a maximal clique.
Therefore, the clustering problem is the NP-Hard maximal cliques problem (Bomze et al., 1999) making Definition 2.2 prohibitively expensive to implement.
In the example shown in Fig. 2, the vertices representing the assignments to a, b, c and d are all in each others backward slices and hence satisfy the definition of a backward-slice cluster.
These vertices also satisfy the definition of a forward-slice cluster as they are also in each others forward slices.
As dependence is not transitive, a statement can be in multiple slice-based clusters.
For example, in Fig. 2 the statements d and e are mutually dependent upon each other and thus satisfy the definition of a slice-based cluster.
Statement d is also mutually dependent on statements a, b, c, thus the set {a, b, c, d} also satisfies the definition of a slice-based cluster.
Same-slice clusters
An alternative definition uses the same-slice relation in place of slice inclusion (Binkley and Harman, 2005).
This relation replaces the need to check if two vertices are in each others slice with checking if two vertices have the same slice.
The result is captured in the following definitions for same-slice cluster.
The first uses backward slices and the second forward slices.
Definition 2.3
Same-slice MDS and cluster (Harman et al., 2009)
A same-backward-slice MDS is a set of SDG vertices, V, such that
∀x, y∈V:BSlice(x)=BSlice(y).
A same-backward-slice cluster is a same-backward-slice MDS contained within no other same-backward-slice MDS.
A same-forward-slice MDS is a set of SDG vertices, V, such that
∀x, y∈V:FSlice(x)=FSlice(y).
A same-forward-slice cluster is a same-forward-slice MDS contained within no other same-forward-slice MDS.
Because x∈BSlice(x) and x∈FSlice(x), two vertices that have the same slice will always be in each other's slice.
If slice inclusion were transitive, a backward-slice MDS (Definition 2.2) would be identical to a same-backward-slice MDS (Definition 2.3).
However, as illustrated by the examples in Fig. 1, slice inclusion is not transitive; thus, the relation is one of containment where every same-backward-slice MDS is also a backward-slice MDS but not necessarily a maximal one.
For example, in Fig. 2 the set of vertices {a, b, c} form a same-backward-slice cluster because each vertex of the set yields the same backward slice.
Whereas the set of vertices {a, c} form a same-forward-slice cluster as they have the same forward slice.
Although vertex d is mutually dependent with all vertices of either set, it does not form the same-slice cluster with either set because it has an additional dependence relationship with vertex e.
Although the introduction of same-slice clusters was motivated by the need for efficiency, the definition inadvertently introduced an external requirement on the cluster.
Comparing the definitions for slice-based clusters (Definition 2.2) and same-slice clusters (Definition 2.3), a slice-based cluster includes only the internal requirement that the vertices of a cluster depend upon one another.
However, a same-backward-slice cluster (inadvertently) adds to this internal requirement the external requirement that all vertices in the cluster are affected by the same vertices external to the cluster.
Symmetrically, a same-forward-slice cluster adds the external requirement that all vertices in the cluster affect the same vertices external to the cluster.
Coherent dependence clusters
This subsection first formalizes the notion of coherent dependence clusters and then presents a slice-based instantiation of the definition.
Coherent clusters are dependence clusters that include not only an internal dependence requirement (each statement of a cluster depends on all the other statements of the cluster) but also an external dependence requirement.
The external dependence requirement includes both that each statement of a cluster depends on the same statements external to the cluster and also that it influences the same set of statements external to the cluster.
In other words, a coherent cluster is a set of statements that are mutually dependent and share identical extra-cluster dependence.
Coherent clusters are defined in terms of the coherent MDS:
Definition 2.4
Coherent MDS and cluster (Islam et al., 2010b)
A coherent MDS is a MDS V, such that
∀x, y∈V: x depends on a implies y depends on a and a depends on x implies a depends on y.
A coherent cluster is a coherent MDS contained within no other coherent MDS.
The slice-based instantiation of coherent cluster employs both backward and forward slices.
The combination has the advantage that the entire cluster is both affected by the same set of vertices (as in the case of same-backward-slice clusters) and also affects the same set of vertices (as in the case of same-forward-slice clusters).
In the slice-based instantiation, a set of vertices V forms a coherent MDS if
∀x, y∈V:x∈BSlice(y)the internal requirement of an MDS∧a∈BSlice(x)⇒a∈BSlice(y)x and y depend on same external a∧a∈FSlice(x)⇒a∈FSlice(y)x and y impact on same external a
Because x and y are interchangeable
∀x,y∈V:x∈BSlice(y)∧ a∈BSlice(x) ⇒ a∈BSlice(y)∧ a∈FSlice(x) ⇒ a∈FSlice(y)∧ y∈BSlice(x)∧ a∈BSlice(y) ⇒ a∈BSlice(x)∧ a∈FSlice(y) ⇒ a∈FSlice(x)This is equivalent to
∀x,y∈V:x∈BSlice(y)∧y∈BSlice(x)∧ (a∈BSlice(x)⇔a∈BSlice(y))∧ (a∈FSlice(x)⇔a∈FSlice(y))which simplifies to
∀x,y∈V:BSlice(x)=BSlice(y)∧FSlice(x)=FSlice(y)and can be used to define coherent-slice MDS and clusters:
Definition 2.5
Coherent-slice MDS and cluster (Islam et al., 2010b)
A coherent-slice MDS is a set of SDG vertices, V, such that
∀x, y∈V:BSlice(x)=BSlice(y)∧FSlice(x)=FSlice(y)
A coherent-slice cluster is a coherent-slice MDS contained within no other coherent-slice MDS.
At first glance the use of both backward and forward slices might seem redundant because x∈BSlice(y)⇔y∈FSlice(x).
This is true up to a point: for the internal requirement of a coherent-slice cluster, the use of either BSlice or FSlice would suffice.
However, the two are not redundant when it comes to the external requirements of a coherent-slice cluster.
With a mutually-dependent cluster (Definition 2.1), it is possible for two vertices within the cluster to influence or be affected by different vertices external to the cluster.
Neither is allowed with a coherent-slice cluster.
To ensure that both external effects are captured, both backward and forward slices are required for coherent-slice clusters.
In Fig. 2 the set of vertices {a, c} form a coherent cluster as both these vertices have exactly the same backward and forward slices.
That is, they share identical intra- and extra-cluster dependencies.
Coherent clusters are therefore a stricter from of same-slice clusters, all coherent clusters are also same-slice MDS but not necessarily maximal.
It is worth noting that same-slice clusters partially share extra-cluster dependency.
For example, each of the vertices in the same-backward-slice cluster {a, b, c} is dependent on the same set of external statements, but do not influence the same set of external statements.
Coherent slice-clusters have an important property: If a slice contains a vertex of a coherent slice-cluster V, it will contain all vertices of the cluster:
(1)BSlice(x)∩V≠∅ ⇒ BSlice(x)∩V=VThis holds because:
∀y,y′∈V:y∈BSlice(x) ⇒ x∈FSlice(y) ⇒x∈FSlice(y′) ⇒ y′∈BSlice(x)The same argument clearly holds for forward slices.
However, the same is not true for non-coherent clusters.
For example, in the case of a same-backward-slice cluster, a vertex contained within the forward slice of any vertex of the cluster is not guaranteed to be in the forward slice of other vertices of the same cluster.
Hash based coherent slice clusters
The computation of coherent-slice clusters (Definition 2.5) grows prohibitively expensive even for mid-sized programs where tens of gigabytes of memory are required to store the set of all possible backward and forward slices.
The computation is cubic in time and quadratic in space.
An approximation is employed to reduce the computation time and memory requirement.
This approximation replaces comparison of slices with comparison of hash values, where hash values are used to summarize slice content.
The result is the following approximation to coherent-slice clusters in which H denotes a hash function.
Definition 2.6
Hash-based coherent-slice MDS and cluster (Islam et al., 2010b)
A hash-based coherent-slice MDS is a set of SDG vertices, V, such that
∀x, y∈V:H(BSlice(x))=H(BSlice(y))∧H(FSlice(x))=H(FSlice(y))
A hash-based coherent-slice cluster is a hash-based coherent-slice MDS contained within no other hash-based coherent-slice MDS.
A description of the hash function H along with the evaluation of its precision is presented in Section 3.3.
From here on, the paper considers only hash-based coherent-slice clusters unless explicitly stated otherwise.
Thus, for ease of reading, a hash-based coherent-slice cluster is referred to simply as a coherent cluster.
Graph based cluster visualization
This section describes two graph-based visualizations for dependence clusters.
The first visualization, the Monotone Slice-size Graph (MSG) (Binkley and Harman, 2005), plots a landscape of monotonically increasing slice sizes where the y-axis shows the size of each slice, as a percentage of the entire program, and the x-axis shows each slice, in monotonically increasing order of slice size.
In an MSG, a dependence cluster appears as a sheer-drop cliff face followed by a plateau.
The visualization assists with the inherently subjective task of deciding whether a cluster is large (how long is the plateau at the top of the cliff face relative to the surrounding landscape?) and whether it denotes a discontinuity in the dependence profile (how steep is the cliff face relative to the surrounding landscape?).
An MSG drawn using backward slice sizes is referred to as a backward-slice MSG (B-MSG), and an MSG drawn using forward slice sizes is referred to as a forward-slice MSG (F-MSG).
As an example, the open source calculator bc contains 9438 lines of code represented by 7538 SDG vertices.
The B-MSG for bc, shown in Fig. 3a, contains a large plateau that spans almost 70% of the MSG.
Under the assumption that same slice size implies the same slice, this indicates a large same-slice cluster.
However, "zooming" in reveals that the cluster is actually composed of several smaller clusters made from slices of very similar size.
The tolerance implicit in the visual resolution used to plot the MSG obscures this detail.
The second visualization, the Slice/Cluster Size Graph (SCG) (Islam et al., 2010b), alleviates this issue by combining both slice and cluster sizes.
It plots three landscapes, one of increasing slice sizes, one of the corresponding same-slice cluster sizes, and the third of the corresponding coherent cluster sizes.
In the SCG, vertices are ordered along the x-axis using three values, primarily according to their slice size, secondarily according to their same-slice cluster size, and finally according to the coherent cluster size.
Three values are plotted on the y-axis: slice sizes form the first landscape, and cluster sizes form the second and third.
Thus, SCGs not only show the sizes of the slices and the clusters, they also show the relation between them and thus bring to light interesting links.
Two variants of the SCG are considered: the backward-slice SCG (B-SCG) is built from the sizes of backward slices, same-backward-slice clusters, and coherent clusters, while the forward-slice SCG (F-SCG) is built from the sizes of forward slices, same-forward-slice clusters, and coherent clusters.
Note that both backward and forward SCGs use the same coherent cluster sizes.
The B-SCG and F-SCG for the program bc are shown in Fig. 4.
In both graphs the slice size landscape is plotted using a solid blackline, the same-slice cluster size landscape using a gray line, and the coherent cluster size landscape using a (red) broken line.
The B-SCG (Fig. 4a) shows that bc contains two large same-backward-slice clusters consisting of around 55% and 15% of the program.
Surprisingly, the larger same-backward-slice cluster is composed of smaller slices than the smaller same-backward-slice cluster; thus, the smaller cluster has a bigger impact (slice size) than the larger cluster.
In addition, the presence of three coherent clusters spanning approximately 15%, 20% and 30% of the program's statements can also be seen.
Fig. 3c shows two box plots depicting the distribution of (backward and forward) slice sizes for bc.
The average size of the slices is also displayed in the box plot using a solid square box.
Comparing the box plot information to the information provided by the MSGs, we can see that all the information available from the box plots can be derived from the MSGs itself (except for the average).
However, MSGs show a landscape (slice profile) which cannot be obtained from the box plots.
Similarly, the box plots in Fig. 4c show the size distributions of the various clusters (i.e. a vertex is in a cluster of size x) in addition to the slice size distributions.
Although the information from these box plots can not be derived from the SCGs shown in Fig. 4a and b directly, the profiles (landscapes) give a better intuition about the clusters, the number of major clusters and their sizes.
For our empirical study we use the size of individual clusters and the cluster profile to find mappings between the clusters and program components.
Therefore, we drop box plots in favor of SCGs to show the cluster profile and provide additional statistics in tabular format where required.
Empirical evaluation
This section presents the empirical evaluation into the existence and impact of coherent dependence clusters.
The section first discusses the experimental setup and the subject programs included in the study.
It then presents two validation studies, the first considers the effect of pointer analysis precision and the second considers the validity of hashing in efficient cluster identification.
The section then quantitatively considers the existence of coherent dependence clusters and identifies patterns of clustering within the programs.
This is followed by a series of four case studies, where qualitative analysis, aided by the decluvi cluster visualization tool (Islam et al., 2010a), highlight how knowledge of clusters can aid a software engineer.
The section then presents studies on inter-cluster dependence, and the relationship of program faults and system evolution to coherent clusters.
Finally, threats to validity are considered.
To formalize the goals of this section, the empirical evaluation addresses the following research questions:RQ1
What is the effect of pointer analysis precision on coherent clusters?
RQ2
How precise is hashing as a proxy for comparing slices?
RQ3
How large are the coherent clusters that exist in production source code and which patterns of clustering can be identified?
RQ4
Which structures within a program can coherent cluster analysis reveal?
RQ5
What are the implications of inter-cluster dependence between coherent clusters?
RQ6
How do program faults relate to coherent clusters?
RQ7
How stable are coherent clusters during system evolution?
The first two research questions provide empirical verification for the results subsequently presented.
RQ1 establishes the impact of pointer analysis on the clustering, whereas RQ2 establishes that the hash function used to approximate a slice is sufficiently precise.
If the static slices produced by the slicer are overly conservative or if the slice approximation is not sufficiently precise, then the results presented will not be reliable.
Fortunately, the results provide confidence that the slice precision and hashing accuracy are sufficient.
Whereas RQ1 and RQ2 focus on the veracity of our approach, RQ3 investigates the validity of the study; if large coherent clusters are not prevalent, then they would not be worthy of further study.
We place very specific and demanding constraints on a set of vertices for it to be deemed a coherent cluster.
If such clusters are not common then their study would be merely an academic exercise.
Conversely, if the clustering is similar for every program then it is unlikely that cluster identification will reveal interesting information about programs.
Our findings reveal that, despite the tight constraints inherent in the definition of a coherent dependence cluster, they are, indeed, very common.
Also, the cluster profiles for programs are sufficiently different and exhibit interesting patterns.
These results motivate the remaining research questions.
Having demonstrated that our technique is suitable for finding coherent clusters and that such clusters are sufficiently widespread to be worthy of study, we investigate specific coherent clusters in detail.
RQ4 studies the underlying logical structure of programs revealed by these clusters.
RQ5 looks explicitly at inter-cluster dependency and considers areas of software engineering where it may be of interest.
RQ6 presents a study of how program faults relate to coherent clusters, and, finally, RQ7 studies the effect of system evolution on clustering.
Experimental subjects and setup
The slices along with the mapping between the SDG vertices and the actual source code are extracted from the mature and widely used slicing tool CodeSurfer (Anderson and Teitelbaum, 2001) (version 2.1).
The cluster visualizations were generated by decluvi (Islam et al., 2010a) using data extracted from CodeSurfer.
The data is generated from slices taken with respect to source-code representing SDG vertices.
This excludes pseudo vertices introduced into the SDG, e.g., to represent global variables which are modeled as additional pseudo parameters by CodeSurfer.
Cluster sizes are also measured in terms of source-code representing SDG vertices, which is more consistent than using lines of code as it is not influenced by blank lines, comments, statements spanning multiple lines, multiple statements on one line, or compound statements.
The decluvi system along with scheme scripts for data acquisition and pre-compiled datasets for several open-source programs can be downloaded from http://www.cs.ucl.ac.uk/staff/s.islam/decluvi.html.
The study considers the 30 C programs shown in Table 1, which provides a brief description of each program alongside seven measures: number of files containing executable C code, LoC - lines of code (as counted by the Unix utility wc), SLoC - the non-comment non-blank lines of code (as counted by the utility sloccount (Wheeler, 2004)), ELoC - the number of source code lines that CodeSurfer considers to contain executable code, the number of SDG vertices, the number of SDG edges, the number of slices produced, and finally the size (as a percentage of the program's SDG vertex count) of the largest coherent cluster.
All LoC metrics are calculated over source files that CodeSurfer considers to contain executable code and, for example, do not include header files.
Columns 10 and 11 provide the runtimes recorded during the empirical study.
The runtimes reported are wall clock times captured by the Unix time utility while running the experiments on a 64-bit Linux machine (CentOS 5) with eight Intel(R) Xeon(R) CPU E5450 @ 3.00GHz processors and 32GB of RAM.
It should be noted that this machine acts as a group server and is accessed by multiple users.
There were other CPU intensive processes intermittently running on the machine while these runtimes were collected, and thus the runtimes are only indicative.
Column 10 shows the time needed to build the SDG and CodeSurfer project that is subsequently used for slicing.
The build time for the projects were quite small and the longest build time (2m33.456s) was required for gcal with 46,827 SLoC.
Column 11 shows the time needed for the clustering algorithm to perform the clustering and create all the data dumps for decluvi to create cluster visualizations.
The process completes in minutes for small programs and can take hours and longer for larger programs.
It should be noted that the runtime includes both the slicing phase which runs in O(ne), where n is the number of SDG vertices and e is the number of edges, and the hashing and clustering algorithm which runs in O(n2).
Therefore the overall complexity is O(ne).
The long runtime is mainly due to the current research prototype (which performs slicing, clustering and extraction of the data) using the Scheme interface of CodeSurfer in a pipeline architecture.
In the future we plan to upgrade the tooling with optimizations for fast and massive slicing (Binkley et al., 2007) and to merge the clustering phase into the slicing to reduce the runtime significantly.
Although the clustering and building the visualization data can take a long time for large projects, it is still useful because the clustering only needs to be done once (for example during a nightly build) and can then be visualised and reused as many times as needed.
During further study of the visualization and the clustering we have also found that small changes to the system does not show a change in the clustering, therefore once the clustering is created it still remains viable through small code changes as the clustering is found to represent the core program architecture (Section 3.9).
Furthermore, the number of SDG vertices and edges are quite large, in fact even for very small programs the number of SDG vertices is in the thousands with edge counts in the tens of thousands.
Moreover, the analysis produces an is-in-the-slice-of relation and graph with even more edges.
We have tried several clustering and visualization tools to cluster the is-in-the-slice-of graph for comparison, but most of the tools (such as Gephi Bastian et al., 2009) failed due to the large dataset.
Other tools such as CCVisu (Beyer, 2008) which were able to handle the large data set simply produced a blob as a visualization which was not at all useful.
The underlying problem is that the is-in-the-slice-of graph is dense and no traditional clustering can handle such dense graphs.
Impact of pointer analysis precision
Recall that the definition of a coherent dependence cluster is based on an underlying depends-on relation, which is approximated using program slicing.
Pointer analysis plays a key role in the precision of slicing and the interplay between pointer analysis and downstream dependence analysis precision is complex (Shapiro and Horwitz, 1997).
To understand how pointer analysis precision impacts the clustering of the programs we study the effect in this section.
Usually, one would choose the pointer analysis with the highest precision but there may be situations where this is not possible and one has to revert to lower precision analysis.
This section presents a study on the effect of various levels of pointer analysis precision on the size of slices and subsequently on coherent clusters.
It addresses research question RQ1: What is the effect of pointer analysis precision on coherent clusters?
CodeSurfer provides three levels of pointer analysis precision (Low, Medium, and High) that provide increasingly precise points-to information at the expense of additional memory and analysis time.
The Low setting uses a minimal pointer analysis that assumes every pointer may point to every object that has its address taken (variable or function).
At the Medium and High settings, CodeSurfer performs extensive pointer analysis using the algorithm proposed by Fahndrich et al. (1998), which implements a variant of Andersen's pointer analysis algorithm (Andersen, 1994) (this includes parameter aliasing).
At the medium setting, fields of a structure are not distinguished while the High level distinguishes structure fields.
The High setting should produce the most precise slices but requires more memory and time during SDG construction, which puts a functional limit on the size and complexity of the programs that can be handled by CodeSurfer.
There is no automatic way to determine whether the slices are correct and precise.
Weiser (1984) considers smaller slices to be better.
Slice size is often used to measure the impact of the analysis' precision (Shapiro and Horwitz, 1997), similarly we also use slice size as a measure of precision.
The study compares slice and cluster size for CodeSurfer's three precision options (Low, Medium, High) to study the impact of pointer analysis precision.
The results are shown in Table 2.
Column 1 lists the programs and the other columns present the average slice size, maximum slice size, average cluster size, and maximum cluster size, respectively, for each of the three precision settings.
The results for average slice size deviation and largest cluster size deviation are visualized in Figs.
5 and 6.
The graphs use the High setting as the base line and show the percentage deviation when using the Low and Medium settings.
Fig. 5 shows the average slice size deviation when using the lower two settings compared to the highest.
On average, the Low setting produces slices that are 14% larger than the High setting.
Program userv has the largest deviation of 37% when using the Low setting.
For example, in userv the minimal pointer analysis fails to recognize that the function pointer oip can never point to functions sighandler_alrm and sighandler_child and includes them as called functions at call sites using *oip, increasing slice size significantly.
In all 30 programs, the Low setting yields larger slices compared to the High setting.
The Medium setting always yields smaller slices when compared to the Low setting.
For eight programs, the medium setting produces the same average slice size as the High setting.
For the remaining programs the Medium setting produces slices that are on average 4% larger than when using the High setting.
The difference in slice size occurs because the Medium setting does not differentiate between structure fields, which the High setting does.
The largest deviation is seen in findutils at 29%.
With the medium setting, the structure fields (options, regex_map, stat_buf and state) of findutils are lumped together as if each structure were a scalar variable, resulting in larger, less precise, slices.
Fig. 6 visualizes the deviation of the largest coherent cluster size when using the lower two settings compared to the highest.
The graph shows that the size of the largest coherent clusters found when using the lower settings is larger in most of the programs.
On average there is a 22% increase in the size of the largest coherent cluster when using the Low setting and a 10% increase when using the Medium setting.
In a2ps and cflow the size of the largest cluster increases over 100% when using the Medium setting and over 150% when using the Low setting.
The increase in slice size is expected to result in larger clusters due to the loss of precision.
The B-SCGs for a2ps for the three settings is shown in Fig. 7a.
In the graphs it is seen that the slice sizes get smaller and have increased steps in the (black) landscape indicating that the slices become more precise.
The red landscape shows that there is a large coherent cluster detected when using the Low setting running from approx.
60-80% on the x-axis.
This cluster drops in size when using the Medium setting.
At the High setting this coherent cluster breaks up into multiple smaller clusters.
In this case, a drop in the cluster size also leads to breaking of the cluster in to multiple smaller clusters.
In the SCGs for cflow (Fig. 7b) a similar drop in the slice size and cluster size is observed.
However, unlike a2ps the large coherent cluster does not break into smaller clusters but only drops in size.
The largest cluster when using the Low setting runs from 60% to 85% on the x-axis.
This cluster reduces in size and shifts position running 30% to 45% x-axis when using the Medium setting.
The cluster further drops in size down to 5% running 25-30% on the x-axis when using the High setting.
In this case the largest cluster has a significant drop in size but does not break into multiple smaller clusters.
Surprisingly, Fig. 6 also shows seven programs where the largest coherent cluster size actually increases when using the highest pointer analysis setting on CodeSurfer.
Fig. 7c shows the B-SCGs for acm which falls in this category.
This counter-intuitive result is seen only when the more precise analysis determines that certain functions cannot be called and thus excludes them from the slice.
Although in all such instances slices get smaller, the clusters may grow if the smaller slices match other slices already forming a cluster.
For example, consider replacing function f6 in Fig. 1 with the code shown in Fig. 8, where f depends on a function call to a function referenced through the function pointer p.
Assume that the highest precision pointer analysis determines that p does not point to f2 and therefore there is no call to f2 or any other function from f6.
The higher precision analysis would therefore determine that the forward slices and backward slices of a, b and c are equal, hence grouping these three vertices in a coherent cluster.
Whereas the lower precision is unable to determine that p cannot point to f2, the backward slice on f will conservatively include b.
This will lead the higher precision analysis to determine that the set of vertices {a, b, c} is one coherent cluster whereas the lower precision analysis include only the set of vertices {a, c} in the same coherent cluster.
Although we do not explicitly report the project build times on CodeSurfer and the clustering runtimes for the lower settings, it has been our experience that in the majority of the cases the build times for the lower settings were smaller.
However, as lower pointer analysis settings yield large points-to sets and subsequently larger slices, the clustering runtimes were higher than when using the highest setting.
Moreover, in some cases with the lower settings there was an explosive growth in summary edge generation which resulted in exceptionally high project build times and clustering runtimes.
As an answer to RQ1, we find that in the majority of the cases the Medium and Low settings result in larger coherent clusters when compared to the High setting.
For the remaining cases we have identified valid scenarios where more precise pointer analysis can result in larger coherent clusters.
The results also confirm that a more precise pointer analysis leads to more precise (smaller) slices.
Because it gives the most precise slices and most accurate clusters, the remainder of the paper uses the highest CodeSurfer pointer analysis setting.
Validity of the hash function
This section addresses research question RQ2: How precise is hashing as a proxy for comparing slices?
The section first gives a brief description of the hash function and then validates the use of comparing slice hash values in lieu of comparing actual slice content.
The use of hash values to represent slices reduces both the memory requirement and runtime, as it is no longer necessary to store or compare entire slices.
The hash function, denoted H in Definition 2.6, uses XOR operations iteratively on the unique vertex IDs (of the SDG) which are included in a slice to generate a hash for the entire slice.
We chose XOR as the hash operator because we do not have duplicate vertices in a slice and the order of the vertices in the slice does not matter.
A slice S is a set of SDG vertices {v1,…,vn} (n≥1) and id(vi) represents the unique vertex ID assigned by CodeSurfer to vertex vi, where 1≤i≤n.
The hash function H for S is defined as HS, where
(2)HS=⊕i=1nid(vi)
The remainder of this section presents a validation study of the hash function.
The validation is needed to confirm that the hash values provide a sufficiently accurate summary of slices to support the correct partitioning of SDG vertices into coherent clusters.
Ideally, the hash function would produce a unique hash value for each distinct slice.
The validation study aims to find the number of unique slices for which the hash function successfully produces an unique hash value.
For the validation study we chose 16 programs from the set of 30 subject programs.
The largest programs were not included in the validation study to make the study time-manageable.
Results are based on both the backward and forward slices for every vertex of these 16 programs.
To present the notion of precision we introduce the following formalization.
Let V be the set of all source-code representing SDG vertices for a given program P and US denote the number of unique slices: US=|{BSlice(x):x∈V}|+|{FSlice(x):x∈V}|.
Note that if all vertices have the same backward slice then {BSlice(x):x∈V} is a singleton set.
Finally, let UH be the number of unique hash-values, UH=|{H(BSlice(x)):x∈V}|+|{H(FSlice(x)):x∈V}|.
The accuracy of hash function H is given as Hashed Slice Precision, HSP=UH/US.
A precision of 1.00 (US=UH) means the hash function is 100% accurate (i.e., it produces a unique hash value for every distinct slice) whereas a precision of 1/US means that the hash function produces the same hash value for every slice leaving UH=1.
Table 3 summarizes the results.
The first column lists the programs.
The second and the third columns report the values of US and UH respectively.
The fourth column reports HSP, the precision attained using hash values to compare slices.
Considering all 78,587 unique slices the hash function produced unique hash values for 74,575 of them, resulting in an average precision of 94.97%.
In other words, the hash function fails to produce unique hash values for just over 5% of the slices.
Considering the precision of individual programs, five of the programs have a precision greater than 97%, while the lowest precision, for findutils, is 92.37%.
This is, however, a significant improvement over previous use of slice size as the hash value, which is only 78.3% accurate in the strict case of zero tolerance for variation in slice contents (Binkley and Harman, 2005).
Coherent cluster identification uses two hash values for each vertex (one for the backward slice and other for the forward slice) and the slice sizes.
Slice size matching filters out some instances where the hash values happen to be the same by coincidence but the slices are different.
The likelihood of both hash values matching those from another vertex with different slices is less than that of a single hash matching.
Extending US and UH to clusters, columns 5 and 6 (Table 3) report CC, the number of coherent clusters in a program and HCC, the number of coherent clusters found using hashing.
The final column shows the precision attained using hashing to identify clusters, HCP=HCC/CC.
The results show that of the 40,169 coherent clusters, 40,083 are uniquely identified using hashing, which yields a precision of 99.72%.
Five of the programs show total agreement, furthermore for every program HCP is over 99%, except for userv, which has the lowest precision of 97.76%.
This can be attributed to the large percentage (96%) of single vertex clusters in userv.
The hash values for slices taken with respect to these single-vertex clusters have a higher potential for collision leading to a reduction in overall precision.
In summary, as an answer to RQ2, the hash-based approximation is found to be sufficiently accurate at 94.97% for slices and at 99.72% for clusters (for the studied programs).
Thus, comparing hash values can replace the need to compare actual slices.
Do large coherent clusters occur in practice?
Having demonstrated that hash function H can be used to effectively approximate slice contents, this section and the following section consider the validation research question, RQ3: How large are coherent clusters that exist in production source code and which patterns of clustering can be identified?
The question is first answered quantitatively using the size of the largest coherent cluster in each program and then through visual analysis of the SCGs.
To assess if a program includes a large coherent cluster, requires making a judgement concerning what threshold constitutes large.
Following prior empirical work (Binkley and Harman, 2005; Harman et al., 2009; Islam et al., 2010a,b), a threshold of 10% is used.
In other words, a program is said to contain a large coherent cluster if 10% of the program's SDG vertices produce the same backward slice as well as the same forward slice.
Fig. 9 shows the size of the largest coherent cluster found in each of the 30 subject programs.
The programs are divided into 3 groups based on the size of the largest cluster present in the program.Small:
Small consists of seven programs none of which have a coherent cluster constituting over 10% of the program vertices.
These programs are archimedes, time, wdiff, byacc, a2ps, cflow and userv.
Although it may be interesting to study why large clusters are not present in these programs, this paper focuses on studying the existence and implications of large coherent clusters.
Large:
This group consists of programs that have at least one cluster with size 10% or larger.
As there are programs containing much larger coherent clusters, a program is placed in this group if it has a large cluster between the size 10% and 50%.
Over two-thirds of the programs studied fall in this category.
The program at the bottom of this group (acct) has a coherent cluster of size 11% and the largest program in this group (copia) has a coherent cluster of size 48%.
We present both these programs as case studies and discuss their clustering in detail in Sections 3.6.1 and 3.6.4, respectively.
The program bc which has multiple large clusters with the largest of size 32% falls in the middle of this group and is also presented as a case study in Section 3.6.3.
Huge:
The final group consists of programs that have a large coherent cluster whose size is over 50%.
Out of the 30 programs 4 fall in this group.
These programs are indent, ed, barcode and gcal.
From this group, we present indent as a case study in Section 3.6.2.
In summary all but 7 of the 30 subject programs contain a large coherent cluster.
Therefore, over 75% of the subject programs contain a coherent cluster of size 10% or more.
Furthermore, half the programs contain a coherent cluster of at least 20% in size.
It is interesting to note that although this grouping is based only on the largest cluster, many of the programs contain multiple large coherent clusters.
For example, ed, ctags, nano, less, bc, findutils, flex and garpd all have multiple large coherent clusters.
It is also interesting to note that there is no correlation between a program's size (measured in SLoC) and the size of its largest coherent cluster.
For example, in Table 1 two programs of very different sizes, cflow and userv, have similar largest-cluster sizes of 8% and 9%, respectively.
Whereas programs acct and ed, of similar size, have very different largest coherent clusters of sizes 11% and 55%.
Therefore as an answer to first part of RQ3, the study finds that 23 of the 30 programs studied have a large coherent cluster.
Some programs also have a huge cluster covering over 50% of the program vertices.
Furthermore, the choice of 10% as a threshold for classifying a cluster as large is a relatively conservative choice.
Thus, the results presented in this section can be thought of as a lower bound to the existence question.
Patterns of clustering
This section presents a visual study of SCGs for the three program groups and addresses the second part of RQ3.
Figs.
10-12 show graphs for the three categories.
The graphs in the figures are laid out in ascending order based on the largest coherent cluster present in the program and thus follow the same order as seen in Fig. 9.
Fig. 10 shows SCGs for the seven programs of the small group.
In the SCGs of the first three programs (archimedes, time and wdiff) only a small coherent cluster is visible in the red landscape.
In the remaining four programs, the red landscape shows the presence of multiple small coherent clusters.
It is very likely that, similar to the results of the case studies presented later, these clusters also depict logical constructs within each program.
Fig. 11 shows SCGs of the 19 programs that have at least one large, but not huge, coherent cluster.
That is, each program has at least one coherent cluster covering 10-50% of the program.
Most of the programs have multiple coherent clusters as is visible on the red landscape.
Some of these have only one large cluster satisfying the definition of large, such as acct.
The clustering of acct is discussed in further detail in Section 3.6.1.
Most of the remaining programs are seen to have multiple large clusters such as bc, which is also discussed in further detail in Section 3.6.3.
The presence of multiple large coherent cluster hints that the program consists of multiple functional components.
In three of the programs (which, gnuedma and copia) the landscape is completely dominated by a single large coherent cluster.
In which and gnuedma this cluster covers around 40% of the program vertices whereas in copia the cluster covers 50%.
The presence of a single large dominating cluster points to a centralized functionality or structure being present in the program.
Copia is presented as a case study in Section 3.6.4 where its clustering is discussed in further detail.
Finally, SCGs for the four programs that contain huge coherent clusters (covering over 50%) are found in Fig. 12.
In all four landscapes there is a very large dominating cluster with other smaller clusters also being visible.
This pattern supports the conjecture that the program has one central structure or functionality which consists of most of the program elements, but also has additional logical constructs that work in support of the central idea.
Indent is one program that falls in this category and is discussed in further detail in Section 3.6.2.
As an answer to second part of RQ3, the study finds that most programs contain multiple coherent clusters.
Furthermore, the visual study reveals that a third of the programs have multiple large coherent clusters.
Only three programs copia, gnuedma, and which show the presence of only a single (overwhelming) cluster covering most of the program.
Having shown that coherent clusters are prevalent in programs and that most programs have multiple significant clusters, the next section presents a series of four case studies that looks at how program structures are represented by these clusters.
Coherent cluster and program decomposition
This section presents four case studies using acct, indent, bc and copia.
The case studies form a major contribution of the paper and collectively address research question RQ4: Which structures within a program can coherent cluster analysis reveal? As coherent clusters consist of program vertices that are mutually inter-dependent and share extra-cluster properties we consider such vertices of the cluster to be tightly coupled.
It is our conjecture that these clusters likely represent logical structures representing a high-level functional decomposition of systems.
This study will therefore look at how coherent clusters map to logical structures of the program.
The case studies have been chosen to represent the large and huge groups identified in the previous section.
Three programs are taken from the large group as it consists of the majority of the programs and one from the huge group.
Each of the three programs from the large group were chosen because it exhibits specific patterns.
acct has multiple coherent clusters visible in its profile and has the smallest large cluster in the group, bc has multiple large coherent clusters, and copia has only a single large coherent cluster dominating the entire landscape.
Case study: acct
The first of the series of case studies is acct, an open-source program used for monitoring and printing statistics about users and processes.
The program acct is one of the smaller programs with 2600 LoC and 1558 SLoC from which CodeSurfer produced 2834 slices.
The program has seven C files, two of which, getopt.c and getopt1.c, contain only conditionally included functions.
These functions provide support for command-line argument processing and are included if needed library code is missing.
Table 4 shows the statistics for the five largest clusters of acct.
Column 1 gives the cluster number, where 1 is the largest and 5 is the 5th largest cluster measured using the number of vertices.
Columns 2 and 3 show the size of the cluster as a percentage of the program's vertices and actual vertex count, as well as the line count.
Columns 4 and 5 show the number of files and functions where the cluster is found.
The cluster sizes range from 11.4% to 2.4%.
These five clusters can be readily identified in the Heat-Map visualization (not shown) of decluvi.
The rest of the clusters are very small (less than 2% or 30 vertices) in size and are thus of little interest.
The B-SCG for acct (row one of Fig. 11) shows the existence of these five coherent clusters along with other same-slice clusters.
Splitting of the same-slice cluster is evident in the SCG.
Splitting occurs when the vertices of a same-slice cluster become part of different coherent clusters.
This happens when vertices have either the same backward slice or the same forward slice but not both.
This is because either same-backward-slice or same-forward-slice clusters only capture one of the two external properties captured by coherent clusters (Eq.
(1)).
In acct's B-SCG the vertices of the largest same-backward-slice cluster spanning the x-axis from 60% to 75% are not part of the same coherent cluster.
This is because the vertices do not share the same forward slice which is also a requirement for coherent clusters.
This phenomenon is common in the programs studied and is found in both same-backward-slice and same-forward-slice clusters.
This is another reason why coherent clusters are often smaller in size then same-slice clusters.
Decluvi visualization (not shown) of acct reveals that the largest cluster spans four files (file_rd.c, common.c, ac.c, and utmp_rd.c), the 2nd largest cluster spans only a single file (hashtab.c), the 3rd largest cluster spans three files (file_rd.c, ac.c, and hashtab.c), the 4th largest cluster spans two files (ac.c and hashtab.c), while the 5th largest cluster includes parts of ac.c only.
The largest cluster of acct is spread over six functions, log_in, log_out, file_open, file_reader_get_entry, bad_utmp_record and utmp_get_entry.
These functions are responsible for putting accounting records into the hash table used by the program, accessing user-defined files, and reading entries from the file.
Thus, the purpose of the code in this cluster is to track user login and logout events.
The second largest cluster is spread over two functions hashtab_create and hashtab_resize.
These functions are responsible for creating fresh hash tables and resizing existing hash tables when the number of entries becomes too large.
The purpose of the code in this cluster is the memory management in support of the program's main data structure.
The third largest cluster is spread over four functions: hashtab_set_value, log_everyone_out, update_user_time, and hashtab_create.
These functions are responsible for setting values of an entry, updating all the statistics for users, and resetting the tables.
The purpose of the code from this cluster is the modification of the user accounting data.
The fourth cluster is spread over three functions: hashtab_delete, do_statistics, and hashtab_find.
These functions are responsible for removing entries from the hash table, printing out statistics for users and finding entries in the hash table.
The purpose of the code from this cluster is maintaining user accounting data and printing results.
The fifth cluster is contained within the function main.
This cluster is formed due to the use of a while loop containing various cases based on input to the program.
Because of the conservative nature of static analysis, all the code within the loop is part of the same cluster.
Finally, it is interesting to note that functions from the same file or with similar names do not necessarily belong to the same cluster.
Intuitively, it can be presumed that functions that have similar names or prefixes work together to provide some common functionality.
In this case, six functions that have the same common prefix "hashtab" all perform operations on the hash table.
However, these six functions are not part of the same cluster.
Instead the functions that work together to provide a particular functionality are found in the same cluster.
The clusters help identify functionality which is not obvious from the name of program artefacts such as functions and files.
As an answer to RQ4, we find that in this case study each of the top five clusters maps to specific logical functionality.
Case study: indent
The next case study uses indent to further support the answer found for RQ4 in the acct case study.
The characteristics of indent are very different from those of acct as indent has a very large dominant coherent cluster (52%) whereas acct has multiple smaller clusters with the largest being 11%.
We include indent as a case study to ensure that the answer for RQ4 is derived from programs with different cluster profiles and sizes giving confidence as to the generality of the answer.
Indent is a Unix utility used to format C source code.
It consists of 6978 LoC with 7543 vertices in the SDG produced by CodeSurfer.
Table 5 shows statistics of the five largest clusters found in the program.
Indent has one extremely large coherent cluster that spans 52.1% of the program's vertices.
The cluster is formed of vertices from 54 functions spread over 7 source files.
This cluster captures most of the logical functionalities of the program.
Out of the 54 functions, 26 begin with the common prefix of "handle_token".
These 26 functions are individually responsible for handling a specific token during the formatting process.
For example, handle_token_colon, handle_token_comma, handle_token_comment, and handle_token_lbrace are responsible for handling the colon, comma, comment, and left brace tokens, respectively.
This cluster also includes multiple handler functions that check the size of the code and labels being handled, such as check_code_size and check_lab_size.
Others, such as search_brace, sw_buffer, print_comment, and reduce, help with tracking braces and comments in code.
The cluster also spans the main loop of indent (indent_main_loop) that repeatedly calls the parser function parse.
Finally, the cluster consists of code for outputting formatted lines such as the functions better_break, computer_code_target, dump_line, dump_line_code, dump_line_label, inhibit_indenting, is_comment_start, output_line_length and slip_horiz_space, and ones that perform flag and memory management (clear_buf_break_list, fill_buffer and set_priority).
Cluster 1 therefore consists of the main functionality of this program and provides support for parsing, handling tokens, associated memory management, and output.
The parsing, handling of individual tokens and associated memory management are highly inter-twined.
For example, the handling of each individual token is dictated by operations of indent and closely depends on the parsing.
This code cannot easily be decoupled and, for example, reused.
Similarly the memory management code is specific to the data structures used by indent resulting in these many logical constructs to become part of the same cluster.
The second largest coherent cluster consists of 7 functions from 3 source files.
These functions handle the arguments and parameters passed to indent.
For example, set_option and option_prefix along with the helper function eqin to check and verify that the options or parameters passed to indent are valid.
When options are specified without the required arguments, the function arg_missing produces an error message by invoking usage followed by a call to DieError to terminate the program.
Clusters 3-5 are less than 3% of the program and are too small to warrant a detailed discussion.
Cluster 3 includes 6 functions that generate numbered/un-numbered backup for subject files.
Cluster 4 has functions for reading and ignoring comments.
Cluster 5 consists of a single function that reinitializes the parser and associated data structures.
The case study of indent further illustrates that coherent clusters can capture the program's logical structure as an answer to research question RQ4.
However, in cases such as this where the internal functionality is tightly knit, a single large coherent cluster maps to the program's core functionality.
Case study: bc
The third case study in this series is bc, an open-source calculator, which consists of 9438 LoC and 5450 SLoC.
The program has nine C files from which CodeSurfer produced 15,076 slices (backward and forward).
Analyzing bc's SCG (row 3, Fig. 11), two interesting observations can be made.
First, bc contains two large same-backward-slice clusters visible in the light gray landscapes as opposed to the three large coherent clusters.
Second, looking at the B-SCG, it can be seen that the x-axis range spanned by the largest same-backward-slice cluster is occupied by the top two coherent clusters shown in the dashed red (dark gray) landscape.
This indicates that the same-backward-slice cluster splits into the two coherent clusters.
The statistics for bc's top five clusters are given in Table 6.
Sizes of these five clusters range from 32.3% through to 1.4% of the program.
Clusters six onwards are less than 1% of the program.
The Project View (Fig. 13) shows their distribution over the source files.
In more detail, Cluster 1 spans all of bc's files except for scan.c and bc.c.
This cluster encompasses the core functionality of the program - loading and handling of equations, converting to bc's own number format, performing calculations, and accumulating results.
Cluster 2 spans five files, util.c, execute.c, main.c, scan.c, and bc.c.
The majority of the cluster is distributed over the latter two files.
Even more interestingly, the source code of these two files (scan.c and bc.c) map only to Cluster 2 and none of the other top five clusters.
This indicates a clear purpose to the code in these files.
These two files are solely used for lexical analysis and parsing of equations.
To aid in this task, some utility functions from util.c are employed.
Only five lines of code in execute.c are also part of Cluster 2 and are used for flushing output and clearing interrupt signals.
The third cluster is completely contained within the file number.c.
It encompasses functions such as _bc_do_sub, _bc_init_num, _bc_do_compare, _bc_do_add, _bc_simp_mul, _bc_shift_addsub, and _bc_rm_leading_zeros, which are responsible for initializing bc's number formatter, performing comparisons, modulo and other arithmetic operations.
Clusters 4 and 5 are also completely contained within number.c.
These clusters encompass functions to perform bcd operations for base ten numbers and arithmetic division, respectively.
As an answer to RQ4, the results of the cluster visualizations for bc reveal its high-level structure.
This aids an engineer in understanding how the artifacts (e.g., functions and files) of the program interact, thus aiding in program comprehension.
The remainder of this subsection illustrates a side-effect of decluvi's multi-level visualization, how it can help find potential problems with the structure of a program.
Util.c consists of small utility functions called from various parts of the program.
This file contains code from Clusters 1 and 2 (Fig. 13).
Five of the utility functions belong with Cluster 1, while six belong with Cluster 2.
Furthermore, Fig. 14 shows that the distribution of the two clusters in red (dark gray) and blue (medium gray) within the file are well separated.
Both clusters do not occur together inside any function with the exception of init_gen (highlighted by the rectangle in first column of Fig. 14).
The other functions of util.c thus belong to either Cluster 1 or Cluster 2.
Separating these utility functions into two separate source files where each file is dedicated to functions belonging to a single cluster would improve the code's logical separation and file-level cohesion.
This would make the code easier to understand and maintain at the expense of a very simple refactoring.
In general, this example illustrates how decluvi visualization can provide an indicator of potential points of code degradation during evolution.
Finally, the Code View for function init_gen shown in Fig. 15 includes Lines 244, 251, 254, and 255 in red (dark gray) from Cluster 1 and Lines 247, 248, 249, and 256 in blue (medium gray) from Cluster 2.
Other lines, shown in light gray, belong to smaller clusters and lines containing no executable code.
Ideally, clusters should capture a particular functionality; thus, functions should generally not contain code from multiple clusters (unless perhaps the clusters are completely contained within the function).
Functions with code from multiple clusters reduce code separation (hindering comprehension) and increase the likelihood of ripple-effects (Black, 2001).
Like other initialization functions, bc'sinit_gen is an exception to this guideline.
This case study not only provides support for the answer to research question RQ4 found in previous case studies, but also illustrates that the visualization is able to reveal structural defects in programs.
Case study: copia
The final case study in this series is copia, an industrial program used by the ESA to perform signal processing.
Copiais the smallest program considered in this series of case studies with 1168 LoC and 1111 SLoC all in a single C file.
Its largest coherent cluster covers 48% of the program.
The program is at the top of the group with large coherent clusters.
CodeSurfer extracts 6654 slices (backward and forward).
The B-SCG for copia is shown in Fig. 16a.
The single large coherent cluster spanning 48% of the program is shown by the dashed red (dark gray) line (running approx.
from 2% to 50% on the x-axis).
The plots for same-backward-slice cluster sizes (light gray line) and the coherent cluster sizes (dashed line) are identical.
This is because the size of the coherent clusters are restricted by the size of the same-backward-slice clusters.
Although the plot for the size of the backward slices (black line) seems to be the same from the 10% mark to 95% mark on the x-axis, the slices are not exactly the same.
Only vertices plotted from 2% through to 50% have exactly same backward and forward slice resulting in the large coherent cluster.
Table 7 shows statistics for the top five coherent clusters found in copia.
Other than the largest cluster which covers 48% of the program, the rest of the clusters are extremely small.
Clusters 2-5 include no more than 0.1% of the program (four vertices) rendering them too small to be of interest.
This suggests a program with a single functionality or structure.
During analysis of copiausing decluvi, the File View (Fig. 17) reveals an intriguing structure.
There is a large block of code with the same spatial arrangement (bounded by the dotted black rectangle in Fig. 17) that belongs to the largest cluster of the program.
It is unusual for so many consecutive source code lines to have nearly identical length and indentation.
Inspection of the source code reveals that this block of code is a switch statement handling 234 cases.
Further investigation shows that copiahas 234 small functions that eventually call one large function, seleziona, which in turn calls the smaller functions effectively implementing a finite state machine.
Each of the smaller functions returns a value that is the next state for the machine and is used by the switch statement to call the appropriate next function.
The primary reason for the high level of dependence in the program lies with the statement switch(next_state), which controls the calls to the smaller functions.
This causes what might be termed 'conservative dependence analysis collateral damage' because the static analysis cannot determine that when function f() returns the constant value 5 this leads the switch statement to eventually invoke function g().
Instead, the analysis makes the conservative assumption that a call to f() might be followed by a call to any of the functions called in the switch statement, resulting in a mutual recursion involving most of the program.
Although the coherent cluster still shows the structure of the program and includes all these stub functions that work together, this is a clear case of dependence pollution (Binkley and Harman, 2005), which is avoidable.
To illustrate this, the code was re-factored to simulate the replacement of the integer next_state with direct recursive function calls.
The SCG for the modified version of copiais shown in Fig. 16b where the large cluster has clearly disappeared.
As a result of this reduction, the potential impact of changes to the program will be greatly reduced, making it easier to understand and maintain.
This is even further amplified for automatic static analysis tools such as CodeSurfer.
Of course, in order to do a proper re-factoring, the programmer will have to consider ways in which the program can be re-written to change the flow of control.
Whether such a re-factoring is deemed cost-effective is a decision that can only be taken by the engineers and managers responsible for maintaining the program in question.
This case study reiterates the answer for RQ4 by showing the structure and dependency within the program.
It also identifies potential refactoring points which can improve the performance of static analysis tools and make the program easier to understand.
Inter-cluster dependence
This section addresses research question RQ5: What are the implications of inter-cluster dependence between coherent clusters?
The question attempts to reveal whether there is dependence (slice inclusion) relationship between the vertices of different coherent clusters.
A slice inclusion relationship between two clusters X and Y exist, if ∃x∈X:BSlice(x)∩Y≠∅.
If such containment occurs, it must be a strict containment relationship (BSlice(x)∩Y=Y, see Eq.
1).
Defining this relation using forward slices produces the inverse relation.
In the series of case studies presented earlier we have seen that coherent clusters map to logical components of a system and can be used to gain an understanding of the architecture of the program.
If such dependencies exist that allows entire clusters to depend on other clusters, then this dependence relationship can be used to group clusters to form a hierarchical decomposition of the system where coherent clusters are regarded as sub-systems, opening up the potential use of coherent clusters in reverse engineering.
Secondly, if there are mutual dependency relations between clusters then such mutual dependency relationships can be used to provide a better estimate of slice-based clusters.
All vertices of a coherent cluster share the same external and internal dependence, that is, all vertices have the same backward slice and also the same forward slice.
Because of this, any backward/forward slice that includes a vertex from a cluster will also include all other vertices of the same cluster (Eq.
1).
The study exploits this unique property of coherent clusters to investigate whether or not a backward slice taken with respect to a vertex of a coherent cluster includes vertices of another cluster.
Note that if vertices of coherent cluster X are contained in the slice taken with respect to a vertex of coherent cluster Y, then all vertices of X are contained in the slice taken with respect to each vertex of Y (follows from Eq. 1).
Fig. 18 shows Cluster Dependence Graphs (CDG) for each of the four case study subjects.
Only the five largest clusters of the case study subjects are considered during this study.
The graphs depict slice containment relationships between the top five clusters of each program.
In these graphs, the top five clusters are represented by nodes (1 depicts the largest coherent cluster, while 5 is the 5th largest cluster) and the directional edges denote backward slice22
A definition based on forward slices will have the same results with reversed edges.
 inclusion relationships: A→B depicts that vertices of cluster B depend on vertices of cluster A, that is, a backward slice of any vertex of cluster B will include all vertices of cluster A (∀x∈B:BSlice(x)∩A=A).
Bi-directional edges show mutual dependencies, whereas uni-directional edges show dependency in one direction only.
In the graph for copia(Fig. 18a), the top five clusters have no slice inclusion relationships between them (absence of edges between the nodes of the CDG).
Looking at Table 7, only the largest cluster of copiais truly large at 48%, while the other four clusters are extremely small making them unlikely candidates for inter-cluster dependence.
For acct(Fig. 18b) there is a dependence between all of the top five clusters.
In fact, there is mutual dependence between clusters 1, 2, 3 and 4, while cluster 5 depends on all the other four clusters but not mutually.
Clusters 1 through 4 contain logic for manipulating, accessing, and maintaining the hash tables, making them interdependent.
Cluster 5 on the other hand is a loop structure within the main function for executing different cases based on command line inputs.
Similarly for indent (Fig. 18c), clusters 1, 2, 4, and 5 are mutually dependent and 3 depends on all the other top five clusters but not mutually.
Finally, in the case of bc (Fig. 18d), all the vertices from the top five clusters are mutually inter-dependent.
The rest of this section uses bcas an example where this mutual dependence is used to identify larger dependence structures by grouping of the inter-dependent coherent clusters.
At first glance it may seem that the grouping of the coherent clusters is simply reversing the splitting of same-backward-slice or same-forward-slice clusters observed earlier in Section 3.6.3.
However, examining the sizes of the top five same-backward-slice clusters, same-forward-slice clusters and coherent clusters for bcillustrates that it is not the case.
Table 8 shows the size of these clusters both in terms of number of vertices and as a percentage of the program.
The combined size of the group of top five inter-dependent coherent clusters is 70.43%, which is 15.67% larger than the largest same-backward-slice cluster (54.86%) and 37.91% larger than the same-forward-slice cluster (32.35%).
Therefore, the set of all (mutually dependent) vertices from the top five coherent clusters when taken together form a larger dependence structure, an estimate of a slice-based cluster.
As an answer to RQ5, this section shows that there are dependence relationships between coherent clusters and in some cases there are mutual dependences between large coherent clusters.
It also shows that it may be possible to leverage this inter-cluster relationship to build a hierarchical system decomposition.
Furthermore, groups of inter-dependent coherent clusters form larger dependence structures than same-slice clusters and provides a better approximation for slice-based clusters.
This indicates that the sizes of dependence clusters reported by previous studies (Binkley et al., 2008; Binkley and Harman, 2005, 2009; Harman et al., 2009; Islam et al., 2010b) maybe conservative and mutual dependence clusters are larger and more prevalent than previously reported.
Dependence clusters and bug fixes
Initial work on dependence clusters advised that they might cause problems in software maintenance, and thus even be considered harmful, because they represent an intricate interweaving of mutual dependencies between program elements.
Thus a large dependence cluster might be thought of as a bad code smell (Elssamadisy and Schalliol, 2002) or a anti-pattern (Binkley et al., 2008).
Black et al. (2006) suggested that dependence clusters are potentially where bugs may be located and suggested the possibility of a link between clusters and program faults.
This section further investigates this issue using a study that explores the relationship between program faults and dependence clusters.
In doing so, it addresses research question RQ6: How do program faults relate to coherent clusters?
Barcode, an open source utility tool for converting text strings to printed bars (barcodes) is used in this study.
A series of versions of the system are available for download from GNU repository.33
http://gnu.mirror.iweb.com/gnu/barcode/.
 There are nine public releases for barcode, details of which are shown in Table 9.
Column 1 shows the release version, columns 3-6 show various metrics about the size of the system in terms of number of source files and various source code size measures.
Columns 7-9 report the number of SDG vertices, SDG edges and the number of slices produced for each release.
Finally, Column 10 reports the number of faults that were fixed since the previous release of the system.
In Table 9 the size of barcode increases from 1352 lines of code in version 0.90 to 3968 lines of code in version 0.98.
The total number of faults that were fixed during this time was 39.
Fault data, gathered by manually analyzing the publicly available version control repository44
cvs.savannah.gnu.org:/sources/barcode.
 for the system, showed that total number of commits for barcode during these releases were 137.
Each update was manually checked using CVSAnaly (Robles et al., 2004) to determine whether the update was a bug fix or simply an enhancement or upgrade to the system.
Those commits that were identified as bug fixes were isolated and mapped to the release that contained the update.
All the bug fixes made during a certain release cycle were then accumulated to give the total number of bugs fixed during a particular release cycle (Column 10 of Table 9).
The reported number only includes bug fixes and does not include enhancement or addition of new functionality.
Fig. 19 shows the backward slice size plots for all versions of barcode in a single graph.
The values of the axises in Fig. 19 are shown as vertex counts rather than relative values (percentages).
This allows the growth of barcode to be easily visualized.
From the plots it is seen that the size of the program increases progressively with each new release.
The graphs also show that a significant number of vertices in each revision of the program yields identical backward slices and the proportion of vertices in the program that have identical backward slices stays roughly the same.
Overall, the profile of the clusters and slices remains consistent.
The graph also shows that the plots do not show any significant change in their overall shape or structure.
Interestingly, the plot for version 0.92 with 9 fault fixes is not different in shape from revision 0.94 where only a single fault was fixed.
As coherent clusters are composed of both backward and forward slices, the stability of the backward slice profile itself does not guarantee the stability of coherent cluster profile.
The remainder of this section looks at how the clustering profile is affected by bug fixes.
Fig. 20 shows individual SCGs for each version of barcode.
As coherent clusters are dependent on both backward and forward slices, such clusters will be more sensitive to changes in dependences within the program.
The SCGs show that from the initial version barcode-0.90 there were two coherent clusters in the system.
The smaller one is around 10% of the code while the larger is around 40% of the code.
As the system evolved and went through various modifications and enhancements, the number of clusters and the profile of the clusters remained consistent other than its scaled growth with the increase in program size.
It is also evident that during evolution of the system, the enhancement code or newly added code formed part of the larger cluster.
This is why in the later stages of the evolution we see an increase in the size of the largest cluster, but not the smaller one.
However, we do not see any significant changes in the slice and cluster profile of the program that can be attributed to bug fixes.
For example, the single bug fixed between revisions 0.93 and 0.94 was on a single line of code from the file code128.c.
The changes to the line is shown in Fig. 21 (in version 0.93 there is an error in calculating the checksum value, which was corrected in version 0.94).
As illustrated by this example, the data and control flow of the program and thus the dependencies between program points are not affected by the bug fix and hence no change is observed between the SCGs of the two releases (Fig. 20).
If dependence clusters correlated to faults, or, if dependence clusters were directly related to the number of faults in a program, then a significant difference would be expected in the shape of the SCG when faults were rectified.
The SCGs for program barcode (Fig. 20) show no change in their profile when faults within the program are fixed.
This provides evidence that faults may not be dictated by the presence or absence of dependence clusters.
As an answer to RQ6, the study of barcode finds no correlation between the existence of dependence clusters and program faults and their fix.
We have to be careful in generalising the answer to this question because of the small dataset considered in this study, further extended research is needed to derive a more generalised answer.
Moreover, this does not exclude the possibility that most program faults occur in code that are part of large clusters.
In future we plan to extend this experiment in a qualitative form to study whether program faults lie within large or small clusters, or outside them altogether.
Clusters and system evolution
The previous section showed that for barcode the slice and cluster profiles remain quite stable through bug fixes during system evolution and its growth of almost 2.5 times over a period of 3 years.
This section extends that study by looking for cluster changes during system evolution.
It addresses RQ7: How stable are coherent clusters during system evolution? using longitudinal analysis of the case studies presented earlier.
From the GNU repository we were able to retrieve four releases for bc, four releases for acct and 14 releases for indent.
As copia is an industrial closed-source program, we were unable to obtain any previous versions of the program and thus the program is excluded from this study.
The graphs in Fig. 22 show backward slice size overlays for every version of each program.
Fig. 22a and c for bc and indent show that these systems grow in size during its evolution.
The growth is more prominent in indent (Fig. 22c) where the program grows from around 4800 vertices in its initial version to around 7000 vertices in the final version.
The growth for bc is smaller, it grows from around 6000 vertices to 7000 vertices.
This is partly because the versions considered for bc are all minor revisions.
For both bc and indent the slice-size graphs show very little change in their profile.
The graphs mainly show a scale up that parallels the growth of the system.
For acct (Fig. 22b) the plots do not simply show a scale up but show a significant difference.
In the 4 plots, the revisions that belong to the same major release are seen to be similar and show a scaling, whereas those from different major releases show very different landscapes.
The remainder of this section gives detail of these clustering profile changes.
Fig. 23 shows the BSCGs for the four versions of bc.
Initially, the backward slice size plots (solid black lines) show very little difference.
However, upon closer inspection of the last three versions we see that the backward slice size plot changes slightly at around the 80% mark on the x-axis.
This is highlighted by the fact that the later three versions show an additional coherent cluster spanning from 85% to 100% on the x-axis which is absent from the initial release.
Upon inspection of the source code changes between versions bc-1.03 and bc-1.04 the following types of updates were found:1
bug fixes,
2
addition of command line options,
3
reorganization of the source tree, and
4
addition of new commands for dc.
The reorganization of the program involved significant architectural changes that separated out the code supporting bc's related dc functionality into a separate hierarchy and moved files common to both bc and dc to a library.
This refactoring of the code broke up the largest cluster into two clusters, where a new third cluster is formed as seen in the SCG.
Thus, the major restructuring of the code between revisions 1.03 and 1.04 causes a significant change in the cluster profile.
Almost no other change is seen in the cluster profile between the remaining three bc revisions 1.04, 1.05, and 1.06, where no significant restructuring took place.
Fig. 24 shows the SCGs for the four versions of acct considered in this study.
The slice profile and the cluster profile show very little change between acct-6.3 and acct-6.3.2.
Similarly, not much change is seen between acct-6.5 and acct-6.5.5.
However, the slice and the cluster profiles change significantly between major revisions, 6.3.X and 6.5.X.
The change log of release 6.5 notes "Huge code-refactoring." The refactoring of the code is primarily in the way system log files are handled using utmp_rd.c, file_rd.c, dump-utmp.c and stored using hash tables whose operations are defined in hashtab.c and uid_hash.c.
Finally, Fig. 25 shows the SCGs for the 14 versions of indent.
These revisions include two major releases.
It is evident from the SCGs that the slice profile during the evolution hardly changes.
The cluster profile also remains similar through the evolution.
The system grows from 4466 to 6521 SLoC during its evolution which is supported by Fig. 22c showing the growth of the system SDG size.
Indent is a program for formatting C programs.
A study of the change logs for indent did not reveal any major refactoring or restructuring.
The changes to the system were mostly bug fixes and upgrades to support new command line options.
This results in almost negligible changes in the slice and cluster profiles despite the system evolution and growth.
As an answer to RQ7, this study finds that unless there is significant refactoring of the system, coherent cluster profiles remain stable during system evolution and thus captures the core architecture of the program in all three case studies.
Future work will replicate this longitudinal study on a large code corpus to ascertain whether this stability holds for other programs.
Threats to validity
This section presents threats to the validity of the results presented.
Threats to three types of validity (external, internal and construct) are considered.
The primary external threat arises from the possibility that the programs selected are not representative of programs in general (i.e., the findings of the experiments do not apply to 'typical' programs).
This is a reasonable concern that applies to any study of program properties.
To address this issue, a set of thirty open-source and industrial programs were analyzed in the quantitative study.
The programs were not selected based on any criteria or property and thus represent a random selection from various domains.
However, these were from the set of programs that were studied in previous work on dependence clusters to facilitate comparison with previous results.
In addition, all of the programs studied were C programs, so there is greater uncertainty that the results will hold for other programming paradigms such as object-oriented or aspect-oriented programming.
Internal validity is the degree to which conclusions can be drawn about the causal effect of the independent variables on the dependent variable.
The use of hash values to approximate slice content during clustering is a source of potential internal threat.
The approach assumes that hash values uniquely identify slice contents.
Hash functions are prone to hash collision which in our approach can cause clustering errors.
The hash function used is carefully crafted to minimize collision and its use is validated in Section 3.3.
Furthermore, the identification of logical structure in programs were done by the authors of the paper who are not involved in the development of any of the case study subjects.
This brings about the possibility that the identified structures do not represent actual logical constructs of the programs.
As the case studies are Unix utilities, their design specification are not available for evaluation.
Future work will entail consultation with the development team of the systems to further validate the results.
Construct validity refers to the validity that observations or measurement tools actually represent or measure the construct being investigated.
In this paper, one possible threat to construct arises from the potential for faults in the slicer.
A mature and widely used slicing tool (CodeSurfer) is used to mitigate this concern.
Another possible concern surrounds the precision of the pointer analysis used.
An overly conservative, and therefore imprecise, analysis would tend to increase the levels of dependence and potentially also increase the size of clusters.
There is no automatic way to tell whether a cluster arises because of imprecision in the computation of dependence or whether it is 'real'.
Section 3.2 discusses the various pointer analysis settings and validates its precision.
CodeSurfer's most precise pointer analysis option was used for the study.
Related work
In testing, dependence analysis has been shown to be effective at reducing the computational effort required to automate the test-data generation process (Ali et al., 2010).
In software maintenance, dependence analysis is used to protect a software maintainer against the potentially unforeseen side effects of a maintenance change.
This can be achieved by measuring the impact of the proposed change (Black, 2001) or by attempting to identify portions of code for which a change can be safely performed free from side effects (Gallagher and Lyle, 1991; Tonella, 2003).
A recently proposed impact analysis framework (Acharya and Robinson, 2011) reports that impact sets are often part of large dependence clusters when using time consuming but high precision slicing.
When low precision slicing is used, the study reports smaller dependence clusters.
This paper uses the most precise static slicing available.
There has also been recent work on finding dependence communities in software (Hamilton and Danicic, 2012) where social network community structure detection algorithms are applied to slice-inclusion graphs to identify communities.
Dependence clusters have previously been linked to software faults (Black et al., 2006) and have been identified as a potentially harmful 'dependence anti-pattern' (Binkley et al., 2008).
The presence of large dependence cluster was thought to reduce the effectiveness of testing and maintenance support techniques.
Having considered dependence clusters harmful, previous work on dependence clusters focuses on locating dependence clusters, understanding their cause, and removing them.
The first of these studies (Binkley and Harman, 2005; Harman et al., 2009) were based on efficient technique for locating dependence clusters and identifying dependence pollution (avoidable dependence clusters).
One common cause of large dependence clusters is the use of global variables.
A study of 21 programs found that 50% of the programs had a global variable that was responsible for holding together large dependence clusters (Binkley et al., 2009).
Other work on dependence clusters in software engineering has considered clusters at both low-level (Binkley and Harman, 2005; Harman et al., 2009) (SDG based) and high-level (Eisenbarth et al., 2003; Mitchell and Mancoridis, 2006) (models and functions) abstractions.
This paper extends our previous work which introduced coherent dependence clusters (Islam et al., 2010b) and decluvi (Islam et al., 2010a).
Previous work established the existence of coherent dependence clusters and detailed the functionalities of the visualization tool.
This paper extends previous work in many ways, firstly by introducing an efficient hashing algorithm for slice approximation.
This improves on the precision of previous slice approximation from 78% to 95%, resulting in precise and accurate clustering.
The coherent cluster existence study is extended to empirically validate the results by considering 30 production programs.
Additional case studies show that coherent clusters can help reveal the structure of a program and identify structural defects.
We also introduce the notion of inter-cluster dependence which will form the base of reverse engineering efforts in future.
Finally, we also present studies which show the lack of correlation between coherent clusters and bug fixes and show that coherent clusters remain surprisingly stable during system evolution.
In some ways our work follows the evolutionary development of the study of software clones (Bellon et al., 2007), which were thought to be harmful and problematic when first observed.
Further reflection and analysis revealed that these code clone structures were a widespread phenomena that deserved study and consideration.
While engineers needed to be aware of them, it remains a subject of much debate as to whether or not they should be refactored, tolerated or even nurtured (Bouktif et al., 2006; Kapser and Godfrey, 2008).
We believe the same kind of discussion may apply to dependence clusters.
While dependence clusters may have significant impact on comprehension and maintenance and though there is evidence that these clusters are a widespread phenomena, it is not always obvious whether they can be or should be removed or refactored.
There may be a (good) reason for the presence of a cluster and/or it may not be obvious how it can be removed (though its presence should surely be brought to the attention of the software maintainer).
These observations motivate further study to investigate and understand dependence clusters, and to provide tools to support software engineers in their analysis.
In support of future research, we make available all data from our study at the website http://www.cs.ucl.ac.uk/staff/s.islam/decluvi.html.
The reader can obtain the slices for each program studied and the clusters they form, facilitating replication of our results and other studies of dependence and dependence clusters.
The visualizations used in this paper are similar to those used for program comprehension.
Seesoft (Eick et al., 1992) is a seminal tool for line oriented visualization of software statistics.
The system pioneered four key ideas: reduced representation, coloring by statistic, direct manipulation, and capability to read actual code.
The reduced representation was achieved by displaying files in columns with lines of code as lines of pixels.
This approach allows 50,000 lines of code to be shown on a single screen.
The SeeSys System (Baker and Eick, 1995) introduced tree maps to show hierarchical data.
It displays code organized hierarchically into subsystems, directories, and files by representing the whole system as a rectangle and recursively representing the various sub-units with interior rectangles.
The area of each rectangle is used to reflect statistic associated with the sub-unit.
Decluvi builds on the SeeSoft concepts through different abstractions and dynamic mapping of line statistics removing the 50,000 line limitation.
An alternative software visualization approach often used in program comprehension does not use the "line of pixels" approach, but instead uses nested graphs for hierarchical fish-eye views.
Most of these tools focus on visualizing high-level system abstractions (often referred to as 'clustering' or 'aggregation') such as classes, modules, and packages.
A popular example is the reverse engineering tool Rigi (Storey et al., 1997).
Summary and future work
Previous work has deemed dependence clusters to be problematic as they inhibit program understanding and maintenance.
This paper views them in a new light, it introduces and evaluates a specialized form of dependence cluster: the coherent cluster.
Such clusters have vertices that share the same internal and external dependencies.
The paper shows that such clusters are not necessarily problems but rather can aid an engineer understand program components and their interactions.
Developers can exploit knowledge of coherent clusters as they aid in program comprehension as the clusters bring out interactions between logical constructs of the system.
We also lay a foundation for research into this new application area and encourage further research.
Moreover, future research could compare the aspects of various definitions of dependence clusters and the properties they capture.
This paper presents new approximations that support the efficient and accurate identification of coherent clusters.
Empirical evaluation finds that 23 of the 30 subject programs have at least one large coherent cluster.
A series of four case studies illustrate that coherent clusters map to a logical functional decomposition and can be used to depict the structure of a program.
In all four case studies, coherent clusters map to subsystems, each of which is responsible for implementing concise functionality.
As side-effects of the study, we find that the visualization of coherent clusters can identify potential structural problems as well as refactoring opportunities.
The paper also discusses inter-cluster dependence and how mutual dependencies between clusters may be exploited to reveal large dependence structure that form the basis of reverse engineering efforts.
Furthermore, the paper presents a study on how bug fixes relate to the presence of coherent clusters, and finds no relationship between program faults and coherent clusters in barcode.
Finally, a longitudinal study of three subjects shows that coherent clusters remain surprisingly stable through system evolution.
The paper is one of the first in the area of dependence clusters to suggest that dependence clusters (coherent clusters) are not problematic but represent program structure and give evidence to that cause.
Future work in this area is rife with opportunities beginning with enabling the use of coherent clusters in a program comprehension and reverse engineering tools.
The inter-cluster dependence study lays out the ground work in this context.
There is also room for further research aimed at understanding the formation and impact of coherent clusters on software quality.
For example, by studying how well dependence clusters can capture functionality.
Furthermore, application of dynamic slicing in formation of dependence clusters might by considered as static analysis can suffer from over approximation caused by its conservative nature.
Acknowledgements
This work is supported by EPSRC (EP/G060525/2, EP/F059442/2), EU (ICT-2009.1.2 no 257574), and NSF (CCF 0916081).
Data from the EPSRC-funded portions of this work may be available by contacting Dr. Krinke.
Please note that intellectual property or other restrictions may prevent the full disclosure of this data.

Short-term metabolic and growth responses of the cold-water coral Lophelia pertusa to ocean acidification

Abstract
Cold-water corals are associated with high local biodiversity, but despite their importance as ecosystem engineers, little is known about how these organisms will respond to projected ocean acidification.
Since preindustrial times, average ocean pH has decreased from 8.2 to ~8.1, and predicted CO2 emissions will decrease by up to another 0.3 pH units by the end of the century.
This decrease in pH may have a wide range of impacts upon marine life, and in particular upon calcifiers such as cold-water corals.
Lophelia pertusa is the most widespread cold-water coral (CWC) species, frequently found in the North Atlantic.
Here, we present the first short-term (21 days) data on the effects of increased CO2 (750ppm) upon the metabolism of freshly collected L. pertusa from Mingulay Reef Complex, Scotland, for comparison with net calcification.
Over 21 days, corals exposed to increased CO2 conditions had significantly lower respiration rates (11.4±1.39 SE, µmolO2g-1tissuedryweighth-1) than corals in control conditions (28.6±7.30 SE µmolO2g-1tissuedryweighth-1).
There was no corresponding change in calcification rates between treatments, measured using the alkalinity anomaly technique and 14C uptake.
The decrease in respiration rate and maintenance of calcification rate indicates an energetic imbalance, likely facilitated by utilisation of lipid reserves.
These data from freshly collected L. pertusa from the Mingulay Reef Complex will help define the impact of ocean acidification upon the growth, physiology and structural integrity of this key reef framework forming species.

Introduction
Cold-water corals are among the most three-dimensionally complex deep-sea habitats known and are associated with high local biodiversity (Roberts et al., 2006, 2009).
However, their remoteness and the relatively short history of ecological research in these habitats mean that to date, we have little information on how these ecosystems will fare in the face of predicted future climate change.
Similar to their tropical counterparts, cold-water corals (referred to as CWC herein) are under potential threat from increasing sea temperatures and ocean acidification.
Ocean acidification, also referred to as the 'other CO2 problem' or the 'evil twin of global warming', is caused by CO2 dissolving into the oceans.
As atmospheric CO2 levels increase, more CO2 dissolves into the oceans and forms carbonic acid, which dissociates to form hydrogen and bicarbonate ions.
This process has led to a decrease in pH by 0.1 units since the industrial revolution, and increasing amounts of atmospheric CO2 are projected to further decrease ocean pH by another 0.3-0.4 pH units by the end of the century in addition to altering seawater carbonate chemistry (Caldeira and Wickett, 2003; Guinotte et al., 2006; Kleypas et al., 1999; Orr et al., 2005).
The shift in carbonate chemistry associated with ocean acidification also reduces the saturation state of aragonite, which is a naturally occurring polymorph of calcium carbonate from which most framework-building corals build their skeletons.
The aragonite saturation depth or 'horizon' (ASH) (Guinotte et al., 2006) is predicted to become shallower (shoal), making it more difficult for calcifying organisms near this depth to maintain their calcified structures, thus affecting net reef growth.
CWC are under particular threat as they inhabit a much larger bathymetric range than tropical corals, and as such are closer to the ASH (Fautin et al., 2009; Form and Riebesell, 2012; Guinotte et al., 2006; Kleypas, 2006; McCulloch et al., 2012a; Roberts et al., 2006; Thresher et al., 2011; Turley et al., 2007).
Only 5% of CWC are found below the ASH at present (Form and Riebesell, 2012), as net calcification below the ASH would require considerable energetic input at the detriment of other energetic processes.
Shoaling of the horizon could thus have potentially severe consequences for CWC.
To date, only a handful of studies have been conducted on the responses of Lophelia pertusa to ocean acidification.
These focus on growth rate, ranging from very short term 24 hour experiments (Maier et al., 2009) on freshly collected coral, to longer-term 6 month (Form and Riebesell, 2012) experiments on laboratory-kept specimens.
Ocean acidification has very varied effects on the growth of calcifying organisms, with different phyla, and even species within phyla showing highly variable responses in experimental studies to date (Ries et al., 2009; Wicks and Roberts, 2012).
For scleractinian corals (i.e. calcareous skeleton forming corals), many studies demonstrate a reduction in growth (net calcification rates) in response to ocean acidification (Gattuso et al., 1998; Kleypas and Langdon, 2006; Krief et al., 2010; Langdon and Atkinson, 2005; Marubini et al., 2003).
However, corals can actively increase the pH in the organic matrix where calcification occurs.
Thus even in situations where the surrounding water is undersaturated with respect to aragonite, calcification can still occur (McCulloch et al., 2012a, 2012b).
The process of increasing pH at calcification sites within corals is driven by Ca2+ATPase, which pumps Ca2+ ions into the sub-calicoblastic space in exchange for H+ ions (Allemand et al., 2004; Cohen and McConnaughey, 2003; Mass et al., 2012).
However, this process is energy intensive and thus may require increased food intake (Edmunds, 2011).
The low abundance of CWC below the ASH suggests that increased energetic demands cannot usually be met, and that dissolution of exposed skeletal framework may be greater that net calcification by tissue-covered skeleton.
To fully understand the impact of increased CO2 on live CWC, it is important to combine growth rates with measures of metabolism, which has not been done previously.
The research presented here addressed the question of whether ocean acidification will impact upon the metabolism and net calcification rate of the CWC Lophelia pertusa in a short-term experiment.
To this end, respiration and net calcification rates were calculated in corals exposed to increased CO2 conditions every 7 days for a total period of 21 days.
Methods
Sample collection
Colonies of Lophelia pertusa were collected from Area 1 within the Mingulay Reef Complex (Roberts et al., 2005, 2009), 56° 49.38 N, 7° 22.56 W (Figs.
1 and 2), during RRS Discovery cruise 366/7 in July 2011 (Achterberg and Richier, 2012).
The Mingulay complex is a relatively shallow inshore seascape of Lophelia reefs that developed through the Holocene with oldest currently dated coral from within the reef mounds at 7.68ka (Douarin et al., in press).
They are the only known inshore Lophelia reef in UK waters.
Colonies were collected using a modified video assisted van-Veen grab (Dodds et al., 2007).
All colonies were collected from 141-167m.
Upon return to the surface, corals were placed in a holding tank at ambient seabed temperature for 2 days, to recover from collection.
Corals were then carefully fragmented into smaller pieces for experiments.
These fragments had 5-20 polyps, and were taken from the top of sampled colonies to ensure that relatively young polyps were used consistently, as polyp age can determine physiological response with younger polyps known to be the fastest-growing (Maier et al., 2009).
Fragments were attached to pre-labelled bases made of PVC pipe with Grotech Korafix epoxy.
Treatments
Two tanks (each ~350L seawater within 430L volume (0.9×0.6×0.8m)) were established for experiments; one at ambient reef conditions of 9.5°C, 380ppm and the other at 9.5°C, 750ppm in accordance with the IPCC IS92a CO2 emission scenario.
Elevated CO2 750ppm gas was purchased pre-mixed (BOC) and generously bubbled directly into a corner of the experimental tank near a powerhead to ensure gas and water mixing and dispersal throughout the tank.
To check that bubbling was sufficient, tank pH was checked with a Mettler-Toledo SevenGo SG2 pH meter.
Ambient air was pumped into the control tank and checked for consistency with a Li-820 gas analyser (Licor).
Both tanks were equipped with chillers, filtration units and powerheads to provide adequate temperature control, filtration and circulation.
Water circulation in this closed system from the tank to the filtration unit and chiller was 300lh-1.
Thirty per cent water changes were conducted on each tank every 2 days.
Tank temperatures and salinity were checked throughout the experimental period with a YSI (30) salinity and temperature meter.
Salinity and temperature for the control and experimental tanks were 35.2±0.04 and 35.3±0.03, and 9.72±±0.13°C and 9.86±0.07°C respectively.
Corals (n=32 in each treatment) were fed a mixture of live Artemia and Skeletonema marinoi every two days.
Time points were Time Zero, +7 days, +14 days and +21 days.
Following measurements at time zero, 750ppm gas bubbling was initiated in the treatment tank.
Fragments from individual colonies were split evenly between treatments and time points to avoid colony pseudo replication.
Site and experimental carbonate chemistry
Mingulay carbonate chemistry was assessed on two cruises; on D366/7 (July-August 2011) and on RRS James Cook 073 (Roberts et al., 2013) (May-June 2012) in surface waters (<20m) and near the reef crest (~120m).
Borosilicate glass bottles with ground glass stoppers were used to collect seawater from Niskin bottles on the CTD rosette, and sample bottles were rinsed and filled according to standard procedures detailed in Dickson et al. (2007).
Samples were poisoned with mercuric chloride, and duplicate samples were taken from the same Niskin bottle.
Samples were bought to room temperature (approx.
23°C) and analysed for total inorganic carbon and total alkalinity within 24h of collection.
Total Alkalinity (AT) and Dissolved Inorganic Carbon (CT) were calculated according to depth-specific salinity, and also normalised to a salinity of 35 for comparative purposes across depths where salinity changed.
AT was corrected for the addition of mercuric chloride.
Carbonate parameters were calculated using CO2sys (Pierrot et al., 2006) with dissociation constants from Mehrbach et al. (1973), refit by Dickson and Millero (1987) and KSO4 using Dickson (1990).
AT and CT samples were collected (as described above) from the middle of experimental tanks at each time point.
Although there was only one experimental tank per treatment, the turnover of seawater between time points (through regular maintenance water changes) ensured that carbonate chemistry was not pseudo-replicated across time points.
For full details of instruments used to assess AT and CT for experimental purposes, and for site carbonate chemistry, please refer to Supplementary Material.
Physiology
The ash-free dry mass (AFDM) of each sample was determined by adding homogenised material to a pre-weighed porcelain crucible and noting the weight of the crucible and the sample.
The crucibles were covered and placed in a muffle furnace (Nabertherm Controller B170).
The temperature of the furnace increased to 450°C over a 30-min period and then remained at that temperature for a further 4h.
After this time the crucibles were re-weighed and the difference in the weight of the sample minus the ashed weight gave the amount of organic matter or AFDM.
Net calcification
Two methods were used to assess L. pertusa growth through net calcification; the alkalinity anomaly technique (Smith and Key, 1975) and uptake of labelled carbon (Marshall and Wright, 1998).
To determine if there was any increase or decrease in tissue mass, the AFDM was compared to dry coral weight.
For additional comparison to recent CWC studies (Form and Riebesell, 2012; Maier et al., 2012, 2009), coral polyp numbers were also counted to determine calcification per polyp.
Different fragments were used for both techniques (Table 1).
Alkalinity anomaly
Following techniques from Smith and Key (1975), and Ohde and Hossain (2004) calcification rates were calculated in L. pertusa by measuring the change in seawater alkalinity in respiration chambers housing coral fragments for 4h.
Samples of incubation water were taken at the beginning and end of the experimental period, and total alkalinity determined using an automatic titrator (Metrohm 702 SM Titrino).
Calcification (µmolCaCO3g-1h-1) was estimated following Eq.
(1),(1)Calcification=0.5(ΔTA)·V/ΔT/AFDMwhere ΔTA is the change of total alkalinity (µmol/l), V is the volume of experimental seawater (L) and ΔT is the experimental period (h).
Calcification rates were calculated per hour (and extrapolated to per day) through a linear function.
However, this may potentially underestimate growth rates if all newly accreted material contributes to calcification.
Further study is needed to calculate whether L. pertusa growth over time is better represented by linear or exponential growth functions.
With regard to nutrients, corrections applied to total alkalinity to account for the release of nutrients during incubations are considered negligible, especially for tropical corals (Riebesell et al., 2010; Smith, 1995).
Considering that the ~10% underestimation of net calcification rate due to nutrient omission is small compared to natural variation in CWC calcification (Maier et al., 2012), nutrients were considered negligible for experiments here.
Changes in aragonite, CT and pH during incubation were not quantified.
For comparison to other CWC studies, calcification rates were also expressed as a percentage compared to initial skeletal weight of the corals to give growth as %d-1.
Radioisotope
Calcification rates were measured by incorporation of 14C (from sodium bicarbonate, CHNaO3), into new coral skeleton.
Fragments of live corals (different from those used for respiration/alkalinity anomaly) from both treatments were placed in 30ml filtered seawater (FSW) maintained at tank temperature (9.5°C) in 50ml falcon tubes (20ml headspace).
After 1h acclimation, 120μl of 14C stock solution was added to each tube to a final activity of 3kBq/ml (0.08101uCi/ml) (Al-Horani et al., 2005).
Two controls were included; one of a dead coral fragment (previously placed in a 4% formalin solution), and one of Filtered Sea Water (FSW).
Tubes were kept in floating tube racks, so ship movement and a pump maintained gentle movement of the tubes to ensure isotope mixing.
Immediately following addition of the radioisotope, 100µl of sample water was removed and added to 4ml of Optisafe scintillation cocktail with 200µl ß-phenylethylamine to assess total activity.
Following 6h incubation, another 100µl was taken to assess the decrease in total activity in incubation water.
Coral fragments were then placed in FSW for 1h to remove unbound tracer.
This step was repeated before corals were preserved at -20°C for analysis following the expedition.
No samples of water carbonate chemistry were taken from the experiment tubes before or after incubation.
To determine the amount of 14C incorporated in skeleton, frozen samples were dried overnight at 60°C.
Samples were weighed and tissue was then hydrolysed with a known volume of 2N NaOH at 90°C.
Hydrolysate (100µl) was sampled for uptake of 14C in coral tissue.
The skeleton was rinsed with distilled water and dried overnight again for re-weighing.
Tissue dry weight was calculated as the difference between skeleton and tissue weight, and skeleton weight.
To calculate uptake of 14C, skeletons were placed in flasks fitted with an outlet to a swinnex filter holder which contained a GF/C filter impregnated with ß-phenylethlyamine.
Excess acid (HCl) was added to flasks to dissolve the skeleton and evolve CO2.
Evolved 14C was trapped in the GF/C filter, and following complete skeletal dissolution, the filter was placed in 4ml of Optisafe scintillation cocktail for counting on a Packard 1900CA Tri-Carb Liquid Scintillation Analyser.
Results
Mingulay seawater parameters
From CTD casts in June 2012, salinity increased with depth at Mingulay whereas temperature slightly decreased from the surface to the reef crest (ca.
120m) (Table 2).
CT increased with depth, both when normalised to the changing salinity and when normalised to 35 for comparison with surface values (Table 2).
AT increased with depth when using depth-specific salinity values, but when normalised to 35, a decrease in AT was observed with depth.
In situ pHT decreased with depth as pCO2 increased (Table 2).
Deeper CTD casts taken in July 2011 at the side of the Mingulay Reef mounds correlate with Table 1; such that AT and CT increased with depth (AT 2332.7±1.18; DIC 2149.0±2.08 at 172m).
Physiology
The respiration rates for coral fragments at Time Zero before fragments were exposed to experimental conditions was 23µmolO2g-1h-1.
For fragments in the control treatment at 380ppm, 9.5°C, pH 8.07 (Table 3), there was no significant change in respiration rates from Time Zero to 21 days.
For fragments exposed to increased CO2 at 750ppm, 9.5°C, pH 7.77 (Table 3), respiration rates were significantly lower than control fragments at 14 days with 12.1±1.30 SE at 750ppm versus 23.7±5.13 SE at 380ppm, and at 21 days with 11.441.39 SE at 750ppm versus 28.6±7.30 SE at 380ppmµmolO2g-1tissuedryweighth-1 (two sample t-test; t=2.19, p<0.05; t=2.31, p<0.05 respectively), see Fig. 3.
Microbial respiration typically ranged from 0.005 to 1.3µmolO2h-1, but on average was ~0.5µmolO2h-1 in the experimental chambers.
There was no difference in the relationship between AFDM and whole coral (skeleton+tissue) dry weight between 380ppm and 750ppm treatments after 21 days, so tissue mass did not change in coral fragments under acidified conditions compared to control individuals.
Fragments from 380ppm and 750ppm were thus combined for correlation analysis between AFDM and whole coral dry weight.
The linear regression was strong (r2=0.85, Fig. 4A, at T+21 days; y=0.0428x+0.08473) even with the presence of one anomalous point.
If this point was removed from the regression, y=0.0415x+0.0849, r2=0.97, p<0.0001, with tissue dry weight approximately 5% of dry coral weight.
Respiration rates for each treatment at T+21 days were compared to AFDM, with positive relationships for both 380ppm (r2=0.62, p=0.04) and 750ppm (r2=0.78, p=0.008) fragments (Fig. 4C).
However, these regressions were partially driven by one larger coral in each treatment.
Comparisons of AFDM and respiration rates to fragment polyp numbers elicited poor relationships; 380ppm (r2=0.14, p=0.40), 750ppm (r2=0.08, p=0.59) (Fig. 4D).
Net calcification
Net calcification rates at time zero, measured with the alkalinity anomaly technique, were ca.
1.5µmolCaCO3g-1tissuedryweighth-1.
This did not change significantly for either treatment over the 21 days, or between treatments at any time point (Fig. 5A).
Changes in AT (blank corrected) across both treatments were on average 34.7 (±3.34)µmolkg-1 and ranged from 9 to 59µmolkg-1.
Growth rates measured as %d-1 did not change significantly over the 21 days either, and averaged~0.028%d-1 (Fig. 5B).
Calcification rate as determined by 14C uptake did not differ between treatment and control fragments at either the experiment start (380ppm; 19.3×10-2 (±SE 2.76), 750ppm; 21.8×10-2 (±SE 7.36)µmolCaCO3g-1tissuedryweighth-1) or end (380ppm; 1.68×10-2 (±SE 0.60), 750ppm; 4.38×10-2 (±SE 1.6)µmolCaCO3g-1tissuedryweighth-1).
However, there was a significant decrease in calcification rates measured by 14C uptake in 380ppm fragments over time (380ppm, two sample t-test; t=7.28, p=0.001), which was not significant in 750ppm fragments due to very high variability.
This decrease was not observed through the alkalinity anomaly technique.
14C uptake into L. pertusa tissue was approximately the same for all treatments and time points (ca.
60×10-2µmol14Cg-1tissuedryweighth-1) and did not significantly change between treatments or over time.
Discussion
Physiology, calcification and ocean acidification
Lophelia pertusa respiration rate was significantly lower in fragments exposed to increased CO2 than in control fragments after 2 weeks.
Metabolic respiration is needed to produce ATP, which in turn is used to support energy-requiring processes (Al-Horani et al., 2003).
This includes calcification, tissue production, mucus-production (Wild et al., 2009) (which accounts for a large proportion of tropical corals' carbon budget (Muscatine et al., 1984)) and driving the transport protein Ca2+ATP-ase.
This transport protein is essential for calcification to occur, as it actively pumps Ca2+ ions into the sub-calicoblastic space in exchange for H+ ions, thereby increasing Ca2+ concentration and making conditions favourable for calcification to occur by increasing pH (McCulloch et al., 2012a).
The decreased respiration found in the present study under acidified conditions may indicate a decreased requirement of ATP production by L. pertusa under increased levels of CO2 after a few weeks.
This may be due to a change in energetic requirements, a reallocation of resources, or changing metabolic pathways (Findlay et al., 2011).
Form and Riebesell (2012) noted discrepancies in L. pertusa physiology between very short (24h) and longer-term (months) experiments; whereby responses observed during the first 24h were no longer exhibited after some months.
This indicates that L. pertusa may have both 'shock' and acclimation responses to changes in CO2 conditions.
The two-week period observed here before any noticeable differences were observed between experimental and control fragments may indicate a switching of metabolic pathways cued by extended exposure to elevated CO2.
However, due to coral respiration in closed-system experimental chambers, it is likely that there will have been a significant rise in CT and corresponding decrease in pH during the respiration incubation periods (Form and Riebesell, 2012).
Although unquantified in this experiment, reductions in pH may have matched those reported by Maier et al. (2009) where decreases over 24h (in smaller vessels) ranged from 0.1-0.5 pH units.
For low pH treatments (such as the 750ppm), reductions could be potentially larger due to the reduced buffering capacity.
Although this is important to consider, the fact that respiration was not significantly lower than T0 in the experimental treatment at T+7 days, but only after 14 days, indicates that it was likely to be a result of experimental conditions rather than changes during incubations.
This is supported by observations that there are no significant differences in respiration rates in corals in the first 30min of respiration incubation compared to after T+4h in the same incubation water (Hennige, S., personal observation).
Respiration rates for L. pertusa documented in this study are higher than those reported by Dodds et al. (2007).
However, this experiment was conducted on freshly collected L. pertusa fragments, and not on colonies kept in aquaria conditions for several months.
Indeed, respiration rates by Dodds et al. (2007) are very comparable to rates observed in L. pertusa fragments that have been maintained in aquaria at Heriot-Watt University for more than a year (Hennige, S., personal observation).
This is likely due to the feeding limitations of aquaria in comparison to the in situ food supply.
Freshly collected fragments will be acclimated to the conditions they were removed from in situ, but may exhibit an unquantified and unknown element of stress due to collection and pressure differences.
However, while long-term aquarium fragments may not have any unknown 'stress' and can be kept at very specific conditions, they may be acclimated to some unrepresentative aquarium conditions.
Both approaches are equally valid but it has to be considered that freshly collected corals may not always exhibit similar physiological responses to long-term aquarium corals.
Results here were normalised to AFDM in preference to polyp number, as respiration rates of fragments from both treatments significantly correlated with tissue mass, and not polyp number.
Although coral polyps are the centres of heterotrophic feeding, dry tissue weight, which includes both polyp tissue and the surrounding coenosarc (which also respires), is therefore a potentially more useful normalisation parameter than polyp number.
The relationship between AFDM and dry coral weight means that dry skeletal weight can provide a convenient proxy of tissue weight, without the need to kill the coral.
Net calcification rates as measured by the alkalinity anomaly technique and 14C uptake, did not differ between control and experimental fragments, even though experimental fragments were at a much lower aragonite saturation level (still>1).
This complements a 24-h study by Maier et al. (2012) and a longer-term experiment by Form and Riebesell (2012), where growth rates of L. pertusa did not significantly change under different CO2 conditions.
The stable net calcification rates reported in this study are in contrast to the reduced respiration in experimental fragments.
This suggests an energetic imbalance between the production of ATP, and the use of ATP to actively provide Ca2+ needed for calcification.
In a recent study, Kaniewska et al. (2012) found that under high CO2 conditions, metabolic activity in the tropical coral Acropora millepora was suppressed after a period of weeks, and changes were observed in genes regulating membrane cytoskeletal interactions and cytoskeletal remodelling.
Results by Kaniewska et al. (2012) also indicated a possible breakdown of lipid reserves, which could provide the energy needed to maintain net calcification rates even with suppressed metabolism as observed.
This highlights the importance of considering energetic budgets and wider cellular processes in studies such as this (Findlay et al., 2009; Kaniewska et al. 2012).
The consistent AFDM: coral dry weight between fragments exposed here to ambient and 750ppm conditions did not indicate any major change in tissue mass in coral fragments from either treatment.
However, it is probable that an increase in lipid breakdown would not be apparent through changes in tissue mass over this relatively short time period.
Net calcification rates reported here from the alkalinity anomaly technique are comparable to previously published rates by Maier et al. (2012, 2009), and Form and Riebesell (2012).
Net calcification rates reported here for L. pertusa were slightly lower than those reported by Maier et al. (2009) where 45Ca was used to determine net calcification, within variability of alkalinity anomaly derived data in Maier et al. (2012), and slightly higher than those reported in Form and Riebesell (2012).
Net calcification rates in L. pertusa are often highly variable, even within studies where fragments are collected from the same locale and time of year.
It is perhaps expected then, that differences exist between L. pertusa from different reefs, depths, and age.
Further differences may be elicited through irregular growth episodes or 'spurts' (Form and Riebesell, 2012; Mortensen et al., 2001), which is typical in scleractinian corals.
The calcification rates measured by 14C uptake were significantly lower than rates calculated through the alkalinity anomaly technique.
This is not surprising, as labelled bicarbonate is not guaranteed to be taken up solely into the coral skeleton as calcium carbonate, and its uptake can be significantly less than 45Ca (Marshall and Wright, 1998).
The reduction in 14C uptake from time zero to the end of the experiment in both control and experimental fragments indicates that some physiological process may have altered during this time.
This may indicate a degree of acclimation to tank conditions (changing food source/change in circulation) that may have impacted the control of internal coral chemistry and cellular processes, so less 14C bicarbonate was incorporated into the skeleton.
Non-quantified changes in carbonate chemistry during radioisotope experimental incubations may also have impacted upon these calcification rates.
Considering the similarity of 14C incubations to the 45Ca incubations by Maier et al. (2009), it is possible that pH may have changed during incubations by up to 0.5 units, even though incubations here were four times shorter.
However, considering that calcification rates (as measured through the alkalinity anomaly technique) did not change over the course of the 21 days, it is likely the reduction in 14C uptake from T0 represents experimentally induced and not incubation induced changes.
In general, the impact of ocean acidification upon scleractinian corals; both tropical and cold, seems to be inconsistent, with different species exhibiting negative (Ohde and Hossain, 2004), no measureable response (Reynaud et al., 2003), or variable responses (Gattuso et al., 1998) to a change in conditions (Wicks and Roberts, 2012).
This is further complicated by suggestions that corals may be more or less susceptible to ocean acidification depending upon their ontogenetic stage.
Albright et al. (2010) demonstrated that the tropical coral Acropora palmata is negatively impacted by increasing CO2 with respect to fertilisation, settlement of larvae, and growth of juveniles.
Impacts on these aspects in L. pertusa, and indeed any CWC remain unknown to date.
Scleractinian responses to ocean acidification may also not be observable in physiology, and may be enacted through changes in biomineralisation (Cohen and Holcomb, 2009; Holcomb et al., 2010).
However, when comparing existing ocean acidification studies, care has to be taken, as methodologies often differ with respect to length of exposure to increased CO2 levels, the speed at which organisms are subjected to change (instantly versus increasing intermediate levels) and the way in which pH is reduced, i.e. acid addition versus CO2 bubbling (Wicks and Roberts, 2012).
A particularly interesting area for future focus may be whether the internal pH upregulation noted in certain coral species (Anagnostou et al., 2012; McCulloch et al., 2012a; McCulloch et al., 2012b; Venn et al., 2009) is consistent across species, and across different simulated future conditions.
Environmental conditions at Mingulay
The increases observed in CT and AT from the surface to the reef crest were primarily driven by increasing salinity with depth.
When normalised to a salinity of 35, AT decreased from the surface to the reef.
Proximity to the reef, where active calcification is occurring, would explain this decrease in alkalinity through calcification (Kleypas and Langdon, 2006), as for every mole of CaCO3 produced by the coral, total alkalinity of the water decreases by two moles.
Differences observed between normalised CT indicate that there is either an addition of CT at the reef, or a drawdown of CT at the surface.
This may be a combination of phytoplankton photosynthesis at the surface (Riebesell, 2004), and respiration from the coral reef, which would act to increase CT.
It has been noted in tropical corals that as ΩAragonite decreases from 3 or 4 (Kleypas and Langdon, 2006) to ca.
2, significant reduction in calcification rates can occur (Wicks and Roberts, 2012).
The ΩAragonite at Mingulay (Table 1), which is considered relatively shallow for a Lophelia pertusa reef (Roberts et al., 2006), is about half that of many tropical reefs (Kleypas and Langdon, 2006).
However, ΩAragonite is still >1, so dissolution should not occur.
At Mingulay Reef, corals experience a periodic downwelling of surface water (Davies et al., 2009).
Recently characterised in terms of changing seawater carbonate chemistry (Findlay et al., 2013), this surface water will bring with it an influx of food from the surface, as well as periods of warmer water.
Thus, corals at Mingulay are exposed to regularly fluctuating water conditions, both in terms of temperature and also water chemistry.
This raises the possibility that corals at Mingulay may have higher tolerance for changing seawater conditions than those in more stable, bathyal environments.
These and related questions form the basis of on-going investigations.
Conclusions
From this experiment, we present the first short-term data on the effects of increased CO2, (750ppm) upon the metabolism of freshly collected L. pertusa from Mingulay Reef Complex, Scotland, and its comparison with net calcification rates.
The sustained net calcification rates of L. pertusa under elevated CO2 conditions corresponds with other studies, but the observed decrease in respiration rate highlights an energetic imbalance, whereby L. pertusa may be forced to use energetic reserves to maintain calcification rates.
However, the observed decrease in respiration in response to ocean acidification is potentially detrimental in the longer term, as expending energetic reserves is a finite strategy.
Thus, it is crucial to perform longer-term experiments on Lophelia pertusa metabolism and growth to assess the acclimation potential, and ultimately the success, of this deep-sea ecosystem engineer to predicted increases of CO2 and warming.
Finally it is important to note that the rapid rise in atmospheric CO2 is not only causing ocean acidification but warming.
Further studies examining the combined effects of warming and acidification alongside other predicted stressors are urgently needed if we are to truly appreciate the significance of global climatic change on cold-water corals and other vulnerable marine ecosystems.
Acknowledgments
This paper is a contribution to the UK Ocean Acidification Research Programme (NE/H017305/1 to JMR; funded by the Natural Environment Research Council, the Department for Energy and Climate Change, and the Department for Environment, Food and Rural Affairs) and the European Commission's Seventh Framework Programme projects EPOCA (Grant agreement no.
211384) and HERMIONE (Grant agreement no.
226354).
SJH, LCW and JMR acknowledge support from Heriot-Watt University's Climate Change Theme.
We thank Juan Moreno-Navas for GIS assistance and multibeam maps, the Scottish Association for Marine Science for the loan of coral sampling equipment, and the captains, crew and scientific participants of RRS Discovery cruise 366/7 and RRS James Cook cruise 073 for assistance at sea.
Supplementary Information
Supplementary data associated with this article can be found in the online version at 10.1016/j.dsr2.2013.07.005.
Supplementary materials
Supplementary Material

Tidal turbine array optimisation using the adjoint approach

Abstract
Oceanic tides have the potential to yield a vast amount of renewable energy.
Tidal stream generators are one of the key technologies for extracting and harnessing this potential.
In order to extract an economically useful amount of power, hundreds of tidal turbines must typically be deployed in an array.
This naturally leads to the question of how these turbines should be configured to extract the maximum possible power: the positioning and the individual tuning of the turbines could significantly influence the extracted power, and hence is of major economic interest.
However, manual optimisation is difficult due to legal site constraints, nonlinear interactions of the turbine wakes, and the cubic dependence of the power on the flow speed.
The novel contribution of this paper is the formulation of this problem as an optimisation problem constrained by a physical model, which is then solved using an efficient gradient-based optimisation algorithm.
In each optimisation iteration, a two-dimensional finite element shallow water model predicts the flow and the performance of the current array configuration.
The gradient of the power extracted with respect to the turbine positions and their tuning parameters is then computed in a fraction of the time taken for a flow solution by solving the associated adjoint equations.
These equations propagate causality backwards through the computation, from the power extracted back to the turbine positions and the tuning parameters.
This yields the gradient at a cost almost independent of the number of turbines, which is crucial for any practical application.
The utility of the approach is demonstrated by optimising turbine arrays in four idealised scenarios and a more realistic case with up to 256 turbines in the Inner Sound of the Pentland Firth, Scotland.
Highlights
•
The layout optimisation of tidal farms was formulated as a PDE-constrained optimisation problem.
•
Gradient-based optimisation and the adjoint approach allow the optimisation of many turbines with realistic flow models.
•
A shallow water model predicts the flow and the farm performance in each optimisation iteration.
•
The capability of this approach is demonstrated by optimising 256 turbines in a farm in the Inner Sound of the Pentland Firth, Scotland.
•
The software framework is open-source and available at http://opentidalfarm.org.

Introduction
With the increasing cost of energy, tidal turbines are becoming a competitive and promising option for renewable electricity generation.
A key advantage of tidal energy is that the power extracted is predictable in advance, which is highly attractive for grid management.
In order to amortise the fixed costs of installation and grid connection, arrays consisting of hundreds of tidal turbines must typically be deployed at a particular site.
This raises the question of where to place the turbines within the site and how to tune them individually in order to maximise the power output; finding the optimal configuration is of huge importance as it could substantially change the energy captured and possibly determine whether the project is economically viable.
However, the determination of the optimal configuration is difficult because of the complex flow interactions between turbines and the fact that the power output depends sensitively on the flow velocity at the turbine positions.
This problem has heretofore been addressed in two different ways.
One approach is to simplify the tidal flow model such that the solutions are either available as explicit analytical expressions, or are extremely fast to compute.
This means that the optimum can be analytically derived, or that the whole parameter space of possible configurations can be rapidly explored.
For example, Bryden and Couch [5] and Garrett and Cummins [15] optimised simplified models to derive an estimate for the maximum energy that can be extracted from a tidal basin.
Vennell [39,40] used simple one-dimensional models to demonstrate the importance of tuning each turbine individually to account for the channel geometry, turbine positions, and the tidal forcing.
Thus, optimisation of farms is a crucial step needed to achieve their full potential.
However, Vennell [42] observes that this optimisation requires many model runs (if performed naively), thus making it computationally infeasible to use expensive, physically-accurate flow models for this task.
While this approach can provide a coarse estimate for the power potential of a site, these simplified models cannot accurately capture the complex nonlinear flow interactions between turbines.
The second approach is to use more complex flow models to accurately predict the tidal flow, the turbine wakes, and the resulting power output.
These models are usually formulated as numerical solutions to partial differential equations (PDEs).
The computational expense of these models prohibits the exploration of the whole parameter space [38].
Consequently, typically only a handful of manually identified turbine configurations are investigated in a given scenario [1].
Divett et al. [8] compared the power output of four different layouts in a rectangular channel by solving the two-dimensional nonlinear shallow water equations and was able to improve the power outcome by over 50% compared to a regular layout.
Lee et al. [28] used a three-dimensional model to investigate how the distance between adjacent rows in a regular array layout impacts the turbine efficiency and showed an efficiency decay for distances of less than three times the turbine diameter.
While these studies show the potential of improving the performance by changing the turbine positions, such manual optimisation guided by intuition and experience becomes difficult in a realistic domain with complex bottom bathymetry, flow dynamics and hundreds of turbines.
In this paper, we present a novel technique for maximising the power extraction of array configurations that combines the physical fidelity of PDE-based flow models with advanced automated optimisation techniques.
This approach allows the identification of optimal solutions in a computationally feasible number of iterations, circumventing the computational limitations noted in Ref.
[42].
The turbine configuration problem is formulated as a PDE-constrained optimisation problem, which is a major topic of research in applied mathematics [20,21].
The resulting maximisation problem is solved using a gradient-based optimisation algorithm that takes orders of magnitude fewer iterations than genetic algorithms or simulated annealing approaches (see e.g.
Ref.
[3]).
In this paper, the power extracted by an array configuration is predicted using a two-dimensional nonlinear shallow water model, which captures the interactions between the geometry, the turbines, and the flow.
The gradient of the power is efficiently computed using the adjoint technique of variational calculus, which solves an auxiliary system that propagates causality backwards through the physical system.
This yields the gradient at a cost almost independent of the number of turbines to be optimised, which is crucial for the method to be applied to large arrays.
This gradient is used by the optimisation algorithm to automatically reposition the turbines and to adjust their tuning parameters.
The flow solution is re-evaluated, and the algorithm iterated until an optimum is found.
This approach has several key advantages.
Firstly, it closes the optimisation loop, by accounting for the effects of the turbines on the flow field itself.
This is necessary to find the actual optimum of the nonlinear optimisation problem.
Secondly, unlike gradient-free methods, the approach requires a relatively small number of model evaluations and scales to large numbers of turbines, which is necessary for the optimisation of industrial arrays.
For example, in Section 6, an array of 256 turbines is optimised in a realistic domain at an approximate cost of 200 flow solutions.
Thirdly, the optimisation algorithm can incorporate complex constraints such as minimum separation distances, bathymetry gradient constraints, and legal site restrictions.
Finally, the same mathematical framework extends naturally to more realistic flow models such as the Reynolds-averaged Navier-Stokes equations, and to other functionals such as profit or environmental impact.
The approach is implemented in an open-source software framework called OpenTidalFarm; all code and examples from this paper are available at http://opentidalfarm.org.
Optimisation algorithms
Optimisation algorithms can be divided into two categories: gradient-free and gradient-based algorithms.
Gradient-free optimisation algorithms use the functional of interest (in this case, power extracted by the array) as a black box.
They proceed by evaluating the functional at many points in parameter space and use these values to decide which areas merit further exploration.
While these methods tend to be robust and can, under certain smoothness conditions, provably find globally optimal solutions [33], they typically require a very large number of functional evaluations that scales linearly or superlinearly with the number of parameters to be optimised.
For example, Bilbao and Alba [3] used a genetic algorithm that mimics the process of natural evolution to optimise the location of 8 wind turbines.
The algorithm was able to improve the power output by about 70% compared to the initial layout after 17,300 functional evaluations.
This large number of evaluations clearly introduces a practical upper limit for the number of turbines that can be optimised.
This difficulty is compounded if a more realistic (and hence more expensive) model is used.
By contrast, gradient-based optimisation algorithms use additional information to update the position in parameter space at each iteration: the first or higher derivatives of the functional of interest with respect to the parameters.
Depending on the problem, this can lead to a significant reduction in the number of iterations required compared to gradient-free algorithms, making these the only feasible choice for large scale optimisation problems [20].
One caveat of applying gradient-based optimisation algorithms is that they find only local optima.
This issue can be circumvented by using hybrid approaches [22].
The main difficulty of applying gradient-based methods is that the implementation of the gradient computation can be difficult for complex models, as it involves differentiating through the solution of a partial differential equation.
One way to obtain the derivative information is to approximate the gradient using finite differences.
However, a major disadvantage of this approach is that a single gradient evaluation requires a large number of functional evaluations that scales linearly with the number of optimisation parameters.
This sets a practical upper bound on the number of turbines to be optimised, and discards the main advantage of gradient-based optimisation algorithms.
Alternatively, the tangent linearisation of the model (i.e. the derivative of the model evaluated at a particular solution) can efficiently compute the derivative of all outputs with respect to a single input, while the adjoint linearisation can efficiently compute the derivative of a single output with respect to all inputs [19].
For the turbine optimisation problem, we wish to maximise a single output (the power extracted) with respect to many input parameters (the positions and tuning parameters of the turbines); this means that the adjoint approach is the natural choice, as the required gradient information can be computed in a number of equation solves that is independent of the number of turbines.
The development of adjoint models is generally considered as very complicated [17,30].
However, this problem has been solved in recent work for the case where the forward model is discretised using finite elements, in the high-level FEniCS framework [11].
This allows for the extremely rapid development of optimally efficient adjoint models, which significantly reduces the development effort required to implement gradient-based optimisation algorithms for PDE-constrained optimisation problems [13].
To the best of our knowledge, this paper presents the first application of the adjoint method to the optimisation of turbine arrays.
While the examples are shown in the marine context, it is expected that the presented techniques can also be applied to the optimisation of wind farms.
As the wind turbine layout problem is both closely related and better studied, we next review techniques proposed for its solution.
Wind farm optimisation
Layout optimisation for wind farms has been addressed in numerous studies, most of which are based on gradient-free optimisation algorithms.
In particular, evolutionary methods [2] are known to yield good results (Ref.
[35] and the references therein).
These algorithms mimic the process of natural evolution by considering a population of candidate solutions on which it executes an evolutionary process to find the "fittest" solution.
A related method is particle swarm optimisation [24], which considers a population of candidate solutions called particles, that move through the parameter space and influence each other to drive the swarm to the best solution.
Wan et al. [43] applied particle swarm optimisation on an analytical wake model to optimise the location of 39 wind turbines and showed that this approach can yield better optimal solutions than genetic algorithms.
Simulated annealing algorithms [26] are probabilistic optimisation methods that exploit an analogy between the way in which a metal heats and cools into a minimum energy crystalline structure (the annealing process) and the search for a minimum in a more abstract system.
Bilbao and Alba [3] used an analytical wake model to compare a simulated annealing algorithm with a genetic optimisation algorithm.
In a scenario with 47 turbines, the number of model evaluations was reduced from 1,036,200 with the genetic algorithm to 61,802 with simulated annealing.
However, such a large number of evaluations would be infeasible for a more complex PDE-based model.
Few publications solve the layout problem with gradient-based optimisation algorithms.
Lackner and Elkinton [27] optimised the position of two wind turbines by applying a gradient-based optimisation algorithm to a simplified energy production model with an analytical expression.
Huang [22] combined a genetic algorithm with steepest ascent to accelerate convergence to an optimal solution.
With this additional derivative information, the number of iterations was reduced by approximately an order of magnitude to less than 300 iterations, for a similar power extraction.
Finally, Fagerfjäll [10] showed how mixed integer linear programming techniques can be used to optimise for both the number and position of turbines.
All of these publications use very simplified flow models for which the gradient is either available analytically or can be easily approximated.
Furthermore, none of the reviewed papers use gradient-based methods with the adjoint technique to find an optimal configuration in a physically realistic model.
While this prior research on wind farm optimisation is relevant, there are key differences between wind and tidal turbine arrays.
Firstly, the flow in a tidal channel is dominantly driven by the predictable tidal forcing, while wind flow modelling is inherently stochastic and needs to include the temporal uncertainty in the magnitude and direction of the wind forcing.
Secondly, the ratio of turbine height to free-surface elevation is significantly different: while the rotor diameter of a wind turbine is small compared to the height of the atmosphere, tidal turbines typically have diameters of around 20 m and are deployed in water depths of approximately 50 m or less.
This leads to little undisturbed flow above the turbine which could contribute to wake recovery and thus potentially increases the length of the wake compared to wind turbines [6,8].
Finally, if a tidal farm occupies a large fraction of the channel's width and depth then the presence of turbines significantly affects the flow velocity upstream of the farm [14,39].
In wind farms, this is not the case [41].
This interaction adds an additional complication to the optimisation of tidal farms.
The remaining sections are organised as follows.
In Section 2, we formulate the turbine configuration problem as an optimisation problem constrained by the shallow water equations.
Section 3 discusses the discretisation and implementation, which is then verified in Section 4.
In Section 5, we demonstrate the capabilities of the proposed approach on four idealised scenarios with 32 turbines.
Section 6 presents an application of this approach where the positions of up to 256 turbines are optimised in a geometry motivated by the Inner Sound of the Pentland Firth, Scotland.
Finally, we make some concluding remarks in Section 7.
Problem formulation
In this section we formulate the optimal configuration of turbines in an array as a PDE-constrained optimisation problem in the following abstract form:(1)maxz,mJ(z,m)subjecttoF(z,m)=0,bl≤m≤bu,g(m)≤0,where J(z,m)∈R is the functional of interest, m are the design parameters, F(z,m) is a PDE operator parameterised by m with solution z, bl and bu are lower and upper bound constraints for the design parameters, and g(m) enforces additional restrictions on the design parameters.
In this work z = (u,η) is the solution (horizontal velocity, free-surface displacement) of the shallow water equations written in the form F(z,m) = 0, and m contains the configuration (position and/or tuning parameters) of the turbines.
The bounds bl and bu are used to enforce that the turbines remain in a prescribed area (here assumed a rectangle for simplicity), while g(m) is used to enforce a minimum distance between any two turbines (e.g. as a multiple of the turbine diameter).
The optimisation problem (1) can be reduced by using the fact that the constraint equation F(z,m) = 0 implicitly maps any choice of m to a unique solution z.
Hence, the solution z can be considered as an implicit function of the optimisation parameters: z ≡ z(m).
By substituting this operator into the functional of interest, we obtain the reduced optimisation problem:(2)maxmJ(z(m),m)subjecttobl≤m≤bu,g(m)≤0.
While it is possible to solve the optimisation problem in unreduced form (1), we choose to solve the reduced form, as it is usually preferable for time-dependent governing equations [29].
The design parameters
For the turbine optimisation problem considered here, the design parameter m is a vector containing the positions, and optionally the tuning parameters, of the turbines.
If the turbine tuning parameters are fixed, then the design parameters contain only the (x,y) positions of the N turbines encoded in the form:
If additionally the turbines are to be individually tuned, then the vector m is extended to contain the friction coefficient Ki of each turbine:
This could be further generalised to account for any number of turbine parameters.
The PDE constraint
The constraint equation F(z,m) = 0 enforces the laws of physics in the optimisation problem (1).
For gradient-based optimisation, the constraint equation must fulfill some properties.
Firstly, for every m it must yield a unique solution z so that z can be written as an implicit function of m.
Secondly, F must be differentiable and its derivative with respect to z must be continuously invertible.
In this work, the physical laws are modelled by the nonlinear shallow water equations.
Let Ω be the domain of interest and let (0,T) be the simulation period.
Then the equations read:(3)κ∂u∂t+u·∇u-ν∇2u+g∇η+cb+ct(m)H‖u‖u=0,κ∂η∂t+∇·(Hu)=0,where the unknowns u:Ω×(0,T]→R2 and η:Ω×(0,T]→R are the depth-averaged velocity and the free-surface displacement, respectively, H:Ω→R is the water depth at rest, g∈R is the acceleration due to gravity, ν∈R is the viscosity coefficient, and cb:Ω→R and ct(m):Ω→R represent the quadratic bottom friction and the turbine parameterisation, respectively.
The parameter κ ∈ {0,1} specifies if the stationary (κ = 0) or the non-stationary problem (κ = 1) is considered; in the stationary case the time-dependency of the variable definitions above can be neglected.
The boundary conditions are as follows: on the domain inflow boundary ∂Ωin a Dirichlet boundary condition is applied to the velocity.
On the outflow boundary ∂Ωout the free-surface displacement is set to zero.
Elsewhere, a no-slip or a no-normal flow boundary condition with a free-slip condition for the tangential components is imposed.
Note that these boundary conditions imply that the effect of the array on the free-stream flow is negligible, which is not the case for large farms [39]: for large arrays, it would be more appropriate to force the model with tidal boundary conditions on the free surface displacement or to ensure that the size of the domain includes the far-field region around the farm.
The turbine parameterisation
A turbine is modelled here via an increased bottom friction over a small area representative of an individual turbine [8].
The sum of the bottom friction associated with all turbines is denoted as ct(m) in equation (3).
This individual turbine approach is in contrast to previous work (e.g. [9,14]), where uniformly increased bottom drag over the array area was used to parameterise groups of turbines.
In Ref.
[8]; the authors set the friction to a constant value at the turbine locations and zero everywhere else.
This constant parameterisation is problematic in the context of gradient-based optimisation because the friction becomes a non-differentiable function of the turbine position.
For this reason, the turbine parameterisation used in this work smoothly increases the friction value at each turbine position.
The associated friction function is constructed from bump functions, i.e. smooth functions with finite support.
A bump function in one dimension is:(4)ψp,r(x)≡{ⅇ1-1/(1-‖x-pr‖2)for‖x-pr‖<1,0otherwise,where the two parameters p and r are the centre and the support radius of the bump function, respectively.
A two-dimensional bump function is obtained by multiplying equation (4) in both independent dimensions.
With that, the friction function of a single turbine parameterised by a friction coefficient Ki centred at a point (xi,yi) is given by:(5)Ci(m)(x,y)≡Kiψxi,r(x)ψyi,r(y).
A plot of the resulting friction for Ki = 1, xi = 10, yi = 10 and r = 10 is shown in Fig. 1.
The turbine friction function ct in the governing PDE (3) is defined to be the sum of the friction functions (5) for all N turbines:(6)ct(m)≡∑i=1NCi(m),where a single value for r is used based on the assumption that the deployed turbines are of equal size.
Note that turbine properties can be calibrated by modifying the friction parameter K in equation (5).
For example, the amount of energy that is extracted from an individual turbine (e.g. due to different pitch settings of the turbine blades) can be controlled.
As a further extension this could be used to handle cut in/out velocities in which the turbines are operational, but this is not considered in this work.
Finally, other more sophisticated turbine parameterisations such as extensions of actuator disc theory could be employed [32].
The functional of interest
The functional of interest J in equation (1) defines the value of interest that is to be maximised.
For gradient-based optimisation, J must be differentiable.
A natural choice for the functional of interest is the time-averaged power extracted due to the increased friction by the turbines [36,41].
In the non-stationary case (κ = 1) this is expressed as:(7)J(u,m)=1T∫0T∫Ωρct(m)‖u‖3ⅆxⅆt,where ρ is the fluid density.
In the stationary case (κ = 0) the functional is defined to be the power extracted from the increased friction:(8)J(u,m)=∫Ωρct(m)‖u‖3ⅆx.
Note that this value represents kinetic power extraction rather than electrical power generation, since it does not incorporate losses due to the turbine support structures and the conversion to electricity.
More advanced functional choices could instead maximise the profit of the turbine farm, by including installation and service costs depending on turbine size and the deployment location [1].
Another alternative would be to incorporate potential environmental impacts.
These are not considered in this study.
Box and inequality constraints
The box and inequality constraints in the generic optimisation problem (1) are used to define the feasible values for the optimisation variables.
In the context of the turbine layout problem, a typical condition is to restrict the area in which the turbines may be placed to the development site.
The numerical examples in this work have rectangular shaped deployment sites and therefore box constraints are sufficient to enforce this restriction.
For more general site shapes appropriate inequality constraints can be used instead.
Another common condition is to ensure that individual turbines do not overlap.
This is implemented by enforcing a minimum distance dmin between any two turbines:(9)‖pi-pj‖2≥dmin2∀i,j:1≤i<j≤N.
In order for the optimisation to be well-posed, the constraints must satisfy a constraint qualification [21].
The box constraints and inequality constraint (9) are concave functions, and hence satisfy the Concave Constraint Qualification (CCQ) (Ref.
[7]; theorem 5.4).
More advanced constraints could for example enforce a minimum or maximum deployment depth or limit the maximum local bathymetry steepness where turbines may be installed, but these are not investigated in this work.
Numerical setup
Optimisation algorithm
A typical gradient-based optimisation algorithm implements the following iteration:•
Choose an initial guess m0 for the design parameters.
•
for i = 0,1,… 1.
Solve the forward problem F(zi,mi) = 0 for zi and evaluate the functional of interest J(zi,mi).
2.
Compute the functional gradient d J/dm(zi,mi).
3.
Stop if the termination criteria are fulfilled.
4.
Find improved design parameters mi+1 using the results of 1 and 2.
Optimisation algorithms differ mainly in their implementation of step 4.
The main task is to identify an improved parameter choice so that the algorithm converges quickly to the optimal solution while satisfying the imposed constraints.
In this paper we solve the turbine configuration problem using sequential quadratic programming (SQP), which is considered to be one of the most efficient optimisation algorithms [4].
The implementation used here is the SLSQP algorithm available through the SciPy optimisation package [23] and is described in detail in Ref.
[25].
The optimisation problem was formulated and solved with the PDE-constrained optimisation framework described in Ref. [13].
The SQP implementation used is not scale-invariant, i.e. scaling the functional of interest can impact the convergence of the algorithm (even though it does not change the optimal configuration).
Preliminary numerical investigation found that such rescaling was necessary to achieve fast convergence.
Therefore, for each numerical experiment presented here the problem was internally rescaled such that the maximum absolute value of the initial functional gradient with respect to the turbine positions was ten times the turbine radius.11
More precisely, we compare the Riesz representer of the functional gradient with respect to turbine positions, which has the same units of length as the turbine radius.
The functional gradient computation with the adjoint approach
The second step of the optimisation algorithm requires the computation of the functional derivative with respect to the optimisation parameters d J/dm.
Its efficient computation is achieved using the adjoint approach.
For reasons of brevity, only the key equations are mentioned; for fuller explanations, see Refs. [11,17,20].
Let |J| be the number of functional outputs (in this case 1, the power), let |m| be the number of parameters, and let |z| be the number of degrees of freedom of the discretised state vector.
The adjoint approach computes the gradient in two steps.
Firstly, the adjoint equation is solved to obtain the adjoint solution λ (with the matrix dimensions indicated below the braces):∂F∂z∗︸|z|×|z|λ︸|z|×|J|=∂J∂z∗︸|z|×|J|,where * denotes the Hermitian transpose.
The adjoint solution plays the role of the Lagrange multiplier enforcing the PDE constraint in the Lagrangian associated with the optimisation problem (1).
Secondly, the functional gradient is obtained by:(10)ⅆJⅆm︸|J|×|m|=-λ∗︸|J|×|z|∂F∂m︸|z|×|m|+∂J∂m︸|J|×|m|.
The functional gradient is the key piece of information that makes the optimisation of large numbers of turbines tractable.
For the optimisation problem considered here, the adjoint shallow water equations are:(11)-κ∂λu∂t+(∇u)∗λu-(∇·u)λu-u·∇λu-ν∇2λu-H∇λη+cb+ct(m)H(‖u‖λu+u·λu‖u‖u)=∂J∂u∗,-κ∂λη∂t-g∇·λu=0.where λ ≡ (λu,λη) is the vector containing the unknown adjoint velocity and adjoint free-surface displacement, respectively.
The derivation of these equations can be found in Ref.
[12].
The non-stationary adjoint equations (κ = 1) have a final-time condition for the adjoint velocity and free-surface displacement; this condition is homogeneous, as the functional J has no term evaluated at the end of time.
The adjoint equations are solved from the final time to the initial time, propagating information backwards in time.
The boundary conditions for the adjoint equations are the homogeneous versions of the boundary conditions of the forward equations, again because the functional has no boundary integral terms.
The functional derivative (∂J/∂u)∗ appears as the source term for the adjoint velocity equation and is easy to evaluate as the functional is available as an analytical expression.
Note that the adjoint equations are linear while the forward equations are nonlinear, and therefore solving the adjoint equations is typically much cheaper.
If the time-dependent equations are solved, the entire forward trajectory is required to assemble the adjoint equations; if the forward trajectory is too large to store at once, then a checkpointing algorithm must be used [11,18].
In this work, rather than deriving, discretising and implementing the adjoint equation (11) by hand, we apply the high-level algorithmic differentiation approach described in Ref.
[11].
This efficiently and automatically derives and implements the discrete adjoint model from the implementation of the forward model (12), without user intervention.
This significantly reduces the effort and expertise required to implement adjoints of complex nonlinear forward models.
The second step of the adjoint approach (equation (10)) evaluates the functional gradient using the adjoint solution λ.
This step only requires the computation of a matrix-vector product and consequently its computational cost is negligible.
This allows for the computation of the gradient of the functional with only the solution of one adjoint system.
While the cost of the matrix-vector product technically depends on the number of turbines, in practice the cost of the adjoint PDE solution dominates, and so the adjoint technique yields the gradient at a cost independent of the number of turbines.
This is a key property if many turbines are to be optimised.
Discretisation
The governing PDEs (3) are discretised with the finite element method.
The weak form is derived by multiplying the equations with test functions (Ψ,Φ) from suitable function spaces, integrating over the computational domain and applying integration by parts to selected terms.
The weak form of equations (3) is: find (u,η) such that ∀(Ψ,Φ):(12)κ〈∂u∂t,Ψ〉Ω+〈u·∇u,Ψ〉Ω+ν〈∇u,∇Ψ〉Ω+g〈∇η,Ψ〉Ω+〈cb+ct(m)H‖u‖u,Ψ〉Ω=0,κ〈∂η∂t,Φ〉Ω-〈Hu,∇Φ〉Ω+〈Hu·n,Φ〉∂Ωin∪∂Ωout=0,where 〈.
, .〉Ω denotes the L2(Ω) inner product.
The no-normal flow/free-slip conditions on ∂Ω\(∂Ωin ∪ ∂Ωout) are weakly imposed by excluding the associated surface integrals.
The Dirichlet boundary conditions are strongly imposed by restricting the function spaces to functions that yield the correct boundary values.
The discretised problem is obtained by choosing discrete function spaces in the weak formulation (12).
In this work, these are constructed from a suitable triangulation of the computational domain Ω using the Taylor-Hood finite element pair which uses piecewise quadratic functions on each triangle for the velocity and piecewise linear functions on each triangle for the free-surface displacement [37].
If the non-stationary problem is considered (κ = 1), then the spatially discretised equations (12) must also be discretised in time.
In this paper, the time discretisation was performed using the implicit Euler method, due to its unconditional stability and simplicity.
Finally, the integral evaluation of the functionals of interest (7) and (8) used the same quadrature rules as used in the underlying finite element formulation of the problem.
Verification
Verification of the forward model
The shallow water model implementation was verified by order-of-convergence analysis.
The analytical solution is constructed using the method of manufactured solutions [31,34]: a desired analytical solution is chosen and then substituted into the governing PDE, which yields a non-zero remainder.
By adding this remainder as a source term to the governing equations, the selected solution becomes an analytical solution to the modified PDE.
To determine the spatial order of convergence of the model, the time step was fixed to a small value of Δt = T/16,800 s, to ensure that the numerical error of the spatial discretisation dominates the overall discretisation error.
Then, the forward model with the added source term was run on four uniform, increasingly fine meshes and the error in the numerical solution (u,η) measured as:E=(‖u-uexact‖Ω×(0,T)2+‖η-ηexact‖Ω×(0,T)2)12.
The resulting errors plotted in Fig. 2 show the second-order convergence that is expected from the Taylor-Hood finite element pair.
For determining the temporal order of convergence, a mesh with 2.5 m element size in the x-direction and 160 m element size in the y-direction was generated (the analytical solution does not vary in the y-direction and hence a relatively large mesh element size can be used).
This fine mesh resolution ensures that the numerical error of the temporal discretisation dominates the overall discretisation error.
The forward model was then run with a set of different time steps.
The resulting errors plotted in Fig. 2b show the first-order convergence expected for the implicit Euler time discretisation.
Verification of the gradient computation
The gradient computation was verified using the Taylor remainder convergence test.
Let Jˆ(m)≡J(z(m),m).
The first-order Taylor expansion states that:(14)|Jˆ(m+δm)-Jˆ(m)-ⅆJˆⅆmδm|=O(‖δm‖2).
Examining the convergence order of the remainder term is a strong test that the adjoint model and the gradient evaluation are implemented correctly: for nonlinear functionals, the gradient computed using the discrete adjoint approach is correct if and only if the Taylor remainder converges at second order.
Firstly, a simple configuration with a single turbine was set up with the turbine parameterisation and the functional of interest as described above.
The Taylor remainder convergence test in many random perturbation directions δm yielded the expected second-order convergence (not shown).
Secondly, the Taylor remainder convergence test was applied on several of the numerical examples presented in the following section (not shown).
All yielded second-order convergence, giving confidence that the adjoint model and the gradient computation are implemented correctly.
Examples
The following numerical examples solve the optimal layout problem in four idealised scenarios motivated by [9] (Fig. 3).
Additionally, in scenario 3 we optimise for the positions and the tuning of the individual turbines.
In all examples, 32 turbines are to be deployed in a rectangular turbine site of size 320 m × 160 m.
The idealised domains simplify the subsequent interpretation of the optimised configurations.
Nevertheless, the domains are chosen such that they resemble structures that can be found in practical deployment sites.
The main three objectives for the numerical examples are to investigate: by how much can optimisation increase the energy extraction? Can the optimisation algorithm reliably improve the energy extraction for different scenarios? How does the choice of the optimisation variables and the constraints impact the resulting configuration?
The parameter choices for the experiments are listed in Table 1.
The stopping accuracy of the SLSQP optimisation algorithm was set to 10-6 unless otherwise stated.
The stopping criteria demand that the solution is feasible (i.e. that the L1 norm of the constraint residuals is less than the tolerance) and that the solution is optimal (i.e. that the gradient in the search direction is also less than the tolerance).
This tolerance is extremely tight, as can be seen in the convergence plots, and could be weakened for efficiency reasons.
Scenarios 1 and 3 were modelled using the stationary shallow water equations with the following boundary conditions.
On the inflow boundary a constant inflow velocity of 2 m s-1 was enforced and on the outflow boundary the free-surface displacement was set to zero.
A no-normal flow boundary condition with a free-slip condition for the tangential components was applied on the remaining boundaries.
Scenarios 2 and 4 solved the non-stationary shallow water equations with boundary conditions explained in the associated sections.
All examples used unstructured meshes with a uniform mesh element size of h = 20 m outside the site area.
Inside the site area, the mesh was structured with an element size of h = 2 m.
The higher resolution in the turbine site ensures that each individual turbine is well resolved, independent of its location within the site; this obviates the need for regridding when the turbines are moved.
Doubling the resolution for the problem considered in Section 5.1 changed the power extracted by less than 0.5%.
It is therefore assumed that the problems are sufficiently well resolved.
The resulting meshes consisted of approximately 33,000 triangles for scenario 1 and 2, 63,000 triangles for scenario 3 and 45,000 triangles for scenario 4.
All meshes were generated using Gmsh [16].
In all numerical experiments, the optimisation algorithm was initialised with the 32 turbines deployed in a regular 8 × 4 grid and with box constraints for the turbine positions to ensure that the turbines remain inside the site areas.
A single turbine
As a preliminary test, a single turbine was deployed in the setup of scenario 1.
The turbine was placed 640/3 m × 320/2 m away from the bottom left corner, as shown in Fig. 4.
This setup was used to study the dependency of the power extraction J on the friction coefficient K that occurs in the turbine parameterisation (5).
Fig. 4c shows the power extraction for a range of K values.
The graph shows a defined single peak where the power extraction is maximised, and is similar to previous studies [14,40,41].
The reason for this peak is that as K → 0, the turbine friction function ct approaches 0, which in turn results in no power extraction since the power function (8) is multiplied by ct; similarly, as K → ∞, the flow is deflected around the turbine and results in the observed power drop.
The power extraction peaks for K = 21, which was used for all following numerical tests if not otherwise stated.
With this choice the single turbine extracted 3.2 MW from the flow.
Scenario 1
Firstly, the layout problem for scenario 1 (Fig. 3a) was solved without enforcing a minimum distance between turbines, i.e. the turbines can be placed arbitrarily inside the site area and may even overlap.
With that setup the optimisation algorithm terminated after 135 iterations (134 gradient evaluations, 231 functional evaluations).
The results are shown in Fig. 5.
Compared to the initial regular layout (Fig. 5a) the optimisation algorithm was able to increase the farm power extraction by 76% from 54.5 MW to 95.7 MW (Fig. 5c).
The optimised layout (Fig. 5b) has the turbines aligned in the shape of two ⊐s with the open end facing the inflow.
An intuitive interpretation of this layout is that the water is funnelled by the two sides of the ⊐s and then forced through the dense turbine 'wall' at their closed ends.
This interpretation is confirmed by an increasing free-surface displacement (not shown) and velocity difference along the sides of the ⊐s and the large jump along their closed end (Fig. 5d).
An additional experiment was performed where inequality constraints were included to enforce a minimum distance of 30 m (3 turbine radii) between each turbine.
With this setup, the optimisation algorithm terminated after 54 iterations (53 gradient evaluations, 112 functional evaluations) and the farm power extraction increased by 38% from 54.5 MW to 75.0 MW.
The reduced optimised power extraction compared to the previous setup is expected since the inequality constraints add further restrictions to the feasible turbine positioning.
In particular, the previous optimised turbine layout is not a feasible solution for this setup.
The optimised alignment differs significantly from the previous one (Fig. 6b).
The two main characteristic structures are a > shaped alignment close to the inflow boundary and a wall of turbines near the outflow boundary.
Furthermore, the turbines are staggered to avoid placing one turbine in the direct wake of another turbine.
Finally, the free-surface displacement (not shown) and the velocity magnitude decrease more gradually towards the outflow compared to the previous setup (Fig. 6d).
Scenario 2
The layout problem for scenario 2 (Fig. 3b) was solved using the non-stationary shallow water equations.
The boundary conditions were as follows.
On the top, bottom and right boundaries a no-slip boundary condition was applied.
On the left boundary a Dirichlet boundary condition enforced a sinusoidal in-/outflow velocity:u(x,y,t)=(-2sin(2πt/P)0),where P is the tidal period time.
Due to the small basin size, a realistically long tidal period would lead to an excessively large tidal range.
Therefore, the tidal period was defined to be P ≡ 10 min, which resulted in a tidal range of ±12 m.
The simulation time was set to one full tidal period with a time step of Δt = 12 s.
No spin up phase was applied, as its effect is assumed to be small due to the relatively short extent of the domain.
The optimisation was performed without enforcing a minimum distance between the turbines.
After 97 optimisation iterations (96 gradient evaluations, 217 functional evaluations) the relative functional improvement in each iteration dropped below the tolerance and the optimisation was terminated.
The results are shown in Fig. 7.
The average power extracted during one tidal cycle increased by 96% from 10.5 MW to 20.6 MW (Fig. 7e).
The optimised layout (Fig. 7b) resembles the result of scenario 1 (Fig. 5b), with the difference that the opening of the ⊐ structure faces the closed basin side.
Scenario 3
The domain of the third scenario is shown in Fig. 3c.
First, only the layout problem is solved.
For this test, inequality constraints were applied to enforce a minimum distance of 30 m between each turbine.
The optimisation algorithm terminated after 56 iterations (55 gradient evaluations, 73 functional evaluations).
The optimised farm layout extracts 40.6 MW, which corresponds to an increase of 31% compared to the initial layout (30.9 MW) (Fig. 8c).
The optimised layout features a distinct ⋄-shaped alignment with an opening on the inflow facing side (Fig. 8b).
Fig. 8d shows the velocity magnitude and suggests that this hole acts to trap and push the flow through the downstream turbines similar to the previous examples.
Second, the optimisation parameters were extended to include the friction coefficients K of each turbine in the parameterisation (5).
Vennell [39,40] showed the necessity of varying the friction coefficients in order to achieve optimal farm performance.
Each K coefficient was constrained such that 0 ≤ K ≤ 21.
With this setup, the optimisation algorithm terminated after 92 iterations (88 gradient evaluations, 148 functional evaluations).
The results are presented in Fig. 9.
Compared to the initial configuration, the farm power extraction increased by 39% from 30.9 MW to 42.9 MW - the additional freedom of varying the K coefficients resulted in a higher optimised power extraction than the previous setup.
The optimised turbine configuration is similar in shape to the previous solution but with a less distinct hole on the inflow facing side.
Most notably, the friction coefficients of most turbines are significantly reduced.
Only the turbines on the downflow edges of the ⋄ take the maximum value, but nevertheless this configuration extracts 6% more energy than the optimal solution of the previous setup where only the positions were optimised.
This example shows that optimising the friction coefficient (which can be viewed as reducing the size of the turbine or controlling the blade pitch) can lead to a significant increase in the power extraction of the farm.
Scenario 4
The final scenario (Fig. 3d) was solved with the non-stationary shallow water equations.
The simulation time consisted of one P = 12 h sinusoidal period with a time step of Δt = 864 s.
No spin up phase was applied, as its effect is assumed to be small due to the relatively short extent of the domain.
Dirichlet boundary conditions on the left and right boundaries enforced the following sinusoidal in-/outflow velocity:u(x,y,t)=(-2sin(2πt/P)0).
On the remaining boundaries a no-slip boundary condition was applied.
The setup optimised the turbine positions and applied the inequality constraints to enforce a minimum turbine distance of 30 m.
The optimisation algorithm terminated after 55 iterations (54 gradient evaluations, 75 functional evaluations).
The results are shown in Fig. 10.
The averaged power extracted during one cycle increased by 22% from 48.4 MW to 59.0 MW (Fig. 10e).
Since the computational domain is symmetric and the simulation time covered one full period, the optimised layout is expected to be symmetric in the x-direction.
The numerical solution, shown in Fig. 10b, indeed shows an almost symmetric result.
The turbine alignment consists of two distorted ∨ shapes whose open ends face the in/-outflow boundaries.
Similar to the previous example, an interpretation of this alignment is to divert the stream towards the corner of the ∨ where turbines can extract large amounts of power.
An additional row of turbines can be seen parallel to the bottom of the domain.
These turbines are positioned to capture energy from the flow passing along the boundary.
Farm optimisation in the Inner Sound of the Pentland Firth
A key design feature of the framework is that it should scale to problems in realistic oceanographic domains with large numbers of turbines in parallel.
To demonstrate this capability, the layout optimisation of a farm in a semi-idealised geometry modelled on the Inner Sound of the Pentland Firth (Fig. 11a) was conducted on the Stampede supercomputer.
This site is one of the most promising locations in the UK, and is currently under development by MeyGen Ltd.
The computational domain is shown in Fig. 11b.
The pink area marks the turbine site location which roughly approximates the area used by the MeyGen project.
The discretised domain consists of a regular mesh in the turbine site area with 2 m element size, and an unstructured mesh elsewhere with element sizes ranging from 1.5 to 200 m.
The mesh was generated using Gmsh [16] using the GSHHS shoreline database [44].
The mesh consists of 1.25 × 106 elements, which induces a discretisation with a total of 5.6 × 106 degrees of freedom with the Taylor-Hood finite element pair.
In this idealised problem, the bathymetry is assumed constant at H = 50 m.
The addition of bathymetry is straightforward, and will be presented in future work, but would obscure the physical interpretation of the results presented here.
The farm optimisation was modelled during the flood tide using the stationary shallow water equations.
The simulations were performed with the same parameter settings as in the previous section (Table 1), but with an increased viscosity coefficient of ν = 30 m2 s-1 and a decreased turbine friction coefficient of K = 10.5.
This was to ensure the existence of a steady-state solution.
Again, the unsteady case is straightforward and will be presented in future work.
For efficiency reasons, the convergence tolerance of the SLSQP optimisation algorithm was changed to 1.
The same boundary conditions were used as in the steady-state scenarios of the previous section, with the inflow condition imposed on the western boundary and the outflow condition enforced on the eastern boundary.
Two optimisation runs were conducted, with 128 and 256 turbines respectively.
Both were performed on the Stampede supercomputer at the Texas Advanced Computing Center on 64 cores; both took between 24 and 48 h of real time to complete (one functional evaluation took approximately 270 s, one gradient evaluation took approximately 90 s).
The 128 turbine case converged in 197 iterations (197 gradient evaluations, 349 functional evaluations), and increased the power extraction from 600 MW to 741 MW, an increase of 24% (Fig. 12a).
The 256 turbine case converged in 133 iterations (133 gradient evaluations, 185 functional evaluations), and increased the power extraction from 607 MW to 804 MW, an increase of 33% (Fig. 12b).
Even though eight times as many turbines are to be optimised compared to the previous section, the number of iterations has hardly increased at all, suggesting the suitability of the method for larger arrays.
The initial and optimised configurations are shown in Fig. 13.
In both cases, the optimisation algorithm builds very similar structures.
The key features of the optimised layouts are the following:•
Walls of turbines aligned with the north and south boundaries; the southern wall extends for the entire zonal length, while the northern wall only extends for the eastern two-thirds of the site.
The turbine density on the western side of the northern wall appears to decay in a similar way in both cases.
•
Eastern and western 'barrages' of densely packed turbines; in the 128 turbine case, the barrages consist of a single meridional column, while for the 256 turbine case the extra turbines are used to form additional columns.
The optimisation algorithm automatically staggers these extra columns to minimize wake shadowing (Fig. 13d).
These barrages are aligned perpendicular to the flow field (Fig. 14a and b) and extract the majority of the power (Fig. 14c and d).
•
'Spurs' arcing from the southern wall in a north-easterly direction; in the 128 turbine case two spurs can be seen, while there are three spurs in the 256 turbine case.
Again, the optimisation aligns the spurs perpendicular to the flow field.
The spur length is chosen such that the majority of streamlines only intersect with two rows of turbines (Fig. 14c and d).
We hypothesise that these structures serve the following functions:•
The main purpose of the southern and northern walls is to funnel the flow into and retain the flow inside the site boundary.
Similar features are found in all scenarios of the previous section.
The water predominantly flows in from the northwest, which is why the northern wall stops short on the western side.
We hypothesise that the decay of the turbine density on the western side of the northern wall acts to funnel water through the barrages.
A similar density decay is visible in the optimised layout of scenario 1 (Fig. 5b).
•
The flow trapped inside the site domain attempts to escape through the northern and southern walls, which causes the arcing of the western and eastern barrages close to the northern and southern boundaries (Fig. 14a and b).
Since the prevailing incoming flow is towards the southeast, more turbines are placed on the southern wall to retain the flow.
This motivates the deployment of the spurs on the western side of the southern wall.
It is not physically meaningful to compare the optimised power extractions for the 128 and 256 turbine cases, as a realistic power curve was not used.
The maximum velocity in the site for the 128 turbine case was 3.7 m s-1, while it was 3.0 m s-1 for the 256 turbine case.
Due to the cubic dependence of the power extraction on the speed, the power per turbine is approximately doubled in the 128 turbine case, and so the total power extraction of the two cases is almost the same.
However, if the turbine model enforced a rated speed beyond which no extra power was extracted, this approximate equality would not hold.
Conclusions
In this work, the optimal configuration of tidal turbine farms was formulated as an optimisation problem constrained by partial differential equations describing the flow.
This formulation allows the direct application of sophisticated mathematical techniques to its solution, particularly the adjoint technique for rapidly evaluating gradients.
The optimisation of tidal turbine farms is necessary to realise their full potential, but without gradient-based optimisation techniques the computational cost is prohibitive [42].
The approach presented here has several key advantages.
It fully accounts for the nonlinear interactions between the geometry, the turbines, and the flow throughout the optimisation.
The use of gradient-based optimisation algorithms combined with the adjoint technique enables the use of physically-realistic flow models, even for a large number of turbines.
Once the flow model inputs are specified (domain, boundary conditions, initial array configuration, etc.), the optimisation is fully automatic.
The approach extends naturally to more realistic flow models, and to different functionals of interest such as profit or environmental impact.
The algorithm was first applied to the optimisation of four idealised scenarios, both to demonstrate the capability of the method and to build physical intuition.
In all cases, the optimisation algorithm was successful in significantly increasing the power extracted by the farm, at a computationally feasible cost.
The algorithm was then successfully applied to a more realistic optimisation problem, involving a site of major industrial interest, accurate shoreline geometry, and an industrially relevant number of turbines.
Four main extensions are required to apply this in an industrial setting.
Firstly, the simulations must be driven by realistic tidal forcing, and incorporate a wider model domain around the site to be investigated.
This also includes extending the forward model to be forced by a head loss instead of a fixed inflow velocity to incorporate the impact of large arrays on the free-stream velocity [39].
Secondly, bathymetric effects must be accounted for.
Thirdly, the flow model must then be validated against real-world measurements.
Finally, the wake modelling should be improved via a turbulence closure and a realistic power curve used.
All of these advances are the subject of ongoing work.
The source-code of the turbine farm optimisation software and all examples are open-source and available at http://opentidalfarm.org.
Acknowledgements
This work is supported by the Grantham Institute for Climate Change, a Fujitsu CASE studentship, EPSRC grants EP/I00405X/1, EP/J010065/1, EP/K030930/1 and EP/K503733/1, an EPSRC Pathways to Impact award, and a Center of Excellence grant from the Research Council of Norway to the Center for Biomedical Computing at Simula Research Laboratory.
High-performance Computing support was provided by the Texas Advanced Computing Center.
The authors would like to acknowledge helpful discussions with S. C.
Kramer, A.
S.
Candy, A.
Avdis of Imperial College London, and R.
Caljouw and S. Crammond of MeyGen Ltd.
The authors would also like to thank R.
Vennell of the University of Otago for his extremely thorough and constructive review.
Supplementary material
The following are the supplementary data related to this article:Video S1
Layout optimisation of 128 tidal stream turbines in the Inner Sound of the Pentland Firth, Scotland.
The top part of the video shows the turbine positions at each optimisation iteration zoomed into the site area.
The bottom part of the video shows the convergence plot.
Video S2
Layout optimisation of 256 tidal stream turbines in the Inner Sound of the Pentland Firth, Scotland.
The top part of the video shows the turbine positions at each optimisation iteration zoomed into the site area.
The bottom part of the video shows the convergence plot.
Supplementary material
Supplementary material associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.renene.2013.09.031.



All reagents were of analytical grade and were used as obtained from commercial sources without further purification. Mn[P2O5(OH)2] can be synthesized under hydrothermal conditions. Typically, we charged 4 mmol Mn(CH3COO)2*4H2O (or MnCl2*4H2O) into a 50 ml Teflon reactor, then added 3 ml concentrated H3PO4 and sealed the reactor into a stainless steel container. After they were heated at 200 degC for 5 days, the solid products were cooled to room temperature and washed with hot water (80 degC). The obtained Mn[P2O5(OH)2] products are colorless and tablet crystals. Sometimes, a known phase, MnP2O7, is presented as a by-product. The purity of the title compound can be testified by the powder X-ray diffraction.

Experimental study of bore-driven swash hydrodynamics on impermeable rough slopes

Abstract
A detailed experimental investigation of the hydrodynamics of large-scale, bore-driven swash on impermeable, rough beach slopes is described.
The experiments were carried out on 1:10 impermeable, rough slopes with three different surface roughnesses based on 1.3mm sand, 5.4mm gravel and 8.4mm gravel.
The large-scale discrete swash event was produced by the collapse of a dambreak-generated bore on the beach.
Simultaneous depths and velocities were measured using LIF, Laser-induced fluorescence, and PIV, particle image velocimetry, respectively, yielding better resolution of the hydrodynamics than previous studies.
Depth time-series, instantaneous velocity profiles, depth-averaged velocities, instantaneous turbulent kinetic energy profiles, depth-averaged turbulent kinetic energy, Reynolds stress profiles and bed shear stresses are presented for five cross-shore measurement locations in the swash.
The measurements resolve the backwash shoreline position, and the late backwash period when depths are shallow and velocities are high.
The detailed data can be used to test and develop advanced numerical models for bore-driven waves on rough slopes.
Highlights
►Detailed hydrodynamics of bore-driven swash on impermeable, rough slopes.
►Large-scale laboratory experiments measuring simultaneous depths and velocities.
►Shows effects of slope roughness on bulk flow and turbulence properties.
►Yields measures of time-varying bed shear stress across the swash zone.

Introduction
Swash on steep beaches is characterised by wave breaking followed by bore collapse on the beach slope.
The flow velocities generated by bore collapse can be very high, up to 5m/s according to Hughes (1992), and the corresponding run-up on the beach can extend many metres.
The swash zone is therefore a highly dynamic region of the beach, complicated in hydrodynamic terms because the flow is turbulent, highly unsteady and highly non-uniform, and in morphological terms because sediment fluxes are high and vary greatly across the swash zone.
While many studies of swash carried out in the field have yielded useful insights (e.g. Austin and Masselink, 2006; Hughes, 1992; Masselink and Hughes, 1998; Puleo et al., 2000) fundamental aspects of swash are arguably best studied in controlled laboratory experiments because of the complexity of the processes at work.
Table 1 lists previous laboratory studies which have investigated breaking wave and bore hydrodynamics in the swash zone, focused on the detailed hydrodynamics, the turbulence and, in some cases, the bed shear stress for smooth and rough impermeable beach slopes.
The majority of the studies have been carried out in small-scale laboratory wave flumes.
There are two disadvantages in using small wave flumes to study bore collapse on beaches.
The first is that the relatively small scale leads to substantial scale effects in relation to roughness, permeability and sediment behaviour; the second is that the magnitudes of swash depths, velocities and maximum run-up are relatively small, making it more difficult to obtain accurate measurements of the key fundamental processes.
An alternative method for generating a controlled swash event in the laboratory is to generate a large-scale bore through dam-break.
A deep reservoir of water is separated from a shallow body of water by a partition that can be removed at high speed.
After partition removal, the released water plunges and generates a bore travelling away from the reservoir.
Subsequent bore collapse on a sloping beach located downstream results in a large-scale swash event with run-up that is an order of magnitude larger than swash run-up generated by waves on a sloping beach in a typical laboratory wave flume.
The large scale of the swash means that scale effects are avoided and accurate measurements of the more detailed hydrodynamics are possible.
This kind of set-up was used by Yeh (1991) and more recently by Barnes et al. (2009) and O'Donoghue et al. (2010).
The dam-break swash experiment belongs to a larger family of dam-break experiments, which includes, for example, dam-break induced sediment transport on horizontal beds (e.g. Zech et al., 2008), 2D dam-break hydraulics (e.g. Aureli et al., 2008) and dam-break bore travel on a downward inclining slope (e.g. Chanson, 2004).
The present paper reports new experiments designed to study the detailed hydrodynamics of large-scale bore-driven swash on impermeable, rough beach slopes.
The experiments were carried out on three impermeable 1:10 slopes with different surface roughness in order to investigate the effect of roughness on the hydrodynamics.
The experiments use a similar setup to that of O'Donoghue et al. (2010) but add substantially to the previous work by extending the range of beach roughness and by better resolution of the hydrodynamics through improved measurement methods, namely, the use of Laser-induced fluorescence (LIF) to better resolve flow depth and the use of an improved particle image velocimetry (PIV) system capable of measuring bed-normal as well as bed-parallel velocities.
In addition, flow depth and velocity are measured simultaneously in the present study allowing a more thorough and rigorous analysis of the depth-averaged and ensemble-averaged velocities and the turbulence quantities.
The paper is structured as follows.
Section 2 describes the experimental set-up and measurement methods and demonstrates the high repeatability and consistency of the measurements.
Section 3 presents the experimental results: ensemble-averaged results for flow depth, shoreline position, depth-averaged velocity and velocity profiles are presented first, followed by results for turbulent kinetic energy and Reynolds stress; results for bed shear stress complete the section.
The main conclusions of the study are presented in Section 4.
Experimental setup
Facility
The experiments were carried out using an updated version of the swash rig described by O'Donoghue et al. (2010).
It consists of a water reservoir built into one end of a 20m long, 0.9m high and 0.45m wide, glass-sided flume (Fig. 1).
The reservoir is fronted by a gate which can be raised at high speed to produce a large plunging wave leading to a bore which propagates towards an impermeable beach located downstream.
The reservoir is constructed from Perspex and has inside dimensions of 983mm×394mm×832mm (length×width×height).
The reservoir exit is streamlined to ensure a smooth transition for the flow from reservoir to flume, and the gate and seal are designed to enable the gate to be opened at high speed while forming a near watertight seal when in situ.
A steel cable connects the gate through pulleys to a 15kg mass suspended at an elevation of 2.5m above the laboratory floor.
When released, the mass falls freely for 2m before tensioning the cable and raising the gate.
The gate is raised to 0.8m in approximately 0.2s.
Experiments were conducted for three impermeable, rough beaches located downstream of the water reservoir.
Each beach had slope 1:10 and was made of marine plywood with a layer of sediment glued to the surface.
The plywood was bolted onto a steel frame positioned to within 1 to 2mm tolerance of the 1:10 slope.
The three sediments used were 1.3mm sand (d10=1.00mm, d50=1.32mm and d90=1.89mm), 5.4mm gravel (3.64mm, 5.4mm and 7.76mm) and 8.4mm gravel (6.46mm, 8.41mm and 10.4mm) (Fig. 2).
The experiments were conducted for one initial condition only: a water depth in the reservoir (hd) of 600mm and a water depth in front of the gate (h0) of 62mm (Fig. 3).
The origin of the x-z coordinate system is at the initial shoreline position, located 4.82m from the reservoir gate; the x-axis is parallel to the beach slope and is positive shoreward; the z-axis is perpendicular to the slope.
The corresponding velocity components are u and w, respectively.
Measurements were triggered at the moment the gate is raised, which is defined as t=0.
Instrumentation
Velocities were measured using cross-correlation Particle Image Velocimetry (PIV) at one location "seaward" of the beach and at 5 cross-shore locations on the beach.
Narrow slots (100-200mm long and 5mm wide) fitted with Perspex windows were cut into the flume floor and the plywood beach panels to enable Laser illumination of the flow for the PIV measurement.
Illuminating vertically upwards from below eliminates the negative effects of free surface reflections associated with illuminating vertically downwards from above.
The Laser was a New Wave Solo III double-pulsed, frequency-doubled Nd YAG Laser.
Its beam was focused and spread into a light sheet through a series of spherical and cylindrical lenses.
Near neutrally-buoyant, silver-coated, hollow glass spheres with mean diameter 15μm were used for seeding.
Velocities were measured from the beach face to the free surface.
Maximum swash flow depth decreases with increasing distance from the initial shoreline, which means that a smaller flow area needs to be measured for locations further up the beach slope, with consequent increase in accuracy of the velocity measurement.
The reflected Laser light from the seeding particles was captured by a Flowsense 2M, b/w digital video camera, fitted with a 60mm fixed focal length lens and a 532nm narrowband green filter.
The camera records two images, separated in time by 1ms.
The camera was rotated to be aligned with the 1:10 slope of the beach, so that measured velocities are bed-parallel and bed-normal.
To avoid recording the reflections from the free surface when flow depths are small, the camera was rotated slightly backwards so that the camera view was below the free surface at all times (Fig. 4).
The software controlling the PIV system timing, image acquisition and image analysis was DANTEC Dynamic Studio v1.45.
Flow depths were measured using Laser-induced fluorescence (LIF) (Sue et al., 2006).
Fluorescent dye was added to the water, with a concentration of approximately 0.1mg/l and illuminated by the Nd-YAG Laser.
The emitted light from the fluorescent dye was recorded by a Kodak Megaplus ES1.0 b/w digital video camera fitted with a 50mm fixed focal length lens and a Hasselblad orange filter.
The camera was aligned with the 1:10 slope of the beach and rotated forwards so that the camera view was at all times above the free surface (Fig. 4).
To measure flow velocity and depth simultaneously, the PIV and LIF systems were combined into a single PIV-LIF system.
The PIV software controlled the timing of the Laser pulses and the synchronisation of the PIV camera.
By inserting a bnc-splitter in the connection between the timing box and the PIV camera, the timing box was also connected to the frame-grabbing board of the LIF computer.
This allowed the trigger and the grabbing frequency of the LIF images to be controlled by the PIV computer and to be synchronised with the PIV camera.
The combined PIV-LIF system was triggered at the moment that the gate was raised (time t=0).
Fig. 5 shows example simultaneously-recorded PIV and LIF images.
In the bottom right of both images water from the incoming bore, travelling from right to left, is visible.
The similar positions of the water droplets suspended in mid-air in the two images are evidence that both images were recorded simultaneously.
The images are not exactly the same because the PIV camera is rotated slightly backward and records a smaller area than the LIF camera which is rotated slightly forward.
Calibrations were carried out at each position to enable the measurements to be converted from pixel-space into mm-space.
A target grid, secured inside a calibration cell, was positioned on the 1:10 slope so that it coincided with the Laser Sheet.
The PIV calibration image of the grid was recorded while the calibration cell was filled with water, whereas for the LIF the cell was empty.
Measurements
For each swash experiment, simultaneous flow depths (h), bed-parallel (u) and bed-normal (w) velocities were measured over a window centred at six cross-shore locations.
Five were located on the beach at x=0.072, 0.772, 1.567, 2.377 and 3.177m, and one seaward of the beach at x=-1.802m (Fig. 3).
During each experiment 140 PIV image pairs and 140 LIF images were recorded at a frequency of 13.5Hz over approximately 10s.
For a short time immediately following bore arrival at a location, velocities could not be measured because of the relatively high percentage of entrained air in the bore front.
The delay between bore arrival and the first velocity measurement was approximately 0.5s at x=0.072m and approximately 0.2s at locations further up the slope.
Individual experiments were repeated 50 times in order to determine ensemble-averaged as well as turbulent quantities.
In addition to the combined PIV-LIF measurements at the 6 locations, a second set of LIF-only measurements was carried out to measure the swash lens, i.e. the instantaneous surface water profile over the whole of the swash extent.
For these measurements the Laser was positioned above the flume and illuminated approximately 300mm of the cross-shore extent of the lens.
The complete lens was measured by combining measurements from approximately 16 cross-shore locations (the exact number dependent on maximum runup).
For these LIF-only measurements, 140 images were recorded at a frequency of 13.5Hz; individual experiments were repeated ten times to obtain the ensemble-averaged lens.
Altogether 1380 swash runs were carried out on the three impermeable beaches.
The PIV images were analysed using an adaptive cross-correlation algorithm applied in 3 iterations, ending with 32×32 pixels.
The overlap between interrogation areas was 50%, giving a 99×74 velocity vector grid with a spatial resolution between 1 and 2.5mm, depending on the measurement area, and a random error of 5 to 15mm/s in the instantaneous velocity components, u and w.
In the LIF images water is light and air is dark.
A MATLAB-based edge detection algorithm was used to detect the interface, which was successful in determining the flow depth with an error of approximately onepixel, giving instantaneous depth measurement with a spatial resolution and random error of 0.1 to 0.3mm.
Determining depth at the longitudinal centerline of the flume was complicated by the fact that the flow is not always perfectly two-dimensional throughout the swash cycle, especially for a short time at and immediately after bore arrival at the measurement location.
If the flow depth is greater on the side of the flume closest to the camera than it is in the centre of the flume (where the measurement is required), it is impossible for the camera to see the free surface at the centre.
Fig. 6 shows example LIF images for a range of conditions and illustrates the algorithm successfully determining the air-water interface except on the right side of the image in Fig. 6(a), which was recorded immediately after the arrival of the bore at the measurement location.
Repeatability of swash events using the rig was excellent, which meant that measurements from many repeats of the same event could be used to produce a very detailed dataset for the swash event and to obtain turbulence measurements.
Repeatability is illustrated in Fig. 7 which shows the depth time-series and the depth-averaged bed-parallel velocity time-series at x=0.072m for the 1.3mm sand-rough beach; results from the 50 individual experiments are shown together with the ensemble-averaged result.
Variability in the depth is greatest just after bore arrival.
There is also some variability coinciding with fluctuations in the ensemble-averaged result, caused by the plunging wave collapsing on the water in front of the gate, generating small waves that propagate with the bore towards the beach.
The fluctuations, and therefore the variability in flow depth, diminish with time and increasing distance up the slope.
There is very little variability in the depth-averaged velocity results up to the time of maximum backwash velocity, after which the relatively small flow depths and high velocities cause high levels of bed-generated turbulence throughout the flow column.
Note that, unlike capacitance gauges which have been used in previous swash measurements, LIF enables depth measurements to be made late into the backwash when depths are very small and velocities are high.
For example, for the 1.3mm sand-rough beach measurements were possible down to 1mm flow depth.
This advantage, combined with the high-quality PIV camera, made it possible to measure backwash velocities for flow depths down to approximately 5mm.
Much more of the backwash is therefore captured by the present measurements compared to O'Donoghue et al. (2010), whose backwash measurements were limited to h>15mm.
In addition, because flow depth and velocity are measured simultaneously in the present study, and include bed-normal velocities (absent from O'Donoghue et al. (2010)), a more thorough and rigorous analysis can be made to determine the depth-averaged and ensemble-averaged velocities and the turbulence quantities compared to previous studies.
A measure of the data consistency was obtained by comparing the ensemble-averaged flow rate per unit width, q, determined from the combined PIV-LIF measurements via(1)q(x,t)=∫0h¯(x,t)u¯(x,t)dzwith q determined from the LIF swash lens measurement via(2)q(x,t)=1Δt∫xxs(t+Δt/2)h¯(x,t+Δt/2)dx-∫xxs(t-Δt/2)h¯(x,t-Δt/2)dxwhere u¯ is ensemble-averaged, depth-averaged bed-parallel velocity (overbar denotes ensemble-average and angled brackets denote depth-average), h¯ is ensemble-averaged depth and xs is shoreline position.
Example comparisons of results from the two methods are shown in Fig. 8; the good agreement seen was typical of all such comparisons.
Results
Swash depth
Time series of ensemble-averaged flow depth, h¯, are presented in Fig. 9 for all six measurement locations on the three beaches.
The general behaviour of the flow depth seen in Fig. 9 is similar to that described by previous researchers for swash depths measured in the field (e.g. Hughes and Baldock, 2004; Hughes et al., 1997; Masselink and Hughes, 1998) and in the laboratory (Barnes et al., 2009; Cowen et al., 2003; O'Donoghue et al., 2010).
At the most seaward measurement location, x=-1.802m, the time series of ensemble-averaged depth for the three beaches are initially in good agreement, reflecting the repeatability of the dam-break-generated bore.
Later on, especially after the flow reversal, the results for the 1.3mm sand-rough beach lag slightly behind the other two.
This is a consequence of the greater volume of water that passed the initial shoreline location during the uprush for the 1.3mm sand-rough beach, resulting in a greater maximum runup and later flow reversal.
As mentioned earlier, the depth time series show small periodic fluctuations which result from the wave plunging.
These fluctuations are largest at x=-1.802m, diminish very quickly on the lower end of the slope, and all but disappear further up the slope.
For all locations, the flow depth rapidly increases after bore arrival.
At the lowest locations on the beach (x=0.072m and x=0.772m), maximum depth is followed by a period of slowly-decreasing depth, which lasts approximately 3s for x=0.072m and approximately 2s for x=0.772m.
Further up the slope the depth decays rapidly immediately after maximum depth.
The rate of decreasing depth increases during the backwash until very late in the backwash when the depths have become small.
The effects of different beach roughness are better visible at locations higher up the slope, where the greater differences in depth reflect the greater differences in the volume of water flowing passed, caused by the difference in roughness.
An alternative perspective on the swash depths is shown in Fig. 10, which presents the swash lenses measured at nine times within the swash cycle for the 1.3mm sand-rough and the 8.4mm gravel-rough beaches.
The corresponding lenses for the 5.4mm gravel-rough beach are not shown because these lie very close to those of the 8.4mm gravel-rough beach.
The differences between the measured lenses are rather small for most of the uprush and only start to become significant as the time of maximum run-up is approached when swash on the 1.3mm sand-rough beach reaches further up the slope than on the 8.4mm gravel-rough beach.
While differences between the uprush lenses of the two beaches are only apparent at the lens tip, differences in the backwash lenses are seen over the whole swash lens, with backwash depths for the 8.4mm gravel-rough beach being lower than those of the 1.3mm sand-rough beach.
This is because maximum volume of water on the beach is lower, resulting in a lower maximum run-up and flow reversal occurring earlier in the case of the 8.4mm beach.
These results are consistent with the findings of O'Donoghue et al. (2010), who measured lenses on a smooth Perspex beach and a 5-6mm gravel-rough beach.
Time-series of shoreline position obtained from the swash lenses are presented for each of the three beaches in Fig. 11.
The shoreline is here defined as that location on the lens where the water depth is equal to 5mm.
5mm was chosen because it was the smallest depth that could be accurately measured for all three beaches.
During uprush the position of the shoreline is not sensitive to the choice of water depth defining its position (O'Donoghue et al., 2010).
However, in the backwash the slope of the flow profile becomes very gradual and shoreline position is then sensitive to the choice of flow depth used to define it.
Following gate opening it took approximately 2s for the bore to arrive at the initial shoreline location.
The effect of roughness on the shoreline position is negligible for the first 1.5s following bore arrival.
Thereafter the higher roughness results in greater deceleration and lower maximum runup: maximum runup is 3.95m, 4.12m and 4.52m for the 8.4mm gravel-rough, 5.4mm gravel-rough, and 1.3mm sand-rough beach respectively.
The time of maximum runup is approximately the same for the three beaches at around t=5.5s.
Fig. 11 also shows the times of flow reversal (based on the ensemble-averaged depth-averaged velocity time-series, Fig. 12) at the five PIV measurement locations on the beach.
There is a small difference in the flow reversal time at each position between the three beaches, with reversal occurring slightly sooner on the rougher beaches.
The increasingly later time of flow reversal for locations further up the beach clearly illustrates the divergent nature of the flow and is consistent with the characteristics of the velocity obtained in the field by Masselink and Hughes (1998).
Finally, the results for uprush in Fig. 11 echo those of O'Donoghue et al. (2010) for shoreline position on a smooth Perspex beach and a gravel-rough beach, but the present results for backwash go beyond those of O'Donoghue et al. (2010) who were unable to resolve the backwash shoreline position from their capacitance gauge depth measurements.
Depth-averaged velocity
Time series of ensemble-averaged, depth-averaged, bed-parallel velocity, u¯ are presented in Fig. 12 for the six measurement locations and the three beaches.
The measurements for the location seaward of the beach, x=-1.802m, echo the depth measurements for the same location (Fig. 9): for all 3 beaches the u¯ time-series are nearly identical for the majority of the uprush period and are similar during the backwash, with u¯ time series for the 1.3mm sand-rough beach lagging slightly behind the other two.
Features of the ensemble-averaged depth-averaged time-series are in agreement with those from measurements obtained in the field (e.g. Hughes and Baldock, 2004; Masselink and Hughes, 1998; Puleo et al., 2000) and the laboratory (e.g. O'Donoghue et al., 2010).
Maximum uprush velocity at a given location occurs at the time of bore arrival and the flow decelerates during uprush.
Maximum values of u¯ exceed 2m/s, which is of similar order of magnitude as high swash velocities measured in the field (Elfrink and Baldock, 2002).
The longer time gap between the time of bore arrival and the first velocity measurement explains why maximum uprush velocity at x=0.072m appears to be lower than the maximum uprush velocity at locations higher up the beach.
The flow accelerates in the backwash until it reaches a maximum backwash velocity when the retarding force due to bed friction balances the driving force due to the slope component of weight of water on the beach.
Maximum uprush velocity is higher than maximum backwash velocity (although the actual maximum uprush velocity is not measured here because of the delay already described), the flow decelerates during the final stage of the backwash when the bed friction exceeds the driving force.
Because of the shallow water depths, velocities are difficult to measure during the late stage of the backwash period.
To the authors' knowledge only O'Donoghue et al. (2010) have previously obtained velocity measurements during this late stage, but their measurements are limited to backwash flow depth greater than 15mm.
The effect of beach roughness on the u¯ time-series is relatively small during uprush and for a significant part of the backwash.
It becomes significant later in the backwash: for the rougher beaches, maximum backwash velocity is reached earlier and has lower magnitude.
Velocity profiles
Ensemble-averaged velocity profiles were generated from the simultaneous measurements of depth and velocity.
Example profiles of bed-parallel velocity are shown for the 1.3mm sand-rough beach in Fig. 13(a) and for the 8.4mm gravel-rough beach in Fig. 13(b).
The bottom-most measurement in each profile corresponds to z~3mm.
The velocity profiles are similar to the rough-beach profiles presented by O'Donoghue et al. (2010): the profiles have a typical "forward-leaning" shape during uprush, become close to depth-uniform at initial stages of the backwash, and become progressively more non-uniform as the velocity increases and depth decreases in the later stages of the backwash.
Velocity gradients are highest close to the bed and, where the depth is large enough, approach near depth uniformity higher up the water column.
Increasing bed roughness tends to increase the extent of the near-bed, high gradient flow; the effect is best seen by comparing the 1.3mm sand-rough and the 8.4mm gravel-rough profiles for x=0.772m.
In the backwash, the flow depth decreases rapidly and the flow velocity increases (until it reaches the maximum backwash velocity).
This results in bed-parallel velocity profiles with very steep near-bed gradients.
For example, at the time of maximum backwash velocity on the 1.3mm sand-rough beach at x=0.772m, the velocity increases from zero at the bed to -1.8m/s at the surface within a depth of 25mm.
Example profiles of bed-normal velocity are shown for x=0.772m for the three beaches in Fig. 14.
The bed-normal velocities are close to zero for most of the swash event with the exception of the time just after bore arrival and at the late stage of the backwash, just before the time of maximum bed-parallel velocity.
The short-lived, positive (upward) vertical velocities seen soon after bore arrival are likely due to the effect of the plunging wave: the resulting clockwise rotation (Miller, 1968; Petti and Longo, 2001) causes the fast fluid close to the free surface to hit the bed and move initially stagnant fluid forward and upward.
In the backwash there is a tendency towards negative bed-normal velocity close to the bed.
The reason for these negative velocities is unclear, but might be the local effect caused by the thin gap in bed roughness created to allow the light sheet to penetrate into the flow.
Turbulent kinetic energy and Reynolds stress
O'Donoghue et al. (2010) presented turbulent kinetic energy results for their smooth and rough beaches, but were limited by measuring one component of velocity (u) only.
The present measurements include bed-normal velocities and so allow a fuller assessment of the flow turbulence.
Fig. 15 presents profiles of turbulent kinetic energy, TKE=u'u'¯+w'w'¯ (where prime denotes a fluctuation, e.g.
u'=u-ū, and overbar denotes ensemble-average), at selected times for three cross-shore locations (x=0.072, 0.772 and 1.567m) for the 1.3mm sand-rough beach (Fig. 15a) and the 8.4mm gravel-rough beach (Fig. 15b).
Each panel shows the TKE profile at times corresponding to selected values of the depth-averaged velocity (indicated in the panel) for uprush and backwash.
Note that the same depth-averaged velocity can occur twice in the backwash: once during backwash prior to the time of maximum backwash velocity and again during the late stage of backwash (see Fig. 12).
Consider first the results for the 1.3mm sand-rough beach in Fig. 15a.
Close to the initial shoreline location (x=0.072m) and soon after bore arrival, TKE is relatively high and approximately constant throughout the flow column until just above the bed.
This turbulent kinetic energy is generated by the wave collapse and is transported with the flow onto the beach.
The turbulent kinetic energy decays once the bore front has passed the measurement location and this trend persists over the upper section of the water column during the remainder of the uprush.
Near the bed, high velocity gradients and intense shearing generate turbulence, so very soon after bore arrival the TKE profiles show a peak close to the bed.
At later stages of the uprush, advected turbulence from lower locations is transported upwards and the peak in the TKE profile moves slightly away from the bed.
At the same time the flow decelerates, thus decreasing TKE production and enhancing dissipation, so that the bed-generated TKE never reaches very high into the water column.
By the end of the uprush most of the bore-generated turbulence has dissipated.
The flow accelerates following flow reversal, creating high velocity gradients at the bed, developing the boundary layer and generating near-bed turbulence.
These processes take time with the result that TKE is much lower than for the uprush (for the same depth-averaged velocity), and reaches significant levels only by the final stages of the backwash.
Bore-generated turbulence dissipates as the bore progresses up the beach.
At the same time bed-generated turbulence from lower locations on the beach is advected shorewards.
The net result is TKE profiles that are much less depth-uniform compared to the TKE profiles at x=0.072m.
For the 8.4mm gravel-rough beach, Fig. 15b, TKE production and dissipation undergo similar behaviour over the swash cycle as for the 1.3mm sand-rough beach.
As expected, the incoming bore-generated turbulence is approximately similar for the two beaches, but bed-generated turbulence is higher for the rougher 8.4mm beach.
The results presented in Fig. 15 echo those of O'Donoghue et al. (2010) who presented TKE profiles based solely on bed-parallel velocity measurements and for somewhat lower depth-averaged velocities (0.8m/s or less).
Fig. 16 presents time-series of depth-averaged turbulent kinetic energy, 〈TKE〉, for the five measurement locations on the 1.3mm sand-rough and the 8.4mm gravel-rough beaches.
During the uprush 〈TKE〉 decreases due to dissipation of the bore-generated turbulence.
Lower down the slope, 〈TKE〉 is slightly higher for the 8.4mm gravel beach because of the higher production of bed-generated turbulence.
Higher up the slope, this difference between the TKE appears to all but disappear, however this is related to the later bore arrival at this location on the rougher slope.
The decrease in 〈TKE〉 resulting from the dissipation continues at all locations well past the time of u¯ reversal which is in agreement with measurements of Sou et al. (2010) on a smooth beach.
Since the TKE transport at early stages of the backwash is still very low, the lowest level of 〈TKE〉 probably corresponds to a short period when dissipation is in balance with the bed-generated production.
After this moment, 〈TKE〉 rapidly increases due to both higher production and higher transport resulting from the increasing flow velocities.
At x=0.072m the increase continues until the time of maximum backwash velocity, while for locations further up the slope it continues for somewhat longer.
At the final stage of backwash, 〈TKE〉 for the 8.4mm gravel-rough beach is less than for the 1.3mm sand-rough beach.
This is related to the higher maximum backwash velocities in the latter case.
Field estimates of TKE based on the root mean square of the vertical velocity by Hughes et al. (2007) showed that the mean turbulence intensity was higher during the uprush than the backwash, which is consistent with the results presented in Fig. 16.
In a similar manner to Fig. 15 for the TKE profiles, Fig. 17 presents profiles of Reynolds stress, -u'w'¯, for three of the five measurement locations on the 1.3mm sand-rough (Fig. 17a) and the 8.4mm gravel-rough (Fig. 17b) beaches.
After bore arrival at x=0.072m, Reynolds stress is non-zero near the free surface.
This is probably the signature of the free surface turbulence remaining after the passage of the bore front; it diminishes further up the beach as expected.
Generally the -u'w'¯profiles are consistent with the bed-parallel velocity profiles, with increasing stress nearer the bed where velocity gradients are steepest.
The -u'w'¯ profiles are similar to the TKE profiles shown in Fig. 15, except that the incoming turbulence comes with the shear stress close to zero.
During uprush the bed-generated peak in turbulent shear stress moves higher into the flow column, indicating bed-normal transport of turbulent momentum and the associated development of the boundary layer.
The magnitude of -u'w'¯ decreases as the flow decelerates and the near-bed velocity gradients become lower.
Close to flow reversal the upper, more energetic part of the flow continues to move shoreward whereas the slower fluid close to the bed changes direction somewhat earlier.
As a result, the Reynolds stress first changes sign near the bed.
The change in the sign of -u'w'¯ near the bed lags behind the change in the near-bed velocity direction by 0.6s at the higher end of the beach and 0.9s at the lower end.
The delay is caused by the time it takes to generate turbulence at the bed level, and transport it in the bed-normal direction to the level of the lowest measurement point.
This agrees with results from studies of unsteady flow generated by accelerating fluid in a pipe (He and Jackson, 2000; He et al., 2008) or by suddenly changing bed roughness (Chen and Chiew, 2003), where it was found that transfer of turbulence in the wall-normal direction is slower than other transfer processes resulting from flow unsteadiness or flow non-uniformity.
In the early stages of the backwash -u'w'¯ is therefore much lower than in the uprush, for the same velocity.
However, at later stages of the backwash, -u'w'¯ values close to the bed are similar to the uprush values.
By this time the turbulent shear stress has been transported throughout the whole (rapidly decreasing) flow depth.
Bed shear stress
The detailed measurements of velocity offer the possibility, in principle, to estimate indirectly the intra-swash bed shear stress using a number of different methods: (i) application of momentum balance to a control volume; (ii) logarithmic profile fitting to the measured velocity profiles; (iii) from the near-bed peak in the turbulent shear stress; and (iv) relating the shear velocity to the turbulent kinetic energy dissipation rate obtained using the second order structure function.
Of these methods, only momentum balance is strictly applicable to unsteady flow; each of the other three is based on principles that apply to steady flow and hence assume that local turbulent production equals local turbulence dissipation.
At the same time, practical considerations can limit the applicability of a particular method.
Kikkert et al. (2009) assessed the theoretical and practical viability of applying the different methods in the present context and showed that, because the second order structure function assumes homogeneous and isotropic turbulence, bed shear stress estimates based on turbulent kinetic energy dissipation rate could only be determined at times around flow reversal, when bed shear stress values are very low.
For this reason only methods (i) to (iii) are considered further here.
For the momentum balance method, the individual terms of the Reynolds-Averaged Depth-Integrated Navier Stokes equations were evaluated using control volumes within the PIV field of view.
The length of the control volume therefore decreased from approximately 100mm for the location closest to the toe of the beach to 50mm for the location furthest up the slope.
The accuracy of the estimates increased when the height of the control volume was limited to approximately 30mm.
The log-law was applied in the same way as O'Donoghue et al. (2010), fitting to the six velocity measurements immediately above the bed and only accepting profiles with a 0.95 correlation between data and fit.
This criterion eliminated velocity profiles close to the time of flow reversal: flow near the bed changes direction before the flow in the rest of the profile, and in the backwash it takes some time for log layer to re-establish itself.
The Reynolds stress method was applied by extrapolating the lower part of the Reynolds stress profile down to 2mm above the bed (approximately the height of the measurement closest to the bed).
Fig. 18 presents typical bed shear stress time-series estimated from the velocity measurements via the momentum balance, log-law and Reynolds stress methods.
The example results shown are for x=0.772m on the 1.3mm sand-rough and 5.4mm gravel-rough beaches.
The results from the 1.3mm sand-rough beach show a good overall agreement between the momentum balance and the log law method.
The agreement is poorer close to flow reversal, when the momentum balance method is least accurate because all balance terms are very small.
The Reynolds stress method agrees with the other two methods in the uprush, but in the backwash it produces results that lag behind them by up to 1s.
This delay is likely due to the same reason as that causing the delay in near-bed turbulent shear stress relative to the near-bed velocity, since the near-bed velocity is closely related to the bed shear stress.
For the gravel-rough beaches the accuracy of the measurements was not sufficient to obtain useful estimates from the momentum balance method (lower panel of Fig. 18).
The limitation of the control volume size relative to the size of the sediments appears to have negatively affected the results.
Therefore, even though the momentum balance method is better-founded theoretically than the log-law method, in the following we concentrate on bed shear stress results obtained using the log-law method.
Another benefit of the log-law method is that it has been extensively used in the literature, making comparison of our results with previous results more straightforward.
Fig. 19 presents time-series of bed shear stress (obtained using the log-law method) for the five cross-shore measurement locations on the 1.3mm sand-rough (Fig. 19a), the 5.4mm gravel-rough (Fig. 19b) and the 8.4mm gravel-rough (Fig. 19c) beaches.
The time-series shown in Fig. 19 have a similar shape for the three different beaches and five different locations.
The bed shear stress is highest at the start of the uprush, although the maximum bed shear stress has not been captured because of the gap in time between bore arrival and first measurements of the velocities.
During uprush, the logarithm of shear stress decays at nearly constant rate.
After the time of flow reversal (between 5 and 6s), bed shear stress first increases very quickly, followed by a tendency to plateau as the depth-averaged backwash velocity reaches its maximum.
For a given beach, the highest backwash bed shear stress occurs at the most seaward location where backwash velocities are also highest.
There is some indication that the shear stress decreases towards the very end of the backwash as both velocity and depth become low, but the effect is not completely captured because the very small flow depths did not allow the logarithmic profile to be fitted to the measurement data.
The magnitude and time-varying behaviour of the bed shear stress are in good agreement with previous results reported by Barnes et al. (2009), who measured swash bed shear stress directly using a shear plate.
Comparison of the present results with those reported by O'Donoghue et al. (2010) shows consistently lower bed shear stresses in the present experiments, especially in the backwash.
O'Donoghue et al. (2010) attributed the discrepancy between their measures of bed shear stress and those of Barnes et al. (2009) to the experimental difficulties in measuring high velocities combined with low depths in the backwash.
In the present experiments these difficulties have been overcome by using the better PIV set-up and a non-intrusive depth measurement.
Swash models commonly predict bed shear stress using a simple quadratic resistance law (τ=cfρU2/2, where ρ is the density and U a representative velocity) with constant friction factor, cf.
Friction factor time-series for the 1.3mm sand-rough and 8.4mm gravel-rough beaches are presented in Fig. 20, calculated from the measured instantaneous bed shear stresses and depth-averaged velocities.
The general behaviour in cf for the two beaches is very similar.
During uprush the friction factors are approximately constant until close to flow reversal, when they start to increase.
During the early stages of the backwash the friction factors are much lower.
They first increase rapidly, then more gradually, and reach an approximately constant value that is somewhat higher than the near-constant value seen in the uprush.
These results are consistent with Masselink and Hughes (1998) who obtained an average friction factor for swash events in the field of 0.01 and uprush friction factor estimates of 0.005-0.01 and backwash estimates of 0.01-0.03 by Puleo and Holland (2001) inferred from uprush and backwash trajectories.
However, the current results seem to contradict previously-published results from the field (Conley and Griffen, 2004) and from the laboratory (Barnes et al., 2009; Cowen et al., 2003; Cox et al., 2000) that have tended to conclude that uprush friction factors are greater than backwash friction factors.
But we note that previous laboratory results for cf are not based on simultaneous measurements of shear stress and velocity profile, as they are here, while the method used in the field to obtain uprush and backwash velocities only allowed estimates for the friction factor during the time of bore arrival and late in the backwash.
The magnitudes of the friction factors for the two beaches are very similar over the majority of the swash cycle.
This may be somewhat surprising since one would expect higher friction factors for the rougher beach.
(For reference, friction factors calculated assuming steady and uniform flow and evaluated from the measured depths and velocities, with equivalent roughness of 1.3mm and 8.4mm, are consistently higher for the rougher beach by approximately 50%.) The fact that the friction factors are not very different in the present experiments suggests that the high flow unsteadiness and non-uniformity of swash have significant influence on the bed shear stress for a given instantaneous velocity.
The boundary layer development is likely to play a significant role and its effect can be observed in the results during the uprush.
At x=0.072m the boundary layer has developed to only a limited extent, so the friction factors for the two beaches are very close.
Further up the slope, beyond x=2.377m, the friction factors are higher for the coarser 8.4mm beach.
The increase in friction factor as flow reversal is approached is consistent with cf behaviour in uniform, steady flow in that cf is increasing with decreasing Reynolds number and higher relative roughness.
In contrast, cf behaviour during the early stage of the backwash is different from cf behaviour in uniform, steady flow in that cf is initially low, despite the Reynolds number being low and the relative roughness being high.
Somewhat later, cf values for the 8.4mm beach catch up on the values for the 1.3mm beach and then become higher than the 1.3mm beach values, but only for locations higher up the beach.
At the two lowest locations on the beach cf values are surprisingly similar during the backwash.
This means that the boundary layer development is not sufficient to explain cf behaviour throughout the whole swash cycle.
Fig. 21 shows the effect of beach location (hence available length for boundary layer development) on cf, which is presented as a function of the Reynolds number (Re=u¯h¯/ν, where ν is the dynamic viscosity).
cf values corresponding to two locations on the 1.3mm sand-rough beach are shown, one at a low beach position (x=0.072m) and one at a high beach position (x=2.377m).
cf is plotted separately for uprush and backwash and the temporal sequence of the data points is indicated by arrows.
(In the uprush the sequence is from right to left because the Reynolds number is initially highest and decreases thereafter as velocity and depth decrease; in the backwash the sequence is left-right-left because Re first increases as flow accelerates, it reaches a maximum, and then decreases late in the backwash as the flow decelerates and becomes shallower.) There are two distinct periods when the friction factors behave differently from what would be expected for steady uniform flow.
In the late uprush, friction factors at the lower end of the beach are much higher than for same Reynolds number at the upper end of the beach; friction factors are also higher than the corresponding steady uniform flow friction factors (calculated using the Colebrook-White equation with roughness equal to the sediment size).
In the early backwash, friction factors at all locations are much lower than the corresponding steady uniform flow values, and they increase with increasing Reynolds number.
Both effects can be explained by the boundary layer development.
In the late uprush the behaviour typical for smooth wall persists until higher Reynolds numbers at the lower end of the beach, where the boundary layer has little time to develop.
By the time the flow has reached the upper end of the beach the boundary layer has developed and the friction factors are closer to the corresponding steady uniform flow values.
In the early backwash however, the development of the boundary layer occurs in accelerating flow which started from zero velocity.
Low velocities, combined with the developing boundary layer, result in low values of friction factors.
Conclusions
A detailed experimental investigation of the hydrodynamics of large-scale, bore-driven swash on impermeable, rough beach slopes has been conducted.
The experiments were carried out on three 1:10 impermeable, rough slopes with different surface roughness, adding substantially to the existing range of conditions for experiments of this type.
Simultaneous measurements of depth and velocity using LIF and PIV respectively have resulted in better resolution of the hydrodynamics than previous studies.
In particular, the new experiments resolve the backwash shoreline position and the late backwash period when depths are shallow and velocities are high, and they provide measures of bed-normal as well as bed-parallel velocities, enabling determination of the two-component turbulent kinetic energy and the Reynolds stress throughout the swash cycle.
The following are the main conclusions from the analysis of the experimental data.i.
As expected, increased beach roughness reduces the shoreward reach of the swash lens and the maximum run-up.
ii.
Roughness has limited effect on depth-averaged velocities during uprush.
Differences in depth-averaged velocities between beaches of different roughness become significant only at the late stages of backwash, when flow over a rougher slope achieves lower (in absolute value) maximum velocity and starts to decelerate earlier than flow over a smoother slope.
In contrast to previous studies, the late stages of backwash are well captured by the present measurements.
iii.
The shapes of the velocity profiles are similar to those reported by O'Donoghue et al. (2010).
Increasing bed roughness tends to increase the extent of the near-bed, high gradient flow.
iv.
The present results for TKE based on two velocity components echo the TKE results presented by O'Donoghue et al. (2010) based on one velocity component (the streamwise component).
The results show the signature of incoming turbulence, developing bottom boundary layer and rapid TKE dissipation as flow decelerates; dissipation continues during the early stages of backwash, followed by enhanced generation as the bottom boundary layer develops.
v.
Reynolds shear stress profiles are consistent with the velocity profiles.
As expected, the turbulent shear stress has a maximum close to the rough bed, where intense shearing generates turbulence.
As the flow decelerates during uprush, the maximum shear stress is transported higher into the water column.
vi.
Bed shear stresses evaluated using momentum balance and log-law show good overall agreement for the sand-rough beach; bed shear stresses evaluated via the Reynolds stress profiles agree reasonably well with the other methods in the uprush, but show a lag in time compared to the other methods in the backwash; for the gravel-rough beaches, measurement accuracy was insufficient to obtain bed shear stress estimates via momentum balance.
vii.
Estimates of uprush bed shear stress based on log profile fitting to the measured velocities agree well with direct shear plate measurements reported by Barnes et al. (2009), including for the late stages of the backwash.
viii.
Friction factors are of the same order of magnitude cf≈0.01 for all three rough beaches.
Factors other than Reynolds number and relative roughness have significant influence on the bed shear stress for given instantaneous velocity.
Friction factors in the backwash appear somewhat higher than in the uprush, a result that contradicts conclusions from previous studies.
ix.
Fiction factors during the late stage of uprush and the early stage of backwash are affected by boundary layer development.
At the lower end of the beach, friction factors during late uprush are higher than those for the corresponding steady uniform flow values; for all locations on the beach, friction factors during early backwash are lower than the corresponding steady uniform flow values.
x.
The results for bed shear stress and friction factor show that the usual velocity squared parameterization of bed shear stress, with constant or variable friction factor (calculated using a steady uniform flow friction factor formula) cannot give accurate prediction of the bed shear stress.
It seems that location on the beach and whether the flow is accelerating or decelerating have significant effect on the bed shear stress.
The experiments have yielded detailed data that can be used to test and develop advanced numerical models for bore-driven waves on rough slopes.
The complete data are available on request to the authors.
Acknowledgements
The research was funded by the UK's Engineering and Physical Sciences Research Council (EPSRC) through awards EP/E011330/1 and EP/E010407/1 to the University of Aberdeen and the University of Nottingham respectively.



Synthesis and gas permeability of highly elastic poly(dimethylsiloxane)/graphene oxide composite elastomers using telechelic polymers
First, a known amount of telechelic PDMS (for 1 wt % GO composites, 990 mg) was dissolved in THF (6 ml) while stirring to ensure complete dissolution of the polymers. Then, dried GO (10 mg) was suspended in the solution with vigorous stirring, and a homogeneous dispersion of GO in this solution was promoted by sonication using a 400 W probe sonicator (Branson Digital Sonifier 450) with 10% amplitude for 10 min (24 kJ) in an ice bath. It is noteworthy to point out that, although THF is not a good solvent for GOs, amine terminated telechelic PDMS can effectively act as a surfactant to assist in dispersing GOs homogeneously in THF through hydrogen bonding between amine ends and oxygen containing groups on GO surfaces. A good dispersion after sonication was confirmed by examining the residual liquid film in the vial while rolling the vial containing the solution and after pouring the majority of the solution into a PTFE dish for solution casting. In either case, the liquid film in the vial was essentially transparent (although brownish-black from the GO content) for all solution concentrations and no aggregates could be identified with the naked eye. This solution was immediately poured into a Teflon dish and covered in order to slowly evaporate the solvent thereby solution casting a film at room temperature. Solvent was evaporated for at least 2 days and then the sample was vacuum dried at room temperature for an additional day to make a homogenous PDMS/GO uncrosslinked liquid sol. In order to form a crosslinked elastomer, the sol precursor was heated to 160 degC in a vacuum oven for 24 h and cooled slowly to room temperature before use.ZnO nanowire/reduced graphene oxide nanocomposites for significantly enhanced photocatalytic degradation of Rhodamine 6G

ZnO NWs were prepared by a modified carbothermal reduction method in our previous work [23]. The mixture of ZnO powder and graphite (500 mesh) with a weight ratio of 1:0.8 was placed in the middle of the furnace. The furnace was heated to 1150 degC and purged with mixed gas of air (0.1 L/min) and N2 (4.5 L/min) as reactive and carrier gas. After about 5 min, white snowflake-like product was carried out by the carrier gas and collected by a 5000 mL flask at the downstream of the furnace.

GO was prepared by a modified Hummers' method. The procedure for the preparation of ZnO NW/RGO nanocomposites is illustrated in Fig. 1. In a typical experiment, 0.5 g graphite oxide was ultrasonicated in deionized water with 0.1 g PVP to form GO solution. Then 0.5 g ZnO NWs were added into GO solution. After that, the mixture was stirred at room temperature for 2 h, followed by centrifuging and washing with deionized water to remove impurities. Finally, the nanocomposites were desiccated at 50 degC overnight and further thermally reduced at 300 degC.
Synthesis of CuO nanocrystalline and their application as electrode materials for capacitors
0.03 mol Cu(NO3)2*3H2O was dissolved in 750 ml distilled water under stirring (solution A). The mixture of PEG (Mw = 6000) and 0.24 mol NH3*H2O was added dropwise into solution A at 30 degC, resulting in a blue solution. The final mixture was rapidly heated to 80 degC and stirred vigorously. After 10 h, the precipitate was filtered and washed for several times with deionized water and alcohol, and then dried at 60 degC in air overnight. Product was signed as sample A.What is claimed is:
1. A precursor of transition metal oxide represented by the following chemical formula 1:
NiaMnbCo1-(a+b+c+d)ZrcMd[OH(1-x)2-y]A(y/n)[Chemical formula 1]
wherein, M is at least one of W and Nb, A is one or more anions except OH, 0.3≤a≤0.9, 0.05≤b≤0.5, 0<c<0.05, 0<d<0.05, a+b+c+d≤1, 0<x<0.5, 0≤y≤0.05, n is the oxidation number of A,
wherein c:d mole ratio is between 2:1 and 3:2 when the M is W or Nb, and
the A is at least one selected from the group consisting of PO4, CO3, BO3, and F.2. The precursor of transition metal oxide according to claim 1, wherein the A includes PO4and F.3. The precursor of transition metal oxide according to claim 1, wherein the precursor of transition metal oxide has a tap density of between 1.0 and 2.5 g/cc.Inelastic neutron scattering study of binding of para-hydrogen in an ultra-microporous metal-organic framework

Graphical abstract
Abstract
Metal-organic framework (MOF) materials show promise for H2 storage and it is widely predicted by computational modelling that MOFs incorporating ultra-micropores are optimal for H2 binding due to enhanced overlapping potentials.
We report the investigation using inelastic neutron scattering of the interaction of H2 in an ultra-microporous MOF material showing low H2 uptake capacity.
The study has revealed that adsorbed H2 at 5K has a liquid recoil motion along the channel with very little interaction with the MOF host, consistent with the observed low uptake.
The low H2 uptake is not due to incomplete activation or decomposition as the desolvated MOF shows CO2 uptake with a measured pore volume close to that of the single crystal pore volume.
This study represents a unique example of surprisingly low H2 uptake within a MOF material, and complements the wide range of studies on systems showing higher uptake capacities and binding interactions.

Introduction
Hydrogen (H2) is a promising alternative energy carrier not only because it can potentially achieve zero-carbon emission at the point of use, but also because H2 has a high energy density (33.3kWh/kg) compared to hydrocarbons (12.4-13.9kWh/kg) [1].
The major scientific challenge for on-board H2 applications is that of inventing effective and efficient H2 storage materials, and there is an ever-increasing worldwide interest in meeting the United States Department of Energy's (DoE) H2 storage targets of 5.5wt% gravimetric and 40gL-1 volumetric by 2017.
It is important to note that the DoE targets refer to storage within the whole system rather than within the storage medium alone, with a target operating temperature of -40 to 60°C and an operating pressure below 100atm.
Although solid-state H2 storage based on chemisorption and physisorption has been extensively studied over recent years, so far no material is able to meet this DoE target thus presenting a major impediment for the realisation of the "Hydrogen Economy".
Nevertheless, physisorption of molecular H2 based upon the non-dissociative interaction in porous solids is an especially attractive option since it shows fast kinetics and favourable thermodynamics over multiple adsorption and release cycles [2].
Thus, enormous efforts have been focused on developing new porous solid materials for high capacity H2 storage.
Metal-organic framework (MOF) complexes are a sub-class of porous solids which show great promise for gas storage and separation due to their high surface area, low framework density, and tuneable functional pore environment [3].
MOF materials are usually built up from metal ions or clusters bridged by organic linkers to afford 3D extended frameworks with the formation of cavities ranging from microporous to mesoporous region.
Several members within this MOF family have achieved impressively high H2 adsorption capacities (albeit at cryogenic temperatures, typically at 77K) [4] with a record of ∼16wt% total uptake capacity observed in NU-100 [5] and MOF-200 [6].
However, these high uptake capacities drop dramatically with increasing temperature, and thus none is a practical material.
There is thus particular emphasis on optimising the interactions between MOF hosts and adsorbed H2 molecules, and the identification of specific binding interactions and properties of gases within confined space represents an important methodology for the development of better materials that may lead us to systems of practical use.
In situ neutron powder diffraction (NPD) at below 10K has been used previously to determine the locations of D2 within a few best-behaving MOF materials incorporating exposed metal sites [7-12].
It has been found that D2 can bind directly to vacant sites on metal centres, and that the adsorbed D2 molecules have molecular separations comparable to that to D2 in the solid state.
These studies have provided invaluable structural rationale for their observed high gas adsorption capacities.
Research has thus focused understandably on MOFs with high H2 uptake capacities, while materials showing very low H2 uptake and/or incorporate fully coordinated metal centres are often ignored for this study.
Therefore, information on binding interactions within those low-uptake MOF systems is entirely lacking, but can still give important complementary data and potential understanding for the subsequent design and optimisation of hydrogen storage materials.
It is critical to the success of the NPD technique that the MOF complex adsorbs a significant amount of D2 to boost the observed signal.
This technique therefore has disadvantages when studying the binding interaction within MOFs with low uptakes.
Furthermore, static crystallographic studies cannot provide insights into the dynamics of the adsorbed gas molecules.
Thus, it is very challenging to probe experimentally the H2 binding interactions within a porous host system which has very low gas uptake due to the lack of suitable characterisation techniques.
We report herein the application of the in situ inelastic neutron scattering (INS) technique to permit direct observation of the dynamics of the binding interactions between adsorbed H2 molecules and an aluminium-based porous MOF, NOTT-300, exhibiting moderate porosity, narrow pore window and very low uptake of H2.
This neutron spectroscopy study reveals that adsorbed H2 molecules do not interact with the organic ligand within the pore channels, and form very weak interactions with [Al(OH)2O4] moieties via a type of through-spacing interaction (Al-O⋯H2).
Interestingly, the very low H2 adsorption has been successfully characterised as weak binding interactions and, for the first time, we have found that the adsorbed H2 in the pore channel has a liquid type recoil motion at 5K (below its melting point) as a direct result of this weak interaction to the MOF host.
Experimental
Synthesis
Synthesis of [Al2(OH)2(C16O8H6)](H2O)6 (NOTT-300-solvate) and of the desolvated material NOTT-300 was carried out using previously reported methods [13].
Gas adsorption isotherms
H2 sorption isotherm was recorded at 77K (liquid nitrogen) on an IGA-003 system at the University of Nottingham under ultra-high vacuum from a diaphragm and turbo pumping system.
H2 gas used was ultra-pure research grade (99.999%) purchased from BOC.
In a typical gas adsorption experiment, ∼100mg of NOTT-300 was loaded into the IGA, and degassed at 120°C and high vacuum (10-10bar) for 1day to give fully desolvated NOTT-300.
Inelastic neutron scattering
Results and discussion
Crystal structural analysis and gas adsorption
NOTT-300 crystallises in a chiral space group I4122 and has an open structure comprising infinite chains of [AlO4(OH)2] moieties bridged by biphenyl-3,3',5,5'-tetracarboxylate ligands L4- (Fig. 1(a)).
The Al(III) ion in NOTT-300 has an octahedral coordination environment with six oxygen atoms, four of which are from carboxylate groups and two of which are hydroxyl groups, giving an [AlO4(OH)2] moiety.
These aluminium oxide moieties are further linked to each other via the corner-sharing hydroxyl groups μ2-OH.
Al(III)-carboxylate MOFs are usually constructed from the 1D aluminium oxide chains linked by the carboxylate ligands (Fig. 1(c)) [14-19].
Two distinct types of aluminium oxide chains have been reported previously.
The aluminium chain in MIL-120 is composed of [AlO2(OH)4] octahedra linking to each other via a common edge defined by two μ2-(OH) groups [14].
The different positions of the common edge in the two crystallographically distinct Al sites induce a cis-trans connection mode of the octahedral units, and thus zigzag chains are generated.
The aluminium oxide chains in MIL-53 and MIL-118 are composed of [AlO4(OH)2] octahedra linked to each other via vertex-sharing μ2-(OH) groups [16-18].
In both case, the connections of the [AlO4(OH)2] octahedra adopt trans configurations, generating straight, rod-like aluminium building blocks.
Depending on the coordination mode of the carboxylate groups, the aluminium chains in MIL-53 and MIL-118 show small differences in linkage of the octahedral nodes.
However, in NOTT-300 the corner-sharing [AlO4(OH)2] octahedra in the aluminium oxide chains display a cis configuration, and in order to accommodate the hydroxide groups, adjacent [AlO4(OH)2] octahedra are rotated by 90o with respect to each other, thereby generating 41 screw axes.
This type of connection is distinct from the other two examples, and, therefore, represents a new type of aluminium oxide building block (Fig. 1(c)).
The chirality of the NOTT-300 framework therefore arises from the formation of helical chains of [AlO4(OH)2] octahedra induced by the cis-configuration of μ2-OH groups.
This overall connectivity affords a porous extended framework structure with square-shaped 1D channels with hydroxyl groups protruding into them, endowing the pore environment with free hydroxyl groups over four different directions (Fig. 1(b)).
The diameter of the channel window, taking into account the van der Waals radii of the surface atoms, is approximately 6-7Å.
Desolvated NOTT-300 has a pore volume of 0.38ccg-1 and a BET surface area of 1370m2g-1 and so the general porosity of NOTT-300 is moderate within the family of MOF complexes.
The H2 isotherm (Fig. 2) at 77K for NOTT-300 shows exceptionally low adsorption uptakes (26ccg-1 or 0.22wt%), albeit NOTT-300 shows very high uptakes of CO2 (3.30Å) and SO2(4.11Å), both of which have a larger kinetic diameter than that of H2 (2.89Å).
The uptake of H2 increases sharply in the low pressure region and reaches saturation at ∼1bar.
By using the pore volume of NOTT-300 and the liquid density of H2 at its boiling point (20.3K), it is estimated that NOTT-300 can hold a maximum of 2.7wt% H2 (302ccg-1) at saturation.
Surprisingly, the experimental uptake is 10 times lower than this estimation, suggesting that NOTT-300 has unusually weak binding interaction to H2 molecules, even though the pore size of NOTT-300 (6-7Å) is believed to be optimal to afford strong overlapping potential to H2 molecules and thus boost the adsorption uptakes.
This anomalous H2 adsorption behaviour motivated us to further investigate the interactions between adsorbed H2 molecules and NOTT-300 host, and thus to understand its very low uptake.
Inelastic neutron scattering (INS) study
Direct visualisation of the interaction between adsorbed H2 molecules and the NOTT-300 host is crucial to understanding the detailed mechanism of interaction and hence rationalising the unusually low observed uptake capacity.
INS is a powerful neutron spectroscopy technique which has unique advantages in probing H2 binding interactions by exploiting the high neutron scattering cross-section of hydrogen (82.02barns) [20].
As a result, the INS spectrum is ultra-sensitive to the vibrations of hydrogen atoms and the rotations of the hydrogen molecule, with hydrogen being ten times more visible than other elements.
In this study, we successfully used the INS technique to investigate the binding interaction for the NOTT-300/H2 system albeit it has such a low H2 uptake capacity.
The INS spectrum for the bare NOTT-300, collected at ∼5K to minimise the thermal motion of the adsorbed H2 and the framework host, was found to be similar to those for MOFs containing polyphenyl rings [18,21-23], and this experimental spectrum is in good agreement with the INS spectrum obtained from DFT calculation (Fig. 3(a)) [24].
Upon loading with H2 (0.25 H2/Al and 0.50 H2/Al) at 40-50K, the background of the INS spectra increases due to the recoil of the H2 molecules, and a broad hump is observed at low energy transfers (<30meV) confirming uptake of H2 by NOTT-300 (Fig. 3(b)).
The difference plots, calculated by subtraction of the background spectrum (bare MOF material and sample container) from the data collected for each H2 loading, display a broad hump centred at ∼20meV with only one small energy transfer peak at 8.8meV (Figs.
3(c) and (d)).
The rotational transitions of molecular H2 give a molecular proof of the local environment that the H2 molecules experience when adsorbed on a solid surface or strongly hindered site.
The rotational energy levels for a diatomic molecule are given by (1):(1)EJM=J(J+1)Brotwith J and M the angular momentum number and Brot is the rotational constant, that in the case of H2 Brot=7.35meV . There are two nuclear spin isomers of molecular hydrogen, para-hydrogen (p-H2) with spins paired or antiparallel (↑↓) and ortho-hydrogen (o-H2) with the spins unpaired or parallel (↑↑).
Because quantum mechanical restrictions of the symmetry of the wave-function are responsible for the existence of both species, transitions between them are forbidden in optical spectroscopy, but in the case of INS the transitions between p-H2 and o-H2 are allowed because the neutron can exchange spin states with the molecule.
For p-H2 in the solid state, the environment is isotropic and the main rotational transition is J(1←0) that manifests itself as a very sharp peak at 14.7meV (Fig. 3(e)).
Such a peak has also been observed on high loadings of H2 on MgO thin films, indicating that H2 molecules are not interacting with the material surface [25].
In addition, a strong and broad shoulder with some weak overlying features appears at higher energies.
This shoulder peak, centered at ca.
37meV, is smooth except for a sharp curtailment at energies below the rotational transition, the intensity in this shoulder coming from rotational transitions displaced by the translational recoil of the H2 molecule.
While the rotational line at 14.7meV disappears completely and the onset of recoil occurs below the rotational transition for H2 in the liquid state, only the recoil features are apparent for H2 within NOTT-300 (Fig. 3(f)).
Fig. 3(c) clearly shows the spectra of adsorbed H2 to be in a liquid-like state within the pore channel of NOTT-300 and not in the form of a solid on the solid surface.
Even at higher loading where H2 adsorption in NOTT-300 reaches saturation, a very weak and broad peak at 14.7meV observed (Fig. 3(d)).
This peak indicates the presence of a very small amount of bulk H2 populated on the surface of NOTT-300, but the predominant feature is the recoil signal for H2 in the liquid state.
This observation is distinct from previous studies on adsorbed H2 which show binding to open metal sites which induce strong host-guest interactions to H2 molecules [12,26].
Thus, comparison of the INS spectra suggests that adsorbed H2 molecules have very weak interactions to the NOTT-300 host and, therefore, can rotate freely in the channel to give recoil rotational motion.
For H2 adsorption on the surface of a solid material or strongly hindered active site, the degeneracy of a transition at 14.7meV can be lifted as the freedom for the H2 molecule to rotate in all directions is restricted, thus resulting in splitting of the peak.
Depending on the energy of the interaction, the peak can split differently, and the shift and splitting of the peaks can therefore provide important information regarding the different adsorption sites and their geometry [27].
Further information can also be gleaned from the change in the peaks on loading with H2, demonstrating site saturation and/or site interference.
In this study, in addition to the broad hump at ∼20meV, a small peak appeared at low energy transfer, and the centre of mass of the rotational line of H2 is significantly shifted to 8.8meV, indicating the presence of a type of specific NOTT-300-H2 interaction.
To probe the site of the H2-framework interactions, it is important to evaluate the accessible voids of the MOF host.
The Al(III) centre in NOTT-300 is coordinated via six oxygen atoms to form a full octahedral coordination sphere, and can therefore be considered to be unavailable for direct interaction with adsorbed H2 molecules.
The small increase in the intensity of this peak in the difference spectra upon increasing the H2 loading from 0.25 to 0.5 H2/Al suggests that the interacting site reaches saturation quickly, presumably owing to space constraints.
Analysis of the crystal structure of NOTT-300 offers two possible void sites that could interact with adsorbed H2 molecules: the organic benzene rings and the inorganic [Al(OH)2O4] moieties.
The surface area around the benzene rings is sufficiently large to hold one H2/Al.
However, INS studies of carbon materials show that the phenyl ring only forms weak interactions with adsorbed H2 molecules, resulting in a small splitting or shift in the 14.7meV rotational line.
For example, in the INS spectra for H2-loaded activated carbon materials, the splitting of the free rotor is very small, with peaks observed at 12.5 and 15meV [28].
An even smaller splitting was observed in H2-loaded carbon nanotubes, with peaks at 13.5 and 14.5meV [29].
Such a small energy shift implies that the adsorbed H2 molecules are encountering relatively little hindrance for rotation, probably because of the weak van der Waals interactions between hydrogen and carbon.
In addition, no change to the molecular motion of the aromatic hydrogen atom on the phenyl rings of NOTT-300 (at ∼125meV suggested by the DFT calculation) was observed upon H2 loading.
Therefore, the population of the INS peak in this study cannot be attributed to H2 interaction with the benzene rings due to the observation of a significant shift in the rotational line to 8.8meV.
Therefore, this leaves the [Al(OH)2O4] moiety as the likely sites within the channel to interact with the H2 molecules.
It has been found that the interaction between H2 and oxygen atoms can cause a significant shift in the rotational line of H2.
For example, the INS spectra for H2 adsorbed on MgO surface show that the rotational line is shifted to 11meV [25].
Furthermore, INS studies on MOF-5, in which the Zn(II) centres are also fully coordinated by oxygen donors to form a [Zn4O(O2CR)6] building block, show two distinct peaks at 10meV and 12meV, which are attributed to H2 interactions with oxygen atoms from the [Zn4O(O2CR)6] building block and with the benzene ring, respectively [21,30].
Consistent results were also obtained from DFT calculations of H2-loaded MOF-5, where the adsorbed H2 molecules are found to interact most strongly with the [Zn4O(O2CR)6] clusters and least strongly with the benzene rings [31].
The peak at 8.8meV in the spectra for NOTT-300·nH2 (n=0.5,1.0) in this study is entirely consistent with the INS peak (10meV) observed for MOF-5, confirming that the adsorbed H2 molecules in NOTT-300 are interacting with [Al(OH)2O4] moieties (Fig. 4).
Due to the space constraints and the possible repulsive interaction with the active hydroxyl groups μ2-OH around [Al(OH)2O4] moieties, this site is saturated very quickly upon H2 loading consistent with the observed low uptake capacity.
Conclusions
INS studies on the H2-loaded material NOTT-300 have revealed the detailed binding interaction within this system.
The adsorbed H2 molecules in NOTT-300 are found to have recoil motion along the pore channel with freedom to rotate in all directions, reminiscent of the behaviour of liquid H2.
[Al(OH)2O4] moieties within the channel can only provide binding interactions to few H2 molecules and reach saturation quickly consistent with the experimentally observed low H2 uptake for this material.
The unusually low uptake of H2 in this study is related to the quantum effect of H2 which has a very low molecular mass [32] and the very weak interaction between the H2 and the NOTT-300 host is thus not sufficient to overcome this quantum effect, and therefore very little H2 is taken up by this porous material.
We have also confirmed that the ultra-low H2 uptake is not due to incomplete activation or MOF decomposition as the desolvated MOF shows very high CO2 uptake with a measured pore volume close to that of the single crystal pore volume.
Acknowledgements
SY gratefully acknowledges receipt of a Leverhulme Trust Early Career Research Fellowship and a Nottingham Research Fellowship, and MS receipt of an ERC Advanced Grant and EPSRC Programme Grant.
We are especially grateful to the STFC ISIS Neutron Facility for access to the TOSCA Beamline.
We thank the user support group at ISIS (Chris Goodway and Mark Kibble) for the technical help at TOSCA beamline.
Supplementary data
Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.chemphys.2013.11.004.
Supplementary data
Supplementary data 1
This doc contains supplementary information.

1. A transition metal precursor for preparation of a lithium transition metal oxide, in which a ratio of tap density to average particle diameter D50 of the precursor satisfies a condition represented by Equation 1 below:
0<TapdensityAverageparticlediameterD50oftransitionofmetalprecursor<3500(g/cc·cm).(1)2. The transition metal precursor according to claim 1, wherein the transition metal precursor comprises at least two transition metals.3. The transition metal precursor according to claim 2, wherein the at least two transition metals are at least two selected from the group consisting of nickel (Ni), cobalt (Co), manganese (Mn), aluminum (Al), copper (Cu), iron (Fe), magnesium (Mg), boron (B), chromium (Cr), and period 2 transition metals.4. The transition metal precursor according to claim 3, wherein the at least two transition metals comprise two transition metals selected from the group consisting of Ni, Co, and Mn, or all thereof.5. The transition metal precursor according to claim 1, wherein precursor particles constituting the transition metal precursor are transition metal hydroxide particles.6. The transition metal precursor according to claim 5, wherein the transition metal hydroxide particles are a compound represented by Formula 2 below:
M(OH1-x)2(2)
wherein M is at least two selected from Ni, Co, Mn, Al, Cu, Fe, Mg, B, Cr, and period 2 transition metals; and 0≦x≦0.5.7. The transition metal precursor according to claim 6, wherein M comprises two transition metals selected from the group consisting of Ni, Co, and Mn, or all thereof.8. The transition metal precursor according to claim 1, wherein the transition metal precursor has an average particle diameter D50 of 1 to 30 μm.9. A lithium transition metal oxide in which a ratio of average particle diameter D50 of the lithium transition metal oxide to average particle diameter D50 of a transition metal precursor for preparation of the lithium transition metal oxide satisfies the condition represented by Equation 3 below:
0<AverageparticlediameterD50oflithiumtransitionmetaloxideAverageparticlediameterD50oftransitionmetalprecursor<1.2.(3)10. The lithium transition metal oxide according to claim 9, wherein the lithium transition metal oxide comprises at least two transition metals.11. The lithium transition metal oxide according to claim 10, wherein the lithium transition metal oxide is a compound represented by Formula 4 below:
LiaNixMnyCOzMwO2-tAt(4)
wherein 0<a≦1.2, 0≦x≦0.9, 0≦y≦0.9, 0≦z≦0.9, 0≦w≦0.3, 2≦a+x+y+z+w≦2.3, and 0≦t<0.2;
M is at least one metal cation selected from the group consisting of Al, Cu, Fe, Mg, B, Cr, and period 2 transition metals; and
A is at least one monovalent or divalent anion.12. The lithium transition metal oxide according to claim 11, wherein, in Formula 4, x>y and x>z.13. The lithium transition metal oxide according to claim 11, wherein the lithium transition metal oxide comprises at least two transition metals.14. A lithium secondary battery in which a unit cell comprising a positive electrode comprising the lithium transition metal oxide according to claim 9, a negative electrode, and a polymer membrane disposed between the positive electrode and the negative electrode is accommodated in a battery case.15. The lithium secondary battery according to claim 14, wherein the lithium secondary battery is a lithium ion battery.16. The lithium secondary battery according to claim 14, wherein the lithium secondary battery is a lithium ion polymer battery.17. The lithium secondary battery according to claim 14, wherein the lithium secondary battery is a lithium polymer battery.Enhanced visible-light photocatalytic activity of strontium-doped zinc oxide nanoparticles

To begin the synthesis of Sr-doped ZnO-NPs, analytical grade zinc nitrate hexahydrate Zn(NO3)2*6H2O, strontium nitrate Sr(NO3)2, gelatin (type B from bovine skin), and distilled water were used as starting materials. All of the materials used were purchased from Sigma-Aldrich. The precursors were taken in the stoichiometric amounts of Zn1-xSrxO (x=0, 0.02, 0.04, and 0.06) to obtain final products. First, a gelatin solution was prepared by adding gelatin (3.65 g) to distilled water at 60 degC. The metal nitrates were dissolved separately in a minimal amount of distilled water at room temperature, and then these were added to the gelatin solution. After that, the compound solutions were stirred and heated at 80 degC until gels were obtained. The gels were calcined at 550 degC for 5 h, at a heating rate of 2 degC/min. The resulting powders were characterized using several tools to check their quality.
1. A precursor of transition metal oxide represented by the following chemical formula 1:
NiaMnbCo1−(a+b+c+d)ZrcMd[OH(1-x)2-y]A(y/n)[Chemical formula 1]
wherein, M is at least one of W and Nb, A is one or more anions except OH, 0.3≦a≦0.9, 0.05≦b≦0.5, 0<c<0.05, 0<d<0.05, a+b+c+d≦1, 0<x<0.5, 0≦y≦0.05, n is the oxidation number of A.2. The precursor of transition metal oxide according to claim 1, wherein the c:d mole ratio is between 2:1 and 3:2 when the M is W or Nb.3. The precursor of transition metal oxide according to claim 1, wherein the A is at least one selected from the group consisting of PO4, CO3, BO3, and F.4. The precursor of transition metal oxide according to claim 1, wherein the A includes PO4and F.5. The precursor of transition metal oxide according to claim 1, wherein the precursor of transition metal oxide has a tap density of between 1.0 and 2.5 g/cc.6. A composite of lithium and transition metal oxide comprising a result obtained from calcination of the precursor of transition metal oxide of claim 1 and a lithium compound.7. The composite of lithium and transition metal oxide according to claim 6, wherein the lithium compound is at least one of lithium hydroxide, lithium carbonate, and lithium oxide.8. The composite of lithium and transition metal oxide according to claim 6, wherein the lithium compound is between 0.95 and 1.2 moles per 1 mole of the precursor of transition metal oxide.9. The composite of lithium and transition metal oxide according to claim 6, wherein the calcination is performed at between 600 and 1000° C.10. A positive electrode comprising the composite of lithium and transition metal oxide of claim 6.11. A secondary battery comprising the positive electrode of claim 10.High catalytic activity of magnetic CuFe2O4/graphene oxide composite for the degradation of organic dyes under visible light irradiation
CuFe2O4/GO (20 wt%) was synthesized by a hydrothermal method. In a typical synthesis, 60 mg of GO was dispersed in 40 mL deionized water with sonication for 1 h. The mixture of 0.200 g Cu(CH3COO)2*H2O, 0.811 g Fe(NO3)3*9H2O and 20 ml deionized water was then added to GO aqueous solution dropwise and stirred for 1 h. The pH value of this suspension was adjusted to 10.0 using 6 M NaOH solution. After stirring for 30 min, the suspension solution was transferred into a 100 mL Teflon-lined stainless steel autoclave, sealed and intained at 220 degC for 18 h. The final products were centrifugated and dried in a vacuum oven at 60 degC.Thermoelectric properties and nonstoichiometry of GaGeTe

Polycrystalline samples Ga1+xGe1-xTe (x=-0.03/0.07) and GaGeTe1-y (y=-0.02/0.02) were synthesized from elements Ga, Ge and Te of 5 N purity. The mixture of elements was heated in evacuated quartz ampoules at 1220 K for 75 min and quenched in air. The ampoules were further annealed at 770 K for 3 day to reach equilibrium. Products were powdered for one minute in a vibrating mill under hexane and identified by X-ray diffraction (XRD). Bulk samples, rectangular with dimensions of 15x3.5x2 mm3 (for all measurements except thermal conductivity) and round ones of diameter d=12 mm and thickness 2 mm (for thermal conductivity) were prepared from the powder using a high-pressure/high-temperature (820 K/85 MPa) technique in a graphite die. The density of the samples (calculated from geometry and weight) was always higher than 95% of its theoretical value.
Performance and durability of carbon black-supported Pd catalyst covered with silica layers in membrane-electrode assemblies of proton exchange membrane fuel cells
Pd/CB was supplied from Tanaka Kikinzoku Kogyo Co., Ltd. The specific surface area of the CB supports was ca. 550 m2 g-1, and the content and the average diameter of Pd particles was 29 wt.% and ca. 5 nm, respectively. Pd/CB was covered with silica layers by the successive hydrolysis of 3-aminopropyltriethoxysilane (APTES) and tetraethoxysilane (TEOS) [16] and [17]. 550 mg of Pd/CB was ultrasonically dispersed in 220 mL of pure water at 60 degC, and the pH of this solution was adjusted to 10.5 by the addition of triethylamine. The hydrolysis of APTES was performed at 60 degC over 30 min by the addition of 290 mg of APTES (Tokyo Chemical Industry Co., Ltd.) to the solutions. Subsequently, 1150 mg of TEOS (Kanto Chemical Co., Inc.) was added to the solutions to cover the samples with thick silica layers. After the solutions were stirred for 3 h at 60 degC, they were filtered and washed several times with pure water. The samples thus obtained were heat-treated at 350 degC under H2/N2 (H2: 5 vol.%) atmosphere for 2 h. Silica-coated Pd/CB is denoted SiO2/Pd/CB hereafter. The morphology of the silica layers were observed by transmission electron microscopy (TEM, HD-2700, Hitachi High-Technologies Corp.) For comparison, Pt/CB (Pt loading: 46 wt.%, TEC10E50E) was also supplied from Tanaka Kikinzoku Kogyo Co., Ltd.Ethanol steam reforming over NiLaZr and NiCuLaZr mixed metal oxide catalysts


Nickel nitrate Ni(NO3)2*6H2O, copper nitrate (Cu(NO3)2*2.5H2O, lanthanum nitrate (La(NO3)3*5H2O, 70% (w/w) zirconium propoxide Zr(OCH2CH2CH3)4 in 1-propanol, and oxalic acid (HO2CCO2H) were purchased from Sigma-Aldrich. Ethanol was purchased from Biopack. All chemicals were reagent-grade and were used as received.

NiLaZr and NiCuLaZr catalysts were prepared by co-precipitation following the addition of oxalic acid to an ethanolic solution of Cu(NO3)2, Ni(NO3)2, La(NO3)3 and Zr(OCH2CH2CH3)4. The precipitates were washed with ethanol, dried at 80 degC, and calcined for 2 h at different temperatures: 700 degC, 850 degC, 900 degC and 950 degC. The resulting NiLaZr catalysts contained 5% of Ni, and the NiCuLaZr catalysts contained 4% of Ni and 1% of Cu, on a weight basis. Where required, the final calcination temperature (degC) is indicated in the catalyst denotation.

1. A method of preparing a positive electrode active material precursor for a lithium secondary battery comprising:adding a reaction solution including a first transition metal-containing solution, a second transition metal-containing solution, an ammonium ion-containing solution, and a basic aqueous solution to a batch reactor, wherein the adding of the reaction solution is done while continuously discharging a portion of the reaction solution in the batch reactor to outside of the batch reactor when the batch reactor is full,wherein an initial input flow rate of the reaction solution added to the batch reactor satisfies following Equation 1, anda pH in the batch reactor satisfies following Equation 2:1.5×V/t≤υ1+υ2+υ3≤10×V/t[Equation 1]wherein, in Equation 1,V is a volume of the batch reactor, t is total reaction time (minutes), υ1is a total initial input flow rate (mL/min) of the first transition metal-containing solution and the second transition metal-containing solution, υ2is an initial input flow rate (mL/min) of the ammonium ion-containing solution, and υ3is an initial input flow rate (mL/min) of the basic aqueous solution, andpH0−{([Ni]0−[Ni]t1)×0.05}≤pHt1≤pH0−{([Ni]0−[Ni]t1)×0.005}  [Equation 2]wherein, in Equation 2,pHt1is a pH in the batch reactor at time t1, pH0is an initial pH in the batch reactor, [Ni]0is a molar concentration of nickel (Ni) in the transition metal-containing solution initially added, and [Ni]t1is a molar concentration of Ni in the transition metal-containing solution added at time t1.
adding a reaction solution including a first transition metal-containing solution, a second transition metal-containing solution, an ammonium ion-containing solution, and a basic aqueous solution to a batch reactor, wherein the adding of the reaction solution is done while continuously discharging a portion of the reaction solution in the batch reactor to outside of the batch reactor when the batch reactor is full,
wherein an initial input flow rate of the reaction solution added to the batch reactor satisfies following Equation 1, and
a pH in the batch reactor satisfies following Equation 2:1.5×V/t≤υ1+υ2+υ3≤10×V/t[Equation 1]
1.5×V/t≤υ1+υ2+υ3≤10×V/t[Equation 1]
wherein, in Equation 1,
V is a volume of the batch reactor, t is total reaction time (minutes), υ1is a total initial input flow rate (mL/min) of the first transition metal-containing solution and the second transition metal-containing solution, υ2is an initial input flow rate (mL/min) of the ammonium ion-containing solution, and υ3is an initial input flow rate (mL/min) of the basic aqueous solution, andpH0−{([Ni]0−[Ni]t1)×0.05}≤pHt1≤pH0−{([Ni]0−[Ni]t1)×0.005}  [Equation 2]
pH0−{([Ni]0−[Ni]t1)×0.05}≤pHt1≤pH0−{([Ni]0−[Ni]t1)×0.005}  [Equation 2]
wherein, in Equation 2,
pHt1is a pH in the batch reactor at time t1, pH0is an initial pH in the batch reactor, [Ni]0is a molar concentration of nickel (Ni) in the transition metal-containing solution initially added, and [Ni]t1is a molar concentration of Ni in the transition metal-containing solution added at time t1.2. The method ofclaim 1, wherein the first transition metal-containing solution and the second transition metal-containing solution each independently comprise a cation of at least one transition metal of nickel, manganese, or cobalt, wherein concentrations of the cation included in the first transition metal-containing solution and the second transition metal-containing solution are different.3. The method ofclaim 2, wherein the first transition metal-containing solution comprises 50 mol % to 98 mol % of nickel, 1 mol % to 40 mol % of manganese, and 1 mol % to 40 mol % of cobalt.4. The method ofclaim 2, wherein the second transition metal-containing solution comprises 20 mol % to 80 mol % of nickel, 1 mol % to 60 mol % of manganese, and 1 mol % to 60 mol % of cobalt.5. The method ofclaim 1, wherein the first transition metal-containing solution and the second transition metal-containing solution are added to the batch reactor after being mixed using a static mixer.6. The method ofclaim 1, wherein the ammonium ion-containing solution comprises NH4OH, (NH4)2SO4, NH4NO3, NH4Cl, CH3COONH4, or NH4CO3.7. The method ofclaim 1, wherein the basic aqueous solution comprises NaOH, KOH, or Ca(OH)2.8. The method ofclaim 1, wherein the pH in the batch reactor is controlled by an input flow rate of the basic aqueous solution.9. The method ofclaim 8, wherein the input flow rate of the basic aqueous solution satisfies following Equation 3:υ3,0×{1−(0.02×([Ni]0−[Ni]t2))}≤υ3,t2<υ3,0[Equation 3]wherein, in Formula 3,υ3,t2is an input flow rate of the basic aqueous solution at time t2, υ3,0is an initial input flow rate of the basic aqueous solution, [Ni]0is a molar concentration of Ni in the transition metal-containing solution initially added, and [Ni]t2is a molar concentration of Ni in the transition metal-containing solution added at time t2.
υ3,0×{1−(0.02×([Ni]0−[Ni]t2))}≤υ3,t2<υ3,0[Equation 3]
wherein, in Formula 3,
υ3,t2is an input flow rate of the basic aqueous solution at time t2, υ3,0is an initial input flow rate of the basic aqueous solution, [Ni]0is a molar concentration of Ni in the transition metal-containing solution initially added, and [Ni]t2is a molar concentration of Ni in the transition metal-containing solution added at time t2.10. The method ofclaim 1, wherein the discharge of the reaction solution is performed by using a tube including a filter.11. A method of preparing a positive electrode active material for a lithium secondary battery, comprising:mixing a lithium-containing raw material with a positive electrode active material precursor prepared by the method ofclaim 1to form a mixed resultant, andsintering the mixed resultant.
mixing a lithium-containing raw material with a positive electrode active material precursor prepared by the method ofclaim 1to form a mixed resultant, and
sintering the mixed resultant.12. A positive electrode for a lithium secondary battery, comprising:a positive electrode active material prepared by the method ofclaim 11.
a positive electrode active material prepared by the method ofclaim 11.13. A lithium secondary battery comprising the positive electrode ofclaim 12.Synthesis, acid properties and catalysis by niobium oxide nanostructured materials
Dichloromethane and acetonitrile were purified with a LC Technology Solutions Inc. SPBT-1 Bench-Top Solvent Purification System. Compound 1 was synthesized according to literature procedures.9 NbCl5 and Nb(OEt)5 used in the preparation of T-I and T-II were purchased from Sigma-Aldrich and used as received. Commercial Nb2O5 (Puratronic(r), 99.9985%) was purchased from Alfa Aesar and used as received. Pluronic (P123) and dodecylamine 98% were used as structure-directing agents and were purchased from Sigma-Aldrich. Milli-Q water (resistivity 18.2 MΩ cm-1 at 25 degC; 0.22 μm filter) was used in all preparations and reactions requiring aqueous conditions.
Synthesis of the Nb2O5 materials discussed within this contribution have been adapted from previously reported procedures. The methods used are briefly described in the following paragraphs:
5 g of NbCl5 (18.5 mmol) were dissolved in 99% ethanol (10 mL) under constant stirring. To this solution NH4OH (1 M) was slowly added to the aforementioned solution until a white precipitate was observed. The precipitate was then separated from the solution by centrifugation (3000 rpm), washed four times using Milli-Q water and dried at 120 degC for 24 hours.
5 g of NbCl5 (18.5 mmol) were dissolved in 200 mL Milli-Q water. Immediately after addition, the yellow powder became white. After stirring for 3 h at room temperature, the white precipitate was centrifuged (3000 rpm) and washed four times with Milli-Q water until the filtrate was at neutral pH. The obtained solid was dried at 120 degC for 24 hours.
1.5 g of P-123 were dispersed in 11 mL of Milli-Q water and 45 mL of HCl (2 M) at 40 degC. 5 g of Nb(OEt)5 (15.7 mmol) was added to solution under stirring. The resulting mixture was kept at 40 degC for 24 hours and then precipitated in a Teflon-lined autoclave at 100 degC for 48 hours. The obtained solid was filtered, washed with Milli-Q water and dried in air at room temperature. Excess surfactant was removed by Soxhlet extraction first by washing with 1% HCl in 95% EtOH (300 mL) and then with 95% EtOH only (120 mL). The final, template-free mesoporous Nb2O5*nH2O was dried in the oven at 120 degC for 18 hours.
A mixture of Nb(OEt)5 (3.25 g, 10.3 mmol) and dodecylamine (0.57 g, 3.1 mmol) was heated until a homogeneous solution was obtained. Subsequent addition of 65.4 mL of Milli-Q water with manual stirring resulted in the formation of a gel-like precipitate. Further addition of 27.6 μL (0.3 mmol) concentrated HCl afforded a white precipitate. The reaction mixture was allowed to stand overnight, sealed and heated at 40 degC for 30 hours, 60-65 degC for 66 hours, 80 degC for 48 hours, and 95-100 degC for 5 days. The solid was then cooled down, collected by filtration and dried at 95-100 degC for 2 hours, 120 degC for 48 hours, and 140 degC for 48 hours. The white solid was then washed four times to remove the presence of excess surfactant. Each washing cycle was conducted for 24 hours under vigorous stirring followed by Buchner filtration. 645 mg (3.4 mmol) of p-toluenesulfonic acid in 13 mL of ether were added to 100 mL of MeOH for the first wash. For the second wash, 65 mg (0.3 mmol) of p-toluenesulfonic acid in 3 mL of ether were added to 100 mL of MeOH. Third and fourth washings were done using MeOH (100 mL) only. The final, template-free mesoporous Nb2O5*nH2O was dried into the oven at 120 degC for 24 hours.Impact of temperature on CO2 storage at the Ketzin site based on fluid flow simulations and seismic data

Highlights
•
Seismic modeling demonstrates significant effects of CO2 on 4D seismic data (Ketzin).
•
CO2 saturation is insensitive to the reservoir temperature in Ketzin.
•
CO2 density is considerably lower for the higher reservoir temperature in Ketzin.
•
Qualitative impact of reservoir temperature is low on 4D seismic data from Ketzin.
•
Quantitative impact of temperature is considerable on 4D seismic data from Ketzin.
Abstract
Temperature is one of the main parameters influencing the properties of CO2 during storage in saline aquifers since it along with pressure and co-constituents controls the phase behavior of the CO2/brine mixture.
When the CO2 replaces brine as a free gas it is well known to affect the elastic properties of porous media considerably.
In order to track the migration of geologically stored CO2 at the Ketzin site, 3D time-lapse seismic data were acquired by means of a baseline (pre-injection) survey in autumn 2005 and a first monitor survey in autumn 2009.
During this period the temperature in the storage reservoir near the injection well was observed to have increased from 34°C to 38°C.
This temperature increase led us to investigate the potential impact of temperature on the seismic response to the CO2 injection and on the CO2 mass estimations based on the Ketzin 4D seismic data.
Two temperature scenarios in the reservoir (34°C and 38°C) were studied using multiphase fluid flow modeling.
The simulations show that the impact of temperature on the seismic response is minor, but that the impact of the temperature on the CO2 mass estimations is significant and can, with the help of the multiphase fluid flow simulations, be explained mostly by the impact on the density of the CO2.

Introduction
It is well known that temperature is a major parameter influencing CO2 storage and migration in saline aquifers along with pressure and co-constituents (e.g. Kumar et al., 2005; Bachu and Bennion, 2009).
For example, the trapping of CO2 at irreducible saturation is a direct function of temperature, as well as in situ pressure.
However, only few experimental data are reported in the temperature and pressure range of interest for geological CO2 storage (Kumar et al., 2005).
At the Ketzin pilot site, Germany, CO2 is being injected at about 640m depth with temperature data being continuously acquired with a permanently installed system (Giese et al., 2009).
These temperature measurements are performed over the entire length of the Ketzin boreholes using a distributed temperature sensing system (Henninges et al., 2011).
Ketzin is the first European onshore pilot scale project for CO2 storage in a saline aquifer and was initiated in 2004 with the aim to evaluate and develop methods for CO2 storage monitoring (Würdemann et al., 2010; Martens et al., 2012).
The site is situated on the southern flank of an anticlinal structure, which has its crest approximately 1.5km to the north of the site (Fig. 1).
The anticline is the eastern part of the Roskow-Ketzin double anticline (Norden et al., 2010) and hosts the sandstones of the Triassic Stuttgart Formation that serve as the storage reservoir (Fig. 2).
The Stuttgart Formation is lithologically heterogeneous, consisting not only of sandstones with good reservoir properties, but also alternating with muddy rocks of poor reservoir quality.
The sandstones vary in thickness between 15 and 30m (Förster et al., 2006) and are present in the depth range of 620-650m beneath the injection site.
The Stuttgart Formation is sealed by an approximately 200m thick cap rock section of playa-type mudstones of the Weser and Arnstadt Formations (Förster et al., 2006) (Fig. 2).
Monitoring at the Ketzin site is performed by means of surface-based and borehole-based methods.
The latter use the injection well (Ktzi201/2007) and additionally two observation wells (Ktzi200/2007 and Ktzi202/2007), which are located at distances of 50m and 112m from the injection well, respectively (Fig. 2).
The wells were drilled in 2007 to depths of approximately 800m (Prevedel et al., 2008) after baseline characterization that included a 3D surface seismic survey (Juhlin et al., 2007).
CO2 injection started in June 2008 and more than 60 kilotons of CO2 had been injected by November 2012.
Nearly all of the CO2 injected to date at Ketzin has been food grade quality, except for about 1500 tons of CO2 (purity 99.7%) from the oxyfuel pilot plant Schwarze Pumpe (Vattenfall) that were injected within a trial period from May 4, 2011 to June 13, 2011 (Martens et al., 2012).
Therefore, the phase behavior of the CO2 is not expected to change due to the effects of co-constituents, e.g.
SO2.
However, in cases where the level of co-constituents is high changes in the phase behavior of the CO2 can be expected (Kummerow and Spangenberg, 2011).
Numerical modeling of multiphase flow is an essential tool to ensure the viability of long-term and safe CO2 storage in geological formations (Kumar et al., 2005; Bryant et al., 2008).
Thus, a number of reservoir simulations have been performed to enhance the understanding of the CO2 migration at the Ketzin site (Kempka et al., 2010; Lengler et al., 2010; Bergmann et al., 2010).
Successful integration of reservoir simulations and 4D seismic data analysis at the Sleipner CO2 storage site (e.g. Arts et al., 2004; Chadwick et al., 2010) motivated us to integrate both methods also at the Ketzin site.
The 3D baseline seismic survey at the Ketzin site was acquired in autumn 2005 (Fig. 1) and revealed a sequence of clear reflections from approximately 150ms to 900ms two-way traveltime (TWT) in the stacked volume (Juhlin et al., 2007).
In the autumn of 2009, a subset of this baseline survey was acquired around the injection well after approximately 22-25 kilotons of CO2 had been injected (Ivanova et al., 2012) (Fig. 1).
This first 3D seismic repeat survey showed a pronounced time-lapse amplitude anomaly at the top of the storage reservoir (Ivandic et al., 2012; Ivanova et al., 2012).
The extent of this anomaly was approximately 250m in the S-N direction and 350m in the W-E direction.
This anomaly, as well as delayed arrival times of reflections below the reservoir ("velocity push-down effect": Arts et al., 2004; Chadwick et al., 2010), demonstrated that CO2 injected at the Ketzin site could be monitored by means of surface-based seismic methods.
As a follow up, the time-lapse seismic images were used to make estimates on the imaged amount of CO2, important for assessing storage efficiency and monitoring potential leakage.
The minimum degree of accuracy is a crucial issue in these investigations.
Such minimum thresholds establish the smallest amount of CO2 that is possible to be monitored by means of surface-based methods (JafarGandomi and Curtis, 2011).
At Ketzin, quantification of the mass of the injected CO2 was performed using the time-lapse seismic data, petrophysical investigations on core samples, and in situ CO2 saturations from pulsed neutron gamma (PNG) logging as input (Ivanova et al., 2012).
The uncertainty range in the order of 5-7% encompasses the true injected mass CO2.
In order to investigate the impact of the reservoir temperature variation on the interpretation of the 4D seismic data at Ketzin, we deduce quantitative CO2 mass estimates for both temperature limits in this study.
In the first step we apply seismic forward modeling and fluid substitution using the so far established petrophysical models for the Ketzin reservoir sandstone (Kummerow and Spangenberg, 2011; Ivanova et al., 2012).
Subsequently, CO2 mass estimations based on reservoir isothermal simulations for both temperature scenarios are compared to those obtained by in situ PNG logging (Ivanova et al., 2012).
Seismic modeling
Kazemeini et al. (2010) investigated the surface seismic response at the Ketzin site to various levels of CO2 saturation in the reservoir and established petrophysical models based on Gassmann's equations (1951) for homogeneous and patchy CO2 distributions.
The seismic response for different CO2 distribution geometries and saturation levels were modeled using 1D elastic and 2D acoustic finite difference methods.
In this contribution, forward modeling of the synthetic seismic response has been carried out using the reflectivity method (Wang, 1999) while incorporating results from petrophysical experiments (Kummerow and Spangenberg, 2011; Ivanova et al., 2012) and PNG logging (Henninges et al., 2011; Ivanova et al., 2012).
By using the reflectivity method, we assume that the geology around the wellbores can be approximated as 1D media (Fuchs and Mueller, 1971; Margrave and Manning, 2004).
Input to the modeling consisted of the compressional wave velocity (Vp), shear wave velocity (Vs), and density (ρ) near the injection well obtained from borehole logging data (Fig. 4).
Vp and ρ were vertically averaged from the logs to remove high frequency fluctuations.
Vs values from the well Ktzi202/2007 were vertically averaged over the main lithological units (Förster et al., 2010).
The resulting Vs model was linearly interpolated to the injection well using the interpreted lithological horizons after Kling (2011).
The input wavelet was extracted from the 3D seismic baseline data (Juhlin et al., 2007) (Fig. 4), yielding a dominant frequency of 40Hz.
Seismic modeling with the reflectivity method using the previously described Vp, Vs and density models as input parameters resulted in a synthetic trace corresponding to a 3D surface seismic baseline trace near the injection well (Fig. 4).
In order to study the impact of a variable degree of CO2 saturation in a reservoir layer of constant thickness and the impact of a variable reservoir thickness at constant CO2 saturation near the injection well we applied fluid substitution models for Vp, Vs and ρ and repeated the seismic modeling for each case.
Results from petrophysical experiments on two core samples from the target reservoir (Kummerow and Spangenberg, 2011; Ivanova et al., 2012) showed a near linear relationship between Vp and the CO2 saturation (Fig. 5).
A least-squares fit of the data yields the following equation:(1)ΔVpVp=-0.46×S(CO2)where ΔVp/Vp is the relative change of Vp and S(CO2) is the corresponding level of CO2 saturation, respectively.
This relationship was then used to scale the baseline Vp model in the reservoir layer.
At Ketzin, the average velocity from ultra-sonic laboratory experiments with 100% formation brine saturation (three measurements on two samples) is 3135m/s, while on the logging data the average P-wave velocity is 3012m/s in the three wells at the Ketzin site in the reservoir sands.
This velocity is close to what is observed on crosshole seismic data between the two observation wells.
Although velocity dispersion is probably present in the Ketzin reservoir rocks, we do not consider it to be large enough to considerably affect interpretation of our time-lapse seismic data (Ivanova et al., 2012).
The petrophysical experiments by Kummerow and Spangenberg (2011) showed that Vs is almost unaffected by CO2/brine fluid substitution.
Therefore, we used an unchanged baseline Vs model (Fig. 4) throughout the modeling.
The density of the composite fluid (the brine and the CO2) was calculated by the following equation:(2)ρfluid=ρbrine(1-SCO2×ρCO2)where ρfluid is the density of the composite fluid, ρbrine is the formation brine density - 1164.59kg/m3 (after Kummerow and Spangenberg, 2011), SCO2 is CO2 saturation and ρCO2 is the CO2 density at the injection well at the time of the 3D seismic repeat acquisition - 266.62kg/m3 after Ivanova et al. (2012).
We used the porosities (ø) from borehole logging (Förster et al., 2010) for the calculation of bulk densities in the brine/CO2 saturated reservoir units using:(3)ρsat=ϕ×ρfluid+(1-ϕ)×ρmatrixwhere ρmatrix is the density of the matrix which can be calculated using the density of the rock fully saturated with brine ρsatbrine by the equation:(4)ρmatrix=ρsatbrine-ϕ×ρbrine1-ϕ
Based on these equations, the modeled decrease in density due to CO2 injection in the reservoir near the injection well in the autumn of 2009 was approximately 4% (at 50% CO2 saturation).
The impact of a variable degree of CO2 saturation in a reservoir layer of constant thickness and the impact of a variable reservoir thickness at constant CO2 saturation are illustrated with Fig. 6a and b, respectively.
The latter case considers a CO2 saturation level of 50%, which is the average value of measured CO2 saturation at the injection well during the acquisition of the 3D seismic repeat survey (Ivanova et al., 2012).
According to Eq.
(1), this yields a relative change in Vp of about -25%.
Considering that the average baseline Vp is 3142m/s, the average time-lapse Vp used for the CO2 response modeling is then 2372m/s.
Both the amplitude changes and "push-down effects" (up to 6ms) can be observed in the synthetic data for the case of increasing the thickness of the CO2 layer (Fig. 6b).
The amplitude change for variable reservoir thickness further shows a "tuning effect" over the range of thicknesses tested.
The time window used for this amplitude calculation is 515-540ms.
Maximum tuning occurs at approximately 20m (Fig. 6b).
Arrival time delays of wave coda from below the reservoir reach values of up to 6ms for both cases (Fig. 6a and b).
The push-down velocity time delay is nearly linear with increasing CO2 saturation in Fig. 6a and with increasing layer thickness in Fig. 6b.
Multiphase fluid flow simulations
Next we apply the multi-phase fluid flow simulations of Lengler et al. (2010) to account for the lateral variability in the petrophysical properties of the storage formation at Ketzin.
Lengler et al. (2010) performed simulations on multiple realizations of the Ketzin reservoir using a stochastic Monte Carlo approach in order to take into account the high degree of uncertainty in the reservoir characterization at the scale required for fluid flow simulations.
In this paper, we use a similar approach, but this time to investigate the impact of the reservoir temperature on the fluid migration and, in turn, on the 4D seismic data.
Hydrogeological studies at the Ketzin site (Norden et al., 2010) have shown that a 2D radially symmetric model of the upper part (33m) of the Stuttgart Formation can be used to interpret the 3D data acquired near the injection well (Fig. 7).
This model accounts for the presence of channel sandstones in the reservoir that are the most favorable for CO2 migration and contains effective porosities in the range of 20-25% (Förster et al., 2010).
As known from core and log analysis of the injection well Ktzi201/2007 and the first observation well Ktzi200/2007, the reservoir is composed of two high porosity sandstone layers.
These layers are separated by a thin strongly cemented sandstone layer (Norden et al., 2010).
Since the thickness of this layer is in the decimeter range, it cannot be detected with the seismic wavelengths typically available from surface-seismic measurements.
However, this layer is known to be a significant constraint to fluid migration due to its low permeability (Wiese et al., 2010).
This cemented sandstone layer is absent in the second observation well Ktzi202/2007, where only one sandstone layer is observed, which is situated at a distance of only 112m away from the injection well.
This observation shows the high degree of heterogeneity in the reservoir.
However, pumping tests have demonstrated that the three wells are hydraulically connected (Wiese et al., 2010).
Therefore, the conceptual model presented here assumes a simple connection between the sandstone intervals in the three wells.
Initial reservoir conditions and rock properties within the reservoir sandstone and the surrounding mudstone are listed in Table 1.
They are assumed to be spatially constant for the flow simulations.
The model boundary was set 10km away from the injection well to ensure that the simulated flow would not be affected by the boundary conditions.
The generated mesh was regularly spaced at 0.3m in the vertical direction.
In the study area, the lateral discretization was 5.0m, but with closer spacing within a radius of 7.5m around the injection well.
To simulate the vertical injection well, the vertical permeability of the well elements was set 104 times higher than that of the reservoir sandstone, and injection was simulated at 4 elements of the well column.
This approach resulted in all elements of the well column being charged.
The injection rate was modeled at a constant rate of 2tons/h (the average rate until 1 October 2009), and 3tons/h (the average rate until 29 October 2009 during the time of the 1st repeat seismic survey).
The simulations were performed using the numerical program TOUGH2 version 2.0 (Pruess et al., 1999) with the fluid property module ECO2N, which was designed for application to the geologic sequestration of CO2 in saline aquifers (Pruess, 2005).
Two cases were considered, one where the reservoir temperature is 34°C and the other where the temperature is 38°C.
Fig. 7 displays distributions of CO2 saturation and CO2 density in the reservoir for both the 34°C and 38°C scenarios.
The CO2 saturation does not differ significantly between the two scenarios (less than 5%), whereas the CO2 density is notably lower for the higher temperature case.
However, the difference in CO2 density decreases with decreasing pressure (Fig. 3) and, therefore, with distance from the injection well (Table 2).
In the vicinity of the injection well, the difference in CO2 density is up to 20% and on average 12%.
The slight increase in CO2 saturation with the isothermal temperature of 38°C follows from the small increase in pressure that, in turn, is due to a lower CO2 density.
The same mass of CO2 has to be injected and, therefore, a larger CO2 volume is present in the higher temperature case, as evident in the greater lateral extent and volume of the CO2 plume (Table 2).
With an increasing volume of the CO2 plume, the contact area between the CO2 and the formation water increases.
Temperature effects on the seismic data
In order to investigate the impact of temperature in the reservoir on the 4D seismic data at the Ketzin site, the CO2 saturation and CO2 density, as well as the thickness of the CO2 layer obtained by multiphase fluid flow simulations were used as input to seismic modeling (see Section 2 of this paper).
Porosity was assumed to be constant in the reservoir (20%) for modeling the temperature effects on the seismic data.
It is well known that the seismic velocity in sandstones saturated with brine does not depend on temperatures in the range of 34-38°C (e.g. Mavko, 2005).
To demonstrate this we calculated the difference in Vp between both temperature scenarios present at the Ketzin site at the time of the 1st 3D seismic repeat campaign (Ivanova et al., 2012) using Gassmann's equations (1951) for 50% CO2 saturation.
The resulting difference between the scenarios in Vp is less than 5m/s.
Considering the uncertainties (e.g. ±5% error in CO2 saturation) in the petrophysical experiments (Kummerow and Spangenberg, 2011; Ivanova et al., 2012) corresponding to ±70m/s in Vp (Fig. 5), we did not take into account the Vp changes (Fig. 5, Eq.
(1)) due to the different temperature scenarios (34 and 38°C).
The resulting synthetic seismic differences (Fig. 8) of both the 34°C and 38°C options look very similar and also show some similarity to the real data, also shown in Fig. 8.
The synthetic difference (repeat-base) seismograms from near the top of the reservoir agree reasonably well with the real difference seismograms (repeat-base) for the Ktzi201/2007 and Ktzi200/2007 wells reported by Ivanova et al. (2012).
However, obvious disagreements are found at the Ktzi202/2007 well, which may be due to the fact that the velocity model used at this location is not sufficiently correct.
Seismic amplitude differences between the 38°C and 34°C scenarios correspond to less than 1% of the amplitude values of the baseline (Fig. 8).
Since the normalized root mean square (NRMS) differences in the 3D time-lapse data are greater than 10% (Kashubin et al., 2011) these temperature effects in the reservoir will not be resolvable with surface seismic methods at the Ketzin site.
Although it is not possible to determine the reservoir temperature from the seismic data, the temperature does have a significant impact when estimating the volume of CO2 injected based on the above modeling.
We show this here by applying the method of volumetric estimation of Ivanova et al. (2012) to both the 34°C and 38°C reservoir temperature scenarios.
We calculate minimum and maximum limits for the two temperature cases based on the bounds suggested by the total amount of injected CO2 for the period in which the repeat survey was active over the injection area (1 October 2009 to 28 October 2009) in this study.
In Ivanova et al. (2012) results of two PNG logging runs before and after the 1st seismic 3D repeat campaign were averaged for minimum and maximum scenarios.
Fig. 9 shows the resulting CO2 mass distribution maps for both temperature cases and reveals that they look similar to that from Ivanova et al. (2012).
The minimum total mass (∼25.6 kilotons) and the maximum total mass (∼29.3 kilotons) for the 34°C option are considerably higher than the amount of injected CO2 at the time of the repeat survey in 2009 (21.1-24.2 kilotons).
However, for the 38°C option, the minimum mass (∼22.3 kilotons) and maximum mass (∼22.8 kilotons) are completely within the bounds of the amount of injected CO2 (21.1-24.2 kilotons) and match well with the CO2 mass estimation from Ivanova et al. (2012) (∼20.5-23 kilotons).
These calculations confirm that the impact of reservoir temperature is considerable when trying to quantify the amount of CO2 in the subsurface and that it needs to be accurately estimated.
Based on these calculations it appears that a significant portion of the reservoir containing CO2 was at 38°C at the time of the repeat survey in 2009.
Discussion
Seismic modeling and observations show that the effects of the injected CO2 on the 4D seismic data from Ketzin are significant, both regarding seismic amplitudes and time delays.
However, reservoir heterogeneity and seismic resolution, as well as random and coherent seismic noise are negative factors to be considered in seismic monitoring.
It is likely that the simulated scenarios of 38°C and 34°C are representative in the vicinity of the injection well and in the remaining reservoir, respectively, in October 2009.
This is based on a measured temperature of approximately 38°C at the injection well at the monitoring time, while at the well Ktzi200/2007 the temperature increased to only slightly above 34°C and at the well Ktzi202/2007 the temperature remained at 34°C during the injection period.
Since most of the CO2 is concentrated around the injection well, the higher temperature value plays an important role in estimating the mass of CO2 from the seismic data.
The integration of seismic modeling and multiphase fluid flow simulations generated synthetic difference seismograms (repeat-base) that demonstrate the main features of the real difference seismograms (repeat-base) (Ivanova et al., 2012) (Fig. 8).
Taking into account the assumptions made constructing the model we consider the correlation between the synthetic and real seismic sections to be satisfactory.
However, the following points should be considered when evaluating the modeling results.
Firstly, the constant 20% reservoir porosity (Förster et al., 2009) used for modeling of the temperature effects is probably an over-simplification since the reservoir is quite heterogeneous (Förster et al., 2006).
Secondly, sound waves may have a frequency dependent propagation velocity (e.g. White, 1975; Müller et al., 2010) so that the higher the frequency the higher the speed.
Although velocity dispersion is probably present in the Ketzin reservoir rocks, we do not consider it to be large enough that it could considerably affect the qualitative and quantitative interpretation of our time-lapse seismic data (Ivanova et al., 2012).
Thirdly, Eq.
(1) was derived using results of petrophysical experiments at 40°C.
For brine saturated sandstone, the seismic velocity does not depend on temperatures in the range of 34-40°C (Mavko, 2005), resulting in that there are only minor differences for Vp at conditions near the critical point (Han et al., 2010).
Via the Gassmann equations (1951) this translates into a maximum difference in Vp of 9m/s for 50% CO2 saturation for the Ketzin site comparing the 40°C and 34°C scenarios.
Therefore, the effect of temperature on the velocity CO2 at the Ketzin site can be disregarded.
The volumetric estimations of the CO2 mass based on the Ketzin 4D seismic data (Fig. 9) shows that the impact of temperature is significant for the calculations due to its impact on CO2 density.
Hence, temperature monitoring is an important component for quantitative seismic interpretations at Ketzin, and probably at other sites.
Using the temperature measured at the injection well for the mass estimations gives the best result for the CO2 mass quantification (Fig. 9).
It is completely within the bounds of the known injected CO2 mass at the beginning and end of 3D seismic repeat acquisition campaign and in very good agreement with the CO2 mass estimation based on in situ CO2 saturation PNG logging (Fig. 9).
Nevertheless, the quantitative analysis contains considerable uncertainties as discussed above and in Ivanova et al. (2012).
Future issues to be considered include expanding the temperature range (34-38°C in this study) to be investigated and the resulting effects on the seismic response, as well as the role of the reservoir heterogeneity.
In addition, it would be important to investigate the impact of temperature on CO2 storage at other sites with favorable P-T conditions in the reservoir (Fig. 3).
A similar approach applied to studying the impact of pressure in the reservoir would be also important for CO2 monitoring using 3D time-lapse seismic methods.
Conclusions
By integrating seismic modeling and multiphase fluid flow simulations, we have estimated the impact of the reservoir temperature on the 4D seismic data from Ketzin.
We studied two cases, one where the injection was performed at 34°C and the other at 38°C.
Results from the multiphase fluid flow simulations show that the difference between the two cases is small for the CO2 migration.
Likewise, the temperature does not affect significantly the seismic amplitude response, although the CO2 density is considerably lower for the higher temperature case.
The difference in CO2 density between 34°C and 38°C decreases with decreasing pressure and, therefore, with increasing distance from the injection well.
Therefore, the modeled time-lapse seismic differences for the two temperature scenarios is minor regarding the qualitative analysis of the 4D seismic data from the Ketzin CO2 storage site (Fig. 8).
However, the volumetric estimation of the CO2 based on the 4D seismic data from Ketzin using results from the multiphase fluid flow simulations (Fig. 9) shows that the impact of temperature in the reservoir at the monitoring time is significant for these estimations.
This is mostly due to its impact on CO2 density, which strongly depends on temperature.
In addition to temperature effects, the simulated CO2 saturation levels also influence volumetric estimation.
Our results show that temperature monitoring is very important for quantitative seismic interpretation at the Ketzin site.
Using the higher temperature scenario, corresponding to that measured at the injection well, gives the best result for the CO2 mass quantification.
The estimate is completely within the bounds of the true amount of injected CO2.
Acknowledgements
The authors cordially thank Manfred Stiller, Julia Götz, Michael Widenbeck and Magdalena Gil for helpful comments on the paper.
Alexei Petrunin is acknowledged for further discussions.
We thank anonymous reviewers for their constructive criticism.
We would like to acknowledge the European Union (project CO2SINK, no.
502599) for funding the 3D seismic baseline survey in Ketzin in 2005 and the German Federal Ministry of Education and Research, BMBF (project 3DRep1, AZ 03G0679A (Geotechnologien program), and project CO2MAN, Grant 03G0770A), for funding the 3D repeat survey in 2009 and current studies.
We would like to thank the following industrial partners of CO2MAN project: VNG, Vattenfall, RWE, Statoil, Dillinger Hüttenwerke, Saarstahl and OMV.
This is the Geotechnologien paper number GEOTECH2097.

Hydrothermal synthesis of variety low dimensional WS2 nanostructures

Synthesis of samples: All chemical reagents were of analytic purity and applied directly without further purification. First, sodium tungstate (Na2WO4*2H2O) (0.005 mol), hydroxylamine hydrochloride (NH2OH*HCl) (0.01 mol), sulfourea (CH4N2S) (0.02 mol) were dissolved in 30 ml deionized water. The surfactants were then added into the solution under constant stirring. The pH value of the mixture was adjusted to 6 by dropping 2 mol/L hydrochloric acid (HCl) or ammonia water (NH3*H2O). The final solution was transferred into a 50 ml Teflon-lined stainless steel autoclave, which was sealed and treated at 180 degC for 24 h. The products obtained were black in color. After calcination, the as-prepared samples were washed several times with distilled water and absolute ethanol, and dried in air at 60 degC for 10 h. To better describe the samples, the precipitates using no additive was named sample I, while powders obtained with the addition of CTAB (0.18 g, 0.24 g) and PEG (0.18 g ) were named as sample II, III and IV, respectively, as shown in Table 1.

Sb2MoO6, Bi2MoO6, Sb2WO6, and Bi2WO6 flake-like crystals: Generalized hydrothermal synthesis and the applications of Bi2WO6 and Bi2MoO6 as red phosphors doped with Eu3+ ions
All chemicals are of analytical grade and used as received without further purification. In this work, for the sake of obtaining a series of Sb2MoO6, Bi2MoO6, Sb2WO6, and Bi2WO6 samples, all experiments were carried out in a 50 mL Teflon-lined stainless steel autoclave at 180 degC for 12 h by simply adjusting the initial molar ratios of SbCl3-to-NaOH (or BiCl3-to-NaOH) while keeping the initial molar ratio of SbCl3-to-Na2MoO4 (BiCl3-to-Na2MoO4, SbCl3-to-Na2WO4, or BiCl3-to-Na2WO4) as 2:1.

SbCl3 (2 mmol) and Na2MoO4*2H2O (1 mmol) were in turn added into 20 mL distilled H2O under magnetic stirring, resulting in dark green precipitates. After being stirred for 20 min, aqueous solution (20 mL) containing 4 mmol NaOH was gradually poured into the above suspension. Next, the above suspension was transferred into a 50 mL Teflon-lined stainless steel autoclave, which was then sealed and kept at 180 degC in an electrical oven. After 12 h, the resulting dark green product was filtered off, washed with distilled water and absolute ethanol for several times, and then dried under vacuum at 60 degC for 6 h.


The nano-Li4Ti5O12/CNTs composite was prepared by a liquid phase deposition method. The predetermined amount of CNT in nano-Li4Ti5O12/CNTs composite is 10 wt%. Typically, 0.3 g purified CNTs were dispersed in 24 mL ethanol with sonication for 1 h and then 10.2 g tetrabutyl titanate was added under vigorous magnetic stirring. 1.6632 g lithium acetate was dissolved into 26 mL of ethanol-water mixture (12:1 in volume) and slowly dropped into the above suspension. After 24 h continuous stirring, the mixture was aged at 60 degC over 60 h to remove solvents gradually and ground in a mortar. The precursor was calcined at 900 degC under N2 for 1 h with a heating rate of 5 degC min-1 to obtain the nano-Li4Ti5O12/CNTs composite.

CLAIMS
1. A cobalt oxide precursor powder for use in preparing a positive electrode active material, wherein the precursor composition comprises particles having a formula Coi-<sub>y</sub>A<sub>y</sub>Ox, wherein l<x<4/3, 0£y<0.05, A comprising at least one element from the group consisting of Ni, Mn, Al, Mg, Ti, and Zr, wherein the particles composition has a Fd-3m crystal structure and wherein the particles have a D50 > 15pm and a compressive strength of at least lOOMPa and at most 170MPa.2. A cathode active material obtained by a process comprising the steps of:
- providing the precursor powder according to claim 1,
- providing a Li-comprising precursor powder and mixing said Li-comprising precursor powder with said precursor powder according to claim 1 to obtain a mixture having molar ratio of Li to (Co + A) > 1.00, and
- sintering said mixture in an oxygen comprising atmosphere during a period of 5-20 hours at a temperature of at least 900°C and at most 1200°C, thereby obtaining said lithium cobalt oxide based active material powder, said cathode active material having a DQ1 of at least 192 mAh/g.3. The cathode material of claim 2, having a pressed density equal to or greater than 3.7 g/cm3.4. The cathode active material of claim 2, wherein the material has a remaining capacity (QR%) after fifty cycles of at least 40%, preferably of at least 50%.5. A method for making the cobalt oxide precursor powder according to claim 1, comprising the steps of:
- heating a composition comprising cobalt salt particles at a temperature of at least 300°C to no more than 450°C in a dry inert atmosphere for 2-5 hours to form a composition comprising intermediate cobalt precursor particles, and
- heating the composition comprising intermediate precursor particles at a
temperature of at least 300°C to no more than 600°C in a dry oxidizing atmosphere for 2-5 hours to form the powder comprising cobalt oxide precursor particles.Development of a pearl millet Striga-resistant genepool: Response to five cycles of recurrent selection under Striga-infested field conditions in West Africa

Highlights
•
A Striga-resistant genepool was developed in West African cultivated pearl millet.
•
We examine the response of the diversified genepool to five cycles of selection.
•
Selection significantly increased panicle yield, Striga and downy mildew resistance.
•
The observed genetic variation and heritabilities enable further selection gains.
•
The derived varieties will contribute to integrated Striga control in West Africa.
Abstract
Striga hermonthica (Del.) Benth. is a persistent threat to pearl millet [Cenchrus americanus (L.) Morrone, comb.
nov.] production, especially in West Africa.
This study aimed at evaluating the response of a diversified pearl millet genepool to five cycles of recurrent selection targeting Striga resistance and panicle yield, and to a lesser extent downy mildew [Sclerospora graminicola (Sacc.) J.
Schroet.] resistance.
Two-hundred full-sib families (FS) representing the C5 selection cycle were evaluated together with the genepool parental landraces, experimental varieties derived from previous cycles and local checks in Striga-infested fields at Sadoré (Niger) and Cinzana (Mali).
Substantial and mostly significant selection progress could be documented.
The accumulated percentage gain from selection amounted to 51%/1% lower Striga infestation (measured by area under Striga number progress curve, ASNPC), 46%/62% lower downy mildew incidence, and 49%/31% higher panicle yield of the C5-FS compared to the mean of the genepool parents at Sadoré/Cinzana, respectively.
Experimental varieties selected from previous cycles also revealed lower ASNPC and mostly higher yield compared to genepool parents at their selection sites.
Significant genetic variation among the C5-FS and operative heritabilities of 76% (Cinzana), 84% (Sadoré) and 34% (combined across locations) for ASNPC will enable continued selection gain for Striga resistance.
High genotype×environment interaction variances for all target traits suggest that different experimental varieties need to be extracted from the genepool for different sites.
The genepool-derived varieties will be further validated on-farm and are expected to contribute to integrated Striga control in pearl millet in West Africa.

Introduction
Pearl millet [Cenchrus americanus (L.) Morrone, comb.
nov.] (Chemisquy et al., 2010) is the staple food and fodder crop of millions of poor rural families in Africa and India (Hash et al., 2000; Senthilvel et al., 2008).
In the arid and semi-arid regions where this cereal is cultivated, pearl millet plays an important role in food security.
However, grain yields are generally low (circa 350-600kgha-1).
Grain yield is limited by abiotic stresses like drought and low soil fertility, and biotic stresses such as downy mildew disease and the parasitic weed Striga.
Striga hermonthica (Del.) Benth. is the most widespread and destructive witchweed affecting cereals.
In the Striga-prone regions of sub-Saharan Western and Central Africa, S. hermonthica remains the major biotic and persistent threat to pearl millet production and productivity (Wilson et al., 2004).
About 40% of the cereal producing areas is severely infested with Striga in sub-Saharan Africa (Gurney et al., 2006).
Grain yield losses up to 100% have been reported in susceptible cereal cultivars under high infestation levels, particularly under drought conditions (Gurney et al., 2006; Ejeta, 2007; Amusan et al., 2008).
Several options have been recommended for Striga control: hand weeding, herbicides, rotations of cash or trap crops with cereals, improved soil fertility, intercropping and biological control (Haussmann et al., 2000).
However, the diversity of the farming systems and of the parasite have rendered the use of a single control method ineffective, leading to the development of integrated Striga control packages that combine several control measures (e.g., Badu-Apraku, 2010).
Breeding cultivars that can withstand parasite infection is considered to be central to integrated approaches to Striga control in pearl millet.
Employing host resistance to S. hermonthica is simple and economical for subsistence farmers to adopt (Ejeta, 2007; Menkir and Kling, 2007; Wilson et al., 2000).
Inheritance of traits associated with resistance to Striga spp.
has been reported for sorghum, maize and rice (Haussmann et al., 2001a, 2004; Amusan et al., 2008; Gurney et al., 2006).
The low stimulation of S. asiatica seed germination has been reported to be under the control of a single recessive gene in the sorghum cultivars Framida, 555, SRN 6496, and SRN 39 (Hess and Ejeta, 1992; Ezeaku and Gupta, 2004).
One major gene and several minor genes are involved in stimulation of S. hermonthica seed germination in sorghum (Haussmann et al., 2001a), and the major gene has recently been mapped (Satish et al., 2011).
Field resistance to Striga in maize is considered a quantitatively inherited trait (Kim, 1994; Gethi and Smith, 2004; Amusan et al., 2008).
The heredity of host-plant resistance to Striga in pearl millet is not well documented.
Few pearl millet cultivars have been reported as partially resistant or tolerant to Striga, and resistance (or at least a lower level of susceptibility) in certain pearl millet materials was shown to be dominant (Ramaiah, 1987).
However, the very existence of Striga resistance in cultivated pearl millet has been questioned by others (Chisi and Esele, 1997).
Lack of precise and validated information about Striga resistance in pearl millet may be partially due to the fact that the pearl millet/S. hermonthica pathosystem is particularly complex.
Both host and parasite are highly out-crossing species, which results in each plant in a pearl millet landrace, improved open-pollinated variety or genepool population having a different genotype and therefore carrying potentially different alleles for Striga resistance or susceptibility.
Similarly, each S. hermonthica plant in a natural population carries potentially different alleles for virulence.
Partial quantitative resistance to S. hermonthica was reported in wild pearl millet relatives from Africa (Wilson et al., 2000, 2004).
However, from a breeder's point of view, it appears preferable to work within a dynamic and diversified genepool of cultivated pearl millets using recurrent selection methods as wild millet relatives carry many undesirable alleles that need to be eliminated, thereby slowing down selection progress.
Downy mildew [caused by the oomycete Sclerospora graminicola (Sacc.) J.
Schroet.] is a pseudo-fungal disease of pearl millet, causing up to 60% crop loss (Singh, 1990; Thakur et al., 2011) and needs to be considered in pearl millet improvement.
Both Striga and downy mildew pathogens are highly variable, and different ecotypes can be present at different locations (Haussmann et al., 2001b; Thakur et al., 2006; Yoshida et al., 2010), underlining the need to perform selection at contrasting locations to exploit genotype×location interactions and/or identify stable varieties.
According to our knowledge, there is no Striga-resistant pearl millet variety released in West Africa.
To date, increased resistance to Striga through recurrent selection has not been reported for pearl millet.
Since 2006, the International Crops Research Institute for the Semi-Arid Tropics (ICRISAT) in Niger, in partnership with the national agricultural research program in Mali (Institut d'Economie Rurale, IER), has conducted recurrent selection in a diversified pearl millet genepool to increase frequencies of desirable alleles for Striga and downy mildew resistance and panicle yield using two test sites in West Africa (Sadoré in Niger and Cinzana in Mali).
In total, this genepool was subjected to five cycles of recurrent selection, and provides a unique opportunity to examine the effectiveness of recurrent selection for improving Striga and downy mildew resistance, and yield performance in cultivated pearl millet.
The objectives of this study were (i) to describe the genepool development and selection process, (ii) to examine the response to five cycles of recurrent selection for panicle yield and resistance to S. hermonthica and downy mildew, (iii) to estimate quantitative-genetic parameters and predict possible future selection gains from the genepool's C5-Full-sib population.
Materials and methods
Striga-resistant genepool development
The initial step for developing the pearl millet Striga-resistant genepool consisted of field evaluation of 64 pearl millet landraces that had been collected in Striga-infested fields jointly by IRD (the French Research Institute for Development) and ICRISAT in 2003 (Bezançon et al., 2009).
The 64 landraces were evaluated in a four-replicate field trial under artificial Striga infestation at the ICRISAT-Sadoré research station (13°15′ N, 2°18′ E) in the 2005 rainy season.
Six landraces were identified as less Striga-sensitive and relatively higher yielding (M141, M239, M029, M197, M017 and KBH).
During the 2005/2006 off-season, both full-sib (FS) progenies (intra- and inter-population) and S1 (self-pollinated) progenies were created from these six selected genepool parents (Table 1).
These progenies were subjected to the first cycle of evaluation in the 2006 rainy season in a four-replicate field trial at ICRISAT-Sadoré.
Separate, artificially infested fields (S1 and S2, sown within two days interval) were used for evaluating FS and S1 progenies in this and the following years.
Evaluation of Striga-resistant genepool progenies at Cinzana, Mali (13°17′ N, 5°56′ W) was started later in the 2008 rainy season in a naturally infested farmer's field near the station.
This field had been carefully selected for strong and uniform Striga infestation in the previous year.
An overview of the diversified Striga-resistant genepool development process is presented in Fig. 1.
In the first and subsequent cycles, selection was based on the following criteria: above-average Striga resistance [i.e.
below-average area under Striga number progress curve (ASNPC) (Haussmann et al., 2000) or Striga emergence supported by the progeny], above-average panicle yield, and below-average downy mildew incidence (DM) (number of infested hills recorded during plant tillering and updated after flowering, divided by the total number of hills after emergence, multiplied by 100).
In later cycles also an overall agronomic suitability score was taken into account, and only entries with above-average scores were selected.
The percentage of selected entries ranged from 13 to 54%, and depended on the number of entries tested, the number of superior entries identified for the desirable trait combinations, and the desire to have at least 50 progenies contributing to the next generation, in order to avoid inbreeding depression in subsequent generations of the genepool.
In the following selection cycles, S1 and FS selection were alternated, with new FS progenies being created from selected S1 progenies and both new FS progenies and S1 progenies being created from the selected FS progenies.
During each recombination step, crossing plans were established to assure that each selected progeny was crossed to several other selected progenies and also that each selected progeny contributed about the same number of progeny to the next generation.
This was done to maximize the effective population size in the following generations of the genepool, and thereby to avoid inbreeding depression.
The genepool was kept open, so that potentially interesting landraces (reported by farmers to be Striga resistant) were included as test entries during the evaluation cycles (all four-replicate field trials), and if above-average Striga resistance and acceptable yield performance was proven, the landrace was included in the FS and S1 creation for the next selection cycle.
A total of five new germplasm were included at two recombination cycles (in C1 and C2, Fig. 1).
Based on selections made from 2008 and 2009 rainy season data, the first experimental varieties were extracted from the genepool (after the 3rd and 4th selection cycle).
For the experimental varieties derived in 2008, 15 best progenies at Sadoré with long panicles (EV08-Sad-LP), and 15 best progenies at Sadoré with shorter panicles (EV08-Sad-SP) were recombined during the 2008/2009 dry season.
For the experimental varieties derived in 2009, the 15 best progenies from each site, Sadoré (EV09-Sad) and Cinzana (EV09-Cin), and from a combined analysis across two sites (EV09-Comb) were recombined.
With the recombination of 15 best progenies, a higher selection intensity was applied for the experimental varieties compared to the one applied to the genepool.
Only one improved population/genepool resulted from each selection cycle.
The reasons for using S1 recurrent selection in combination with FS recurrent selection were the following: selection among S1 progenies gives higher chances to select for desirable recessive resistance alleles and has theoretically the advantage of larger genetic variance among the tested entries.
The variable numbers of S1 and FS progenies included in each selection cycle (Table 1) were due to variable success in FS/S1 creation in the off-season nurseries - due to irrigation problems and concomitant heat stress, grasshopper damage or poor seed setting.
A summary of each selection cycle including evaluated units, recombination units, heritability and trial mean is given in Table 1.
Evaluation of selection progress after five selection cycles
Genetic materials
The test of selection progress after five selection cycles was integrated into the recurrent selection progeny evaluation process.
A set of 200 C5-FS progenies were produced in the 2010/2011 dry season by recombining 56 FS progenies and 16 S1 progenies selected from the C4 evaluation cycle, and 3 experimental varieties (EV09-Sad, EV09-Cin and EV09-Comb).
Remnant seeds of each selected progeny were sown in a one-row plot of seven hills and crosses were performed according to a defined crossing plan to assure that each progeny was crossed to at least three different selected progenies and that each progeny contributed the same number of progenies (about three) to the next generation.
For the present study, the 200 C5 derived FS families together with 20 controls [including genepool parental varieties, 5 experimental varieties (see above) and a set of 8 agronomically elite open-pollinated varieties (OPVs)] were evaluated for Striga resistance during the 2011 rainy season.
These elite varieties have acceptable levels of resistance to downy mildew and wide adaptability but are generally susceptible to Striga.
Field experiment procedures
The experiment was conducted at two sites: in Niger under artificial Striga infestation at ICRISAT-Sadoré (13°15′ N, 2°18′ E) and in Mali under naturally highly Striga-infested field conditions near IER-Cinzana (13°17′ N, 5°56′ W).
These environments are contrasting in terms of ecology (Sahelian vs Sudanian), and soil type.
Randomized incomplete block designs with four replications were used in each test environment.
Each full-sib family was grown in a single-row plot of 3m length with inter-row spacing of 1.60m of distance to provide more light for Striga emergence, enough space for Striga counting and to reduce border effects (Haussmann et al., 2000).
Within plot-rows, distance between hills was 0.50m, providing seven hills per row.
Initially about 15 seeds were sown per hill.
The hills were then thinned to two plants per hill three weeks after emergence.
Fertilization of the trials was performed during soil preparation when 50kgha-1 of 15-15-15 NPK fertilizer was applied.
Weeds other than Striga were eliminated by hand on a regular basis to keep the field clean for Striga observations.
The Striga seeds used for artificial infestation at ICRISAT-Sadoré were collected from pearl millet field trials there and farmers' fields near Sadoré at the end of previous seasons and had been stored for at least one year to break seed dormancy.
The Striga seeds were mixed with finely sieved sand that served as carrier and ensured uniform infestation (Kim, 1991).
About 50,000 viable Striga seeds were applied per m2 to ensure a sufficiently high Striga pressure (Haussmann et al., 2000).
Measured variables
Data were collected at both sites on a plot basis for traits related to pearl millet phenological and agronomical characters, downy mildew (DM) incidence, and Striga resistance.
The flowering time (FLO) in days after sowing (das) was recorded when 50% of the plants had exerted stigmas.
During harvest, the number of productive panicles was counted and weighed to determine the panicle yield (HYD) (gm-2).
The number of hills per plot with downy mildew was recorded at tillering, verified/adjusted after flowering and used for calculating the downy mildew (DM) incidence (in percentage): number of infested hills, divided by the total number of hills after emergence, multiplied by 100.
In addition, the number of emerged Striga plants for each plot was recorded at 67, 101 and 121 das.
Successive Striga counts were then used to calculate the "Area under Striga Number Progress Curve" (ASNPC) (Haussmann et al., 2000):ASNPC=∑i=0n-1Yi+Y(i+1)2(t(i+1)-ti)where n is the number of Striga assessment dates, Yi the Striga count at the ith assessment date, ti the days after sowing at the ith assessment date, t0 is 0, and Y0 is 0.
Low ASNPC means values indicate resistance, and high values susceptibility to Striga.
Statistical analysis
Analysis of variance (ANOVA) was performed for all measured traits following the incomplete block design.
Data from the two locations (Sadoré and Cinzana) were analyzed separately and then the adjusted means of each environment were used for combined analysis.
The statistical software PLABSTAT v3A (Utz, 2005) was used for data analysis.
The discrete downy mildew data were subjected to a log-transformation to meet ANOVA assumptions.
The binomial distribution with the Logit function was used for analyzing the transformed downy mildew data.
The generalized linear model (GLM) command implemented in the restricted maximum likelihood (REML) package from the GenStat 14th Ed. software (Payne et al., 2011) were used for the analysis.
Back-transformed means were estimated.
All sources of variation, entries, replications and incomplete blocks within replications, were considered as random effects.
For further analysis, the C5 full-sib families (C5-Full-sib), genepool parent (Parent), agronomic controls (Control) and experimental varieties (EVs) were separated into different groups.
t-tests were performed for paired group means comparisons by dividing the means difference with their standard error of difference.
The selection differential (S) is the difference between the mean of selected units and the overall mean of the base population.
Estimates of broad-sense heritability (h2) on a plot basis in a replicated trial were calculated for all traits following the formula described by Haussmann et al. (2000).
Predicted gain was calculated by multiplying heritability by the selection differential (Falconer, 1981).
The percentage gain per cycle (G%) was calculated following the formula:G%=(X¯n-X¯0)/X¯0×100n,where G% is the average percentage gain per cycle, X¯n is the mean at latest selection cycle, X¯0 is the genepool parents mean, and n is the number of selection cycles.
Results
Assessment of the genepool development: S1 vs FS selection
Entry means, genetic differentiation and correlations among traits
Mean performance of the 200 C5-FS for panicle yield (HYD) amounted to 151gm-2 at Sadoré, which was higher than the mean panicle yield recorded at Cinzana (130gm-2) and across the two sites (141gm-2) (Table 2).
Mean values for the area under Striga number progress curve (ASNPC) were lower at Cinzana (246.9) than at Sadoré (1068.1) (Table 2).
Highly significant differences (P<0.01) were detected among the genotypes for flowering time (FLO), panicle yield (HYD), ASNPC and downy mildew incidence (DM) from the single-site analyses of variance (Table 2).
Excellent plot-based heritabilities of 84 and 76% were estimated for ASNPC at Sadoré and Cinzana, respectively.
Results from the combined analyses of variance showed highly significant differences (P<0.01) among genotypes (G), environments (E) (data not shown) and genotype-by-environment (G×E) interactions for all the measured traits.
G×E interaction variances (VG×E) were larger than genetic variances (VG) for the Striga and downy mildew resistance-related traits.
In contrast, the genetic variance component of pearl millet flowering time was higher and the one for panicle yield similar to the respective G×E variance component (Table 2).
For the present trial, estimates of broad-sense heritability (h2) from the combined analysis were high for flowering time (h2=71%) and panicle yield (h2=50%).
Moderate heritability values (34% and 38%) were observed for Striga and downy mildew resistance traits (Table 2), due to the larger G×E variance components for these traits.
High panicle yield was significantly but only weakly associated with resistance to Striga (coefficient of correlation r=-0.38** and -0.15*) and downy mildew (r=-0.34** and -0.22**) for Sadoré and Cinzana, respectively.
Combined across the two sites, high panicle yield was also weakly and significantly associated with later flowering.
Neither Striga resistance nor downy mildew resistance were related to flowering time.
Test of selection progress
Under Sadoré conditions, the mean of the full-sib families (FS) created from the C5 selection cycle was significantly lower (P<0.01) than the mean of the genepool parents (Parent) for both Striga resistance (ASNPC: -51%) and downy mildew incidence (DM: -46%) (Table 3).
In other words, the advanced C5-FS population presents significantly better resistance to Striga and downy mildew than the genepool parental varieties from which it was derived.
Panicle yield was significantly higher (+49%) in the FS progenies compared to the genepool parental population.
Similarly, significant selection gains were observed at Cinzana for panicle yield (+31%) and downy mildew incidence (-62%), but not for Striga resistance, where the C5-FS mean for area under Striga number progress curve was only 1% lower than the Parent group, and the observed difference was statistically non-significant.
At both sites, no significant difference was detected for flowering time between FS and Parent groups (Table 3), indicating that the selection for the other traits did not result in flowering time changes in the genepool.
The average realized percentage gain from selection for Striga resistance (ASNPC) was -10%cycle-1 at Sadoré and -0.3%cycle-1 at Cinzana, downy mildew susceptibility decreased by 9%cycle-1 at Sadoré and 21%cycle-1 at Cinzana.
The selection gain for panicle yield was equal at both Sadoré and Cinzana, where 10%cycle-1 was quantified (Table 3).
Under Sadoré conditions, most experimental varieties (EVs) derived from the genepool in 2008 (3rd selection cycle) or 2009 (4th selection cycle) present better resistance to Striga and downy mildew than the genepool parental population, except EV09-Cin (for ASNPC), EV08-Sad-SP and EV09-Comb (for DM) (Table 3).
All EVs were higher yielding compared to the genepool parents, except for EV08-Sad-SP.
At Cinzana, the derived varieties EV08-Sad-SP, EV09-Cin and EV09-Comb showed lower mean for ASNPC, while all EVs were more resistant to downy mildew and higher yielding compared to the Parent group, except for EV09-Cin (for HYD).
At both sites, only non-significant changes were observed between EVs and genepool parental means for flowering time.
Under both sites, the Control group supported the highest Striga damage and lowest panicle yield, while its performance was better at Sadoré for downy mildew resistance compared to the genepool parental means (Table 3).
Predicted response from future selection
Considering the 200 C5-FS as the base population for future selection, the significant genetic variation and moderate to high heritabilities for the target traits at both sites individually and in the combined analysis should enable further positive response to selection.
Predicted gainscycle-1 for Striga resistance (recorded on the area under Striga number progress curve) were -572, -81 and -122 for Sadoré, Cinzana and across the two sites, respectively, and ranged from -3% to -6% for downy mildew incidence (Table 4).
For both traits, the expected gainscycle-1 at Sadoré were remarkably higher compared to the two other conditions.
Predicted gains cycle-1 for panicle yield were similar between Sadoré and across the two sites (12gm-2cycle-1).
Comparatively smaller but positive genetic gains for resistance to Striga and downy mildew and for increases in panicle yield may therefore be expected under Cinzana conditions (Table 4).
Results from the combined analyses of variance revealed a high amount of G×E interaction variance especially for resistance to Striga and downy mildew, but also for panicle yield and lower heritabilities for ASNPC and DM compared to the single-site analysis (Table 2).
These suggest that selection for specific adaptation to each of the two test sites may be more promising than selection for wide adaptation across these two sites.
This means that different FS families need to be selected and recombined for the two contrasting sites Sadoré and Cinzana to maximize gains from selection and to derive the best varieties for each location.
Discussion
Recurrent selection: an effective tool for enhancing Striga resistance in pearl millet
Recurrent selection in broad-based populations can be used to enhance quantitative resistance to a target pest or disease.
Such quantitative resistance, which is due to many gene loci with smaller effects, is usually more durable than monogenic, qualitative resistance (Berner et al., 1995; Menkir and Kling, 2007).
According to our knowledge, this is the first report describing significant progress from recurrent selection for quantitative Striga resistance in pearl millet.
Simultaneously, Striga resistance, downy mildew resistance and panicle yield were improved while maintaining flowering time which determines general adaptation to the target sites.
The rigorous evaluation under field conditions using four replicates and ASNPC as combined measure for Striga resistance (resulting in lower experimental error and higher broad-sense heritabilities), and the large number of progeny evaluated in each generation seem to be effective for improving the genepool.
Self-pollinated progeny evaluation is expected to lead to better genetic differentiation, but the S1 evaluation partially suffered from poor plant establishment (seedling inbreeding depression), resulting in lower heritabilities in some seasons.
The FS selection, on the other hand, has less probability to identify and move forward desirable recessive alleles compared to S1 selection, but has the advantage of the test entries being in their natural, highly heterozygous state.
Since both selection schemes were combined which saved time (no additional cycle needed for recombination of S1s), a more detailed comparison of selection efficiencies in S1 vs FS selection is not possible in this study.
Based on the present findings, the recurrent selection objective - improving the Striga-resistant genepool mean while maintaining the genetic variability - is largely attained by the fifth selection cycle, especially at Sadoré.
Different recurrent selection methods have been reported by Rattunde and Witcombe (1993) to improve substantially grain yield and resistance to downy mildew in five pearl millet composites.
These results are in line with our findings which equally revealed large gains over five cycles of selection.
Also, recurrent selection has been used successfully in pearl millet to improve downy mildew resistance, grain yield and other agronomic traits (e.g., Weltzien and King, 1995; Govil et al., 1982; Bidinger and Raju, 2000; Baskaran et al., 2009).
In Nigeria, recurrent selection of maize for grain yield and Striga resistance had been reported to be successful (Badu-Apraku, 2010; Menkir and Kling, 2007).
The lower measured response from selection for Striga resistance at Cinzana may be attributed to the following factors:-
predominance of selection at Sadoré, i.e., lower number and later start of the genepool evaluations at Cinzana;
-
selection of the genepool parents mainly from Niger;
-
significant G×E interaction for ASNPC across Sadoré and Cinzana, combined with predominance of selection at Sadoré might have resulted in specific adaptation of the genepool to the Striga population at Sadoré;
-
fluctuations in selection pressure, resulting from year-to-year variation in the level of parasite infestation, especially under the conditions of natural Striga infestation as was the case at Cinzana (artificial Striga infestation might therefore be generally preferable, but requires strict isolation to prevent any Striga seed to migrate; but such isolation was not possible at Cinzana; obviously, artificial infestation cannot be recommended if farmers' fields are used during evaluation - here careful field selection in the previous year is key to success);
-
the low Striga infestation level at Cinzana during the evaluation of selection progress, which might have been sub-optimal for differentiation among the tested entries - but this is less probable since the estimated plot-based heritability was sufficiently high at Cinzana during the test year.
Future uses of the Striga-resistant pearl millet genepool
In the present study, marked predicted responses were estimated for all target traits, suggesting that enhanced Striga and downy mildew resistance, as well as increased panicle yield are expected also in the next cycle of selection.
The fact that useful genetic variation still exists in the population may be due to the introgression of further resistant germplasm in one selection cycle and maintenance of a large enough effective population size in the genepool in each selection cycle.
The broad-based genepool can therefore serve as base population for further selection in collaborative breeding programs and for extraction of new experimental varieties with specific adaptation to different target sites.
Experimental varieties have been and are being extracted from the genepool for Sadoré and Cinzana, and are being also validated in on-farm trials.
The genepool may also be shared with other pearl millet breeders in West Africa, especially Burkina Faso, Nigeria, and Senegal, so that the resistance is validated and new experimental varieties are extracted for new target sites in those countries.
Such cultivars with quantitative resistance may play an important role in integrated Striga control in pearl millet in West Africa.
Need for selection for specific adaptation
The presence of significant G×E interaction for Striga resistance and other traits in our study may be attributed to the differences in environmental factors such as soil type, amount of rainfall (Sahelian vs Sudanian), and putative differences for virulence between the parasite populations at Sadoré and Cinzana.
The total observed G×E interaction consists of interactions of the pearl millet genepool with environmental factors of the test locations, interactions between pearl millet genepool and S. hermonthica populations at the two test sites and the threefold interaction between pearl millet genepool, S. hermonthica population and environmental factors, but these different types of interaction effects cannot be separated in field trials due to quarantine regulations.
One alternative would be to move beyond the two sites (Cinzana and Sadoré) and rather test at numerous environments in several countries in order to perform an AMMI analysis, a powerful tool for understanding complex G×E interactions.
However, the fact that the G×E interaction variances were considerably higher than the genetic variances for resistance to Striga and downy mildew suggests that best FS families differ between the two evaluation environments, and that the best FS identified at each site should be recombined separately to create experimental varieties with specific adaptation to each site and its broader environmental context.
Also Badu-Apraku (2010) found significant G×E interaction for grain yield and Striga resistance parameters in an extra-early maturing maize population in three test locations in Nigeria, and Haussmann et al. (2001b) reported highly significant G×E interaction for grain yield and area under Striga severity progress curve (ASVPC, a measure of Striga emergence and vigor throughout the season) in African sorghum trials.
Both studies therefore also support the need to select for specific adaptation in Striga resistance breeding, especially if the target sites differ substantially from each other and represent putative different Striga ecotypes.
Future prospects for Striga resistance breeding and management in pearl millet
Intra-species genetic diversity has been reported in S. hermonthica (Yoshida et al., 2010), indicating that the parasite is highly variable, and therefore responds to selection pressure rapidly.
The complex nature of the pathosystem and high adaptability of Striga to a changing host population likely requires that Striga-resistant varieties be managed and combined with integrated Striga control measures, to hinder the parasite to overcome the resistance.
Genetically heterogeneous open-pollinated varieties where different plants carry different resistance alleles may contribute to stability of resistance over time, because they result in non-uniform selection pressure on the parasite.
Maintenance breeding of such Striga-resistant open-pollinated varieties may need to be done under Striga infested conditions, in order to maintain and improve their resistance.
This is especially true in case there are any "costs of resistance" that lead to lower performance of resistant plants in the absence of Striga.
To our knowledge, any costs of Striga resistance - if existent at all - are not yet well understood.
Several mechanisms have been implicated in lowering Striga parasitism, including low germination stimulant production (Hess et al., 1992; Ejeta and Butler, 1993; Haussmann et al., 2001a; Jamil et al., 2011), reduced capacity to elicit haustorial induction of Striga (Gurney et al., 2003; Rich et al., 2004), mechanical barriers (Maiti et al., 1984; Gurney et al., 2006; Amusan et al., 2008; Yoshida and Shirasu, 2009), and hypersensitive response (Mohamed et al., 2003; Cissoko et al., 2011).
Ejeta et al. (2000) had also summarized other potential post-germination mechanisms of resistance that impede attachment and emergence of Striga in crops.
Whether the observed gains in Striga resistance with selection in the pearl millet Striga-resistant genepool are due to these or other mechanisms is yet to be determined.
A better understanding of Striga resistance mechanisms in pearl millet and development of appropriate, cost-efficient screening procedures for component traits could render future recurrent selection more efficient.
The development of a marker-assisted population improvement scheme for enhancing quantitative Striga resistance in pearl millet could help render selection even more effective.
Due to decreasing genotyping costs (Elshire et al., 2011), larger numbers of entries could be screened for markers linked to resistance alleles, followed by field phenotyping of a selected subset of the entries, and this would result in an increased effective selection intensity compared to phenotypic selection alone.
The so-called "non-Striga years" (years with overall limited Striga infestation) that often lead to poor differentiation among the tested entries would no longer hinder selection progress as much as they do at present, because selection would be done using the markers.
In years when the field phenotyping is successful in differentiating the tested entries, the results can be used to re-calibrate the marker-based selection index.
Efforts are presently underway in our group to develop such a marker-assisted FS population improvement scheme for Striga resistance in pearl millet.
Conclusions
Five cycles of phenotypic recurrent selection in a diversified pearl millet genepool resulted in significant improvement for Striga and downy mildew resistance, and increased panicle yield.
Significant genetic variance and sufficiently large broad-sense heritabilities in the C5 population indicate good prospects for further selection within and extraction of new Striga-resistant experimental varieties out of this genepool.
Large G×E interactions observed in the present Striga resistance trials underline the need for multi-location testing to develop new varietal options with specific adaptation to contrasting target environments.
Because of the Striga variability, resistant varieties need to be carefully managed and combined with other integrated Striga control measures, in order to hinder the parasite to overcome the deployed resistance.
Acknowledgements
This work has been undertaken as part of the CGIAR Research Program on DrylandCereals.
The financial support of the McKnight Foundation is gratefully acknowledged (ICRISAT grants no.
06-14 and 10-134).
This research was conducted at ICRISAT-Sadoré and IER-Cinzana; the authors express their appreciation to Boubé Guida, Hamadou Boubé, Adama Traoré and all other field assistants at Sadoré and Cinzana for their great help with the experiments.
Our grateful thanks are extended to Dr. Roger Stern and Zobinou Mawusi (Reading University, UK) for their valuable statistical assistance, and to the resource persons of the e-writeshop organized by the Collaborative Crop Research Program of the McKnight Foundation.
Finally, this publication was made possible through support provided by the IRD-DSF to the first author.

Influence of the growth rate on the morphology of electrodeposited zinc oxide

The electrochemical growth of ZnO was performed by using a potentiostat in the three-electrode configuration with F-doped SnO2 (FTO) supplied by SOLEMS, graphite and Ag/AgCl(sat) as working (WE), counter and reference electrode (RE), respectively. The electrolyte was made by dissolving different amount of Zn(NO3)2*4H2O in deionized water (18.2 MΩ cm), to reach concentrations from 0.01 M to 0.1 M. The bath was then heated at 80 degC under magnetic stirring and the final pH was around 4.6. Electrodeposition was carried out at a potential ranging between -0.9 V/RE and -1.4 V/RE with adjusted deposition time to obtain 350/400 nm-thick layers, in compliance with Faraday's law. Then, the samples were rinsed with deionized water, dried with nitrogen and finally annealed at 200 degC during 30 min at atmospheric pressure.
1. A positive electrode active material for a lithium secondary battery, the positive electrode active material comprising:
a core portion and a shell portion, both of which comprise a nickel-based composite oxide represented by the following formula 1:
< formula 1>
Lia[NixCoyMnz]O2
Wherein a is more than or equal to 0.8 and less than or equal to 1.2, x is more than or equal to 0.05 and less than or equal to 0.9, y is more than or equal to 0.1 and less than or equal to 0.8, z is more than or equal to 0.1 and less than or equal to 0.8, and x + y + z is equal to 1,
wherein the content of nickel in the core portion is greater than the content of nickel in the shell portion, and the core portion comprises acicular particles.2. The positive active material for a lithium secondary battery according to claim 1, wherein the core part or the precursor of the core part has open pores.3. The positive active material for a lithium secondary battery according to claim 1, wherein the core part or the precursor of the core part has a curved surface structure.4. The positive active material for a lithium secondary battery according to claim 1, wherein the shell portion is formed in the following manner: the shell portion penetrates into the open pores of the core portion.5. The positive electrode active material for a lithium secondary battery according to claim 1, wherein the content of the core part is in the range of 40 to 90 parts by weight based on 100 parts by weight of the positive electrode active material, and the content of the shell part is in the range of 10 to 60 parts by weight based on 100 parts by weight of the positive electrode active material.6. The positive active material for a lithium secondary battery according to claim 1, wherein the core portion comprises LiNi0.6Co0.2Mn0.2O2And the shell portion comprises LiNi1/3Co1/3Mn1/3O2。7. The positive active material for a lithium secondary battery according to claim 1, wherein the content of nickel in the core part is in the range of 50 to 90 mol% and the content of nickel in the shell part is in the range of 5 to 49 mol%.8. The positive active material for a lithium secondary battery according to claim 1, wherein the core part has a diameter in the range of 1 to 20 μm, and the shell part has a diameter in the range of 5 to 10 μm.9. A method of preparing a positive active material for a lithium secondary battery, the method comprising:
a first process of mixing a first precursor solution including a nickel salt, a cobalt salt, and a manganese salt with a first base to prepare a first mixture, and then initiating a reaction in the first mixture to obtain a precipitate,
a second process of adding a second precursor solution including a nickel salt, a cobalt salt, and a manganese salt and a second base to the precipitate to obtain a second mixture and initiating a reaction in the second mixture to obtain a composite metal hydroxide; and
mixing the composite metal hydroxide with a lithium salt to obtain a third mixture, and heat-treating the third mixture to prepare a positive electrode active material according to any one of claims 1 to 8,
wherein the content of nickel in the first precursor solution is adjusted to be greater than the content of nickel in the second precursor solution.11. The method of claim 9, wherein the precipitate of the first process is a porous structure having open pores.12. The method of claim 9, wherein the reaction time of the first mixture in the first process is in the range of 5 to 7 hours and the reaction time of the second mixture in the second process is in the range of 6 to 10 hours.13. The method of claim 9, wherein the content of the nickel salt in the first process is in the range of 1 to 1.2 moles based on 1 mole of the cobalt salt.14. The method of claim 9, wherein the pH of the first mixture in the first process is adjusted to be in the range of 10 to 11 and the pH of the second mixture in the second process is adjusted to be in the range of 11.5 to 12.0.15. A lithium secondary battery comprising the positive electrode active material according to any one of claims 1 to 8.16. A positive active material precursor for a lithium secondary battery, the positive active material precursor comprising:
nickel-based composite hydroxide and acicular particles represented by the following formula 2:
< formula 2>
NixCoyMnzOH
Wherein x is more than or equal to 0.05 and less than or equal to 0.9, y is more than or equal to 0.1 and less than or equal to 0.8, z is more than or equal to 0.1 and less than or equal to 0.8, and x + y + z is equal to 1.17. The positive electrode active material precursor according to claim 16, wherein the positive electrode active material precursor has open pores and a curved surface structure.Nano-perovskite carbon paste composite electrode for the simultaneous determination of dopamine, ascorbic acid and uric acid

All chemicals were used as received without further purification. Ammonium hexachloropalladate (IV) (Aldrich, 99.99%), strontium nitrate, citric acid, glycine, urea, nitric acid, ammonium hydroxide (Aldrich), graphite powder (Sigma-Aldrich, <20 μm, synthetic) and Paraffin oil (Fluka) were used as received without further purification. Dopamine (DA), uric acid (UA), ascorbic acid (AA) and potassium ferricyanide were supplied by Aldrich Chem. Co. (Milwaukee, WI. USA). Aqueous solutions were prepared using double distilled water. Phosphate buffer solution PBS (1 mol L-1 K2HPO4 and 1 mol L-1 KH2PO4) of pH 2-11 was used as the supporting electrolyte. pH was adjusted using 0.1 mol L-1 H3PO4 and 0.1 mol L-1 KOH.

Stoichiometric amounts of (NH4)2PdCl6 and Sr(NO3)2 were weighed, dissolved in distilled water, then a sufficient amount of certain fuel (citric acid, glycine, or urea) was added. The pH of the solution was adjusted to certain value (2, 7, or 10) by nitric acid and ammonium hydroxide. The solution was heated on a hot plate to about 250 degC. The precursor complex undergoes dehydration to produce foam which then ignited giving a voluminous black fluffy powder. A ceramic nano-SrPdO3 was obtained by calcination at 750 degC for a given time (3, 5, or 8) h [15].1. A positive active material for a lithium secondary battery containing a lithium transition metal composite oxide having a hexagonal crystal structure in which the transition metal (Me) includes Ni, Co and Mn, wherein in the lithium transition metal composite oxide, a molar ratio of Ni to the transition metal (Me) (Ni/Me) is 0.5 or more and 0.9 or less, a molar ratio of Co to the transition metal (Me) (Co/Me) is 0.1 or more and 0.3 or less, a molar ratio of Mn to the transition metal (Me) (Mn/Me) is 0.03 or more and 0.3 or less, and a value obtained by dividing a half width ratio F(003)/F(104) at a potential of 4.3 V (vs. Li/Li+) by a half width ratio F(003)/F(104) at a potential of 2.0 V (vs. Li/Li+) is 0.9 or more and 1.1 or less.2. The positive active material for a lithium secondary battery according to claim 1, wherein the lithium transition metal composite oxide is represented by the composition formula Li<sub>1+x</sub>(Ni<sub>a</sub>Co<sub>b</sub>Mn<sub>c</sub>)<sub>1-x</sub>O<sub>2 </sub>(−0.1<x<0.1, 0.5≦a≦0.9, 0.1≦b≦0.3, 0.03≦c≦0.3, a+b+c=1).3. The positive active material for a lithium secondary battery according to claim 1, wherein in the lithium transition metal composite oxide, a particle size distribution does not have two or more local maximal values.4. A method for producing a positive active material for a lithium secondary battery containing a lithium transition metal composite oxide having a hexagonal crystal structure in which the transition metal (Me) includes Ni, Co and Mn, wherein when coprecipitating compounds containing Ni, Co and Mn in a solution to prepare a precursor, a solution containing a compound of Ni and Co and a solution containing a Mn compound are simultaneously added dropwise separately, and thereby a precursor of a transition metal composite oxide in which a molar ratio of Ni to the transition metal (Me) is 0.5 or more and 0.9 or less, a molar ratio of Co to the transition metal (Me) is 0.1 or more and 0.3 or less, and a molar ratio of Mn to the transition metal (Me) is 0.03 or more and 0.3 or less, is prepared.5. The method for producing a positive active material for a lithium secondary battery according to claim 4, wherein the precursor has a tap density of 1.4 g/cc or more.6. An electrode for a lithium secondary battery containing the positive active material for a lithium secondary battery according to claim 1.7. A lithium secondary battery including the electrode for a lithium secondary battery according to claim 6.8. An energy storage apparatus configured by assembling a plurality of the lithium secondary batteries according to claim 7.High activity Pt/C catalyst for methanol and adsorbed CO electro-oxidation
The Pt/C(a) was prepared according to Ref. [16].
The preparation of Pt/C(b) was: first a mixture of the required quantity of the carbon support (Vulcan XC-72R activated carbon black) and glycol was agitated ultrasonically and the hexachloroplatinic acid solution was added into the mixture at a temperature of 85deg C then, the formic acid solution was added drop by drop to reduce H2PtCl6 at 85deg C for 1 h. Next, the slurry obtained was filtered and washed with triply distilled water until no Cl- was detected. Finally the slurry was filtered and dried in the vacuum condition at 25deg C. The Pt/C catalyst prepared contained 20 wt.% Pt.Characterization of mechanical and microstructural properties of palm oil fuel ash geopolymer cement paste
The palm oil fuel ash (POFA), obtained from burning of palm oil shells, husk and fibers, was collected from a mill at Johor State, south of Malaysia. The raw palm oil fuel ash was oven dried at 110 +- 5 degC for 24 h , sieved with a 300 μm sieve to remove large unwanted particles and incompletely combusted materials [49] and [50], and then it was ground by a modified Los Angeles machine [42]. The specific surface area after grinding was 0.915 m2/g. The chemical composition of the POFA by XRF test is shown in Table 1. As can be seen, major components are SiO2 and CaO with concentrations of 47.37% and 11.83%, respectively with a low amount of Al2O3 (3.53%). The relatively high amount of CaO available in the POFA is most likely from lime and fertilizer [41]. It is worth to mention that POFA is a not toxic waste material in terms of heavy metals leachability [51].
Sodium hydroxide and sodium silicate were chosen as alkali activators in this investigation. The sodium hydroxide was in industrial-grade with minimum 99% purity. Industrial grade water glass (Na2SiO3) solution was chosen with a chemical composition of 15.33% Na2O, 31.28% SiO2, and 53% H2O. The alkaline activator selection was based on the recommendations in [34], [53], [54] and [55]. The alkali activation solution was prepared by mixing Na2SiO3 with NaOH within ratios ranged between 0.5 and 3.0 [34].
Preliminary experiments were conducted to study the alkali activation of POFA [56]. It revealed that raw POFA cannot be used without sieving and grinding because of the low compressive strength results which is related to the porous structure and high demands for the alkaline activator. Experimental program was designed in order to investigate the ability of POFA to be incorporated in geopolymer technology. NaOH solution was mixed with Na2SiO3 to produce six ratios (0.5, 1.0, 1.5, 2.0, 2.5, and 3) to prepare alkaline activator solution 24 h prior to use. Ground POFA and the alkaline activator were then mixed with two solid-to-liquid ratios (1.0, 1.32) as in [34], [53], [57] and [58]. Two groups of twelve mixes were prepared based on the solid to liquid ratio as shown in Table 2. Mix one with sodium silicate to sodium hydroxide ratio (0.5) was unable to be used; the geopolymer paste had such low workability that could not be cast in molds.
The palm oil fuel ash was mixed directly with the alkaline activator. First the ash was discharged to the mixer pan, and then the alkaline was added and mixed for 1.0 min at a normal speed rate (gear one). Then, the mixer stopped for 10-20 s in order to scrap the un-mixed ash on the sides of the paddle and the pan, then the mixing was continued with a medium speed (gear two) for another 1.0 min in accordance to ASTM C305 [59]. All the geopolymer paste mixes were blended and produced with a small blender (HOBART Mixer). The fresh geopolymer paste was then cast into 50 x 50 x 50 mm iron molds in two layers immediately after mixing. To compact the specimen, each layer was vibrated by a vibrating table for 25-30 s. This procedure was used to remove the bubbles from the paste. After casting and vibrating, oven curing was applied directly at 60 degC for 2 h. The molds were then kept in the laboratory to reach ambient temperature. The specimens were kept in a plastic bags after de-molding to prevent any moisture loss and were stored to the date of testing [55].A robust, gravure-printed, silver nanowire/metal oxide hybrid electrode for high-throughput patterned transparent conductors
Ag NWs dispersed in ethanol were purchased from Blue Nano Inc. (average diameter D = 33 nm, average length = 14 μm). Glass substrates (1 mm thick) were obtained from VWR Scientific. Planarized polyethylene naphthalate substrates (PEN) were provided by Dupont Teijin Films. All solvents and sol-gel precursors were obtained from Sigma Aldrich.
IZO/Ag NW composite inks were prepared from a mixture of Ag NWs dispersed in ethanol and IZO sol-gel precursors (Indium Acetate, 99.99%, Zinc Acetate dehydrate, 99.99%) dissolved in a 2:1 mixture (volume ratio) of 2-methoxyethanol and monoethanolamine. The sol-gel precursor ink was sonicated for 1 hour until complete dissolution occurred, and the solution was subsequently filtered through a 0.2 μm PTFE filter before mixing with the Ag NW dispersion. During storage in a dry environment, the ink remained stably dispersed for more than one month.An electrochemical sensor for H2O2 based on a new Co-metal-organic framework modified electrode

A mixture of Co(AC)2*4H2O (0.25 mM, 62 mg), H2pbda (H2pbda = 3-(pyridine-3-yloxy)benzene-1,2-dicarboxylic acid) (0.25 mM, 78 mg), 4,4-bpy(0.25 mM, 48 mg) and KOH (0.05 mM, 28 mg) was added in 10 ml distilled water and stirred at room temperature for 30 min, then the resulting mixture was sealed in a 25 ml Teflon-lined stainless steel vessel and heated at 160 degC for 3 days. After the reactant was cooled slowly to room temperature with a cooling rate of 5 degC/h, purple block-shaped crystals were obtained in 60% yield (based on Co). Elemental anal. (%): Calcd. For [Co(pbda)(4,4-bpy)*2H2O]n: C 51.44, H 3.12, N 6.67. Found: C 50.13, H 3.32, N 6.78. Crystallographic parameters for (Co(pbda)(4,4-bpy)*2H2O)n: a = 21.9803(8) Å, b = 17.2195(7) Å, c = 9.2491(3) Å, α = 90.00deg, β = 105.929(2)deg, γ = 90.00deg, Mr = 841.48, monoclinic, space group C2/c, V = 3366.3(2) Å3, Z = 4, goodness-of-fit = 1.029, R1 = 0.0559 ([I > 2σ(I)]) and wR2 = 0.0965 (all data). Crystallographic data (CIF file) of [Co(pbda)(4,4-bpy)*2H2O]n have been deposited at the Cambridge Crystallographic Data Centre and CCDC deposition number is 1034563. These data can be obtained free of charge via email deposit@ccdc.cam.ac.uk.

Synthesis of iron-containing nitrogen-doped titania by hydrothermal method and its photocatalytic activity

Fe co-doped TiO2-xNy was prepared by hydrothermal method, similar to that used for only N-doped TiO2 reported in a previous paper [19]. TiCl3 20 wt% aqueous solution was used as the titanium source, and hexamethylenetetramine (HMT) was used as nitrogen source. To co-dope Fe into the samples, FeCl2*4H2O was added to the starting materials. In a typical preparation, a 50 mL mixed aqueous solution of TiCl3, FeCl2*4H2O and hexamethylenetetramine (HMT) was first prepared, which contained 0.03 mol of metal ions and 0.06 mol of HMT. Then, the solution was transferred to a Teflon-lined stainless autoclave, and heated at 190 degC for 2 h. After that, precipitates were collected by centrifugation, and carefully washed with distilled water and acetone. Finally, the products were vacuum-dried at 60 degC overnight. The amount of Fe in the final products was altered in the range from 1 to 5 mol%.

Characterization of glassy carbon electrodes modified with carbon nanotubes and iron phthalocyanine through grafting and click chemistry
Tetrabutylammonium tetrafluoroborate (TBABF4), 4-azidoaniline hydrochloride, iron(II) phthalocyanine (FePc), sodium ascorbate, hydrazine sulphate and 4-ethynylpyridine hydrochloride were obtained from Aldrich. Dimethylformamide (DMF), acetonitrile (ACN), and acetone were purchased from Merck. Copper sulphate and potassium chloride were purchased from Saarchem. Single-walled carbon nanotubes functionalised with carboxylic acid groups (SWCNT-COOH, ~1.5 nm in diameter and 1-5 mm in length, >95% purity by thermal gravimetric analysis) were obtained from NanoLab (USA). 4-Azidobenzenediazonium salt was synthesised as reported in literature [52]. All other chemicals and reagents were of analytical grade and were used as received.
Analysis of SOX2-Expressing Cell Populations Derived from Human Pluripotent Stem Cells

Summary
SOX2 is involved in several cell and developmental processes, including maintenance of embryonic stem cells, differentiation of neural progenitor cells, and patterning of gut endoderm.
To study its role in a human system, we generated a human embryonic stem cell (hESC) line harboring a reporter gene encoding GFP in the SOX2 locus.
This SOX2 reporter line faithfully recapitulates expression of the SOX2 gene in undifferentiated human pluripotent stem cells (hPSCs), neural progenitor cells (NPCs), and anterior foregut endoderm (AFE).
In undifferentiated hESCs, GFP expression corresponds to those cells with highest levels of expression of genes associated with the pluripotent state.
In NPCs, expression of GFP can be employed to isolate cells expressing markers associated with NPC multipotency.
In AFE, we used transcriptome-wide expression analysis to identify cell surface markers with elevated expression in this population, thereby facilitating isolation and purification of this hPSC-derived cell population.
Highlights
A SOX2-GFP hESC line is used to isolate and characterize multiple cell types
A cell surface marker signature allows for the purification of endodermal progeny
Targeted gene insertion with adeno-associated virus (AAV) is highly efficient
SOX2 is expressed during multiple developmental stages.
Brafman, Willert, and colleagues used human embryonic stem cells carrying a SOX2 reporter to characterize its expression in undifferentiated cells and in neural and lung progenitor cells.
This approach led to the identification of a cell surface signature to isolate prospective lung progenitors.

Introduction
Human pluripotent stem cells (hPSCs; including human embryonic stem cells [hESCs] and human induced pluripotent stem cells [hiPSCs]), provide a unique model system to study early human development and generate mature and functional cell types suitable for disease modeling, cell transplantation, and replacement therapies.
Clinical applications of hPSCs will require a detailed understanding of the mechanisms that maintain their pluripotency or result in their differentiation to specific lineages.
A particularly attractive method to study the underlying mechanisms that control pluripotency and differentiation is through the use of marker cell lines in which specific genes known to function in these processes are modified with a "molecular beacon," such as a gene encoding a fluorescent protein.
Expression of such a tagged gene can be used to analyze and characterize the cells in which expression of this gene is either activated or repressed.
Here, we describe the generation and characterization of such a marker line for the gene SOX2, which plays multiple roles in hPSC pluripotency and differentiation (Arnold et al., 2011; Lefebvre et al., 2007).
SOX2 is a member of the SRY-related high-mobility-group box (SOX) transcription factors and controls cell fate and differentiation in a variety of cell types during development (Kiefer, 2007; Lefebvre et al., 2007).
During the initial stages of development, SOX2 is expressed in the inner cell mass of the blastocyst (Lorthongpanich et al., 2008; Rossant, 2004) and along with OCT4 is responsible for regulating the pluripotent precursors that develop into the three germ layers (Avilion et al., 2003).
SOX2 acts in coordination with other factors, such as NANOG (Wang et al., 2006) and OCT4 (Nichols et al., 1998), to maintain ESCs in a pluripotent state.
Furthermore, ectopic expression of SOX2 along with OCT4, KLF4, and c-MYC can induce a pluripotent stem cell state from adult human fibroblasts, giving rise to hiPSCs (Takahashi et al., 2007).
In addition to regulating the pluripotent state, SOX2 controls the formation of several cell types during fetal development, such as the nervous system (Ellis et al., 2004), anterior foregut endoderm (Que et al., 2007), and sensory cells of the taste bud and inner ear (Dabdoub et al., 2008; Kiernan et al., 2005; Okubo et al., 2006).
SOX2 also regulates the progenitor cells in several adult tissues in the brain (Ellis et al., 2004), trachea (Que et al., 2009), and pituitary glands (Fauquier et al., 2008).
A recent genetic lineage tracing study revealed that SOX2 regulates adult stem cells and tissue homeostasis in several adult epithelial tissues in the stomach, cervix, anus, testes, lens, and glands associated with the oral cavity, trachea, and cervix (Arnold et al., 2011).
In this study, we describe the generation and characterization of a hESC line in which the endogenous SOX2 locus was targeted to express GFP.
The targeted reporter line facilitated the flow-cytometry-based purification and genetic assessment of SOX2-positive (SOX2+) cells in pluripotent hESCs as well as hESC-derived neural progenitor cells (NPCs) and anterior foregut endoderm (AFE).
Genome-wide analysis of SOX2+ AFE cells revealed a global gene expression signature that distinguished hESC-derived AFE cells from other cell types.
This signature included two cell surface markers that permitted purification of SOX2+ AFE cells from differentiating hESC cultures.
Therefore, this SOX2-GFP reporter line is a valuable tool to dissect the role of SOX2 in regulating pluripotency, self-renewal, and differentiation.
Results
Generation of a SOX2-GFP Reporter hESC Line by AAV Mediated Homologous Recombination
Using a recombinant adeno-associated viral (rAAV)-based gene-targeting method, we inserted the gene-encoding GFP into the SOX2 locus in H9 hESCs (Figure 1A).
Proper homologous recombination led to the replacement of the SOX2 open reading frame with that of GFP and a neomycin selection cassette (SV40-Neo).
After infection with rAAV and G418 drug selection, a total of 36 clones were expanded and screened by Southern blotting for homologous recombination events.
Among these clones, 26 (72%) were found to carry the GFP-Neo cassette in the SOX2 locus (Figure S1A available online).
No clones in which both SOX2 alleles were disrupted were isolated.
Our subsequent analysis focused on one of these clones, clone 23 (hSOX2-23).
We confirmed appropriate gene targeting in this clone using multiple restriction digests followed by Southern blotting (Figures 1B, S1B, and S1C).
We did not observe nontargeted insertions of the rAAV sequences, and cells exhibited a normal karyotype (data not shown).
Flow cytometry of hSOX2-23 revealed that the majority of the cells expressed GFP (Figure 1C).
By comparison, a drug-selected clone, hSOX2-25, which was negative for targeted insertion (Figure S1A), showed no detectable GFP (Figure S2A).
Despite only having one copy of SOX2, hSOX2-23 had similar levels of SOX2, OCT4, and NANOG expression as hSOX2-25 and wild-type (WT) hESCs (Figure S2B).
Moreover, the percentage of GFP-positive (GFP+) cells in hSOX2-23 was constant over more than 20 passages.
Immunofluorescence (IF) staining of hSOX2-23 showed that 100% of GFP+ cells expressed SOX2 protein (Figure S2C).
Additionally, hSOX2-23 colonies had characteristic hESC morphology (Figure S2D) and expressed markers of the undifferentiated state, such as NANOG (Figure S2E).
These results show that this rAAV-based gene-targeting method can be used to efficiently disrupt genes by homologous recombination.
In addition, the SOX2-GFP hESC marker line can be used to monitor SOX2 expression in undifferentiated hESCs.
SOX2-GFP Marks Undifferentiated hESCs
To investigate whether GFP expression in hSOX2-23 could be used to monitor the differentiation status of hESCs, we performed flow cytometry analysis of hSOX2-23 grown in culture conditions that maintain hESCs in an undifferentiated state.
In these conditions, >90% of the cells were GFP+ (Figure 1C).
Quantitative RT-PCR (qRT-PCR) revealed that expression of SOX2, OCT4, and NANOG was significantly higher in GFP+ compared to GFP negative (GFP-) cells (Figure 1D), indicating that GFP expression marked undifferentiated cells.
To determine if GFP expression could be used to remove differentiating cells from pluripotent hESC cultures, we cultured purified GFP+ and GFP- cells in conditions that support undifferentiated growth for hESCs.
The GFP+ cells grew as compact colonies characteristic of the undifferentiated state, whereas the GFP- cells had a fibroblast-like morphology typical of differentiating hESCs (Figure S2F).
Moreover, cultured GFP+ cells maintained high GFP expression, whereas cultured GFP- cells failed to express detectable levels of GFP (Figure 1E).
Finally, IF staining revealed that cultured GFP+ cells maintained high NANOG and OCT4 expression, whereas cultured GFP- cells showed little NANOG or OCT4 expression (Figure 1E).
These results suggest that the SOX2-GFP marker can be used to monitor the undifferentiated state of hESCs.
Dynamics of SOX2-GFP Expression during Neural Differentiation
In addition to being a master regulator of hPSCs, SOX2 is a marker of multipotent NPCs and is necessary for their maintenance in the nervous system (Ellis et al., 2004).
To assess regulation of the SOX2-GFP marker during neurectoderm differentiation, we developed a serum-free differentiation protocol based on previously published methods (Figure 2A; Chambers et al., 2009; Li et al., 2011).
In brief, NPCs were manually picked from embryoid body-derived rosettes, dissociated, replated, and maintained as proliferative cells in the presence of fibroblast growth factor 2 (FGF2) and epidermal growth factor (EGF) (Shin et al., 2006).
Expression of SOX2 and the neural-specific marker PAX6 peaked upon NPC formation (Figure 2B).
Flow cytometry confirmed the progressive loss of the pluripotency marker TRA-1-81 as hESCs differentiated to rosettes and NPCs (Figure 2C).
Concurrently, GFP expression declined upon differentiation to the rosette stage and then re-emerged in NPCs (Figure 2C).
This pattern of GFP expression is consistent with previous studies (Chambers et al., 2009).
IF of hSOX2-23 NPCs revealed that 100% of GFP+ NPCs were SOX2+ (Figure 2D).
Additionally, a high percentage of GFP+ NPCs coexpressed the NPC marker SOX1 as monitored by flow cytometry (Figure 2E).
Together, these results demonstrate that SOX2-GFP expression can be used to monitor neural differentiation of hESCs.
Isolation of SOX2-GFP+ NPCs from Neural Rosettes
Fluorescence analysis of rosette stage cultures revealed that GFP expression was isolated to the neuroepithelial-like rosette structures that are manually dissected to obtain NPCs (Elkabetz et al., 2008; Figure 2F).
To investigate if GFP expression could allow for the isolation of NPCs without manual dissection, we dissociated rosette-stage cultures into single cells and isolated GFP+ cells using fluorescence-based cell sorting (Figure 2G).
Gene expression analysis of GFP+ and GFP- cell populations by qRT-PCR revealed that the GFP+ rosette stage cells expressed higher amounts of the NPC markers SOX1, SOX2, PAX6, and NESTIN than the GFP- rosette stage cells (Figure 2H).
Subsequent culture of GFP+ rosette stage cells revealed that these cells maintained high expression of GFP and the NPC marker SOX1 (Figure 2I).
Thus, SOX2-GFP expression can be used to isolate NPCs from rosette stage cultures.
SOX2-GFP Marks the Anterior Foregut Endodermal Progeny of Differentiating hESCs
SOX2 is expressed in the developing AFE, with the highest levels in the future esophagus, trachea, and lung (Que et al., 2007).
To investigate if the hSOX2-23 line could be used to isolate cells with an AFE identity from differentiating hESCs, we used a modified version of previously published protocols (Figure 3A; Green et al., 2011; Longmire et al., 2012; Mou et al., 2012).
To generate definitive endoderm (DE), the precursor cell population for AFE, hSOX2-23s were treated with Activin A and Wnt3a (stage 1).
Subsequent differentiation to AFE was achieved through addition of bone morphogenetic protein (BMP) antagonists noggin and SB431542 (stage 2).
Maturation to a lung progenitor cell (LPC) phenotype was achieved through addition of BMP4, FGF2, and Wnt3a.
At stage 1, expression of the DE marker SOX17 peaked while expression of the pluripotency marker NANOG declined (Figure 3B).
Similarly, at stage 2, we observed high expression levels of AFE markers FOXA2 and TBX1 (Figure 3B).
In contrast, expression of the posterior foregut endoderm (PFE) markers HNF6 and PDX1 was not detectable (Figure S3A).
SOX2, as well as TBX1, expression re-emerged during differentiation to AFE (Figure 3B), but not to PFE (Figure S3B), suggesting the SOX2-GFP reporter line can be employed to isolate cells with properties associated with AFE.
Furthermore, LPC markers NKX2.1 and SOX9 showed high levels of expression at stage 3 (Figure 3B).
Consistent with previous publications, this protocol yielded approximately 25% NKX2.1-positive (NKX2.1+) LPCs (Figure 3C).
Next, we tested whether GFP expression in hSOX2-23 hESCs could be used to monitor anterior foregut differentiation and to purify AFE progeny.
After an initial decrease in GFP expression as cells exited the pluripotent state and differentiated toward DE, a GFP+ cell population re-emerged and expanded during the AFE stage (Figure 4A).
Upon subsequent differentiation to LPC, GFP expression disappeared.
This temporal pattern of GFP expression was consistent with our analysis of SOX2 gene expression.
Differentiation of hSOX2-23 into AFE yielded densely packed GFP+ cells, often surrounding an empty lumen-like cavity (Figure 4B).
qRT-PCR analysis revealed that expression of the AFE markers SOX2, TBX1, PAX9, HOXA1, and HOXA2 was highly enriched in the GFP+ population, whereas the PFE markers HNF1B, HNF4A, GATA6, CDX2, and PDX1 were enriched in the GFP- population (Figure 4C).
To test the potential of these cells to develop into LPCs, sorted GFP+ and GFP- AFE cells and unsorted control AFE cells were replated and differentiated to LPCs using previously described methods (Green et al., 2011; Longmire et al., 2012; Mou et al., 2012).
IF analysis for NKX2.1, the earliest marker of LPCs distinguishing it from the remainder of the AFE (Fagman et al., 2011; Que et al., 2009), revealed that >90% of GFP+ cells differentiated into NKX2.1+ lung endoderm (Figure 4D).
In contrast, the unsorted AFE population or the GFP- populations generated significantly fewer SOX2+ and NKX2.1+ cells (Figure 4D).
Together, these results demonstrate that GFP+ cells exhibit properties of AFE and are capable of differentiating in vitro into derivatives of AFE, including NKX2.1+ LPCs.
Genome-wide Analysis of SOX2-GFP Reporter-Expressing Anterior Foregut Endoderm Cells
To define a global gene expression signature of AFE, we performed whole transcriptome sequencing (RNA sequencing [RNA-seq]) of sorted GFP+ and GFP- cells from differentiated AFE cultures (Figure 5A and Table S1).
We identified 1,943 genes with differential expression between these two cell populations, with the expression of 1,038 genes elevated in the GFP+ population and 905 genes elevated in the GFP- population (Figure 5B).
This signature included genes involved in signaling pathways (Wnt, FGF, Notch, BMP, and RA signaling) known to play a role in the patterning of the foregut endoderm.
Moreover, this genetic signature included genes known to define developing AFE and PFE (Figure 5C).
Specifically, expression of AFE markers SOX2, HOXA1, HOXA2, and IRX5 was highly enriched in the GFP+ population (Figure 5C).
Conversely, expression of PFE markers HNF1A, HNF1B, HNF6, and GATA6 as well as the DE markers SOX17 and FOXA1 were increased in the GFP- cells (Figure 5C).
Early markers of tissues derived from AFE, such as the lung (IRX1 and SOX9), thyroid (PAX8), pharynx (FGF8), esophagus (DLX3 and OTX1), and stomach (EYA4), showed higher levels of expression in GFP+ cells (Figure 5C).
In contrast, expression of genes associated with tissues derived from PFE, such as intestine (CDX2), liver (AFP), and pancreas (PDX1 and NGN3), were lower in GFP+ cells (Figure 5C).
Collectively, this RNA-seq analysis suggests that GFP+ cells isolated from differentiating cultures are enriched for cells with an AFE gene expression profile.
Isolation of AFE Using Cell Surface Markers
To develop a cell surface marker "signature" for SOX2+ AFE cells, we mined our RNA-seq data for genes encoding transmembrane proteins with differential expression levels between GFP+ and GFP- cells (Table S2).
qRT-PCR confirmed that several genes encoding cell surface markers were differentially expressed in the GFP+ and GFP- populations (Figures S4A and 4B).
Flow cytometry with antibodies directed against these cell surface markers revealed that staining of CD56 (neural cell adhesion molecule [NCAM]) and CD271 (nerve growth factor receptor [NGFR]) correlated with GFP expression in day 8 AFE cells (Figures 6A and 6B, and S4C).
Consistent with the RNA-seq data, qRT-PCR analysis confirmed that transcripts for both CD56 and CD271 were enriched in the GFP+ populations (Figure S4B).
Similarly, gene expression analysis of cells at various stages of differentiation revealed that CD56 and CD271 expression peaked at AFE (Figure S4D).
Using fluorescence-based cell sorting of day 8 AFE cultures with antibodies to CD56 and CD271 (Figures 6A and 6C), we demonstrated that double-positive CD56+CD271+ cells expressed higher levels of GFP than single-positive CD56+CD271- or CD56-CD271+ cells or double-negative CD56-CD271- cells (Figure 6D).
When AFE cells differentiated from WT H9 hESCs were sorted for these cell surface markers, expression of AFE markers SOX2, TBX1, and PAX9 was increased in double-positive cells compared with double-negative cells (Figure 6E).
Conversely, expression of PFE markers GATA6, HNF1B, HNF4A, CDX2, and PDX1 was higher in double-negative cells compared with double-positive cells (Figure 6E).
To investigate if double-positive CD56+CD271+ cells were capable of differentiating into more mature lung progeny, as assessed by NKX2.1 expression, we replated CD56+CD271+ and CD56-CD271- cells after cell sorting and differentiated them to LPCs.
Gene expression analysis revealed that expression of the LPC markers NKX2.1 and SOX9 was enriched in the CD56+CD271+ population relative to the CD56-CD271- (Figure 6F).
Additionally, IF analysis for NKX2.1 revealed that a higher percentage of the CD56+CD271+ cells differentiated into NKX2.1+ lung endoderm compared to the CD56-CD271- cells (Figure 6G).
Interestingly, cells with highest NKX2.1 expression were clustered with bright staining along the edges, an organization reminiscent of an epithelial cell population as may be expected for lung epithelial precursors.
These data demonstrate that cell enrichment strategies for CD56 and CD271 significantly increase the percentage of cells with AFE gene expression patterns from differentiated hESC cultures.
Although our analysis suggested that CD56 and CD271 marked SOX2+ AFE cells, it was unclear if these cell surface markers specified SOX2+ cells in undifferentiated hESC or neurectoderm cultures.
Gene expression analysis revealed that neither CD56 nor CD271 expression was enriched in GFP+ hESCs (Figure 7A) or neural rosette cells (Figure 7B).
Flow cytometry revealed that neither CD56 nor CD271 correlated with GFP expression in hESCs (Figure 7C) or neural rosette cells (Figure 7D).
Because SOX2 is also expressed in undifferentiated hESCs and neurectoderm cells, we wanted to confirm that we were not enriching these rare cell types in our CD56+CD271+ AFE cultures.
To that end, there was little expression of the pluripotency markers OCT4 and NANOG in CD56+CD271+ AFE cells when compared to undifferentiated hESCs (Figure 7E).
Additionally, there was no difference in expression of these genes between the CD56+CD271+ and CD56-CD271- AFE cells.
Furthermore, expression of the neurectoderm markers SOX1, PAX6, and NES was significantly lower in CD56+CD271+ AFE cells compared to neural rosette cells (Figure 7F).
Finally, there was no difference in expression of these genes among the CD56+CD271+ and CD56-CD271- AFE cells.
Collectively, these studies suggest that CD56 and CD271 expression correlates only with the SOX2+ AFE cell population.
Discussion
In this work, we employed a human SOX2-GFP reporter cell line to characterize distinct cell populations in which SOX2 is known to be expressed, including undifferentiated hPSCs, NPCs, and anterior foregut endodermal cells (AFEs).
We showed that this reporter line can be used to monitor the differentiation status of cells, isolate and purify distinct cell populations, and identify genes with expression patterns associated with these distinct cell populations.
This approach is particularly valuable for the design and development of protocols for the directed differentiation of hPSCs into cell populations suitable for transplantation studies, disease modeling, and drug screening.
Gene Targeting Using AAV
Gene targeting in hPSCs has met many challenges, and to date, methods for homologous recombination (HR) in hPSCs are not as commonplace as in mouse embryonic stem cells (mESCs).
The reasons for differences in gene targeting between mESCs and hPSCs remain poorly understood.
It has been suggested that mESCs represent an early "naive" developmental stage akin to the inner cell mass of the blastocyst, whereas hPSCs represent a later "primed" developmental state that resembles the epiblast (Nichols and Smith, 2009), and that this difference accounts for the differences observed in transgenesis and HR (Buecker et al., 2010).
In fact, Buecker et al. showed that, in hPSCs that had been genetically manipulated to obtain a naive mESC-like state, HR targeting efficiencies approached those typically observed in mESCs (Buecker et al., 2010).
However, conversion of hPSCs to a naive state with biological characteristics similar to mESCs remains technically challenging (Hanna et al., 2010).
Based on several previous publications (Khan et al., 2010, 2011), we explored the utility of adeno-associated virus (AAV) as a method to improve gene targeting efficiencies in hPSCs.
For SOX2, a gene that is highly expressed in undifferentiated hPSCs, gene targeting rates were greater than 70%.
Similar targeting efficiencies in hPSCs using AAV have been reported by others (Asuri et al., 2012; Khan et al., 2010, 2011; Smith-Arica et al., 2003), indicating that AAV offers a highly efficient and robust approach to target genes for HR in hPSCs.
Currently, conventional methods for gene targeting utilize standard transduction methods, such as electroporation, to introduce linearized DNA constructs with homology arms of 3-5 kb flanking positive (e.g., neomycin or hygromycin) and negative (ganciclovir) selection cassettes (Mansour et al., 1988).
These approaches are extremely inefficient, with targeting efficiencies varying between <0.1% and 5%.
An alternative method to improve gene modification efficiencies involves the introduction of site-specific, double-stranded breaks into the genome using zinc finger nucleases (Davis and Stokoe, 2010; Zou et al., 2009), transcription activator-like effector nucleases (Hockemeyer et al., 2011), or the RNA-guided CRISPR-Cas system (Cong et al., 2013; Hou et al., 2013; Mali et al., 2013).
Whereas these approaches are promising, site-directed specificity has been difficult to control and off-target cleavage events are common (Cradick et al., 2013; Radecke et al., 2010).
In addition, bacterial artificial chromosomes (BACs) have been successfully used for site-specific targeting in hPSCs at efficiencies of up to 25% (Song et al., 2010); however, the use of BACs is technically challenging due to complex cloning methods.
In contrast to these methods, AAV offers features that make it an attractive alternative means for gene targeting.
First, the AAV genome is relatively compact (∼4.8 kb) and genetic engineering is accordingly straightforward.
Aside from two flanking palindromic inverted terminal repeats, the entire genome can be engineered to contain the desired genetic elements, including drug selection cassettes, reporter genes, and homology arms to promote HR.
Second, AAV is a single-stranded DNA virus and, upon infection and entry into the cell, this single-stranded piece of DNA provides an ideal substrate for the endogenous DNA repair machinery, thereby significantly increasing gene-targeting efficiencies.
Third, AAV rarely integrates itself nonspecifically into the genome and it consequently has become an attractive system to create viral vectors for gene therapy.
SOX2, a Regulator of Pluripotency
Along with OCT4 and NANOG, SOX2 is one of the master regulators of the pluripotent state in hPSCs (Rizzino, 2009).
However, analysis of SOX2 expression in hPSCs relies on the fixation of cells, which limits their use in subsequent molecular and biological studies.
We were able to use our SOX2-GFP reporter line to detect and enrich for SOX2 expression in live hPSC cultures.
Consistent with previous reports that describe hPSCs as heterogeneous cultures with varying levels of expression of pluripotency-associated genes (Stewart et al., 2006), we observed varying levels of SOX2-GFP expression in our hPSC cultures.
Purification and analysis of these cells revealed higher expression levels of pluripotency-associated genes in SOX2+ versus SOX2- cells.
Moreover, subsequent culture of purified SOX2+ and SOX2- cells revealed that they maintained distinct developmental states.
SOX2, a Regulator of NPC Multipotency
NPCs derived from hPSCs offer a unique model system to study neural development and are a possible source of cells to treat a variety of neurodegenerative disorders.
In the adult brain, SOX2 functions to maintain the multipotent state of endogenous NPCs (Graham et al., 2003).
Further, SOX2 is a marker of multipotent NPCs derived from hPSCs (Chambers et al., 2009; Li et al., 2011).
Consistent with these studies, we were able to use SOX2-GFP reporter expression to isolate a homogeneous population of SOX2+ NPCs from heterogeneously differentiating cultures.
Moreover, we demonstrated that these cells were enriched for neural markers and maintained high expression of NPC markers over subsequent passages.
These SOX2+ NPCs will be useful for future applications, such as neural transplantation, genetic profiling, or epigenetic analysis.
SOX2, a Marker of Gut Tube Patterning
Cells derived from AFE, including those comprising the lung, trachea, and thyroid, are of significant interest for many regenerative medicine and disease-modeling purposes.
SOX2 has been implicated in regulating the patterning of the foregut endoderm along the anterior-posterior axis and specifying AFE (Que et al., 2007).
Using our SOX2-GFP reporter line, we were able to monitor foregut endoderm differentiation and use flow cytometry to isolate a pure SOX2+ AFE population from differentiating cultures and perform subsequent genetic and developmental studies.
Using RNA-seq, we were able to identify a global gene expression signature that defines SOX2+ AFE cells in heterogeneously differentiating hPSC cultures.
Because AFE exists only transiently during in vivo development, our SOX2 reporter line allows for the in vitro study of a developmental stage that is difficult to analyze in vivo.
Finally, we demonstrated that subsequent in vitro differentiation of sorted SOX2+ cells led to the generation of cells that uniformly express NKX2.1, a transcriptional regulator of lung and thyroid development.
Together, this cellular platform will be useful for future studies examining the developmental and genetic programs that contribute to foregut, lung, and thyroid development.
Flow-cytometry-based purification of intermediate progenitor cell populations of differentiating hPSCs followed by subsequent differentiation is an alternative approach for generating highly enriched and well-defined mature cell populations required for cell-based therapies and disease modeling (McKnight et al., 2010).
Recently, transgenic cell marking combined with genome-wide expression profiling and flow cytometry have been used to develop flow-cytometry-based strategies for the purification of DE, immature cardiomyocytes, and pancreatic endoderm (Dubois et al., 2011; Kelly et al., 2011; Wang et al., 2011).
However, flow-cytometry-based strategies have not yet been developed for the purification of AFE.
Using our genome-wide expression analysis of reporter-expressing AFE cells, we identified two cell surface markers, CD56 (also known as NCAM) and CD271 (also known as NGFR), that permitted the isolation of SOX2+ AFE cells.
Although their names imply neural expression (and hence ectodermal origin), CD56/NCAM and CD271/NGFR are not completely restricted to the derivatives of the ectodermal germ layer.
For example, these cell surface markers also define cells of the mesodermal (Evseenko et al., 2010), mesenchymal (Saliem et al., 2012), and other nonneural lineages (Yuan et al., 2011).
In the future, this cell surface panel will allow for the prospective isolation and study of pure AFE cells from potentially any hPSC line.
In conclusion, we have developed a cell-based tool that will allow for the study of SOX2+ cells, not only in pluripotent hPSCs, but also in various endodermal and neural-related cell types.
Furthermore, this reporter cell line will enable high-throughput screening approaches to identify secreted factors or small molecules that promote endodermal or neural differentiation of hPSCs.
Finally, because SOX2 is one of the few regulatory genes expressed in both pluripotent and differentiated cells, future genetic and epigenetic analysis of the SOX2+ cell populations will allow for the identification of common mechanisms that control hPSC pluripotency and differentiation.
Experimental Procedures
Cells and Culture Conditions
Media compositions and sources for all cell lines are listed in the Supplemental Information section.
All hESC cultures were supplemented with 30 ng/ml FGF2.
Mouse embryonic fibroblast-conditioned medium (MEF-CM) was produced by culturing MEFs in hESC medium for 24 hr followed by sterile filtering.
Cells were routinely passaged with Accutase, washed, and replated at a density 4.25 × 104/cm2.
All work with hESCs was reviewed and approved by the University of California at San Diego (UCSD) Stem Cell Research Oversight Committee, project numbers 100210ZX and 090807ZX.
AAV Production and Gene Targeting
The design and construction of the SOX2-targeting vector is described in the Supplemental Experimental Procedures.
Supernatants carrying infectious AAV particles were produced as previously described (Hirata et al., 2002) with a detailed protocol available here: http://vectorcore.salk.edu/protocols/AAV Production Protocol.doc.
H9 cells on Matrigel in MEF-CM were infected with the SOX2-GFP AAV-2 supernatants at approximately 1010 genome copies.
After 24 hr, virus was removed and G418 (50 μg/ml) was applied for 2 weeks.
After 2 weeks, colonies were manually picked and transferred to fresh MEF feeder cells in 96-well plates.
Genomic DNA extracted from G418R clones was analyzed using Southern blot analysis with probes to the left (probe 1) and right (probe 2) homology arms.
Neural Differentiation and NPC Culture
Methods to derive and passage NPCs are described in detail in the Supplemental Experimental Procedures.
Briefly, embryoid bodies formed over 5 days in the presence of 50 ng/ml recombinant mouse noggin (R&D Systems) and 0.5 μM Dorsomorphin (Tocris Bioscience) were cultured in neural induction media.
After 7 days, neural rosettes were isolated, dissociated into single cells, and plated onto poly-L-ornithine (10 μg/ml) and mouse laminin (5 μg/ml)-coated dishes in neural induction media with 10 ng/ml mouse FGF2 and 10 ng/ml mouse EGF2.
Endodermal Differentiation
Methods to differentiate hESCs to endodermal derivatives are described in detail in the Supplemental Experimental Procedures.
Briefly, to generate DE, hESCs were cultured for 3 days in 100 ng/ml recombinant human Activin A with the first day supplemented with 30 ng/ml Wnt3a (Willert et al., 2003).
For differentiation of AFE, DE cells were treated for 5 days with 200 ng/ml noggin and 10 μM SB-431542.
For differentiation to LPCs, AFE cells were treated for 5 days with 100 ng/ml Wnt3a, 10 ng/ml mouse keratinocyte growth factor (KGF/FGF7), 100 ng/ml mouse FGF2, 10 ng/ml mouse BMP4, 10 ng/ml mouse FGF10, and 10 ng/ml EGF.
For differentiation to PFE, DE cells were cultured in 50 ng/ml KGF for 3 days and then in 50 ng/ml noggin, 0.25 μM 3-keto-N-(aminoethyl-aminocaproyl-dihydrocinnamoyl)cyclopamine, and 2 μM retinoic acid for 3 days.
qRT-PCR
RNA was isolated using TRIzol and reverse transcription was performed by means of qScript cDNA Supermix. qRT-PCR was carried out using TaqMan probes (Table S3) and TaqMan Fast Universal PCR Master Mix on a 7900HT Real Time PCR machine.
Gene expression was normalized to 18S rRNA levels.
All experiments were performed with three technical replicates.
IF and FC
Detailed protocols for IF and flow cytometry (FC) are provided in the Supplemental Information section.
For IF, cultures were in 4% (w/v) paraformaldehyde, permeabilized with 0.2% (v/v) Triton X-100, washed, and incubated overnight in primary antibody.
Secondary antibodies were incubated 1 hr.
All antibodies and dilutions are listed in Table S4.
Nucleic acids were stained for DNA with Hoechst 33342 (2 μg/ml).
Cells were imaged on an Olympus Fluoview 1000.
Image quantation was performed by counting a minimum of nine fields at 20× magnification.
For FC, cells were dissociated into single cells with Accutase, washed with fluorescence-activated cell sorting (FACS) buffer, resuspended at 5 × 106 cells per 100 μl, stained with indicated antibodies (Table S4), and analyzed and sorted with a FACSCanto or FACSAria2 (BD Biosciences).
FC data were analyzed with FACSDiva software.
For replating experiments, cells were stained with appropriate antibodies, sorted in FACS buffer, and replated with 10 nM Y27632.
Isotype negative controls are listed in Table S4.
For sorting experiments in which cells were separated on the basis of GFP expression, wild-type nonfluorescing cells were used as a negative control.
High-Throughput RNA-Seq
RNA-seq of RNA from SOX2-GFP+ and SOX2-GFP- AFE cells was performed as described in the Supplemental Information section, and differential gene expression analysis was performed with TopHat and Cufflinks (Trapnell et al., 2012, 2013).
Reads per kilobase of exon per million mapped reads (RPKM) were calculated for each gene and used as an estimate of expression levels.
Acknowledgments
We would like to thank Eric O'Connor of the UCSD/Sanford Consortium for Regenerative Medicine (SCRM) Human Embryonic Stem Cell Core for assistance with cell sorting and the SCRM Viral Vector Core for production of rAAV.
D.A.B. was supported by funding from the UCSD Stem Cell Program and a gift from Michael and Nancy Kaehr.
This research was supported in part by the National Institute of Diabetes and Digestive and Kidney Diseases Beta Cell Biology Consortium (5U01DK089567-02) and the California Institute for Regenerative Medicine (RT2-02064).
This work was made possible in part by the CIRM Major Facilities grant (FA1-00607) to the Sanford Consortium for Regenerative Medicine.
Supplemental Information
Supplemental Information includes Supplemental Experimental Procedures, four figures, and four tables and can be found with this article online at http://dx.doi.org/10.1016/j.stemcr.2013.09.005.
Supplemental Information
Document S1.
Supplemental Experimental Procedures, Figures S1-S4, and Tables S3 and S4Table S1.
RNA-Seq Data of GFP+ and GFP- Day 8 AFE Cells, Related to Figure 5Table S2.
RNA-Seq Data of Cell Surface Markers Differentially Expressed in GFP+ and GFP- Day 8 AFE Cells, Related to Figure 6

1. The power battery recycling waste from a method of production of a nickel cobalt manganate fprintf, characterized in comprising the steps of:
A nickel cobalt lithium manganate power battery dismantling waste into(1), the anode plate;
Grinding positive electrode sheet(2), pyrolysis, sieving, undersize collected;
An(3) undersize dissolved in an acid solution, filtering, the filtrate is containing nickel, cobalt, manganese, lithiated mixed solution;
A mixed solution of step(3) to(4) a nickel salt, cobalt salt, manganese, nickel, cobalt, manganese molar ratio of (1-3): (1-2): 1, then adding ammonia;
A mixed solution of several blocks inserted vertically into the(5) substrate step(4), heating to stand 85-90 °C 1h, distance between the substrates rejustification, stand 12-14 h homothermism, deposited on a substrate a layer of nickel cobalt manganese hydroxide, lithium ion containing solution is still in;
The substrate was removed(6), placed in water, after the ultrasonic oscillations, nickel cobalt manganese hydroxide freed from a substrate, the resulting suspension;
Filtering the suspension liquid(7), and drying the precipitate, nickel, cobalt and manganese hydroxide powder is obtained;
The(8) step(5) of the still sodium carbonate, sodium carbonate solution with a molar amount of lithium in mass ratio of (53-60 g): 1 µM, for 1h, precipitation, filtration, drying, to obtain a lithium carbonate powder;
Lithium carbonate is added to the(9) nickel cobalt manganese hydroxide powder, a mass ratio of the nickel, cobalt and manganese hydroxide with lithium carbonate (2.5-5): 1, mixed, calcined at 4h in 250 °C, and then heat to 600-700 °C, calcined 10-12 h, to obtain a nickel cobalt manganese oxide;
Step(4) 0.90 g/cm has a density of ammonia3, ammionia are added in a total molar ratio of metal elements in the solution (50-60) mL: 1 µM;
Step(5), a pitch between the substrates 0.5-1 CM start, a distance adjusted 2-4 cm.2. Redirects the circulating power waste by nickel cobalt lithium manganate battery prepared according to claim 1, characterized in: step(4) nickel salt is nickel sulfate, nickel chloride or one of nickel, cobalt salt is cobalt sulfate, cobalt chloride or one of cobalt nitrate, manganese salt is a manganese sulfate, manganese nitrate or one of manganese.3. The power battery prepared by nickel cobalt manganate redirects the circulating old method according to claim 1, characterized in: stief(5) conductive glass or silicon wafer substrate.Electrical Properties of Fe-doped Perovskite-like BaNb0.75-xFexNa0.25O3-δ (0.05<x<0.5)

A series of BaNa0.25Nb0.75-xFexO3-δ (BNF) compositions (0.05 < x < 0.5) were prepared by conventional solid state synthesis. Stoichiometric amounts of high purity (> 99.9%, Alfa Aesar) BaCO3, NaNO3, Nb2O5, and Fe2O3 were weighed and the mixtures were ball milled (Pulverisette, Fritsch, Germany) using zirconia balls in 2-propanol for 6 h. The mixture was calcinated at 1000 degC for 16 h, and the subsequent powder ball milled for 6 h. The powder was isostatically pressed and fired at 1200 degC for 24 h. As-prepared samples were characterized by powder X-ray diffraction (PXRD) (Rigaku powder X-ray diffractometer (Cu Kα, 40 kV, 40 mA)). The surface and morphology of the materials were evaluated using scanning electron microscopy (SEM) (Philips XL 30, The Netherlands). The chemical stability in 100% CO2 was investigated at 800 degC for 5 days and also refluxed in H2O for 5 days. The resultant products were investigated by PXRD and weight of the samples was measured before and after the stability tests.



Self-assembly of mono-crystalline NdF3 nanostructures during hydrothermal process
The NdF3 samples were obtained through a simple hydrothermal process. In a typical synthesis procedure, 3 ml of 1M NH4F was added to 10 ml of 0.1 M Nd(NO3)3 solution. The mixture was transferred into a Teflon-lined stainless-steel autoclave with 20 mL capacity. The sealed tank was heated in an oven to 160 degC and maintained there for 10 h, then taken out and cooled down to room temperature under ambiance condition. After the reactions, the composite samples were washed with de-ionized water and pure ethanol, respectively for several times and dried in vacuum at room temperature. Powder X-ray diffraction (XRD) patterns of the samples were recorded by RIGAKU-DMAX2500 X-ray diffractometer with Cu Ka radiation (λ = 0.15406 nm) at a scanning rate of 5deg/min for 2θ ranging from 5deg to 85deg. The microstucture, morphology, and chemical composition of the samples were examined by transmission electron microscope (TEM, JEM-2010) equipped with energy dispersive X-ray spectroscopy (EDS).A controlled anion exchange strategy to synthesize core-shell β-bismuth oxide/bismuth sulfide hollow heterostructures with enhanced visible-light photocatalytic activity
All chemical reagents in our experiments were analytical and used as received without further purification. The synthesis of β-Bi2O3 hollow spheres adopted our previous method [21]. In a typical experiment, Bi(NO3)3*5H2O (2.91 g, 6 mmol) was dissolved in a mixed solution of glycerol (30 mL) and ethanol (30 mL) under vigorous stirring. After 40 min of stirring, the solution was transferred to a 100 mL Teflon-lined stainless steel autoclave, heated to 160 degC and maintained for 3 h. After the autoclave was cooled to room temperature, the products were separated centrifugally and washed with deionized water and absolute ethanol for several times, and then dried under vacuum at 80 degC overnight. Finally, the products were further heated at 2 degC/min heating rate from room temperature to 270 degC and maintained at 270 degC for 2 h to obtain β-Bi2O3 hollow spheres.
The core/shell β-Bi2O3/Bi2S3 hollow heterostructure was synthesized by in situ anion-exchange method. In a typical process, the as-prepared β-Bi2O3 hollow microsphere (0.4660 g, 1 mmol) was dispersed in 50 mL deionized water under vigorous stirring. Then a certain amount CH3CSNH2 (0.0225 g, 0.3 mmol) was added into the solution. After 30 min of stirring at 50 degC, the precipitates were collected, washed and dried under vacuum at 80 degC for 6 h. By controlling the theoretical molar percentage of Bi2S3 to initial Bi2O3, different β-Bi2O3/Bi2S3 heterostructures were obtained. The final samples were labeled as the β-Bi2O3/Bi2S3 (5%, 10%, 25%) heterostructure.1. A nickel-based active material precursor for a lithium secondary battery, comprising:
a porous core; and
a shell having a radially arranged structure with a higher density than the porous core,
wherein the nickel-based active material precursor has a size of about 9 µm to about 14 µm, and
the porous core has a volume of about 5 % by volume to about 20 % by volume based on the total volume of the nickel-based active material precursor.2. The nickel-based active material precursor of claim 1, wherein
a maximum diffusion distance of lithium in the nickel-based active material precursor is 5 µm or less.3. The nickel-based active material precursor of claim 1, wherein
a porosity of the porous core is in a range of about 15 % to about 30 %.4. The nickel-based active material precursor of claim 1, wherein
a porosity of the shell is in a range of about 2 % to about 5 %.5. The nickel-based active material precursor of claim 1, wherein the nickel-based active material precursor comprises a plate particle, and
a major axis of the plate particle is radially arranged.6. The nickel-based active material precursor of claim 1, wherein the nickel-based active material precursor is a compound represented by Formula 1:
[Formula 1]     Ni1-x-y-zCoxMnyMzOH   wherein M in Formula 1 is an element selected from boron (B), magnesium (Mg), calcium (Ca), strontium (Sr), barium (Ba), titanium (Ti), vanadium (V), tungsten (W), chromium (Cr), iron (Fe), copper (Cu), zirconium (Zr), and aluminum (Al), and x≤(1-x-y-z), y≤(1-x-y-z), z≤(1-x-y-z), 0<x<1, 0≤y<1, and 0≤z<1 are satisfied.7. The nickel-based active material precursor of claim 6, wherein an amount of nickel in the nickel-based active material precursor is 1/3 to 0.95 mol% of the total amount of transition metals (Ni, Co, and Mn), and
the amount of nickel is greater than that of Mn and the amount of nickel is greater than that of that of Co.8. The nickel-based active material precursor of claim 1, wherein the nickel-based active material precursor is Ni0.6Co0.2Mn0.2(OH)2, Ni0.5Co0.2Mn0.3(OH)2, Ni1/3Co1/3Mn1/3(OH)2, Ni0.8Co0.1Mn0.1(OH)2, or Ni0.85Co0.1Al0.05(OH)2.9. A method of preparing a nickel-based active material precursor for a lithium secondary battery, the method comprising:
performing a first step of forming a porous core by reacting a mixture of a complexing agent, a pH regulator, and a metal raw material for forming a nickel-based active material precursor; and
performing a second step of forming a shell on the porous core of the first step, the shell having a radially arranged structure with a higher density than that of the porous core.10. The method of claim 9, wherein
a supply rate of the complexing agent and the metal raw material for a nickel-based active material precursor in the second step is reduced as compared to that in the first step.11. A nickel-based active material for a lithium secondary battery, obtained from the nickel-based active material precursor of any one of claims 1 to 8.12. A lithium secondary battery comprising:
a cathode comprising the nickel-based active material of claim 11;
an anode; and
an electrolyte between the cathode and the anode.Potential transgenic routes to increase tree biomass

Highlights
•
Increasing forest biomass has significant economic and environmental impact.
•
Many forestry objectives, e.g., more biomass, are best attained by transgenesis.
•
Transgenes enable plants to display one or a few traits for biomass applications.
•
Multi-trait engineering can generate a superior tree expressing most desired traits.
Abstract
Biomass is a prime target for genetic engineering in forestry because increased biomass yield will benefit most downstream applications such as timber, fiber, pulp, paper, and bioenergy production.
Transgenesis can increase biomass by improving resource acquisition and product utilization and by enhancing competitive ability for solar energy, water, and mineral nutrients.
Transgenes that affect juvenility, winter dormancy, and flowering have been shown to influence biomass as well.
Transgenic approaches have increased yield potential by mitigating the adverse effects of prevailing stress factors in the environment.
Simultaneous introduction of multiple genes for resistance to various stress factors into trees may help forest trees cope with multiple or changing environments.
We propose multi-trait engineering for tree crops, simultaneously deploying multiple independent genes to address a set of genetically uncorrelated traits that are important for crop improvement.
This strategy increases the probability of unpredictable (synergistic or detrimental) interactions that may substantially affect the overall phenotype and its long-term performance.
The very limited ability to predict the physiological processes that may be impacted by such a strategy requires vigilance and care during implementation.
Hence, we recommend close monitoring of the resultant transgenic genotypes in multi-year, multi-location field trials.

Introduction
Forests are rich sources of biodiversity and serve as sanctuaries to organisms under threat of extinction.
They help reduce surface runoff and their roots stabilize the soil to impede erosion.
Forests fix atmospheric CO2 into biomass and consequently mitigate climate change.
Wood, the primary renewable resource from forests, is widely used for energy, pulp and paper-making, textiles, and construction.
Forests also provide the raw materials for non-traditional products such as bioplastics, pharmaceuticals and chemical feed stocks.
Forest trees can flourish with comparatively little human intervention.
They are often relegated to marginal land that is considered unfit for general agriculture or horticulture.
Such areas may feature combinations of steep slopes, degraded soils, limited water, and/or nutrients.
Furthermore, there is pressure on forest resources due to growth in the human population and climate change.
Many forests are harvested unsustainably and global forest area is decreasing [1].
Hence, it is important to examine how tree growth can be improved to satisfy the demand for forest products.
Trees usually require several decades to reach harvest size, so there is considerable interest in fast-growing forest tree species or in ways to make trees grow faster.
Other key objectives of tree improvement in modern forestry include improved wood quality, pest resistance, adaptation to environmental stress, and delayed flowering or ablated reproductive organs [2].
The long reproductive cycle of trees and their extensive requirements for space and resources impede progress in the production of superior trees through traditional breeding and selection.
The intensification of environmental stressors (e.g., flooding, drought, extreme temperatures) associated with climate change complicate the problems faced by breeders.
Genetic engineering complements conventional breeding programs by providing a means to rapidly incorporate foreign genes or to manipulate endogenous genes to improve stress resistance or biomass yield.
Transgenic technology can improve elite genotypes chosen from current breeding programs.
Transgenic approaches to precocious flowering [3] could enable tree crop breeders to use the power of inbreeding followed by selection to purify genomes of deleterious alleles within a reasonable period.
Scientists have successfully manipulated various aspects of tree architecture, photosynthetic processes, and photosynthate assimilation to improve the quantity and quality of biomass.
Increasing sink strength by accelerating downstream processes related to the utilization or storage of photosynthetic products have also significantly improved biomass yield.
Delaying reproductive growth has significantly increased vegetative biomass in annuals and some reports indicate that this may be possible in trees as well [4,5].
Transgenic approaches have improved the ability of plants to survive or thrive even in the presence of various biotic and abiotic stress factors, many of which are likely to be aggravated by climate change.
Some biotic factors, such as weed competition, affect trees only at the seedling/sapling stages, while others, like insect and pathogen infestations, can damage trees at any stage in their life cycle.
Introduction of genes for resistance to abiotic stress such as low temperatures, osmotic extremes (salinity, drought) and heavy metal contamination, have expanded the range of potential sites for cultivation of fast-growing trees to these inhospitable environments.
Most researchers have concentrated on the effects of single genes, but there are synergistically greater opportunities in pyramiding or "stacking" genes that combine superior phenotypes into an individual tree.
We believe that the base of this transgenic pyramid must consist of genes proven to increase biomass, complemented by those designed to offset stress factors that suppress biomass yield.
"Gene stacking" can be done sequentially in annual crop species, by intercrossing stable transgenic lines that over-express different genes.
In trees, simultaneous introduction of many genes by multi-trait engineering, as we discuss in Section 9, would likely be a faster approach than crossing transgenic lines to endow trees with the various characteristics needed to thrive in the forest.
This review focuses on transgenic research that has proven useful in substantially increasing tree biomass or protecting tree species from losing biomass due to stress.
It is divided into sections that deal with transgenic research on crown and root architecture, reproduction, and various stress factors that affect biomass yield.
We propose an approach to the creation of high-yielding transgenic trees resistant to multiple stress factors.
Source and host species, gene identities and functions, as well as their effects on biomass and other traits are presented in tables throughout the manuscript.
Findings validated with field-test data are highlighted in these tables with asterisks.
Readers interested in transgenic approaches to major downstream applications of forest biomass (timber, pulp, biopolymers and bioenergy) may refer to recent reviews on these topics [6-9].
Crown architecture
The components of crown architecture (plant height, branching pattern, foliar arrangement, and morphology) determine the efficiency by which the plant can harvest solar energy.
Taller plants with more vertical branches/leaf orientations capture more solar energy and they can be planted at greater densities for improved yield per unit area.
The genes that are useful for the modification of crown architecture for single-stem and coppiced systems are summarized in Table 1.
The ideal shoot architecture depends on whether the tree is destined for single-stem cultivation or for coppicing.
In single-stem cultivation, the biomass yield is dependent on the rate and extent of vertical stem growth.
For poplar grown in coppice systems, the primary determinants of plant yield [10] are the ability to generate epicormic shoots from buds that lie underneath the bark and sylleptic branching, which is the ability for branches to develop from lateral buds during a single growing season [11].
In both systems, trees need to grow and quickly produce a dense canopy to capture solar energy and shade out competing vegetation.
Optimal utilization of solar energy requires a specific plant architecture that allows photosynthesis throughout the canopy instead of only the top leaves of the canopy [12].
This involves effective manipulation of plant height, branching, foliar arrangement, and leaf morphology.
Broad-leaved angiosperms should feature large and vertically oriented leaves in the upper crown, and rapid abscission of dead branches to allow light penetration into the inner canopy and, in the case of poplars and aspens (Populus spp.), sylleptic branching.
In hybrid poplar (Populus trichocarpa×P. deltoides), sylleptic branching results in the production of more branches, more leaves and expanded photosynthetic capacity that supports the overall growth and biomass yield of the tree at a young age [11].
Populus species are often cultivated in coppice systems, so efficiency in regenerating the harvested shoot biomass is required for high yield.
In contrast, the "crop ideotype" for loblolly pine features a wide crown with thick tufts of leaves borne on long side branches angled at 49° from the tall main stem [13].
Trees with these canopy traits have higher light capture potential.
Tree height
Tree height is an important component of shoot architecture and it is associated with biomass yield [14].
Manipulation of endogenous hormone levels is effective in modifying tree heights.
In general, brassinosteroids and gibberellins have the most direct effects on plant height with the fewest negative side effects on the overall phenotype.
Increasing the biosynthesis and/or signal transduction of these hormones, or conversely, decreasing their conversion to inactive storage forms or catabolic breakdown can produce taller plants and, therefore, more biomass [15].
Brassinosteroids
Brassinosteroids are polyhydroxylated steroid hormones that are essential for plant growth, reproduction, and responses to various abiotic and biotic stresses [16].
Arabidopsis thaliana plants that are unable to synthesize or perceive brassinosteroids are dwarfs with rounded leaves and reduced pollen fertility that show significantly delayed flowering time and leaf senescence [17] (Table 1).
STEROID 22a HYDROXYLASE, encoded by the DWF4 gene, is a key enzyme for brassinosteroid biosynthesis.
Over-expression of DWF4 in A. thaliana produced plants that grew 35-47% taller and produced 33% more seed [17].
The rice mutant dwf4-1 had depressed levels of brassinosteroids and an ideotype characterized by slight dwarfism and erect leaves.
Although individual dwf4-1 plants had reduced biomass yield, their ideotype allowed high-density planting that led to increased grain yield per unit area [18].
The effects of such genes on modifying tree ideotypes still remain unexplored.
Gibberellins
Gibberellins are a large family of diterpenoid compounds that are involved in various aspects of plant growth and development.
In reference to tree height, gibberellins promote cell elongation and cell division and, consequently, stem elongation.
Gibberellins are also required during xylogenesis in the cambium and fiber elongation in the developing xylem [19].
Hence, increasing gibberellin content or activating pathways for gibberellin signaling should increase plant height.
Biosynthesis of gibberellins depends, to a large part, on the activity of the enzyme GA20 OXIDASE, encoded by the GA20OX gene.
Over-expression of GA20OX enhanced biomass production through increased height and girth of the main trunk in hybrid aspen (Populus tremula×P. tremuloides) (Table 1)[20].
Carrizo citrange (Citrus sinensis×Poncirus trifoliata) that over-expressed GA20OX were taller and displayed longer thorns, a sign of juvenility.
In contrast, suppression of GA20OX by gene silencing generated dwarf and bushy phenotypes with shorter thorns and larger, thicker leaves [21].
The activity of gibberellin biosynthetic genes is subject to complex feedback regulation that can nullify efforts to increase endogenous gibberellin levels or be negated by an excessively high activity of the gibberellin deactivating enzyme, GA2 OXIDASE and its encoding gene, GA2OX [22].
Over-expression of GA20OX from Populus trichocarpa in hybrid poplar (P. tremula×P. alba) conferred early shoot regeneration and shoot growth in vitro but the growth effect disappeared after the plants were transferred to the greenhouse [23].
It is therefore essential to verify that the biomass gains observed in vitro or in the greenhouse can be replicated in field trials.
Transcripts of GA20OX from A. thaliana were detected in shoots of transgenic six-month-old hybrid aspen (P. tremula×P. tremuloides) but not in shoots of three-year-old trees.
By contrast, GA20OX transcripts were present in leaves at both growth stages [24].
The disappearance of transcripts from shoots suggests some form of temporal and/or developmental tissue-specific silencing.
Nevertheless, it seems that the residual activity of GA20 OXIDASE produced in the leaves was still sufficient to support superior stem growth: 17-week-old transgenic trees were 1.9 times taller than the wild-type plants [24].
Hybrid poplar (P. tremula×P. alba) that over-expressed genes that promoted gibberellin catabolism (GA2OX) or interfered with GA signaling (gai (GA-INSENSITIVE), rgl (REPRESSOR OF GA1-LIKE)) showed a range of dwarf phenotypes with various reductions in height, branch length, internode length and leaf length, although stem diameter was less affected [25].
However, it has also been reported that over-expression of GAI in hybrid poplar (P. tremula×P. alba) produced semi-dwarf trees that accounted for a higher proportion of total biomass when compared to wild type [26].
High-density planting with such semi-dwarf plants may lead to higher yields per unit area, analogous to the effect of dwf4-1, a mutant rice (Oryza sativa) line with a defective gene for brassinosteroid synthesis (see Section 2.1.1).
Over-expression of GA2OX produced hybrid poplar (P. tremula×P. alba) with small, dark green leaves whereas those that over-expressed rgl exhibited a compact crown with normal-sized leaves [25].
The GA2OX gene family has members with tissue-specific expression that give differential results after molecular manipulation [22].
Over-expression of shoot-specific paralogs of GA2OX generated dwarf hybrid poplars (P. tremula×P. alba) whereas their suppression enhanced biomass growth without adversely affecting rooting.
Suppression of root-specific GA2OX by RNA interference (RNAi; a process by which cellular enzymes recognize and destroy double-stranded RNA and homologous single-stranded mRNA, which leads to post-transcriptional gene silencing) reduced root biomass [22].
Gibberellin levels are determined by genes that encode positive regulators such as GIBBERELLIN INSENSITIVE DWARF 1 (GID1) [27] or negative regulators such as GA-INSENSITIVE (GAI) and REPRESSOR OF GA-1 (RGA) [25].
Hybrid poplar (P. tremula×P. alba) over-expressing gai (mutant gene with a 51-bp in-frame deletion) had narrow compact crowns with shorter main stems and branches than the wild type.
By contrast, those that expressed the wild-type form of the gene (GAI) had phenotypes that were not significantly different from the non-transgenic controls, except for early and high frequency of flowering (Table 1) [25].
DELLA proteins, which contain an N-terminal DELLA (asp-glu-leu-leu-ala) domain essential for GA-dependent proteasomal degradation, repress GA responses [28].
The GID1-GA complex down-regulates DELLA repressor proteins, consequently stimulating plant growth and development [29].
Hybrid aspen (P. tremula×P. tremuloides) over-expressing GID1 were about 40% taller than the wild type after nine weeks in the greenhouse [19].
Auxins
Auxin activity is strongly associated with apical dominance.
Over-expression of the biosynthetic genes for auxin from A. tumefaciens in hybrid aspen (P. tremula×P. tremuloides) unexpectedly produced slow-growing, semi-dwarf plants [30].
Hybrid poplar (P. tremula×P. alba) over-expressing GS1a, the gene encoding GLUTAMINE SYNTHETASE from Pinus sylvestris, showed high levels of indole acetic acid (IAA) in leaves, indicating that the additional IAA may be contributing to the enhanced growth of the transgenic trees [31].
β-Glucoside esters or ether conjugates of plant hormones such as auxins, gibberellins, cytokinins, and ABA are inactive storage forms that are activated through hydrolysis by β-GLUCOSIDASE.
Tobacco (Nicotiana tabacum) plants that over-expressed BGL1, the gene encoding β-GLUCOSIDASE, in their chloroplasts had higher levels of auxin, gibberellin, and zeatin, and almost double the biomass of the untransformed controls [32] (Table 1).
This gene could be a prime candidate for testing in tree species because there are very few genes that have the potential to double the size of trees.
Cytokinins
Induction of high levels of cytokinins by transgenesis often leads to undesirable pleiotropic effects in plants (see Section 2.4.2).
Tobacco plants that over-expressed GOLS2 (a cold tolerance gene from A. thaliana) fused to a gene for ISOPENTENYL TRANSFERASE (ipt), were ∼30-50% taller and had greener leaves than the wild type.
They also had increased cold tolerance and normal root systems.
Apparently, the presence of the termination codon of the GOLS2 gene and the intervening nucleotides between the two transcriptionally fused genes reduced the efficiency of translation of the ipt gene [33].
The resultant phenotype was due to over-expression of the GOLS2 gene and moderate expression of the ipt gene.
Leaf development
Photosynthetic productivity can be improved by manipulating leaf number, shape, size, inclination, and distribution of the branches in a canopy.
Lower levels of brassinosteroids in the dwf4-1 rice mutant were associated with erect leaves that improved light interception and allowed high-density planting [34].
Over-expression of the endogenous DWF-4 in A. thaliana had no effect on leaf angle [17].
This indicates, among other possibilities, differences in the regulation of leaf angle in monocots and dicots.
Increased gibberellin signaling produced hybrid aspen (P. tremula×P. tremuloides) that bore upward-pointing leaves [19] (Table 1), although this effect has not been reported in other attempts to modify gibberellin signaling.
Hybrid aspen (P. tremula×P. tremuloides) that over-expressed GA20OX had larger leaves and longer internodes [20].
The GA2OX family in poplar includes 11 members with various patterns of tissue-specific expression [22].
Down-regulation of GA2OX4 and GA2OX5 by RNAi in hybrid poplar (P. tremula×P. alba) increased the number and size of leaves and, consequently, shoot biomass without adversely affecting root development [22].
Hybrid poplar (P. tremula×P. alba) that over-expressed GA2OX from Phaseolus had dwarf phenotypes with smaller leaves and shorter branches [25].
Hence, it is possible to manipulate canopy development by controlling tissue-specific genes for GA catabolism.
Radial growth and xylogenesis
The most important forestry product is wood, which is essentially the secondary xylem in the tree trunk that conducts water and nutrients absorbed by the roots to the leaves.
Secondary xylem deposition (and thereby radial growth) is governed by the meristematic activity of the vascular cambium, which is found in all woody dicots and gymnosperms.
Lending support to the idea that effects on wood-like traits from transgenic model species can be successfully applied to woody species are the extensive parallels not only in genes that affect cambial development in A. thaliana and trees (reviewed in [35]), and the similarities in wood-like properties between Arabidopsis inflorescence stems and wood [36].
Furthermore, modification of genes to alter hormone levels or signal mechanisms that regulate cambial growth has led to increased biomass.
Phytohormones
Auxin is involved in the induction of vascular tissue development which is the basis for wood formation [37,38].
However, auxin is well-known to affect many other aspects of plant development.
Exogenous auxin represses sylleptic branching while cytokinin promotes it in hybrid poplar (P. trichocarpa×P. deltoides) [11].
Suppression of branching should help reduce the production of multiple stems in species grown in single-stem culture systems (see Section 2.4).
Over-expression of a cytokinin catabolic gene, CKX2 for CYTOKININ OXIDASE (from A. thaliana), compromised radial growth which was evidenced by a reduced number of cambial cell layers in hybrid aspen (P. tremula×P. tremuloides) [39].
Cytokinins promote axillary bud break and, consequently, branching, but high cytokinin levels inhibit root elongation [40].
The adverse effects of overproduction of cytokinins may be mitigated by using tissue-specific promoters or gene fusions to moderate the effect of strong promoters.
For example, A. thaliana over-expressing the ipt gene fused to the 3′ end of the GUS (β-GLUCURONIDASE) gene had normal rooting and double the biomass of wild-type plants (see also Section 2.1.4) [33].
Manipulation of ethylene does not seem to be a promising approach for increasing biomass in trees.
Exposure to an ethylene precursor, aminocyclopropane carboxylate, stimulated cell division in the cambium but inhibited vertical growth and radial expansion of fibers and vessel elements in hybrid aspen (P. tremula×P. tremuloides) [41].
Ethephon is converted into ethylene in plant tissues.
Application of ethephon dissolved in lanolin on bark inhibited lignification of tracheids in the underside of tilted seedlings of Japanese red pine (Pinus densiflora) [42] and interfered with xylem lignification in stems of Monterey pine (Pinus radiata) saplings (Dubouzet, J.G., Donaldson, L., Black, MA, McNoe, L., Liu, V., Lloyd-Jones, G., unpublished data).
Intracellular signaling
SHORT-ROOT1 (SHR1) is a putative transcription factor that negatively regulates cell division and meristem activity in both shoots and roots.
Partial suppression of SHR1 in hybrid aspen (P. tremula×P. tremuloides) led to significantly higher stem biomass [43] (Table 1).
Mutation of a WRKY12 transcription factor in A. thaliana activated the biosynthesis and deposition of cellulose, lignin and xylan into secondary cell walls, consequently increasing biomass density by 50% and shoot biomass by 25% [44].
Hence, RNAi-mediated suppression of WRKY12 orthologs in tree species may increase biomass deposition as well.
Autophagy is an autolytic process triggered at xylem maturation that is crucial to the formation of the hollow, water-conducting tracheids or vessel elements in plants [45].
A Rab GTPase, RabG3bCA, is a positive regulator of autophagy, a catabolic mechanism for the degradation of non-functional cellular components that is associated with programmed cell death.
Ten-week-old hybrid poplars (P. alba×P. tremula var.
glandulosa) over-expressing RabG3bCA exhibited enhanced xylem development, increased stem growth, and double the dry biomass of wild-type control plants.
The transgenic lines also exhibited altered wood composition in reference to polysaccharide composition, with 10% more cellulose and 25% less xylan [46].
Over-expression of RabG3bCA had no effect on lignin content.
Branch development
In addition to tree height, branch diameter and length are the most important determinants of wood production and maximum leaf area index in Populus [47].
CORNGRASS 1 (miR156) is a microRNA that is involved in the regulation of juvenility and flowering.
Constitutive expression of CORNGRASS 1 increased sylleptic branching and leaf number at the expense of height and lignin content in the stems of hybrid poplar (P. tremula×P. alba) [5] (Table 1).
Over-expression of RELATED TO ABI3 AND VIVIPAROUS 1 (RAV1) from Castanea sativa led to precocious development of sylleptic branching in transgenic poplar.
Early sylleptic branching in hybrid poplar (P. tremula×P. alba) is correlated with increased branching and greater biomass accumulation as trees mature [48].
Auxin and apical dominance
In general, apical buds of plants produce auxin that inhibits the growth of sub-apical lateral buds.
Removal of the apical bud leads to lateral bud break and shoot proliferation but this can be inhibited by application of exogenous auxin [49].
Auxin may play a repressive role in sylleptic branching in hybrid poplar (P. trichocarpa×P. deltoides) [11].
In single-stem cropping systems, pruning is needed to produce higher-quality clear wood that is free of knots.
Widely spaced Sitka spruce (Picea sitchensis) tend to have bigger knots (from large branches) and more crooked stems (due to extensive branching), thus producing timber biomass of lower quality.
Hence, high-density planting is favored in Sitka spruce because it suppresses the development of lateral branches [50].
The rol A, B, and C genes from Agrobacterium rhizogenes affect flower morphology, rooting (and auxin sensitivity), and branching in transformed plants, respectively [51].
Hybrid walnut (Juglans hindsii×J. regia) over-expressing rolABC had denser and bushier canopies due to reduced internode lengths and increased lateral branching [51].
Hybrid aspen (P. tremula×P. tremuloides) plants expressing the rolABC genes had reduced apical dominance but the synergistic growth or counterbalancing effects of the simultaneous expression of three transgenes dramatically increased stem biomass [52] (Table 1).
Cytokinins
Cytokinin activity is essential for wood formation from the vascular cambium [10].
The rolC gene from A. rhizogenes has been implicated in cytokinin metabolism.
Hybrid aspen (P. tremula×P. tremuloides) that over-expressed rolC had higher cytokinin levels and were characterized by stunted growth, short internodes, and smaller leaves [53].
ISOPENTENYL TRANSFERASE (ipt) catalyzes a rate-limiting step in cytokinin biosynthesis in A. tumefaciens.
Hybrid poplar (P. trichocarpa×P. deltoides) over-expressing ipt had improved sylleptic branching, increased branch frequency but poor rooting [40].
As discussed in Section 2.1.4, the negative effects of increasing cytokinin levels via transgenesis may be ameliorated by using gene constructs that moderately increase cytokinin production.
Strigolactones
Strigolactones, a novel class of phytohormones, are carotenoid derivatives that are implicated in plant branching.
Strigolactones and cytokinins play opposite roles in the control of bud outgrowth.
Their interaction with auxin, an inhibitor of lateral bud growth, is responsible for the plasticity of axillary branching and plant architecture [54].
Mutants of A. thaliana with defective carotene degradation due to non-functional carotene dioxygenases such as CAROTENOID CLEAVAGE DIOXYGENASE 7 and 8, had low levels of strigolactones, enhanced shoot branching, dwarf habit, and poor rooting [55].
CAROTENOID CLEAVAGE DIOXYGENASE 8 is involved in the synthesis of strigolactones in plant roots.
Suppression of the endogenous CAROTENOID CLEAVAGE DIOXYGENASE 8 gene in kiwifruit (Actinidia chinensis) increased branch development and delayed leaf senescence [56].
Hence, the biosynthetic pathway for strigolactones may be an attractive target for RNAi in poplar.
A possible effect of the down-regulation of strigolactone synthesis, implied by research using mutants for strigolactone biosynthetic genes [55], could be improved lateral root formation and nutrient acquisition from the soil.
Additionally, based on the aforementioned phenotypic effects of silencing the genes for strigolactone biosynthesis, over-expression of these genes may reduce shoot branching and, consequently, pruning requirements in single-stem culture.
As such, these genes are interesting targets for further research.
Root architecture and plant hormones
Root architecture (diameter, angle, and branching) is key to a plant's ability to support its shoot canopy and access nutrients and water in the rhizosphere.
Larger root systems are arguably more effective in nutrient and water uptake and, consequently, more advantageous when such resources are limiting.
However, actively diverting resources to favour root growth via transgenesis can lead to poor shoot development [26].
The genes that have been shown to modify root architecture and function are listed in Table 4.
Young hybrid aspen (P. tremula×P. tremuloides) over-expressing GA20OX had poorer rooting than the control but this negative effect disappeared in older plants [20].
Some GA2OX paralogs in poplar have tissue-specific expression, which may be important with regard to selection of the target gene for manipulation.
Over-expression of GA2OX in hybrid poplar (P. tremula×P. alba) increased rooting ability but retarded shoot growth [57] whilst suppression of GA2OX4 and GA2OX5 promoted above-ground stem and leaf biomass [22].
In A. thaliana, GA INSENSITIVE is a negative regulator of GA signaling.
As discussed in Section 2.1.2, over-expression of this gene in hybrid poplar (P. tremula×P. alba) produced semi-dwarf trees.
Interestingly, these trees had larger root systems that accounted for a higher proportion of the total biomass as compared to wild type [26].
Similarly, induction of cytokinin degradation by over-expression of CYTOKININ OXIDASE/DEHYDROGENASE 3 (CKX3) led to enhanced root growth but poor shoot development in A. thaliana [58].
Genes that actively reduce GA or cytokinin levels by catabolism or signaling may be more effective if controlled by root-specific promoters.
There are numerous instances of constitutive over-expression of biosynthetic genes for cytokinins that led to poor or no rooting (see example in Section 2.4.2).
Instead of constitutive expression, targeted expression using tissue-specific promoters may attenuate their possible negative effects on the rest of the plant, analogous to the natural production of cytokinins and strigolactones in roots and auxin in meristems and distribution of these hormones through the rest of the plant.
Water-use efficiency
Inefficient use of available water is a major factor that depresses biomass yield.
Careful site selection is a prerequisite to successful silviculture but the acceleration of global climate change and the resultant weather disturbances portend that many forest plantations will undergo water stress in their cultivation cycle.
Water-use efficiency (WUE) is the ratio between biomass production and water consumption [59].
Over-expression of GA2OX produced small hybrid poplar (P. tremula×P. alba) plants with dark green leaves that had increased chlorophyll content and increased WUE [26].
However, as discussed in Section 3, the increased biomass in these trees was directed in a greater proportion to roots, which is less desirable from a biomass perspective than improved stem yield.
The ERECTA gene is a leucine-rich repeat receptor-like kinase with a key role in WUE signaling: A. thaliana over-expressing ERECTA from poplar had lower stomatal density, and consequently, higher WUE, lower transpiration rates, and superior growth in both irrigated or drought conditions.
Over-expression of this gene may be useful in generating high-yielding and drought-resistant poplar and other tree species [59].
Aquaporins are proteins that facilitate the flow of water and small molecules across cell membranes.
Over-expression of PIP2, an aquaporin from radish (Raphanus sativus), improved photosynthesis and increased shoot growth in Eucalyptus camaldulensis by 25% [60].
The transgenic plants had lower transpiration rates, but they were not tolerant to drought, possibly because they produced 115% more leaves (i.e., larger transpiring surface) than controls.
Late embryogenesis abundant proteins are associated with various stress conditions (including drought) in plants.
Over-expression of hva1, a gene coding for a late embryogenesis abundant protein from barley (Hordeum vulgare), produced mulberry (Morus indica) plants that grew normally with leaves that tolerated salinity and physiological drought induced by polyethylene glycol (PEG) [61].
These plants also had better photosynthetic yield under salinity and water stress than wild-type plants.
A GSK3/SHAGGY-LIKE protein kinase (GSK1) from A. thaliana is associated with tolerance to water and salt stress.
Over-expression of GSK1 in hybrid poplar (P. alba×P. tremula var.
glandulosa) generated plants that had better growth and higher rates for photosynthesis and transpiration than wild-type plants.
The transgenic plants also grew well under PEG-induced water stress whereas the wild-type plants showed chlorotic and necrotic symptoms [62].
Even in the absence of drought, plants suffer daily from water stress leading to stomatal closure and cessation of photosynthesis even before midday [63] and from intermittent water stress when rainfall is not well-distributed throughout the growing season.
Transgenesis using some of the genes discussed in this section have the potential to confer superior drought-coping mechanisms unto forest trees to improve survival in the face of unstable water supply in the field.
Mineral uptake and utilization
Nitrogen (N), phosphorus (P) and potassium (K) are the major essential elements required for plant growth but many soils are deficient in one or more of these nutrients.
With transgenesis, it is possible to engineer plants that feature more efficient uptake and/or utilization of specific minerals to obtain the best yield from sites with nutrient limitations.
Some genes associated with nutrient utilization that have significant effects on biomass are presented in Table 2.
Nitrogen
Nitrogen-deficient soils are widespread and, consequently, N fertilizers account for the largest portion of fertilizers used in forestry.
Nitrogen fertilization significantly increased stem and whole-plant biomass of P. trichocarpa×P. deltoides [64].
Nitrogen-use efficiency (NUE), or the ratio between biomass yield over input N [65], is partly determined by the ability of the plant to capture and utilize available nitrogen.
Research has centered on the elucidation of soil nitrogen uptake, assimilation of inorganic nitrogen into nitrate and ammonium, and the biosynthesis and recycling of various forms of organic nitrogen [66].
The number of ammonium ions released by photorespiration can be up to 10 times higher than that obtained by plants through primary nitrogen assimilation [67].
It is essential to recycle the ammonium released in the cytosol by various cell processes to improve nitrogen-use efficiency.
Various forms of glutamine synthetase participate in ammonia assimilation in plants.
In angiosperms, cytosolic GLUTAMINE SYNTHASE 1 (GS1) assimilates ammonium obtained from the soil and other metabolic processes [68].
Ammonium released by nitrate reduction or photorespiration is assimilated into glutamine by GLUTAMINE SYNTHASE 2 (GS2) [69].
Gymnosperms evolutionarily predate angiosperms by several hundred million years and their GLUTAMINE SYNTHASE enzyme variants have functions that encompass those performed by GS1 and GS2 in angiosperms [68].
Hybrid poplar (P. tremula×P. alba) over-expressing the cytosolic GS1a from Scots pine (P. sylvestris) displayed considerable improvements in biomass production [70], timber quality, and pulping ability [71].
The increased biomass was presumably due to improved nitrogen uptake and recycling that would otherwise have been lost during normal plant metabolism.
The angiosperm-specific GS2 enzyme is a more evolutionarily recent form of GS, and is specifically adapted for intracellular ammonium recycling in the chloroplasts.
Therefore, ectopic production of GS2 in gymnosperms may have significant effects on nitrogen assimilation, photorespiration, and, consequently, biomass yield.
GLUTAMATE DEHYDROGENASE catalyzes the amination of 2-oxoglutarate in glutamate synthesis.
Microbes such as Aspergillus nidulans rely on GLUTAMATE DEHYDROGENASE for nitrogen assimilation [72] whilst plants use GLUTAMINE SYNTHETASE for the same process.
Potato (Solanum tuberosum) over-expressing gdhA, a microbial gene for GLUTAMATE DEHYDROGENASE, had higher NUE and increased rates of nitrogen and carbon redistribution to sink tissues.
The increase in sink strength was associated with increased photosynthetic rates, and, consequently, a 10% increase in total biomass [72].
ALANINE AMINOTRANSFERASE increases glutamate utilization by catalyzing the synthesis of alanine from pyruvate and glutamate.
Canola (Brassica napus) that over-expressed AlaAT, the gene for ALANINE AMINOTRANSFERASE from barley, required 40% less N fertilizer to achieve yields equivalent to wild-type plants grown with normal N fertilization [73].
Deployment of these genes in trees may help reduce the requirements for N supplementation in forestry.
Phosphorus
Phosphorus (P) is often a growth-limiting factor in highly weathered and acidic volcanic soils worldwide.
Furthermore, mineral P fertilizers are rapidly transformed into poorly soluble mineral salts or organic molecules that are not immediately available to plants.
Phytates (phosphate esters of inositol) account for up to 80% of soil organic P, but plants have limited ability to directly obtain P from this substrate [74,75].
PHYTASE, a phosphatase exuded from plant roots or soil microbes, hydrolyzes soil phytates, thus releasing phosphate ions that can be absorbed by plant roots.
Over-expression of PHY1, the gene encoding PHYTASE from Medicago truncatula, trebled the shoot biomass of transgenic alfalfa (Medicago sativa) fertilized with organic P in sand culture.
When grown in natural soils with no supplemental P, the transgenics generated double the biomass of the control, presumably by mobilizing P from soil organic matter [76].
Hence, transgenic deployment of phytase genes can minimize the need for supplemental P in sites with abundant organic material.
The effectiveness of various phytases in transgenic plants can be improved by increasing the levels of organic acids (citrate, malate, oxalate), which increase the solubility of precipitated phytates in the soil [77].
Over-expression of the genes for the synthesis of these organic acids have led to inconsistent results in plants [78] but it seems quite possible to obtain a synergistic effect from the simultaneous root-specific production of PHYTASE and organic acids in tree species.
Potassium
Potassium (K+) is the most abundant cation in living plant cells where it plays critical roles in various aspects of metabolism and growth [79].
Membrane transporters facilitate the movement of K+ through plant membranes, but despite the large number of investigations into these genes, their effects on biomass remain largely unexplored.
The roles of K+ transporters like HAK5 and AKT1 have been elucidated primarily in A. thaliana mutants, where it was found that HAK5 was responsible for uptake at low K+, whilst AKT1 was most active at high K+ concentrations [79].
Antiporters mediate the exchange of two or more molecules/ions across a membrane.
A proton (Na+/H+) antiporter gene from A. thaliana was over-expressed in poplar (Populus×euramericana 'Neva') to enhance salt tolerance.
Under low or high salt (NaCl) concentrations, the transgenic poplar continued to grow well and accumulated more K+ than the wild-type plants.
The transporter apparently sequestered excess Na+ into vacuoles and improved K+ absorption [80].
This gene may be useful for plants destined for K+-deficient soils.
Sulfur
Sulfur is an essential element that is a major constituent in some plants.
Glutathione accounts for a significant portion of sulfur content in plants.
It has important roles in sulfur storage and transport and in the response of the plant to abiotic stress.
γ-GLUTAMYLCYSTEINE SYNTHETASE is a key enzyme in glutathione biosynthesis.
Over-expression of GSH1, the gene for γ-GLUTAMYLCYSTEINE SYNTHETASE, in poplar led to poor growth, but the lines with the lowest transgene activity displayed increased growth (height and weight) and better photosynthetic rates than the wild-type plants [81].
Attenuating transgene activity by using less powerful promoters or gene fusion, for example, may lead to similar results.
Photosynthesis
Increasing the photosynthetic efficiency or capacity or recapturing losses from photorespiration should improve biomass yield.
There are many factors that determine CO2 uptake in crop plants through photosynthesis.
These include specific aspects of the photosynthetic machinery, carbon flux, photorespiration, photoinhibition, assimilate partitioning, and assimilate utilization.
Genes that have been used to improve photosynthesis and assimilation of photosynthate are listed in Table 3.
Photosynthetic machinery
Most plants are classified as C3 plants because they generate a three-carbon molecule (3-phosphoglycerate) as the first stable product of photosynthesis.
Some plants, classified as C4, have a more efficient process that produces malate, a four-carbon molecule, as the first stable product of photosynthesis.
Attempts to directly manipulate the photosynthetic machinery (Table 3) by importing C4 genes into C3 plants [82], modifying RuBisCO [83], or RuBisCO ACTIVASE [84] have generated mixed results with few practical applications.
This is presumably because C4 plants, in addition to their differences in metabolism, possess anatomical features (i.e., bundle sheath cells) that are absent in C3 species.
Thus, engineering of C4 metabolism in C3 plants may require the engineering of the entire developmental pathway for the construction of bundle sheath cells.
Manipulating carbon flux in the Calvin cycle
SEDOHEPTULOSE-1,7-BISPHOSPHATASE (SBPase) is involved in the generation of pentose sugars.
It has been shown to be a major control point in the Calvin cycle for CO2 fixation in plants [85].
Transgenic tobacco plants that expressed FBP/SBPase, a gene for FRUCTOSE-1,6/SEDOHEPTULOSE-1,7-BISPHOSPHATASE (from cyanobacteria), in their chloroplasts had 40-80% greater photosynthetic activity and, consequently, 80% more dry matter than wild-type plants [86].
The ability of similar genes to improve photosynthesis in tree species should be investigated as well.
Carbon concentration mechanisms
RuBisCO evolved before the development of photosynthesis, under CO2 conditions that were much higher than current levels [87].
Hence, many plants flourish at higher CO2 concentrations [88] and warmer temperatures, so they might be expected to thrive in the future higher CO2 environment that has been predicted with continuing climate change.
An eight-year study of a deciduous forest confirmed that carbon-enrichment increased net photosynthesis by 42-48% relative to controls [89].
Similarly, free-air enrichment by supplementary CO2 in field plots increased biomass yield by 15-27% in three poplar genotypes (P. alba, P. nigra, and Populus×euramericana) [90].
Mechanisms that concentrate cellular CO2 around RuBisCO are found in C4 or CAM (crassulacean acid metabolism) plants [91] that are known for their high capacity to produce biomass.
Trees with the C3 photosynthetic machinery have benefitted from alternative means of concentrating carbon around their RuBisCO units (see below).
In cyanobacteria, carbonic anhydrase dehydrates cellular HCO3- to supply CO2 for RuBisCO but research suggests that the activity of carbonic anhydrase is not rate-limiting relative to the activity of RuBisCO in vivo [91].
Genes involved in carbon-concentrating mechanisms in cyanobacteria, such as bicarbonate transporters (BCT1, SbtA, BicAI), are under study for their probable role in improving photosynthetic yield in C3 plants [92].
GLYCOLATE DEHYDROGENASE (GDH) is a key enzyme in the glycolate catabolic pathway in Escherichia coli.
Transgenic A. thaliana that over-expressed bacterial GDH converted glycolate in the chloroplast directly to glycerate and reduced the metabolic flux towards peroxisomes and mitochondria.
The increased CO2 concentration in the chloroplast enhanced carbon assimilation and increased biomass by about 30% [93] (Table 3).
This may be a key gene for improvement of photosynthesis in C3 trees.
Photorespiration and photoinhibition
Under ambient conditions, about 35% of ribulose-1,5-bisphosphate molecules become oxygenated [94] and generate phosphoglycolate, a strong inhibitor of photosynthetic carbon fixation.
Photorespiration recycles phosphoglycolate but this process can lead to a 25% loss of carbon captured by the photosynthetic apparatus in plants [95].
GLUTAMINE SYNTHASE is associated with nitrogen assimilation, but over-expression of conifer GS1 also provides hybrid poplar (P. tremula×P. alba) with a mechanism to manage photorespiration and maintain high rates of photosynthesis (see Section 5.1) [70].
Photorespiration protects the photosynthetic machinery from photoinhibition and oxidative stress [96].
Even in cold-adapted evergreen conifers (Picea engelmannii and Abies lasiocarpa), cold-induced photoinhibition has been shown to determine the survival of those growing at the alpine tree-line [97].
Deciduous hybrid poplar (P. tremula×P. alba) overproducing GLUTATHIONE REDUCTASE in their chloroplasts had increased resistance to photoinhibition at low temperatures (5°C) and high light intensity (1000μmolm-2s-1) [98].
This ability to fix carbon efficiently under cold but sunny conditions might be useful in increasing the photosynthetic activity of evergreen conifers in winter or even transgenic poplar with delayed or no winter dormancy.
To minimize potential cold injury to the rest of the cellular apparatus, it may be prudent to co-express this gene with known genes that promote whole-plant resistance to cold stress such as those belonging to the COLD BINDING FACTOR/DRE-BINDING (CBF/DREB) family of transcription factors [99] (see also Section 8.1.1).
Assimilate partitioning and utilization
Increasing sink capacity and neutralization of the feedback mechanisms that down-regulate photosynthesis can increase biomass [100].
Wood consists mainly of cellulose, hemicellulose, and lignin, so actively channeling photosynthate into the production of cell-wall polysaccharides or lignin may improve biomass accumulation in trees [101].
Cellulose and other polysaccharides
Improved photosynthesis increases sucrose production but this must be coupled with increased sucrose utilization, storage, or transport to sink tissues to avoid negative feedback regulation [102].
Increased downstream utilization of photosynthetic products has been shown to increase biomass yield [103].
Cellulose and hemicellulose account for ∼70% of wood [104] so over-expression of genes that promote cellulose production should have a significant impact on timber production.
SUCROSE SYNTHASE converts sucrose into fructose and glucose-the latter is the primary substrate for cellulose synthesis.
Hybrid poplar (Populus alba×P. grandidentata) over-expressing SuSy, the gene for SUCROSE SYNTHASE from cotton (Gossypium hirsutum), had increased carbohydrate content late in the growing season.
The transgenic plants had increased wood density but the transgene had no effect on the growth phenotype [103].
SUCROSE PHOSPHATE SYNTHASE is a key enzyme in sucrose synthesis.
Over-expression of SPS, the gene encoding this enzyme, changed the phenology of transgenic hybrid poplars (P. alba×P. grandidentata), effectively reducing the duration of their winter dormancy period [105].
The longer photosynthetically productive period resulting from shorter winter dormancy should lead to increased biomass yield.
These transgenic plants may help clarify the role of carbohydrate reserves in triggering and terminating winter dormancy in deciduous species.
Plant cellulases (ENDO-1,4-β-GLUCANASE) cleave glucose chains from microfibrils in the cell walls.
Over-expression of a cellulase gene (CEL1) from poplar (P. alba) in the tropical legume tree, sengon (Paraserianthes falcataria), increased the stem length and girth and leaf chlorophyll content of the transgenic trees [106].
Poplar (P. tremula) over-expressing the gene for ENDO-1,4-β-GLUCANASE (cel1) gene from A. thaliana had more leaves, as well as longer and thicker main stems with a higher percentage of cellulose and hemicellulose compared to wild type [107].
Both papers suggest that elevated cellulase may break up the cell wall to a sufficient extent to result in decreased xyloglucan cross-linking and thereby increased turgor-driven cell expansion, consequently increasing cell division and growth.
Xyloglucan is a hemicellulose that is a major component of wood.
Xyloglucanases from microbes degrade xyloglucans.
Poplars (P. alba) over-expressing AaXEG2, a gene for XYLOGLUCANASE from Aspergillus aculeatus, had stems with increased cellulose content and higher specific gravity than wild-type plants.
In the growth chamber, the transgenic plants grew faster (∼40% greater height increment, 25% more radial growth in 30 days) and they had larger leaves that were 30% heavier (dry weight) than wild-type plants [108].
However, the transgenic trees grew poorly in field trials, indicating that over-expression of this catabolic enzyme adversely affected plant performance in the field [109].
These results emphasize that it is crucial to confirm gene function by field-testing for longer durations.
Increasing sink strength can lead to higher biomass yield.
PHOTOPERIOD RESPONSE 1 is a gene in potato that has been associated with the accumulation of starch in the tubers.
Over-expression of the endogenous ortholog of this gene in P. trichocarpa resulted in starch accumulation in stem and roots, and, consequently, significantly more stem and root biomass [110].
Many of the transgenic plants had malformed root systems that may have contributed to a 50% mortality rate in the greenhouse.
Subsequently, the survivors were able to develop normal root systems.
Careful selection for growth and vigor in the greenhouse phase and use of bare-root seedlings for transplanting may help ensure that only plants with satisfactory root systems are planted out in the field.
Lignin suppression
Lignin is an integral component of plant cell walls and efforts to reduce its levels or change its composition by molecular means have often generated plants with little potential for increased biomass.
Table 4 has some of the genes that directly modify lignin content.
COUMAROYL 3′-HYDROXYLASE is a key rate-limiting enzyme in the biosynthesis of lignin.
Introduction of hairpin RNAi constructs of C3′H, the gene encoding this enzyme in hybrid poplar trees (P. alba×P. grandidentata), severely inhibited the lignification of cell walls, compromised vascular integrity, and predisposed the tissues to wall failure and cavitation [111].
This led to a substantial reduction in total plant biomass.
The enzyme 4-COUMARATE: COENZYME A LIGASE (4CL) catalyzes the ligation of hydroxycinnamic acid precursors for lignin and flavonoid biosynthesis.
Anti-sense inhibition of 4CL produced taller aspen (P. tremuloides) trees that had significantly lower lignin and higher cellulose [112].
However, these effects were not correlated with the degree of transcript suppression-similar growth was observed in lines that had moderate or almost undetectable levels of transcripts.
Efficient suppression of 4CL by double-stranded RNA (dsRNA) retarded the growth of Monterey pine (P. radiata), which apparently was more sensitive to the adverse effects of silencing of this gene than aspen [113].
This difference may be a reflection of fundamental anatomical differences between angiosperm species, which use vessels for water conduction and fibers for structural strength, and conifers, which use tracheids for both roles [113].
Lignin suppression in conifers led to tracheid collapse and severely deformed plants.
However, undesirable effects from suppression of this gene are also observed in angiosperm species.
Poplar (P. tomentosa) showing 20% reduction in 4CL transcripts grew better after six months in the field than the control or lines with 50% suppression.
Moderate (<30%) suppression of 4CL expression dramatically improved tree growth [114].
Hence, it may be worthwhile to evaluate the use of less efficient gene suppression methods, or selecting lines with lower levels of gene silencing when gene suppression leads to undesirable phenotypes.
For example, anti-sense or co-suppression methods are known to be less effective inducers of post-translational gene silencing than those using "hairpin" RNA constructs [115].
Similar to such effects resulting from transgenesis, cinnamyl alcohol dehydrogenase (cad) hemizygous null mutant plants showed a 14.1% increase in de-barked volume in year 4 compared to wild-type controls [116].
Presumably, similar effects could be achieved from CAD RNAi approaches in this and other conifer species.
Conversely, over-expression of F5H, the gene encoding FERULATE 5-HYDROXYLASE in hybrid poplar (P. tremula×P. alba), generated plants with normal growth phenotypes that produced wood with an increased proportion of syringyl units and improved pulping efficiency [117].
The wood from transgenic trees with the highest syringyl content was subsequently found to resist fungi causing wood decay [118].
Most research indicates that manipulating the lignin biosynthetic pathway to reduce or alter lignin can increase the utility of trees for bioenergy, but not total biomass.
Consequently, genes encoding enzymes for lignin modification, such as F5H, should be coupled with genes for increased biomass in order to obtain a better "bioenergy phenotype".
Reproduction
During the course of development, plants shift from a purely vegetative growth phase into the reproductive stage.
In mature perennial trees, vegetative and reproductive growth occurs in cyclic patterns often influenced by temperature and water availability.
Developmental events in the plant's life cycle can be manipulated to increase the efficiency of biomass production.
The effects of some genes affecting flowering and juvenility on biomass and other traits are summarized in Table 5.
Flowering
Fruit-crop researchers aim for early flowering for breeding and fruit production.
A shorter juvenile period will also favor breeding and selection in forest trees.
TERMINAL FLOWER 1 is a known repressor of flowering in many species.
Suppression of the endogenous gene in apple (Malus domestica) by dsRNA led to extremely reduced growth and highly precocious flowering and abnormal flowers in vitro [119].
Follow-up research in forest trees may benefit by using less efficient silencing techniques such as anti-sense constructs to attenuate the negative effects.
Gene expression in plants can be suppressed in a sequence-specific manner by infection with viral vectors carrying fragments of host genes, a phenomenon that is known as virus induced gene silencing (VIGS) [115].
An advantage with VIGS approaches is that their phenotypic effects are generally not permanent (but see Section 9 for recent developments in this area).
In the case of precocious flowering in apple mediated by Apple Latent Spherical Virus (ALSV) vector, the viral construct and its effect on silencing the Terminal Flower 1 (that induced precocious flowering) gradually disappeared in the infected plants, within two years from inoculation [120].
This phenomenon could have substantial utility in forest tree breeding.
The MADS box domain is a sequence motif that is conserved in a class of transcription factors that regulate the expression of many genes in many aspects of plant development.
The MADS4 gene from birch (Betula pendula) induces early flowering in birch and apple.
Aspen (P. tremula) that over-expressed this gene did not flower early, but the plants had winter senescence/dormancy delayed by up to 10 weeks.
The transgenic saplings maintained their photosynthetic activity under winter conditions.
The plants that underwent no- or short-winter dormancy grew taller and produced more leaves than the wild type [4].
Lack of winter dormancy, coupled with a cold-resistant photosynthetic machinery, may increase biomass yield in frost-free areas (see Section 8.1.1).
FLOWERING PROMOTER FACTOR 1, a gene that can induce early flowering in several annual species, was over-expressed in hybrid aspen (P. tremula×P. tremuloides).
It failed to induce early flowering and it produced significantly smaller plants with reduced wood density, lower lignin content, and higher fractions of cellulose and glucomannan [121].
LEAFY promotes early flowering in A. thaliana.
Over-expression of PTLF, its homolog in Populus, hastened flowering or gender change in hybrid aspen (P. tremula×P. tremuloides).
The slow-growing and precociously flowering transformant had a highly branched, bushy habit with significantly smaller leaves [122].
FLOWERING TIME promotes the transition of the vegetative shoot meristem to a flowering meristem in A. thaliana.
The transcripts for the FLOWERING TIME 2 homolog in poplar (P. deltoides) are rare in juvenile trees but abundant in mature poplar undergoing reproductive growth.
Over-expression of this gene induced flowering within a year but it also led to extreme dwarfing [123].
Over-expression of the genes that promote early flowering have uniformly produced undesirable results from a biomass perspective.
Suppression of these genes by RNAi may lead to delayed flowering and better early vegetative growth-traits that are desired in short-rotation cropping.
Many trees require a long period of vegetative growth before they achieve reproductive maturity, after which the trees enter a recurrent cycle of vegetative and reproductive growth.
Repression of flowering in perennial indeterminate species like snapdragon (Antirrhinum majus) offer insights into how such genes may operate in indeterminate perennial tree species.
Over-expression of CENTRORADIALIS, originally characterized as a floral repressor in snapdragon, has been shown to delay flowering in a variety of annual plant species.
Over-expression of a poplar (P. trichocarpa) ortholog of CENTRORADIALIS (PopCEN1) resulted in an almost complete absence of flowering in six-year-old transgenic poplars.
In contrast, most of their wild-type counterparts had flowered in four years.
Nevertheless, many poplars do not flower until they are seven years old, so a longer period of observation is needed to fully appreciate the effect of CENTRORADIALIS over-expression.
The transgenic plants were also characterized by severely disturbed shoot phenology and crown architecture.
In addition, as the trees grew older, they were noticeably smaller than the wild-type plants.
This was presumably due to the cumulative effects of repeated delays in spring bud flushes and increased shading by nearby wild-type trees with earlier bud flush [124].
Proper blocking and replication in field experiments will reduce the bias brought on by possible shading and competition among transgenic lines that are expected to have gross differences in plant phenotype.
A. thaliana that over-expressed the CORNGRASS 1 microRNA from maize (Zea mays) had extended juvenility and delayed flowering [125].
Hybrid poplar (P. tremula×P. alba) over-expressing this microRNA displayed increased branching, shorter internodes, and dramatically lower lignin content.
Significantly, the transgenic plants showed higher sylleptic branching, a trait associated with high biomass yield in poplar [5].
Updates from this research should clarify whether over-expression of this microRNA will extend the juvenile period of the poplar trees.
Trees grow fastest during their juvenile stage so poplar with longer juvenile phases may produce more biomass as well.
A short reproductive (seed to seed) cycle is crucial not only for conventional breeding but also for gene stacking (see Section 9).
As discussed above, early flowering has been induced by suppressing TERMINAL FLOWER 1 via VIGS.
Apple plants inoculated with VIGS vectors based on the Apple Latent Spherical Virus (ALSV) for TERMINAL FLOWER 1 produced fruit precociously but became virus-free several months later [120].
Over-expression of the FLOWERING LOCUS T gene led to flowering two months after bombardment of a virus-mediated over-expression (VOX) construct into apple seedling cotyledons.
These plants required only seven months to complete a seed to seed cycle and the seed progeny of these inoculated plants were completely virus-free [3].
These developments may pave the way for the development of inbred lines for the production of true F1 hybrids in trees.
Reproductive sterility
At present, researchers in certain jurisdictions are required to cut down their transgenic trees before flowering, to prevent the unwanted spread of transgenes.
The ideal tree in such jurisdictions would be one that does not flower within the duration of its cultivation cycle.
This can be attained by significantly delaying maturity (see Section 7.1) coupled with shorter crop cycles or by producing completely sterile trees.
Complete ablation of the reproductive organs will presumably enable the plant to concentrate its resources to its vegetative structures.
The cytotoxic BARNASE gene from Bacillus amyloliquefaciens was placed under the control of a flower-specific promoter from the silver birch (Betula pendula) MADS1 gene.
MADS1 is a transcription factor that is expressed in the inflorescence meristem and it is needed in the formation of floral organs.
Over-expression of this construct led to the complete ablation of inflorescences in silver birch but this was accompanied by negative pleiotropic effects that led to small leaves, slow growth, and bushiness in most of the transformants [126].
The negative pleiotropic effects on the shoot phenotype may indicate a "leaky" (i.e., not truly floral-specific) promoter that allowed some expression of the cytotoxic gene in non-target cells.
It proved impossible to obtain transgenic hybrid poplar (P. tremula×P. alba) expressing BARNASE under the control of the LEAFY promoter in poplar-the authors speculated that substantial expression of this promoter in non-reproductive tissues may be the reason [127].
A promoter that strictly limits expression to floral tissues coupled with less toxic (mutated) versions of BARNASE [128] may help reduce such negative effects.
Ideally, the transcription of BARNASE should be strictly limited to reproductive organs.
Researchers used promoters specific to the various cell types in the female gametophyte to drive the BARNASE gene to attain complete ablation of the egg cell without affecting non-target tissues.
The resultant A. thaliana transgenics were unable to produce functional ovaries and seed set was completely abolished [129].
Complete abortion of fruits should allow the tree to concentrate photosynthate into vegetative structures.
In addition, mature transgenic trees that remain fruitless would allow forestry researchers to evaluate characters such as wood quality or ideotype effects on harvest yield.
Whilst unable to produce seed, such plants would still produce transgenic pollen which means that, in most jurisdictions, they will have to be destroyed before flowering.
Transgenic freeze-tolerant Eucalyptus that harbored a cytotoxic BARNASE gene from B. amyloliquefaciens under the control of a P. radiata anther-specific promoter (PrMC2) did not produce pollen six years after planting [130].
A similar construct prevented pollen formation in hybrid pine (Pinus rigida×P. taeda) scions for at least four years after grafting onto mature P. taeda [128].
However, these male-sterile trees are still capable of producing seed if fertilized by nearby wild-type pollen.
Again, such trees would still need to be cut down in many jurisdictions before they set seed.
In summary, current regulations against the spread of genetically modified organisms are best addressed by complete flower ablation.
Both male and female reproductive structures need to be ablated by using combinations of more tissue-specific promoters and 'less toxic' (mutated) versions of barnase [128].
If such "sterile" plants are required for breeding sometime in the future, silencing the barnase gene through transient expression methods such as VIGS [120] may temporarily restore fertility.
Stress resistance
Abiotic stress
The quality and quantity of woody biomass are influenced by environmental factors such as soil fertility, precipitation, and (a)biotic stresses.
Some believe that climate change will bring severe droughts on a continental scale, whilst the melting of the polar ice caps will increase sea levels and salinity of low-lying areas used for agriculture.
Imprudent irrigation of arid lands may lead to salinization [131].
Current and future forest plantations will be impacted by the harsher environmental conditions that are associated with climate change.
Transgenesis provides a way to equip trees with multiple genes for resistance to abiotic stress that will help them cope with such conditions in the future.
Some genes that have proven useful in helping the plants survive abiotic stress are listed in Table 6.
Cold tolerance and winter dormancy
Low temperature is the major limiting factor for plant growth in winter.
Plants shut down their photosynthetic apparatus or shed their leaves in order to survive harsh winter conditions.
Solar energy is sufficiently strong to induce photoinhibition even though it is only available for shorter periods during the winter months.
In many temperate climes, deep-rooted trees can obtain water for photosynthesis from deeper soil layers, where temperatures are high enough to keep water liquid or from water that seeps near ground level during periodic thaws in winter.
Hence, except for generally lower temperatures, the main requirements for photosynthesis (water, CO2, and sunlight) are present even in winter.
Transgenesis might aid in maintaining photosynthesis in frost-free climates in plants that undergo winter dormancy.
Such climates will presumably extend nearer to the polar regions as a result of global warming.
Genes that reduce the duration of winter dormancy coupled with genes that protect the plants from the occasional frosts might be useful for deployment in trees destined to be planted in areas that have milder, frost-free winters.
Shorter day lengths and colder temperatures in autumn are signals for deciduous trees to stop growth, shed leaves, and prepare for the coming winter.
There are some exceptions such as pear (Pyrus spp.) and apple (Malus spp.) that do not require specific photoperiods for winter dormancy [132].
Populus was, for a long time, considered to be responsive only to photoperiodic signals but it was recently shown that warmer temperatures modulate growth cessation and dormancy induction in hybrid poplar (Populus×spp.) [133].
High insolation and cold temperatures result in photoinhibition, which seems to be a key process that limits plant productivity and survival in winter.
There is a strong correlation in the tolerance for cold-induced photoinhibition and freezing tolerance in winter hexaploid triticale (×Triticosecale) [134].
Cold-induced photoinhibition limited the ability of snow gum (Eucalyptus pauciflora) to survive and regenerate at the tree line [135].
Over-expression of master regulators for cold tolerance can produce cold-tolerant leaves and photosynthetic apparatus, including tolerance to photoinhibition in winter.
Over-expression of two endogenous cold-binding factor/drought responsive element binding (CBF/DREB)1-like genes in rape (B. napus) increased freezing tolerance and improved photochemical efficiency and photosynthetic capacity.
These transgenics were less affected by photoinhibition induced by low temperatures and high insolation [99].
Similarly, over-expression of two CBF/DREB genes from Eucalyptus gunnii, a species that can withstand winter temperatures down to -18°C, improved drought and freezing tolerance of a cold-sensitive Eucalyptus hybrid (E. urophylla×E. grandis) [136].
Tropical E. grandis×E. urophylla expressing a stress-inducible rd29a promoter-CBF2 transcription factor cassette demonstrated stable tolerance to -8°C in a variety of locations through multiple years with no significant loss in productivity [130].
Glutathione is an antioxidant that has multiple functions in plant stress management.
Over-expression of the gene encoding GLUTATHIONE REDUCTASE in the chloroplasts of hybrid poplar (P. tremula×P. alba) increased the ability of the leaves to resist cold-induced photoinhibition at high light intensities [98].
Such genes may play a key role in increasing photosynthetic yield in tree species that have shorter or no winter dormancy.
Thus, it is possible to extend the duration of photosynthate production well into the cold winter season for evergreen species and for deciduous species that display delayed leaf senescence/dehiscence or minimal winter dormancy.
Interspecific walnut hybrids over-expressing the rolABC genes had delayed fall dormancy, but the transgenic trees suffered tip die-back after an early frost [51].
Similarly, hybrid aspen (P. tremula×P. tremuloides) plants expressing the rolABC genes showed little seasonal variation in growth, whilst the wild type plants exhibited winter dormancy [52] (Table 1).
Such plants would presumably benefit from co-transformation with (CBF/DREB)1-like genes.
Evergreen trees like conifers react to cold temperatures by substantial suppression of photosynthesis [137].
Net photosynthesis is completely suppressed in Scots pine (P. sylvestris) under subfreezing conditions but pines photosynthesize for a few hours during 'warm' winter days.
This indicates that the cold-induced suppression of the photosynthetic apparatus in conifers may be reversed by exposure to relatively warmer conditions during the day [138].
Winter photosynthesis in red spruce (Picea rubens) is usually low in Vermont (USA) but it increases significantly at intermittent thaws during the winter season [139].
The adaptability of the photosynthetic apparatus in these conifers indicates a strong potential benefit for transgenic manipulation to engineer cold-tolerant adaptations for photosynthesis at near-freezing conditions.
PATHOGEN AND FREEZING TOLERANCE-RELATED PROTEIN 1 (PF1) is an ERF/AP2 transcription factor associated with cold tolerance in Capsicum annuum.
Compared to wild-type plants, eastern white pine (P. strobus) over-expressing PF1 had higher survival rates after exposure to extreme drought, freezing, or salt stress with no adverse effects on growth and morphology.
The PF1 gene, controlled by the 35S promoter, apparently helped maintain polyamine levels under drought, freezing, and salt stress.
Polyamines including putrescine, spermidine, and spermine are polycationic compounds that have been implicated in tolerance to multiple abiotic stress factors [140].
By contrast, polyamine levels in wild-type plants significantly decreased after exposure to these stress factors.
Constitutive production of polyamines had no apparent negative effect on growth [141].
Winter temperatures are gradually increasing due to climate change.
It may be prudent to engineer deciduous species that are less responsive to short day length and cold temperatures to shorten their leafless period.
Hybrid aspen (P. tremula×P. tremuloides) over-expressing the FLOWERING LOCUS T (FT) gene from P. trichocarpa displayed an early flowering phenotype and continued to grow even when exposed to short-day conditions that precede winter [142].
However, these plants performed poorly from a biomass perspective, so other genes may be more useful for this purpose.
Overproduction of SUCROSE PHOSPHATE SYNTHASE in hybrid poplar produced trees with a phenology that included a delayed onset of senescence in autumn and earlier bud flush in spring [143].
This presumably increased their photosynthetically productive period on an annual basis.
Heat tolerance
Temperatures above 30°C adversely affect photosynthesis in C3 plants.
In Japan, leaf temperatures of conifers, deciduous trees and evergreens ranged from 36 to 40°C in late summer [144].
Hence, tree leaves are regularly exposed to high temperatures detrimental to photosynthesis particularly (but not only) during summer days.
SEDOHEPTULOSE-1,7-BISPHOSPHATASE (SBPase) was discussed (in Section 6) in relation to increased photosynthesis.
Over-expression of the gene encoding this enzyme prevented the sequestration of RuBisCO activase and improved CO2 assimilation in rice seedlings undergoing high temperature stress [145].
Very young Virginia pine (P. virginiana) saplings can be killed by brief exposure to 48°C.
Heat exposure of young P. virginiana over-expressing an ERF/AP2 transcription factor (PF1) from pepper (C. annuum) led to 40% mortality, but the control suffered 90% mortality [146].
Drought tolerance
Continental droughts and/or salinization may occur as consequences of climate change [147].
In some forest ecosystems, elevated tree mortality rates have been associated with global trends in warming and drought [148].
Marginal arid areas that are prone to salinization require reforestation species with enhanced tolerance to drought and salinity [2].
The photosynthetic process in C3 trees is adversely affected when water stress leads to stomatal closure under high insolation.
Hybrid poplar (P. tremula×P. alba) that over-expressed cytosolic GLUTAMINE SYNTHASE from pine (P. sylvestris) displayed higher net photosynthetic rates before, during, or after exposure to transient water stress [149].
Apparently, the greater photorespiratory activity in the transgenic lines provided protection against high light exposure during water stress.
Leaf architecture, including shape, size, stomatal density, cuticle composition, etc., plays a major role in how plants survive water stress.
Lipid biosynthesis determines the production of wax for deposition on leaf cuticles and jasmonates that participate in stress signaling.
Hybrid poplar ((P. deltoides×(Populus×euramericana)) over-expressing a fatty acyl-acyl carrier protein thioesterase from wintersweet (Chimonanthus praecox) had normal growth phenotypes under non-stressed conditions, but they grew much better and produced much higher levels of oxygen scavengers than wild-type plants under PEG-induced water stress.
Wild-type plants showed growth retardation and leaf curling under similar conditions [150].
Poplar (P. alba) that over-expressed DnaK/HSP70, a gene encoding a molecular chaperone from the salt-tolerant cyanobacterium Aphanothece halophytica, exhibited improved photosynthesis, as well as increased leaf size and plant growth under normal conditions.
The transgenic lines recovered well after exposure to drought stress, whereas the wild-type plants either recovered poorly or died after treatment.
The transgenic plants also exhibited resistance to high light intensities, salt, and cold stress [151].
Plants that accumulate more osmolytes are better able to respond to osmotic stress.
Paper mulberry (Broussonetia papyrifera) plants that over-expressed the Na+/H+ antiporter 5 (NHX5) gene from A. thaliana had increased levels of proline and sugars that enabled them to better tolerate drought and salinity stress [152].
Transgenic poplar (Populus×euramericana 'Guariento') was co-transformed with SacB, a gene for levansucrase that has been associated with drought tolerance, along with other stress tolerance genes (vgb, BtCr3A, JERF36, and OC-1).
The resultant multi-genic transformants exhibited superior shoot growth, greener leaves, and better developed root systems than the control after exposure to extended drought.
Under drought stress, the transgenic plants displayed vigorous growth while the wild-type plants shed most of their leaves and produced less root biomass [153] (see Section 9 for a description of these genes).
Salt tolerance
Manipulation of cation transporters can confer tolerance to soil salinity.
Poplar (Populus×euramericana 'Neva') over-expressing the Na+/H+ antiporter gene NHX1 from A. thaliana had improved growth and photosynthetic capacity under salt stress (150mM NaCl) compared to wild-type plants [80].
An apple gene for SPERMIDINE SYNTHASE (SPDS1) endowed transgenic pear (P. communis 'Ballad') with strong resistance to osmotic, salt, and heavy metal (copper) stress [140].
Ethylene-responsive transcription factors (ERFs) play major roles in stress signaling in plants.
Hybrid poplar over-expressing the JASMONIC ETHYLENE RESPONSIVE FACTOR (JERF) transcription factor gene from tomato (Solanum lycopersicon) were significantly taller than control plants even without salinity stress (Table 6).
The transgenic plants were 30% taller and produced twice as much biomass than the control at a concentration of 300mM NaCl [154].
Over-expression of JERF36 and four genes for stress tolerance (vgb, SacB, Cry3A, and OC-1, see Table 8) produced hybrid poplar (Populus×euramericana 'Guariento') plants that were able to tolerate salt levels as high as 135mM NaCl, growing 60% taller, producing greener leaves, and more extensive root systems.
Their shoot and root biomasses were 35% and 780% heavier than the wild type [153].
Oxidative stress
Reactive oxygen species (ROS) such as oxygen ions and peroxides damage plant tissues, leading to loss of function and cell death.
Low concentrations of paraquat (herbicide) are used to test the resistance of leaf discs to oxidative stress.
Hybrid poplar (P. tremula×P. alba), with increased tolerance to oxidative stress due to over-expression of GLUTATHIONE REDUCTASE, also became resistant to 2μM paraquat [98].
NUCLEOSIDE DIPHOSPHATE KINASE 2 regulates antioxidant gene expression in plants.
Hybrid poplars (P. alba×P. glandulosa) over-expressing this gene had greater antioxidant (catalase, peroxidase) activity and increased tolerance to 1μM paraquat.
Furthermore, the plants had significantly more branches and thicker stem diameters during the first six months of growth in the field [155].
This indicates that over-expression of NUCLEOSIDE DIPHOSPHATE KINASE 2 reduces the adverse effects of ROS on plant tissues and growth.
Plants, however, also rely on the generation of ROS to counteract pathogen incursions so transgenic plants that suppress ROS production need to have alternate ways of coping with inevitable microbial infections.
Peroxidases oxidize various compounds in the presence of hydrogen peroxide (H2O2).
Class III peroxidases oxidize reducing agents, catabolize auxin, actively participate in wound healing and lignin biosynthesis, etc.
Hybrid poplar (P. sieboldii×P. grandidentata) that over-expressed prxC1a, a class III horseradish peroxidase, had enhanced growth and were resistant to oxidative stress [156].
The leaves of the transgenic plants were resistant to a high level (1mM) of paraquat-a dose that was clearly toxic to the wild-type leaves.
This trait may be useful in the creation of forest trees tolerant to herbicide applications (Section 8.2.3).
Biotic stress
The increasing frequency of extreme climatic conditions associated with climate change has the potential to weaken forest trees and predispose them to biotic threats.
Providing transgenic trees with genes for resistance to a wide variety of potential biotic threats could be a prudent strategy to assure a measure of stability in perennial tree plantations.
Plant-pest interaction is a co-evolutionary process involving the creation of novel defensive traits by plants and the pertinent countermeasures by pests [157].
In this 'arms race', transgenes are akin to novel defenses that the pest can surmount only after several generations of selection pressure.
One potential strategy is to equip transgenic trees with genes targeted at potential pests that may not yet be present in the target planting sites.
For example, beetle pests are not a problem in Australia's or New Zealand's pine industries, but it is not far-fetched to imagine that, sometime in the future, a gravid pine beetle (e.g., Dendroctonus spp.) may pass through their biosecurity screening procedures and devastate the more than two million hectares of Monterey pine plantations in these two countries.
Table 7 contains a list of genes that have endowed plants with resistance to biotic adversaries.
Insect resistance
Insects and their host plants co-exist and co-evolve in natural populations.
Insect pest populations are usually kept in check by natural control mechanisms but, occasionally, devastating outbreaks occur.
Increasingly warm conditions allow pest species to survive in previously colder locations where native tree species have not evolved the appropriate defenses [158].
Greater openness and speed in global transport and trade dramatically increase the probability of accidental introduction of novel pests.
In some cases, insect populations may persist at high infestation levels for considerable periods due to the widespread availability of hosts made more susceptible by abiotic stress.
Mountain pine beetle (Dendroctonus ponderosae) and its fungal symbionts (e.g., Grosmannia clavigera) have destroyed ∼16 million hectares of lodgepole pine (P. contorta) forests in western Canada and northwestern USA [159].
The poplar looper (Apocheima cinerarius) and gypsy moth (Lymantria dispar) damaged about 40% of the total poplar plantation in China in 1989 [160].
Controlling such wide-scale infestations with insecticidal sprays is neither practical nor economical.
Long-lived trees have an array of physicochemical defense mechanisms but they also serve as hosts to myriad pests that have co-evolved to overcome such defenses.
Conifers have an array of putative defensive structures (e.g., resin ducts) and chemicals (terpenoids) that have little negative effect on the adapted herbivores.
Pine beetles use the terpene components of the defense response of conifers as semiochemicals or as pheromone precursors to summon kindred pests to the wounded target [159].
However, pests that evolved in angiosperm hosts may have greater difficulty with defense resistance if their hosts are engineered to over-express a variety of defense-related genes from gymnosperms, i.e., the gymnosperm genomes are an unexplored resource of genes that may be useful in improving the defense capabilities of angiosperms (and vice versa).
Polyphenol oxidases are active in the browning reaction to tissue injury.
Annual species engineered to over-express such genes have demonstrable resistance to bacterial pathogens.
Larvae of the forest tent caterpillar (Malacosoma disstria) had higher mortality rates when fed with leaves of hybrid aspen (P. tremula×P. alba) over-expressing a gene for POLYPHENOL OXIDASE from hybrid poplar (P. trichocarpa×P. deltoides) [161].
However, a subsequent study failed to substantiate the efficacy of POLYPHENOL OXIDASE on tree-feeding caterpillars so the researchers suggested the need for more stringent and reliable tests to back up similar claims [162].
These conflicting results emphasize the need to verify gene activity at various stages of the plant's life, from in vitro cultivation to growth in the field.
Bacillus thuringiensis (Bt) is a bacterium that produces a wide variety of δ-endotoxins, toxic polypeptides that are activated within the guts of insect herbivores.
The individual crystalline δ-endotoxins produced by Bt strains are poisonous to specific orders of Insecta (lepidopterans, dipterans, coleopterans, hymenopterans) and nematodes [163].
Hence, simultaneous over-expression of several Bt toxins should produce a tree that would be poisonous to many insect herbivores that are likely to attack the host.
Constitutive expression of a gene for resistance to a negligible pest population may constitute an undesirable metabolic load that can affect biomass.
Nevertheless, any yield penalty that may be due to the overproduction of a foreign gene will be negated by much lower damage from actual herbivory at higher pest infestation levels.
For example, hybrid aspen (P. tremula×tremuloides) producing a modified cry3Aa endotoxin were slightly shorter than wild-type plants in the absence of a leaf beetle (Phratora vitellinae).
The Bt hybrid aspen grew taller than wild-type trees when beetle populations were high enough to cause significant damage by herbivory [164].
Hence, the slight adverse effect of producing Cry endotoxins on growth is more than offset by the much lower losses in case of significant pest infestation.
Four-year-old wild-type cottonwood (P. trichocarpa) suffered significant defoliation at normal infestation levels of the leaf beetle (Chrysomela scripta).
The wild-type trees had a net growth that was 13% less than those over-expressing the cry toxins from Bt.
The trees producing the Cry3A toxin had very low feeding damage [165].
Eucalypts over-expressing a sequence-enhanced version of cry3Aa produced leaves that were toxic to three beetle species [166].
P. radiata producing the Cry1Ac toxin were resistant to feeding damage caused by larvae of the painted apple moth (Teia anartoides) [167].
In China, poplar (P. nigra) over-expressing Cry1A reduced leaf-damage due to the geometer moth (Apocheima cinerarius) and clouded drab moth (Orthosia incerta) to a point where insecticide sprays were deemed unnecessary [160].
Continuous production of any single insecticidal toxin in widely planted transgenic perennials will inevitably lead to the evolution of tolerant insect populations.
Based on the co-evolution concept, pest populations are expected to take a longer time to evolve mechanisms that can overcome multiple genes that are active in different metabolic pathways and originating from very distantly related species.
Hence, it is prudent to generate transgenic trees that have an array of resistance genes for use against known pests.
For example transformation of P. radiata in New Zealand with the Cry3A gene among other resistance genes will prepare them for a potential incursion of the mountain pine beetle (D. ponderosae), which has devastated large swathes of the pine forest in the United States and Canada.
As these transgenics will need to survive in the field for several decades, engineering of multiple resistance traits into tree genomes will provide a measure of security from sudden and unexpected pest outbreaks.
Hybrid triploid poplars (P. tomentosa) over-expressing a TRYPSIN INHIBITOR from cowpea were resistant to lepidopteran defoliators such as Stilpnotia candida, Lymantria dispar, and Malacosoma disstria.
Insect larvae feeding on the foliage of these transgenic poplars had reduced growth and increased mortality [168].
Leaf beetles (Plagiodera versicolora) fed with leaves from transgenic hybrid poplar (Populus×euramericana 'Guariento') that produced cry3A (insect toxin) in addition to other stress-related genes (vgb, SacB, JERF36, and OC-1) had a higher mortality rate than those feeding on control leaves [153].
Molecular modification of known insecticidal genes can extend the variety, range, and efficacy of these toxins as well.
Leaves of poplar (P. simonii×P. nigra) over-expressing novel toxins from the fusion of a toxin gene from a spider (Atrax robustus) and cry1A(b) [169] or a toxin gene from a scorpion (Buthus martensii) fused to the gene for chitinase (chit) [170], have also proven to be toxic to their respective test insects (see Table 7).
Fusing a chitin-binding domain from silkworm (Bombyx mori) onto the endochitinase gene from Beauveria bassiana made the fused protein more potent against insects than the wild-type protein [171].
Transgenic plantation forest trees that produce a variety of insecticidal compounds will create a smaller environmental impact compared to wild types that need extensive aerial sprays of insecticides during outbreaks.
This multi-faceted approach should retard the rise of a pest that can overcome the multiple resistances built into the transgenic trees.
Furthermore, trees generating multiple insecticidal compounds may benefit from possible synergistic effects on toxicity to the target pests.
For example, leaves of poplar clone 741 (P. alba×(P. davidiana×P. simonii)) expressing Cry1Ac or Cry3Aa are toxic to Hyphantria cunea or Plagiodera versicolora, respectively.
Leaves from poplar producing both endotoxins were more toxic to either pest, indicating a synergistic effect of ectopic co-expression of these two bacterial genes [172].
Pathogen resistance
Fungal pathogens
Plants react to pathogen attack by producing pathogenesis-related proteins with antimicrobial functions (chitinases, β-1,3 glucanases, proteinase inhibitors and peroxidases).
The various plant responses are coordinated at the cellular level by genes that control the pertinent signaling pathways.
Salicylic acid is an important signal molecule in the response of plants to pathogen attack.
NON-EXPRESSOR OF PR1 (NPR1) is a salicylic acid receptor that is a key player in systemic acquired resistance in plants.
Over-expression of the endogenous NPR1 homolog in apple generated transgenic plants resistant to two major fungal pathogens, Venturia inaequalis and Gymnosporangium juniperi-virginianae [173].
Cytospora chrysosperma is a fungus that causes stem canker in wild-type poplar.
Chitin is an important component of fungal and insect cell walls.
Beauveria bassiana, on the other hand, is a fungus species that attacks a wide variety of insect pests.
This fungus produces an endochitinase that, when purified, is apparently effective against a wide variety of fungal pathogens as well.
Up to 40% of Chinese white poplar (P. tomentosa) over-expressing an endochitinase gene (chit1) from B. bassiana had almost complete resistance to Cytospora [174].
The LEAF COLOR (Lc) gene from maize is a bHLH transcription factor that is associated with the biosynthesis of flavonoids, a class of plant metabolites with members that participate in resistance to pest attack.
Apple over-expressing this gene had higher resistance against Erwinia amylovora and Venturia inaequalis.
Unfortunately, the plants had poor shoot regrowth after pruning and produced malformed leaves with necrotic lesions typical of a hypersensitive response even in the absence of pathogens [175].
Better results may potentially be obtained by coupling this gene to a promoter of a gene associated with signaling during the earliest stages of pathogenesis.
Syringyl-rich wood of poplars (P. alba×P. tremula) over-expressing the gene for FERULATE 5-HYDROXYLASE from A. thaliana had a high level of resistance to brown and white rot fungi causing wood decay.
The high syringyl/guaiacyl ratio in these transgenic trees may have affected lignin structure and susceptibility to oxidizing agents released by the fungal colonizers [118].
This gene may be useful in conifers, which normally do not have genes for syringyl lignin.
This may also have significant implications in wood treatment in the lumber industry, which currently uses large amounts of noxious copper-based biocides.
Many of the aforementioned genes for resistance are effective only against a very limited range of pathogens.
The gene for an antimicrobial protein, AMP2, is a non-specific lipid transfer protein from mint-related motherwort (Leonurus japonicas).
Chinese white poplar over-expressing AMP2 had significantly milder disease symptoms due to Alternaria alternata and Colletotrichum gloeosporioides compared to wild-type plants [176].
Transgenic poplar over-expressing both chit1 and AMP2 were significantly more resistant to A. alternata than single gene transformants and wild-type trees [177].
Thus, the possible synergistic effects of the introduction of multiple genes for resistance may help the plant cope with a wider variety of pathogens in the field.
Bacterial pathogens
NON-EXPRESSOR OF PR1 binds to salicylic acid and plays a major role, through the activation of the salicylic acid pathway, in systemic plant defense.
Plants that over-express NPR1 have high levels of pathogenesis-related proteins that make them resistant to pathogens.
Apple trees that over-expressed an NPR1 homolog had much lower shoot infection rates after inoculation with Erwinia carotovora, in addition to having significant resistance to two fungal pathogens in apple (see Section 8.2.2.1) [173].
Citrus (C. paradisi and C. sinensis) over-expressing the NPR1 gene from A. thaliana had significant resistance to the canker pathogen Xanthomonas citri subsp.
citri [178].
Over-expression of ethylene responsive transcription factors has endowed resistance to various stress factors in many plant species.
Virginia pine (P. virginiana) over-expressing an ERF/AP2 transcription factor (PF1) from pepper was tolerant to Bacillus thuringiensis and Staphylococcus epidermidis.
In addition to superior shoot growth in vitro, the plants were also tolerant to heat stress and heavy metals such as cadmium, copper, and zinc [141].
Genes with a variety of positive pleiotropic effects are prime candidates for testing in other crop species.
Polyamines, like spermidine, have fundamental roles in the regulation of many cellular and physiological processes in plants.
As discussed above, high levels of spermidine via transgenesis have conferred resistance to many abiotic stress factors (see Section 8.1.5).
A role in biotic stress was also demonstrated by ectopic expression of the apple gene for SPERMIDINE SYNTHETASE (SPDS1) in sweet orange, which reduced susceptibility to canker caused by Xanthomonas axonopodis pv.
citri.
Upon bacterial challenge, the increased production of polyamines triggered a hypersensitive response and activation of defense-related genes [179].
Pathogenic bacteria secrete elicitors that are necessary for pathogenesis in host cells.
Erwinia carotovora produces hrpnN, a gene that encodes a harpin elicitor that causes the expression of a hypersensitive response in host cells.
Over-expression of this harpin gene in pear significantly reduced susceptibility to E. carotovora [180].
Apparently, constitutive expression of this bacterial elicitor stimulated defense mechanisms that enabled the transgenic pear (developed from the highly susceptible cultivar "Passe Crassane") to resist Erwinia more effectively.
This technique would be primarily useful in host species that possess well-developed resistance mechanisms against pathogenesis.
Synthetic antimicrobial peptides like D4E1 can inhibit the germination of microbial spores.
Hybrid poplars (P. tremula×alba) over-expressing D4E1 became resistant to Xanthomonas populi pv.
populi causing bacterial canker but they remained susceptible to Hypoxylon mammatum, which causes fungal canker [181].
Molecular modification of the DNA sequences encoding these antimicrobial peptides could be a way of generating a practically limitless supply of candidate peptides to target microbes.
Weed competition and herbicide resistance
Unwanted plants present a major problem in forest plantations during the early stages of establishment or after coppicing, when fast-growing weed species can outgrow the slower growing/regenerating trees [182].
Transgenes that accelerate tree growth must be an important component of weed management because trees that attain early canopy closure will shade out understory weeds earlier and require less intervention in the form of weed control.
However, it is unlikely that transgenesis can boost tree growth rates to fully outpace the growth of annual weeds, so there is always a need for human intervention through physical or chemical methods of weed control.
In places where labor costs are prohibitive, non-selective herbicide application is the method of choice to deal with unwanted plants in the forest.
Glyphosate is the most widely used non-selective and systemic herbicide.
It inhibits 5-ENOYLPYRUVYLSHIKIMATE-3-PHOSPHATE SYNTHASE (EPSPS) and, consequently, interferes with the synthesis of aromatic amino acids.
Glyphosate resistance has been engineered into a wide variety of crops to facilitate weed management.
Hybrid Populus (P. tremula×P. alba, P. trichocarpa×P. deltoides, P. tremula×P. tremuloides) over-expressing EPSPS from Agrobacterium strain CP4 for resistance against glyphosate suffered little damage even after exposure to a rate of 3.9kg of glyphosate.per hecatre (Table 7) [183].
Glyphosate-resistant European larch (Larix decidua) was created by over-expressing bacterial aroA, a less effective gene that makes plants insensitive to much lower dosages (e.g., 0.6kgha-1) of glyphosate [184].
Glufosinate (phosphinothricin) is a non-selective systemic herbicide that interferes with glutamine biosynthesis and detoxification of ammonia.
PHOSPHINOTHRICIN ACETYL TRANSFERASE (bar) from Salmonella breaks down glufosinate.
The bar gene has been used to produce a variety of transgenic herbicide-resistant forestry species such as Eucalyptus camaldulensis [166], Populus alba [185], Quercus suber [186], P. radiata, and Picea abies [187].
Pine (P. radiata) and Norway spruce (P. abies) over-expressing the bar gene survived sprays equivalent to a dose of 4kg of glufosinate per hectare.
The non-transgenic controls died within eight weeks of spraying [187].
Chlorsulfuron is a systemic sulfonylurea herbicide that blocks the biosynthesis of isoleucine and valine and it is recommended for use against broadleaf weeds.
A mutant gene for ACETOLACTATE SYNTHASE from A. thaliana allows the synthesis of isoleucine and valine even after exposure to sulfonylurea and imidazolinone herbicides.
Over-expression of this gene in hybrid cottonwood led to resistance to chlorsulfuron [188].
Widespread use of glyphosate has led to the rapid emergence of glyphosate-resistant weeds worldwide [189].
Simultaneous development of resistance to herbicides with different modes of action is expected to be rare but it has happened: two ryegrass (Lolium rigidum) populations from Italian olive groves sprayed regularly with glyphosate were resistant to glyphosate but one population was also resistant to another herbicide, fluazip, which disrupts lipid synthesis [190].
Nevertheless, it is still worthwhile to equip forest trees with gene-based resistance to more than one herbicide, to allow herbicide rotations and to foil the possible spread of, say, glyphosate-resistant weed species from horticultural sites into forest plantations.
In annual crops, there is a current effort to incorporate resistance to both glyphosate and the herbicide 2,4-D, to counter the emergence of glyphosate-resistant weeds [191].
An integrated approach will help mitigate problems associated with unwanted plants in managed forests.
This could include boosting plant growth by breeding, transgenesis, and fertilizers; closer planting to attain early canopy closure; introduction of multiple genes for herbicide resistance to allow herbicide rotation; proper timing of spray applications to seasons or growth stages when the trees become naturally resistant to herbicides, etc.
Multi-trait improvement
Molecular strategies targeting multiple traits
Researchers have collectively evaluated a large collection of genes with proven conserved functions across species.
This constitutes a 'toolbox' that molecular engineers can use to customize trees for specific field conditions.
The current practice in transgene deployment deals with one gene or one trait at a time, and this may be partially due to regulatory requirements that obstruct the beneficial use of transgenesis in agriculture and forestry.
A holistic approach that will simultaneously add or improve several traits will likely be more effective.
Pleiotropic effects, or the simultaneous (positive or negative) effects of a given gene on more than one apparently unrelated trait, can confound the eventual utilization of some genes.
For example, increasing the activity of cytokinin pathways by transgenesis has resulted in branch proliferation and poor rooting [40].
Over-expression of OsDREB1 homologues of rice has enhanced resistance to cold and salinity stress in A. thaliana and rice at the cost of severe growth retardation [192].
It is therefore necessary to evaluate transgenes for pleiotropic effects to make sure that these are either desirable or tolerable.
The need to minimize negative pleiotropy, coupled with the "single-gene clean" regulatory requirements in some countries, has contributed to a "one gene-one trait" approach because the latter produces transgenics that are much easier to evaluate and regulate.
However, the multi-faceted objectives of crop improvement are likely best addressed by simultaneous deployment of many genes.
In this review, 'gene stacking' refers to hybridization of stable transgenic genotypes that over-express different transgenes by breeding to combine desired traits.
Gene stacking can overcome some of the limitations of the "one gene - one trait" approach, which is currently prevalent.
Gene stacking has been successfully implemented in annual crops such as maize which now have commercial varieties featuring multiple stress-tolerance traits that were generated by intercrossing transgenic lines, e.g., Agrisure® Duracade™ 5222.
This cultivar has transgenes for tolerance to glufosinate and glyphosate herbicides and resistance to coleopteran and lepidopteran insects (https://www.isaaa.org/gmapprovaldatabase/default.asp).
In silviculture, this is a viable option only in very few tree species with short reproductive cycles and a collection of transgenic lines expressing single genes.
However, this situation could change with the realization that gene stacking can be facilitated by co-introduction of VOX cassettes for early flowering.
Apple plants inoculated with VOX vectors produced fruit precociously but became virus-free several months later (see Section 7.1) [193].
Early flowering by transgenic techniques will be the key technology that may allow, within a reasonable time-frame, the generation of (partly) inbred lines that can produce more uniform hybrid tree progenies.
As discussed earlier, researchers have successfully reduced the seed to seed cycle in apples to seven months [3].
This generation time is only a few months longer than those of annual crops with established inbred-F1 hybrid breeding programs.
These transgenic techniques have now made it possible to establish a breeding strategy based on the hybridization of inbred (or transgenic) lines in tree species not only to produce true "F1 hybrids" but also to facilitate gene stacking.
Nevertheless, the outcrossing nature of most forest species will limit the utility of gene stacking.
Crossing transgenic lines developed from a single heterozygous genotype from an outcrossing species will lead to various levels of inbreeding depression (see below).
Using transgenic lines developed from various heterozygous genotypes will lead to recombination and possible reversion to the mean in traits other than those modified by the transgenes.
The severity of inbreeding depression will depend largely on the natural mating system (selfing, outcrossing, or mixed) of the species [194].
Reports on the production of partially inbred populations from a variety of tree species have revealed slight [195-197] to high [198-200] levels of inbreeding depression.
Except for seed abortion (probably due to recessive lethal alleles), the inbred plants were able to attain reproductive maturity, which is the prerequisite for further inbreeding and purifying selection.
Similar to their annual counterparts, the "inbred" tree lines are expected to be less vigorous than their hybrid progeny.
In contrast to single gene transformation followed by gene stacking, 'multi-gene engineering' involves the simultaneous introduction of several genes in a single transgenic event.
Multi-gene engineering has extended plant metabolic pathways, producing a spectrum of valuable compounds such as multiple β-carotenoids or potential psychoactive carbolines [201-203].
It can accelerate the introduction of desirable traits which is particularly important in tree species with long reproductive cycles.
Multi-gene engineering has been successfully implemented in trees (see Table 8).
Cold-resistant and pollen-sterile transgenic Eucalyptus were created by simultaneously introducing CBF2/DREB1C, a master controller of the salinity and cold responses, and a cytotoxic barnase targeted for expression in pollen, through Agrobacterium-mediated transformation [130].
Simultaneous introduction of the genes for a glycolate catabolic pathway from E. coli, viz., GLYCOLATE DEHYDROGENASE (GDH), GLYOXYLATE CARBOLIGASE (GCL) and TARTRONIC SEMIALDEHYDE REDUCTASE (TSR), into A. thaliana using co-transformation and stacking, improved photosynthetic efficiency and increased biomass [93].
Despite the utility of such an approach, the simultaneous introduction of a number of transgene constructs is severely limited in current implementations of Agrobacterium-mediated transformation, particularly when the transgenes are introduced via separate plasmids.
For example, the functional genomics strategy known as 'Fox Hunting' involves mixing A. tumefaciens cultures containing expression plasmids with individual gene inserts from entire transcriptomes (>10,000 genes).
Fox Hunting has been used in floral dip transformation of A. thaliana [204] or Agrobacterium-mediated transformation of friable rice calli [128].
This strategy produced thousands of transformant lines with predominantly (>50%) single inserts.
Although based on a small sample population of 51 disease-resistant rice lines, 74.5% had single inserts whereas only 15.6% had more than two genes in one insertion locus [204].
For tree species that respond best to Agrobacterium-mediated transformation, binary vectors containing multiple expression cassettes could be one way of obtaining multiple traits from several unrelated genes in a single transformation event.
For example, a tandem construct containing expression cassettes for a mutant anthranilate synthase and tryptophan decarboxylase enabled rice transgenics to produce large quantities of tryptophan derivatives including psychoactive carbolines [203].
Independent minimal expression cassettes can be excised from their host plasmids, concatenated and then cloned to produce multi-gene cassettes in one plasmid.
Eight genes essential for the establishment of Rhizobium symbiosis were inserted into a plasmid in their genomic context (i.e., including promoter, exon, intron, and terminator).
Using such cassettes, multi-gene transformants were obtained in strawberry (Fragaria×ananassa), poplar, tomato and tobacco [205].
From a breeder's viewpoint, a major advantage of this approach is that the linked gene expression cassettes are inherited as a single locus.
This is especially important in plant species that are inefficient in producing transgenics via Agrobacterium-mediated transformation.
A major limitation is that A. tumefaciens vectors such as the pHUGE-Red plasmid vector can accommodate only a limited number of genes [205].
A major advantage of Agrobacterium-mediated transgenesis using binary vectors containing multi-effector gene cassettes is that it will generally result in transgenic plants with the same set of genes.
By contrast, the 'multi-trait' strategy outlined in Fig. 1 is expected to generate a transgenic population varying in the composition and number of genes, relative to the original set of bombarded genes.
A powerful promoter controlling fusions of two or more genes in tandem can drive the simultaneous expression of the fused genes.
This phenomenon has been exploited to produce cold-tolerant A. thaliana with enhanced growth, by fusing isopentenyl transferase (ipt) to the tail end of a CaMV35S-galactinol synthase construct [33].
The transgenic plants exhibited cold resistance due to galactinol synthase and improved growth due to ipt moderately increasing cytokinin levels.
The reduction in expression levels of the second gene is due to interference from the internal untranslated nucleotides between the fused genes and the terminal codon of the first gene (i.e., Kozak's ribosome screening model).
This limits the utility of this method to a very few genes for each construct [33].
Several reports highlighting the use of multi-gene engineering for plants are summarized in Table 8.
Some of the early studies were aimed at introducing the genes for entire photosynthetic pathways (see Section 6) or parts of biosynthetic pathways such as that for Vitamin A in "Golden Rice 2" [206].
On the other hand, researchers in tree improvement aim for a variety of traits (e.g., biomass, stress resistance) that are under the control of a number of independent genes acting in different pathways.
Such multi-faceted objectives may require the simultaneous introduction of many genes.
Biolistic transformation has been shown to generate maize plants with more than 100 copies of transgenes inserted into their genome [207].
Introduction of a large number of genes by biolistic transformation is arguably a more efficient way of generating elite genotypes endowed with a variety of desirable traits.
Efficient transformation systems that can introduce more genes with each 'transformation event' are required to increase the chance of obtaining transformants possessing most of the transgenes.
Multi-gene engineering using biolistic transformation for the transfer of whole metabolic pathways was the subject of a recent review [208].
Rice calli bombarded with 14 expression cassettes for markers in independent plasmids generated plants that had 2-13 of the genes in one or two loci.
Most of the transgenic lines harboring multiple transgenes grew normally.
There was no correlation between sterility and the number of integrated transgenes [209].
The low number of integration sites is probably due to efficient DNA repair mechanisms that ensure that double strand breaks are highly transitory and are present at low frequency [208].
Plants generated after particle gun bombardment with multiple genes will probably possess many, if not all of the set of genes in one or two chromosomal sites due to co-integration enforced by efficient DNA repair mechanisms [208,209].
It is quite possible that there are other factors that affect this integration process.
Five expression plasmids with the CaMV35S promoter fused to different effector genes (vgb for waterlogging resistance; SacB for resistance to drought or salinity; JERF36 for resistance to salinity; BtCry3A, a Chrysomelid-specific endotoxin; and OC-I, a cystatin proteinase from rice that suppresses insect growth) were simultaneously introduced into hybrid poplar (P.×euramericana 'Guariento') by biolistic transformation.
Two transgenic lines exposed to drought or salt stress treatments in the greenhouse generated more biomass relative to the control.
The transgenic lines had enhanced resistance to a leaf beetle (Plagiodera versicolora) as well (Table 8) [153].
Gene constructs, with or without their vector backbones, can be introduced via biolistics or silicon-carbide whiskers.
Co-integration of plasmid vector backbone fragments is a frequent complication of transgenesis using whole plasmids [210], even when using Agrobacterium for transformation [211].
Use of vector-free, minimal expression cassettes amplified by polymerase chain reaction [210] or excised from expression plasmids for bombardment will reduce genetic complications due to the introduction of vector backbones.
Elimination of vector backbones can be implemented in biolistic transformation but it is clearly impossible in Agrobacterium-mediated transformation.
Regulatory aspects
The United States Department of Agriculture's Animal and Plant Health Inspection Service (APHIS) determined that it did not have the authority to regulate transgenic plants that do not involve gene sequences derived from plant pathogens [212].
Effectively, this means that transgenic plants generated by bombarding DNA that are not derived from plant pathogens will not be subject to years of environmental testing and consultation required by APHIS, although depending on the transgene inserts and tree species, other regulatory agencies (e.g., FDA, EPA, etc.) may still have oversight.
There are a number of exciting breakthroughs in the field of genetic transformation, enabling researchers to insert, replace, delete, or mutate sequences in living cells [213] that may likewise escape the regulatory requirements imposed on transgenic plants.
For example, the use of zinc-finger nucleases and transcription activator-like effectors have been deemed to generate "non-transgenic" plants by a number of national regulatory bodies [214-216].
There are indications that other countries will follow suit [215,217].
It is possible to obtain stable but non-heritable gene expression via viral vectors [218,219], among others.
Application of viral vectors is particularly exciting from the standpoint of tree improvement as it does away with regeneration from (transformed) callus, which can be quite long, arduous, expensive, and an uncontrollable source of random somaclonal variants in some species.
More importantly, VIGS and VOX technologies have been proven to work in several tree species [218,219].
Simultaneous inoculation with VIGS/VOX vectors containing different gene constructs may enable the study of the effects of possible interactions among different effector genes on a complex trait or the whole plant phenotype, albeit for a limited duration.
As with zinc finger-based technologies, viral vectors face significant regulatory hurdles and their most realistic application in tree improvement in the foreseeable future might be in functional genomics research.
Recommendations
Researchers have demonstrated the effects of manipulating the expression of many plant genes that have significant impact on biomass yield in plants.
Biomass is the product of many genes, acting through numerous signaling and metabolic pathways.
Hence, researchers could benefit from a more holistic strategy that uses a variety of tools to address the challenge of increasing plant biomass.
Tree researchers often resort to verification of gene function that was previously characterized in annual species such as A. thaliana.
Functional genomics research in annual species is comparatively advanced because it takes far less time and resources.
It would be prudent for tree researchers to base their initial efforts to the testing and deployment of genes that have been fully characterized and proven to generate consistent phenotypes across diverse (annual) plant taxa.
In addition, they may have to rely on annual species for initial functional characterization of novel tree genes [204].
There is a host of factors responsible for the lack of field-testing for most of the work summarized in the tables (references based on field tests are denoted with an asterisk), not the least of which are the need to publish and the short-term nature of most research funding.
Field trials with transgenic trees are particularly expensive to establish because most jurisdictions require the implementation of extreme precautionary measures to prevent the spread of transgenic propagules in addition to security issues.
Trait expression in vitro or in the greenhouse can differ from that in the field for a variety of reasons.
Hence, field trials of sufficient duration, however expensive, are essential components of tree research.
Increased demand for forest products necessitates the rapid introduction of desired traits into commercial forestry species.
Rapidly assembling all the desired traits in one tree by conventional breeding in a reasonable time frame is virtually impossible.
Transgenesis by multi-trait engineering (Fig. 1) will allow simultaneous introduction of several transgenes, each with its own optimal promoter, to improve various aspects of the target-plant phenotype.
This more holistic strategy can generate trees that feature a combination of transgenes for resource acquisition and utilization, stress management, and other special traits useful for post-harvest processing.
The hypothetical transformation of the tree in Fig. 1 has the names of some suggested candidate genes in the initial core-trait set, but the specific composition of the core gene set will obviously depend on the objectives for improving the target genotype.
Superior genotype(s) should be targeted for transgenesis.
Genes that enhance growth rate should be included in multi-trait transgenesis programs for several reasons.
Fast-growing transgenic trees are suited for short-rotation cropping for biomass yield.
Their fast growth will help them compete with weeds in the field, replace tissue lost through herbivory or pathogenesis, and compensate for potentially adverse interactions generated by other components of the core-trait set.
To attain optimal growth, the core-trait set should include genes known to increase biomass production by improving plant architecture, photosynthesis and assimilation, nutrient acquisition, etc., as discussed in previous sections of this review.
Perhaps the greatest advantage of incorporating a proven growth gene in the core-trait set is that it provides a trait that is easily detected and qualitatively assessed-this will facilitate high-throughput selection at various stages of production, from in vitro to field plantings.
There are many candidate genes in Table 1 that can confer clearly visible growth improvement (e.g., greater than 30% increase in height) to transgenic plants growing in vitro or in the greenhouse.
These genes should be prioritized for evaluation in the tree species of interest.
The specific genes that might comprise a core-trait set will obviously depend on the traits that need to be introduced in the target genotype.
Ideally, members of a core-trait set should function in different metabolic or signaling pathways to minimize competition or negative-feedback regulation.
Several core-trait sets can be assembled based on current literature as well as site- and species-specific requirements.
These can be used to transform different batches of transformable tissue to generate diverse populations expressing variable sets of core genes.
Sequential selection at specific developmental stages for target phenotypes should enable the isolation of lines that feature the most effective combinations of the transgenes.
Very few tools can reliably introduce multi-gene constructs in a single transformation event.
Transformation with particle (inflow) guns offer the simplest and most efficient way to deliver multiple unique plasmids, or their minimal (i.e., free of vector backbone) linearized effector constructs.
The propensity of bombarded expression cassettes for insertion in one or two loci in the target genome means that selection for growth will most probably select for some of the other introduced traits as well, further simplifying downstream selection.
However, this will need to be tested in all species of interest.
Co-transformation can be improved by linking the expression constructs into a single plasmid, especially when all the genes need to be present to activate a target metabolic or signaling pathway.
However, there may be some physical limitation to the actual length of introduced genetic material that will not be prone to breakage in the bombardment procedure or within the target cells themselves.
There was a recent and unfortunately failed attempt to endow non-leguminous species with the ability to establish a symbiotic relationship with rhizobial bacteria, which involved linking eight genes in one plasmid for use in Agrobacterium-mediated transformation [205].
A potential problem with linked genes is that negative interactions between specific genes can occur and potentially nullify their beneficial effects or even those of the rest of the linked genes.
Transformation procedures need to be highly efficient, i.e., provide large numbers of independent transformants, for selection to be effective.
Selection should hopefully eliminate any plants that lack the desired phenotypes.
Biolistic transformation can help generate transgenic lines featuring a wide variety of transgenic traits.
A series of phenotypic screens will identify the genotypes that express the traits expected from the core gene list.
Thus, a single experiment that ensures multiple transformation "events" followed by sequential screening can generate a population of elite genotypes expressing various combinations of transgenes for superior growth and resistance to multiple stressors.
The regenerants will vary in the composition and copy number of the individual transgenes in the core set.
This highly variable population will offer many opportunities to select regenerants with the best combination of traits and eliminate those that have transgenes with a negative effect on the overall phenotype.
Superior growth due to the inclusion of a proven "growth-promoting" gene in the core-trait set can be considered as a marker trait, akin to antibiotic resistance.
The initial selection for growth should also eliminate poor performers due to undesirable interactions or non-target mutations (e.g., gene truncation, somaclonal variation) resulting from the transformation process.
The strategy outlined in Fig. 1 requires a relatively insignificant increment in the total time and resources that are required to generate a transgenic line over-expressing one transgene.
It will enable the production of a number of elite trees expressing various combinations of traits from one set of transgenes introduced in one transgenic "event".
The elite trees generated by particle gun bombardment will probably possess many, if not all of the genes from each core trait set.
Co-integration will facilitate downstream selection and breeding operations.
These trees may be propagated as clones, but they may be quite useful in breeding programs when both parents feature different core sets of transgenes (i.e., gene stacking).
Due to the probable heterozygous nature of these transgenic lines, the resultant variable population would need to be subjected to pertinent selection and fixation of desirable genotypes by cloning.
This strategy will help address the problem of low genetic variability associated with the current single-gene transformation systems.
There are many well-characterized candidate genes in Tables 1-8 and the number can be increased by including more genes that have only been tested in several annual species.
There is a need to determine the practical limits to the number and kinds of transgenes that can be simultaneously expressed in an organism.
Judicious use of constitutive, tissue-specific, or inducible promoters of varying efficiencies will optimize transgene efficacy.
The eventual success of multi-trait engineering will largely depend on the regulatory environment which currently prohibits transgenic plants in forestry in many countries around the world.
The absence of transgenic genotypes available for commercial forestry (except in China) is in direct contrast with the substantial number of transgenic annual crops that have been approved for commercial release in many countries.
This conundrum is likely due to the lack of corporate sponsors who are willing to bankroll the application process for transgenic trees.
However, there is evidence that this situation is beginning to change.
China has permitted the cultivation of Bt poplar (P. nigra) in 1998 and clone 741 (P. alba×(P. davidiana×simonii)) in 2001 (https://www.isaaa.org/gmapprovaldatabase/default.asp).
In the United States, the USDA has deregulated two transgenic trees, papaya and plum, as of 2012, although these are fruit trees, not forest trees [220].
Adopters of transgenic technologies for commercial forestry will benefit from the deregulation of multi-trait engineering for use in forest species.
We have discussed multi-trait engineering in the context of forestry, but its underlying principles should prove useful in any plant species that is tractable to genetic modification.
Acknowledgements
The authors would like to thank Mr.
John Smith and Drs.
Lloyd Donaldson, Glenn Thorlby, Elspeth MacRae, Ruth Falshaw, and Rowland Burton (Scion, New Zealand) for helping to improve the manuscript.
The authors would also like to extend their utmost appreciation and gratitude to Prof.
J.
Gressel (review editor for Plant Science) and the team of reviewers that he assembled whose suggestions and opinions have substantially improved the manuscript to its present form.

Unusual CH4 dry reforming performance over Ni-based catalyst separated from nickel metal hydride battery waste
Ni-MH waste used here was collected from the crucible for producing Ni-MH battery provided from Mitsui Mining and Smelting Co. Ltd., Japan. In our previous study, Ni-based compound was successfully separated from Ni-MH waste by a series of acid and base treatments [Environ. Sci. Technol., submitted]. Namely, Ni-MH waste of 10 g was dissolved with 2 M (mol/dm3) HCl solution of 200 ml for 24 h. Then, the dissolved transition and rare-earth metal ions were precipitated at pH 12 with 2 M NaOH solution. The Ni component was extracted from this precipitate by treating with 7.5 M NH3 aq. of 200 ml, through forming a Ni-ammonium complex. After filtration, the purple filtrate was heated around 60 degC to decompose the Ni-ammonium complex and to obtain Ni(OH)2. The sample, s-NiO, was obtained by calcination of the resulting Ni(OH)2 at 1000 degC.Coupled soft-template/hydrothermal process synthesis of mesoporous carbon spheres from liquefied larch sawdust
Larch sawdust (10 g), phenol (30 mL), sulfuric acid (98%, 1 mL), and phosphoric acid (85%, 2 mL) were added into a three-necked glass reactor and heated under reflux at a temperature of 110-120 degC for 1 h. The mixture was filtered and the pH was adjusted to neutral and filtered again. The filtrate was concentrated using vacuum distillation at 40 degC, giving the liquefied larch sawdust.

In a typical synthesis, formaldehyde (37%, 90 mL) and sodium hydroxide (3 g) were added to liquefied larch to generate larch-based resin. Different concentrations of F127 (3%, 6%, 8%, and 10%) were added to the liquefied larch and stirred for 12 h at 40 degC, following adjustment of the pH to 0.5 and performed for 12 h. The reaction mixture was allowed to react for 6 h at 180 degC. The mixture was then carbonized under a N2 atmosphere at different temperatures (500 degC, 600 degC, 700 degC) for 2 h. The resultant CSs were denoted as CS-x-y, where x was the concentration of F127, and y was the carbonization temperature.Hydrothermal chemistry of vanadium oxides with aromatic di- and tri-phosphonates in the presence of secondary metal copper(II) cationic complex subunits
All chemicals were used as obtained without further purification: vanadium(V) oxide, copper(II) acetate hydrate, copper(II) nitrate hydrate, 2,2'-dipyridine, 2,2'-dipyridylamine, 1,10-phenanthroline, 2,2':6,2''-terpyridine, and hydrofluoric acid (48 to 51%) were purchased from Aldrich. The phosphonate ligands 1,4-phenyldiphosphonic acid, 1,3-phenyldiphosphonic acid, and 1,3,5-phenyltriphosphonic acid were prepared according to the literature method.57 The copper mononucleating ligand 2-pyridylcarboxylic acid was prepared by refluxing 2-cyanopyridine in concentrated HCl overnight, and subsequent removal of excess liquid. All syntheses were carried out in 23mL poly(tetrafluoroethylene) lined stainless steel containers under autogenous pressure. The reactants were stirred briefly and initial pH was measured before heating. Water was distilled above 3.0 M Ω in-house using a Barnstead Model 525 Biopure Distilled Water Center. The reactions initial and final pH was measured using Hydrion pH sticks.
A mixture of V2O5 (0.078g, 0.429mmol), cupric acetate hydrate (0.212g, 1.17mmol), 1,10-phenanthroline (0.087 g, 0.483 mmol), 1,3-phenyldiphosphonic acid (0.048 g, 0.202 mmol), and H2O (5.00 mL, 277.47 mmol) in the mole ratio 2.12:5.79:2.39:1.00:1374 was stirred briefly before heating to 150 degC for 72 h. Initial and final pH values of 3.0 and 3.0, respectively, were recorded. Green plates of 1 suitable for X-ray diffraction were isolated in 40% yield. IR (KBr pellet, cm-1): 3436(b), 3069(m), 1170(s), 971(s), 950(s), 912(m), 851(w), 799(m), 778(w), 723(w), 523(w). Anal. Calcd. for C15 H10 Cu N2 O5.50 P V: C, 39.8; H, 2.21; N, 6.20. Found: C, 39.6; H, 2.33; N, 6.03.
A solution of V2O5 (0.078 g, 0.429 mmol), cupric nitrate hydrate (0.201 g, 0.864 mmol), 1,10-phenanthroline (0.083 g, 0.461 mmol), 1,3-phenyldiphosphonic acid (0.051g, 0.214mmol), H2O (5.00mL, 277.47mmol), and HF (0.400mL, 11.60mmol) in the mole ratio 2.00:4.04:2.15:1.00:1297:54.21 was heated at 150 degC for 72h (initial and final pH:1.0 and 1.0, respectively). Blue plates of 2 suitable for X-ray diffraction were isolated in 65% yield. IR (KBr pellet, cm-1): 3435(b), 3052(w), 1585(w), 1521(w), 1428(m), 1169(m), 1108(m), 1051(w), 954(s), 934(m), 848(w), 772(w), 720(w), 625(w), 546(w). Anal. Calcd. for C18 H13 Cu N2 O8 P2 V: C, 38.5; H, 2.31; N, 4.98. Found: C, 38.7; H, 2.11; N, 5.14.
The solution of V2O5 (0.077g, 0.423mmol), cupric acetate hydrate (0.210 g, 1.16 mmol), 2,2':6,2'' terpyridine (0.103 g, 0.442 mmol), 1,3-phenyldiphosphonic acid (0.051 g, 0.214 mmol), H2O (5.00 mL, 277.47 mmol), and acetic acid (0.100 mL, 1.74 mmol) in the mole ratio 1.98:5.42:2.07:1.00:1297:8.13 was heated to 135 degC for 60 h and provided green blocks of 3 in 30% yield (initial pH: 3.0, final pH: 3.0). IR (KBr pellet, cm-1): 3448(b), 3033(w), 1604(w), 1577(w), 1480(m), 1450(m), 1412(w), 1333(w), 1168(s), 1094(m), 1057(s), 971(s), 938(s), 922(m), 781(s), 724(s), 671(w), 654(w), 548(s).
The reaction of V2O5 (0.077 g, 0.423 mmol), cupric acetate hydrate (0.207 g, 1.14 mmol), 2,2'dipyridylamine (0.083 g, 0.485 mmol), 1,3-phenyldiphosphonic acid (0.049 g, 0.206 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.100 mL, 2.90 mmol) in the mole ratio 2.05:5.53:2.35:1.00:1347:14.08 at 135 degC for 48 h produced green plates of 4 in 40% yield (initial pH: 2.0, final pH: 1.0). IR (KBr pellet, cm-1): 3434(b), 3187(w), 3082(w), 1638(m), 1586(m), 1531(w), 1476(s), 1438(m), 1368(w), 1344(w), 1237(w), 1163(m), 1141(s), 1066(m), 962(s), 925(m), 767(m), 707(w), 590(w), 551(w), 520(w). Anal. Calcd. for C26 H26 Cu2 F N6 O10 P2 V: C, 37.1; H, 3.09; N, 9.98. Found: C, 36.8: H, 2.89; N, 9.85.
The reactants V2O5 (0.077 g, 0.423 mmol), cupric acetate hydrate (0.206 g, 1.13 mmol), 2,2'dipyridylamine (0.084 g, 0.491 mmol), 1,3-phenyldiphosphonic acid (0.052 g, 0.218 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.200 mL, 5.80 mmol) in the mole ratio 1.94:5.18:2.25:1.00:1273:26.61 were stirred briefly before heating to 135 degC for 48h. The initial and final pH values were 1.0 and 1.0, respectively. Dark green plates of 5 suitable for X-ray diffraction were isolated in 85% yield. IR (KBr pellet, cm-1): 3448(b), 3093(w), 3050(w), 1652(m), 1590(w), 1537(m), 1485(s), 1435(w), 1423(w), 1232(w), 1095(m), 985(m), 968(m), 876(m), 766(m), 705(w), 582(m), 550(w). Anal. Calcd. for C13 H11 Cu F N3 O5 P V: C, 34.4; H, 2.42; N, 9.26. Found: C, 34.7; H, 2.33; N, 9.01.
A reaction of V2O5 (0.080 g, 0.440 mmol), cupric acetate hydrate (0.201 g, 0.864 mmol), 1,10-phenanthroline (0.083 g, 0.461 mmol), 1,4-phenyldiphosphonic acid (0.051 g, 0.214 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.100 mL, 2.90 mmol) with the mole ratio 2.06:4.14:2.15:1.00:1297:13.55 was stirred briefly before heating to 120 degC for 72h. Initial and final pH values of 1.0 and 1.0, respectively, were recorded. Green blocks of 6 suitable for X-ray diffraction were isolated in 40% yield. IR (KBr pellet, cm-1): 3443(b), 1201(m), 1149(w), 993(w), 959(s), 930(s), 874(w), 847(w), 721(w), 625(w), 604(w), 580(w).
A mixture of V2O5 (0.079 g, 0.434 mmol), cupric acetate hydrate (0.200 g, 0.860 mmol), 1,10-phenanthroline (0.084 g, 0.466 mmol), 1,4-phenyldiphosphonic acid (0.050 g, 0.210 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.400 mL, 11.60 mmol) with the mole ratio 2.07:4.10:2.22:1.00:1321:55.24 was stirred briefly before heating to 120 degC for 72 h (initial and final pH values of 0.5 and 0.5, respectively). Blue plates of 7a were isolated in 65% yield which were suitable for X-ray diffraction. IR (KBr pellet, cm-1): 3448(b), 3062(w), 1585(w), 1521(w), 1431(w), 1387(w), 1192(m), 1149(m), 1067(w), 939(s), 852(w), 722(m), 629(w).
A mixture of V2O5 (0.077 g, 0.423 mmol), cupric acetate hydrate (0.203 g, 0.873 mmol), 1,10-phenanthroline (0.084 g, 0.466 mmol), 1,4-phenyldiphosphonic acid (0.049 g, 0.206 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.100 mL, 2.90 mmol) with the mole ratio 2.05:4.24:2.26:1.00:1347:14.08 was stirred briefly before heating to 150 degC for 72h (initial and final pH values of 1.0 and 1.0, respectively). Green blocks of 7b were isolated in 25% yield and were suitable for X-ray diffraction. IR (KBr pellet, cm-1): 3440(b), 3071(w), 1145(s), 1087(m), 1067(m), 1019(m), 956(s), 849(w), 722(w), 620(w).
A mixture of V2O5 (0.082 g, 0.451 mmol), cupric acetate hydrate (0.201 g, 0.864 mmol), 2-pyridyl carboxylic acid (0.130 g, 1.060 mmol), 1,4-phenyldiphosphonic acid (0.050 g, 0.210 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.200 mL, 5.80 mmol) with the mole ratio 2.15:4.11:5.05:1.00:1321:27.61 was stirred briefly before heating to 120 degC for 72 h. The initial and final pH values were 1.0 and 1.0, respectively. Green plates of 8 were isolated in 35% yield and were suitable for X-ray diffraction. IR (KBr pellet, cm-1): 3450(b), 1603(w), 1421(w), 1302(w), 1051(m), 978(s), 832(w), 707(w), 620(m), 583(w), 506(w), 437(m).
A solution of V2O5 (0.078 g, 0.428 mmol), cupric acetate hydrate (0.201 g, 0.864 mmol), 2,2'-dipyridyl (0.065 g, 0.416 mmol), 1,4-phenyldiphosphonic acid (0.047 g, 0.197 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.200 mL, 5.80 mmol) with the mole ratio 2.17:4.39:2.11:1.00:1408:29.44 was stirred briefly before heating to 120 degC for 72 h with an initial and final pH of 1.0,.5. Dark green plates of 9 were isolated in 40% yield which were suitable for X-ray diffraction. IR (KBr pellet, cm-1): 3454(b), 1446(m), 1150(m), 1092(m), 1070(m), 1034(m), 1021(m), 969(s), 872(m), 773(m), 635(m). Anal. Calcd. for C13 H10 Cu F N2 O5 P V: C, 35.6; H, 2.28; N, 6.38. Found: C, 36.0; H, 2.45; N, 6.22.
A reaction of V2O5 (0.079 g, 0.434 mmol), cupric acetate hydrate (0.201 g, 0.864 mmol), 2,2':6,2'' terpyridine (0.100 g, 0.429 mmol), 1,4-phenyldiphosphonic acid (0.049 g, 0.206 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.200 mL, 5.80 mmol) with the mole ratio 2.11:4.19:2.08:1.00:1347:28.16 was stirred briefly before heating to 180 degC for 48 h. Initial and final pH values of 1.0 and 1.0, respectively, were recorded. Blue blocks of 10 were isolated in 65% yield. IR (KBr pellet, cm-1): 3436(b), 3051(w), 1474(w), 1215(w), 1145(w), 1117(m), 1064(w), 1025(w), 960(m), 938(m), 893(w), 780(m), 645(w), 633(w). Anal. Calcd. for C21 H16 Cu F0.5 N3 O9 P2 V1.50: C, 37.9; H, 2.40; N, 6.31. Found: C, 37.8; H, 2.29; N, 6.21.
A mixture of V2O5 (0.077 g, 0.423 mmol), cupric acetate hydrate (0.202 g, 0.868 mmol), 2,2'-dipyridylamine (0.075 g, 0.438 mmol), 1,4-phenyldiphosphonic acid (0.049 g, 0.206 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.100 mL, 2.90 mmol) with the mole ratio 2.05:4.21:2.13:1.00:1347:14.08 was stirred briefly before heating to 150 degC for 96 h. The initial and final pH values were 1.0 and 1.0, respectively. Dark green blocks of 11 were isolated in 10% yield and were suitable for X-ray diffraction. IR (KBr pellet, cm-1): 3435(b), 3214(w), 3117(w), 1655(s), 1591(m), 1536(w), 1487(s), 1433(w), 1103(m), 1080(m), 1055(m), 1021(w), 981(s), 880(w), 765(w), 626(w). Anal. Calcd. for C13 H11 Cu F N3 O5 P V: C, 34.3; H, 2.42; N, 9.26. Found: C, 34.8; H, 2.25; N, 8.88.
Synthesis of [Cu(bpy)VO2 {(HO3P)3C6H3}]*1.5H2O (12*1.5H2O).
The solution of V2O5 (0.076 g, 0.42 mmol), cupric nitrate hydrate (0.202 g, 0.87 mmol), 2,2'-dipyridyl (0.065 g, 0.42 mmol), 1,3,5-phenyltriphosphonic acid (0.069 g, 0.22 mmol), H2O (5.00 mL, 277.47 mmol) and HF (400 uL, 11.60 mmol) in the mole ratio 1.92:4.00:1.92:1.00:1280:53.46 was heated to 150 degC for 72 h and provided blue blocks of 12 in 60% yield (initial pH:1.0, final pH:1.0). IR (KBr pellet, cm-1): 3392(b), 1607(m), 1577(w), 1474(w), 1446(m), 1180(s), 1058(m), 1034(m), 949(s), 804(w), 777(m), 706(w), 527(m).
The reaction of V2O5 (0.078 g, 0.43 mmol), cupric nitrate hydrate (0.205 g, 0.88 mmol), 2,2'-dipyridyl (0.066 g, 0.42 mmol), 1,3,5-phenyltriphosphonic acid (0.06 7g, 0.21 mmol), H2O (5.00 mL, 277.47 mmol) and HF (100 uL, 2.90 mmol) in the mole ratio 2.03:4.18:2.00:1.00:1315:13.74 at 180 degC for 72 h produced green needles of 13 in 40% yield (initial pH:1.0, final pH:1.0). IR (KBr pellet, cm-1): 3448(b), 1555(w), 1492(w), 996(m), 964(s), 906(w), 815(s), 764(w), 661(m), 535(w).
The reactants V2O5 (0.080 g, 0.44 mmol), cupric nitrate hydrate (0.205 g, 0.88 mmol), 1,10-phenanthroline (0.083 g, 0.46 mmol), 1,3,5-phenyltriphosphonic acid (0.069 g, 0.22 mmol), H2O (5.00 mL, 277.47 mmol) and HF (100 uL, 2.90 mmol) in the mole ratio 2.03:4.06:2.12:1.00:1279:11.34 were stirred briefly before heating to 180 degC for 72 h. The initial and final pH values were 1.0 and 1.0, respectively. Green needles of 14 suitable for X-ray diffraction were isolated in 15% yield. IR (KBr pellet, cm-1): 3435(b), 3064(m), 1428(w), 1165(w), 1135(m), 1072(s), 1002(m), 979(w), 928(m), 859(m), 721(w).
A mixture of V2O5 (0.078 g, 0.429 mmol), cupric nitrate hydrate (0.203 g, 0.873 mmol), 2-pyridyl carboxylic acid (0.060 g, 0.487 mmol), 1,3-phenyldiphosphonic acid (0.048 g, 0.202 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.200 mL, 5.80 mmol) in the mole ratio 2.12:4.32:2.41:1.00:1374:28.71 was heated to 135 degC for 48 h. Initial and final pH values of 1.0 and 1.0, respectively, were recorded. Blue needles of 15 suitable for X-ray diffraction were isolated in 30% yield after slow evaporation of resultant blue solution. IR (KBr pellet, cm-1): 3384(b), 3101(w), 3076(w), 1603(m), 1484(w), 1452(w), 1409(s), 1301(w), 1269(w), 1189(m), 1086(s), 1110(m), 1052(m), 936(s), 792(w), 763(m), 693(m), 526(m), 463(m). Anal. Calcd. for C18 H18 Cu N2 O11 P2: C, 38.3; H, 3.19; N, 4.97. Found: C, 38.5; H, 3.01; N, 4.72.
A mixture of V2O5 (0.079 g, 0.434 mmol), cupric acetate hydrate (0.204 g, 0.877 mmol), 2-pyridyl carboxylic acid (0.128 g, 1.039 mmol), 1,4-phenyldiphosphonic acid (0.049 g, 0.206 mmol), H2O (5.00 mL, 277.47 mmol), and HF (0.400 mL, 11.60 mmol) with the mole ratio 2.11:4.26:5.04:1.00:1347:56.31. The reaction was stirred briefly before heating to 120 degC for 72 h with an initial and final pH of 1.0, 1.0. Blue blocks of 16 were isolated in 85% yield and were suitable for X-ray diffraction. IR (KBr pellet, cm-1): 3490(b), 1700(m), 1560(m), 1399(s), 1299(m), 1268(w), 996(m), 923(s), 765(m), 711(w), 691(w), 585(w), 545(w), 465(w).
A mixture of V2O5 (0.078 g, 0.43 mmol), cupric nitrate hydrate (0.200 g, 0.86 mmol), 2,2'-dipyridyl (0.066 g, 0.42 mmol), 1,3,5-phenyltriphosphonic acid (0.068 g, 0.21 mmol), H2O (5.00 mL, 277.47 mmol) and HF (800 uL, 23.20 mmol) in the mole ratio 2.00:4.02:1.98:1.00:1297:108.41 was heated to 150 degC for 72 h. Initial and final pH values of 1.0 and 1.0, respectively, were recorded. Blue blocks of 17 suitable for X-ray diffraction were isolated in 80% yield. IR (KBr pellet, cm-1): 3372(b), 1605(w), 1446(m), 1180(m), 1138(w), 1057(w), 1034(w), 949(s), 804(w), 777(m), 705(m), 527(s), 459(w).
The solution of V2O5 (0.079 g, 0.43 mmol), cupric nitrate hydrate (0.203 g, 0.87 mmol), 2,2'-dipyridyl (0.067 g, 0.43 mmol), 1,3,5-phenyltriphosphonic acid (0.067 g, 0.21 mmol), H2O (5.00 mL, 277.47 mmol) and HF (1000 uL, 29.00 mmol) in the mole ratio 2.06:4.14:2.03:1.00:1315:137.44 was heated to 120 degC for 72 h to give blue blocks of 18 in 70% yield (initial pH:1.0, final pH:1.0). IR (KBr pellet, cm-1): 3374(b), 3100(w), 1606(m), 1577(w), 1499(w), 1473(w), 1448(m), 1311(w), 1167(m), 1058(m), 928(s), 804(m), 777(m), 704(m), 537(s), 492(w).
The reaction of V2O5 (0.079 g, 0.43 mmol), cupric nitrate hydrate (0.210 g, 0.90 mmol), 1,10-phenanthroline (0.086 g, 0.48 mmol), 1,3,5-phenyltriphosphonic acid (0.067 g, 0.21 mmol), H2O (5.00 mL, 277.47 mmol) and HF (800 uL, 23.20 mmol) in the mole ratio 2.06:4.28:2.26:1.00:1315:109.95 at 180 degC for 72 h produced blue blocks of 19 in 50% yield (initial pH:1.0, final pH:1.0). IR (KBr pellet, cm-1): 3421(b), 1655(m), 1583(w), 1519(m), 1428(m), 1402(w), 1132(m), 1116(m), 1065(w), 935(s), 849(m), 722(m), 695(w), 539(s).
Synthesis of [Cu(bpa){H2O3PC6H3(PO3H)2}]*H2O (20*H2O).
The reactants V2O5 (0.081 g, 0.45 mmol), cupric acetate hydrate (0.211 g, 1.16 mmol), 2,2'-bipyridyl amine (0.083 g, 0.49 mmol), 1,3,5-phenyltriphosphonic acid (0.086 g, 0.27 mmol), H2O (5.00 mL, 277.47 mmol) and HF (800 uL, 23.20 mmol) in the mole ratio 1.65:4.30:1.80:1.00:1028:85.93 were stirred briefly before heating to 135 degC for 72 h. The initial and final pH values were 1.0 and 1.0, respectively. Green blocks of 20 suitable for X-ray diffraction were isolated in 90% yield. IR (KBr pellet, cm-1): 3550(w), 3364(w), 3058(w), 3058(w), 1636(m), 1591(m), 1525(m), 1476(s), 1439(w), 1233(w), 1160(w), 1058(m), 937(s), 807(w), 770(m), 703(w), 533(s), 459(w).1. A carbonate precursor compound for manufacturing a lithium metal (M)-oxide powder usable as an active positive electrode material in lithium-ion batteries, M comprising 20 to 90 mol % Ni, 10 to 70 mol % Mn and 10 to 40 mol % Co, the precursor further comprising a sodium and sulfur impurity, wherein the sodium to sulfur molar ratio (Na/S) is 0.4<Na/S<2, and wherein the sum (2*Na<sub>wt</sub>)+S<sub>wt </sub>of the sodium (Na<sub>wt</sub>) and sulfur (S<sub>wt</sub>) content expressed in wt % is more than 0.4 wt % and less than 1.6 wt %.2. The carbonate precursor compound of claim 1, having the general formula MCO3, wherein M=NixMnyCOzAv, A being a dopant, wherein 0.20≤x≤0.90, 0.10≤y≤0.67, and 0.10≤z≤0.40, v≤0.05, and x+y+z+v=1.3. The carbonate precursor compound of claim 2, wherein A is selected from the group consisting of Mg, Al, Ti, Zr, Ca, Ce, Cr, Nb, Sn, Zn and B.4. A carbonate precursor compound for manufacturing a lithium metal (M)-oxide powder usable as an active positive electrode material in lithium-ion batteries, having the general formula MCO<sub>3</sub>, wherein M=Ni<sub>x</sub>Mn<sub>y</sub>CO<sub>z</sub>A<sub>v</sub>, A being a dopant, wherein 0.10≤x<0.30, 0.55≤y≤0.80, and 0≤z≤0.30, v≤0.05, and x+y+z+v=1, the precursor further comprising a sodium and sulfur impurity, wherein the sodium to sulfur molar ratio (Na/S) is 0.4<Na/S<2.5. The carbonate precursor compound of claim 1, wherein the sodium content is between 0.1 and 0.7 wt %, and the sulfur content is between 0.2 and 0.9 wt %.6. A lithium metal oxide powder for a positive electrode material in a rechargeable battery, having the general formula Li<sub>1+a</sub>M<sub>1−a</sub>O<sub>2 </sub>where M=Ni<sub>x</sub>Mn<sub>y</sub>Co<sub>z</sub>A<sub>v</sub>, A being a dopant, wherein 0.10≤a<0.25, 0.10≤x<0.30, 0.55≤y≤0.80, and 0<z≤0.30, v≤0.05, and x+y+z+v=1, the powder having a particle size distribution with 10 μm≤D50≤20 μm, a specific surface with 0.9≤BET≤5, the BET being expressed in m<sup>2</sup>/g, the powder further comprising a sodium and sulfur impurity, wherein the sum (2*Na<sub>wt</sub>)+S<sub>wt </sub>of the sodium (Na<sub>wt</sub>) and sulfur (S<sub>wt</sub>) content expressed in wt % is more than 0.4 wt % and less than 1.6 wt %, and wherein the sodium to sulfur molar ratio (Na/S) is 0.4<Na/S<2.7. A method for preparing a carbonate precursor compound according to claim 1, comprising:
providing a feed solution comprising Ni-, Mn- and Co-ions, and a source of A, wherein the Ni-, Mn-, Co- and A-ions are present in a water soluble sulfate compound,
providing an ionic solution comprising a carbonate solution and Na-ions, wherein the CO<sub>3</sub>/SO<sub>4 </sub>rate is selected so as to obtain a Na/S molar ratio with 0.4<Na/S<2 and the sodium (Na<sub>wt</sub>) and sulfur (S<sub>wt</sub>) content expressed in wt % yield a sum (2*Na<sub>wt</sub>)+S<sub>wt </sub>of more than 0.4 wt % and less than 1.6 wt %,
providing a slurry comprising seeds comprising M′-ions, wherein M′=NixMnyCozA′n,
A′ being a dopant, with 0≤x′≤1, 0≤y′≤1, 0≤z′≤1, 0≤n′≤1 and x′+y′+z′+n′=1,
mixing the feed solution, the ionic solution and the slurry in the reactor, thereby obtaining a reactive liquid mixture,
precipitating a carbonate onto the seeds in the reactive liquid mixture, thereby obtaining a reacted liquid mixture and the carbonate precursor, and
separating the carbonate precursor from the reacted liquid mixture.8. The method according to claim 7, wherein the M′-ions are present in a water insoluble compound that is selected from the group consisting of M′CO3, M′(OH)2, M′-oxide and M′OOH.9. The method according to claim 7, wherein the molar ratio (M′seeds/Mfeed) of the metal content in the seed slurry to the metal content in the feed solution is between 0.001 and 0.1, and wherein the median particle size of the carbonate precursor is determined by the ratio M′seeds/Mfeed.10. The method according to claim 7, wherein A and A′ are selected from the group consisting of Mg, Al, Ti, Zr, Ca, Ce, Cr, Nb, Sn, Zn and B.11. The method according to claim 7, wherein the concentration of NH3in the reactor is less than 5.0 g/L.12. The method according to claim 7, wherein M=M′13. The method according to claim 7, wherein the ionic solution further comprises either one or both of a hydroxide and a bicarbonate solution, and the ratio OH/CO3, or OH/HCO3, or both these ratios are less than 1/10.14. The method according to claim 7, wherein the seeds have a median particle size D50 between 0.1 and 3 μm.15. A method for preparing the carbonate precursor compound of claim 2, comprising:
providing a feed solution comprising Ni-, Mn- and Co-ions, and a source of A, wherein the Ni-, Mn-, Co- and A-ions are present in a water soluble sulfate compound,
providing an ionic solution comprising a carbonate solution and Na-ions, wherein the CO<sub>3</sub>/SO<sub>4 </sub>rate is selected so as to obtain a Na/S molar ratio with 0.4<Na/S<2 and the sodium (Na<sub>wt</sub>) and sulfur (S<sub>wt</sub>) content expressed in wt % yield a sum (2*Na<sub>wt</sub>)+S<sub>wt </sub>of more than 0.4 wt % and less than 1.6 wt %,
providing a slurry comprising seeds comprising M′-ions, wherein M′=NixMnyCozA′n,
A′ being a dopant, with 0≤x′≤1, 0≤y′≤1, 0≤z′≤1, 0≤n′≤1 and x′+y′+z′+n′=1,
mixing the feed solution, the ionic solution and the slurry in the reactor, thereby obtaining a reactive liquid mixture,
precipitating a carbonate onto the seeds in the reactive liquid mixture, thereby obtaining a reacted liquid mixture and the carbonate precursor, and
separating the carbonate precursor from the reacted liquid mixture.Synthesis and electrochemical properties of submicron sized sheet-like LiV3O8 crystallites for lithium secondary batteries

Analytical pure LiOH*H2O, NH4VO3 and PVP were used as raw materials without any purification. Stoichiometric amounts of LiOH*H2O and NH4VO3 (Li:V = 1:3, molar ratio) were mixed in deionized water with continuous magnetic stirring. The resultant little yellow solution was then transferred to a 100 mL Teflon lined autoclave. The autoclave was heated at 160 degC and maintained for 1 h under the microwave hydrothermal system (Sineo MDS-8, China). After the reaction, the autoclave was cooled to room temperature naturally and a colorless clear solution was obtained. This solution was dried in air at 80 degC for 10 h to prepare an orange gel. The gel was ground to powders and then calcined at 400 degC for 10 h in a muffle furnace under air atmosphere. Finally, the powders were cooled to room temperature naturally and ground again to obtain the final products, named as sample A. The synthesis of sample B was the same as sample A, but PVP was added into the solution before microwave hydrothermal.
Template-Directed Synthesis of Pillared-Porous Carbon Nanosheet Architectures: High-Performance Electrode Materials for Supercapacitors
Porous MgO Layer Template: 100 mg MgO nanoparticles (Sinopharm Chemical Reagent Co. Ltd.) were added into 500 mL aqueous solution under the ultrasonication (KQ-600KDE, 600 W) for 30 min, and subsequently boiled for 2-24 h to form Mg(OH)2 using a reflux apparatus. After the filtration and desiccation at 100 degC for 24 h, the as-obtained material was calcined at 500 degC for 30 min.
Synthesis Process: Briefly, 1 g of coal tar pitch (Taiyuan Steel Co. Ltd., China, softening point: 160 degC) was dissolved in 40 mL tetrahydrofuran (THF) solution under stirring. Subsequently, 2 g of porous MgO was added with stirring until complete homogenization. After that, the mixed solution was aged at 60 degC for 48 h, followed by heating at 900 degC for 2 h with a heating rate of 5 degC min-1 in an N2 flow. After carbonization, the black carbon/MgO composite was washed using 10 wt% HCl to remove the MgO, followed by filtration and drying of the sample. For comparasion, nanofoam, and porous layered carbon materials were prepared by the same process as mentioned above using MgO nanoparticle and Mg(OH)2 nanosheet as the template, respectively.
Incremental approaches for updating approximations in set-valued ordered information systems

Abstract
Incremental learning is an efficient technique for knowledge discovery in a dynamic database, which enables acquiring additional knowledge from new data without forgetting prior knowledge.
Rough set theory has been successfully used in information systems for classification analysis.
Set-valued information systems are generalized models of single-valued information systems, which can be classified into two categories: disjunctive and conjunctive.
Approximations are fundamental concepts of rough set theory, which need to be updated incrementally while the object set varies over time in the set-valued information systems.
In this paper, we analyze the updating mechanisms for computing approximations with the variation of the object set.
Two incremental algorithms for updating the approximations in disjunctive/conjunctive set-valued information systems are proposed, respectively.
Furthermore, extensive experiments are carried out on several data sets to verify the performance of the proposed algorithms.
The results indicate the incremental approaches significantly outperform non-incremental approaches with a dramatic reduction in the computational speed.

Introduction
Granular Computing (GrC), a new concept for information processing based on Zadeh's "information granularity", is a term of theories, methodologies, techniques, and tools that makes use of granules in the process of problem solving [1,2].
With the development of artificial intelligence, the study on the theory of GrC has aroused the concern of more and more researchers [3-5].
Up to now, GrC has been successfully applied to many branches of artificial intelligence.
The basic notions and principles of GrC have appeared in many related fields, such as concept formation [6], data mining [7] and knowledge discovery [8,9].
Rough Set Theory (RST) is a powerful mathematical tool for dealing with inexact, uncertain or vague information [10].
It is also known as one of three primary models of GrC [11].
In recent years, there has been a rapid growth of interest in RST and its applications.
It seems to be of fundamental importance to artificial intelligence and cognitive sciences, especially in the areas of machine learning, decision analysis, expert systems, inductive reasoning and pattern recognition [13-16].
The data acquired for rough set analysis is represented in form of attribute-value tables, consisting of objects (rows) and attributes (columns), called information systems [17].
In real-life applications, data in information systems is generated and collected dynamically, and the knowledge discovered by RST need to be updating accordingly [12].
The incremental technique is an effective method to update knowledge by dealing with the new added-in data set without re-implementing the original data mining algorithm [18,19].
Many studies have been done towards the topic of incremental learning techniques under RST.
Considering the problem of discretization of continuous attributes in the dynamic databases, Dey et al. developed a dynamic discreduction method based on RST and notions of Statistics, which merges the two tasks of discretization and reduction of attributes into a single seamless process, so as to reduce the computation time by using samples instead of the whole data to discretize the variables [20].
Considering the problem of dynamic attribute reduction, Hu et al. proposed an incremental positive region reduction algorithm based on elementary set, which can generate a new positive region reduction quickly when a new object is added into the decision information systems [28].
From the view of information theory, Wang et al. proposed an incremental attribute reduction algorithm based on three representative entropies by considering changes of data values, which can generate a feasible reduct in a much shorter time.
However, the algorithm is only applicable on the case of the variation of data one by one [21].
Furthermore, Wang et al. developed a dimension incremental strategy for attribute reduction based on information entropy for data sets with dynamically increasing attributes [22].
Since the core of a decision table is the start point to many existing algorithms of attribute reduction, Yang et al. introduced an incremental updating algorithm of the computation of a core based on the discernibility matrix, which only inserts a new row and column, or deletes one row and updates corresponding column when updating the discernibility matrix [29].
Considering the problem of dynamic rule induction, Fan et al. proposed an incremental rule-extraction algorithm (REA) based on RST, which updates rule sets by partly modifying original rule sets without re-computing rule sets from the very beginning and the proposal approach is especially useful in a large database, since it does not re-compute the reducts/rules that are not influenced by the incremental data set [23].
Nevertheless, alternative rules which are as preferred as the original desired rules might exist since the maximum of strength index is not unique.
The REA may lead to non-complete rules, then an incremental alternative rule extraction algorithm (IAREA) was proposed to exclude the repetitive rules and to avoid the problem of redundant rules [24].
Zheng et al. developed a rough set and rule tree based incremental algorithm for knowledge acquisition, which is not only obviously quicker than that of classic algorithm, but also has a better performance of knowledge learned by the proposed algorithm to a certain degree [25].
Liu et al. defined a new concept of interesting knowledge based on both accuracy and coverage of the generated rules in the information system, and presented an optimization model using the incremental matrix for generating interesting knowledge when the object set varies over time [26,27].
The main goal of RST is to synthesize approximations of concepts from the acquired data, which is a necessary step for expressing and reducing incomplete and uncertain knowledge based on RST [30-32].
The knowledge hidden in information systems can be discovered and expressed in the form of decision rules according to the lower and upper approximations [36-39].
In order to resolve the problem of high computation complexity in computing approximations under the dynamic information systems, many incremental updating algorithms have been proposed.
Therefore, extensive efforts have been devoted to efficient algorithms for computing approximations.
Li et al. presented an incremental method for updating approximations in an incomplete information system through the characteristic relation when the attribute set varies over time, which can deal with the case of adding and removing some attributes simultaneously in the information system [40].
Since the domain of attributes may change in real-life applications, attributes values may be added to or deleted from the domain, Chen et al. proposed the incremental updating approach of approximations while attributes values coarsening or refining in the complete and incomplete information systems [35].
Zhang et al. discussed the change of approximations in neighborhood decision systems when the object set evolves over time, and proposed two fast incremental algorithms for updating approximations when multiple objects enter into or get out of the neighborhood decision table [33].
Li et al. firstly introduced a kind of dominance matrix to calculate P-dominating sets and P-dominated sets in dominance-based rough sets approach, and proposed the incremental algorithms for updating approximations of an upward union and downward union of decision classes [34].
Instead of considering the incremental updating strategies of rough sets, Cheng proposed two incremental methods for fast computing the rough fuzzy approximations, which are established respectively based on the redefined boundary set and the relation between a fuzzy set and its cut sets [41].
However, to our best knowledge, previous studies on incremental computing approximations mainly concerned in the single-valued information systems, but little attention has been paid to the set-valued information systems.
Set-valued information systems are an important type of data tables, and generalized models of single-valued information systems [42].
In many practical decision-making issues, set-valued information systems have very wide applications, which can be used in intelligent decision-making and knowledge discovery from information systems with uncertain information and set-valued information.
In such systems, some of the attribute values of an object may be set-valued, which are always used to characterize the incomplete information, i.e., the values of some attributes are unknown or multi-values.
On the other hand, we often encounter the scenario where the ordering of properties of the considering attributes plays a crucial role in the analysis of information systems.
Considering attributes with preference-ordered domains is an important characteristic of multi-attribute decision making problems in practice.
Greco et al. proposed the Dominance-based Rough Sets Approach (DRSA) [44,45].
This innovation is mainly based on the substitution of the indiscernibility relation by a dominance relation.
Furthermore, Qian et al. established a rough set approach in Set-valued Ordered Information Systems (SOIS) to take into account the ordering properties of attributes in set-valued information systems, and classified the SOIS into two categories: disjunctive and conjunctive systems [43].
Since the characteristics of the set-valued information systems is different from that of single-valued information systems (such as: some of the attribute values for an object are set-valued), the method for knowledge acquisition in the single-valued information systems cannot be applied directly to the set-valued ones.
For this reason, the incremental method for updating approximations in the dynamic set-valued information systems is discussed in this paper.
In [46], Zhang et al. proposed an incremental method for computing approximations in set-valued information systems under the tolerance relation, when the attribute set varies with time.
In this paper, we focus on updating knowledge under the variation of the object set in SOIS.
Firstly, we discuss the principles of incremental updating approximations when the objects in the universe change (increase or decrease) dynamically in the conjunctive/disjunctive SOIS.
Then two incremental updating algorithms are proposed based on the principles.
Finally, the performances of two incremental algorithms are evaluated on a variety of data sets.
The remainder of the paper is organized as follows.
In Section 2, some basic concepts of RST in SOIS are introduced.
The principles and some illustrated examples for incremental updating approximations with the variation of the object set are presented in Section 3.
In Section 4, we propose the incremental algorithms for computing approximations based on the updating principles.
Performance evaluations are illustrated in Section 5.
The paper ends with conclusions and further research topics in Section 6.
Preliminaries
For convenience, some basic concepts of rough sets and SOIS are reviewed in this section [42,43].
A set-valued information system is an ordered quadruple S=(U,C∪{d},V,f), where U={x1,x2,…,xn} is a non-empty finite set of objects, called the universe.
C is a non-empty finite set of condition attributes and d is a decision attribute with C∩{d}=∅; V=VC∪Vd, where V is the domain of all attributes, VC is the domain of all condition attributes and Vd is the domain of the decision attribute; f is a mapping from U×(C∪{d}) to V such that f:U×{C}→2Vc is a set-valued mapping and f: U×{d}→Vd is a single-valued mapping.
In an information system, if the domain (scale) of a condition attribute is ordered according to a decreasing or increasing preference, then the attribute is a criterion.Definition 1
A set-valued information system S=(U,C∪{d},V,f) is called a SOIS if all condition attributes are criterions.
In real problems, many ways to present the semantic interpretations of set-valued information systems have been provided [47-50].
Qian et al. summarized two types of set-valued information systems with two kinds of semantics, which are known as conjunctive (∀x∈U and c∈C, f(x,c) is interpreted conjunctively) and disjunctive (∀x∈U and c∈C, f(x,c) is interpreted disjunctively) set-valued information systems.
According to the introduction of the following two dominance relations to these types of set-valued information systems, SOIS can be also classified into two categories: conjunctive and disjunctive SOIS [43].
Assume the domain of a criterion a∈C is completely pre-ordered by an outranking relation ⪰a; x⪰ay means "x is at least as good as (outranks) y with respect to criterion a".
For a subset of attributes A⊆C, we define x⪰Ay⇔∀a∈A,x⪰ay, which means "x is at least as good as (outranks) y with respect to all attributes in A".Definition 2
Let S=(U,C∪{d},V,f) be a conjunctive SOIS and A⊆C.
The dominance relation in terms of A is defined as:(1)RA∧⩾={(y,x)∈U×U|y⪰Ax}={(y,x)∈U×U|f(y,a)⊇f(x,a),∀a∈A}
Example 1
Table 1 illustrates a conjunctive SOIS, where U={x1,x2,x3,x4,x5,x6}, C={a1,a2,a3,a4}, d is the decision attribute, VC={e,f,g} and Vd={1,2,4}.
Here, we can obtain that f(x1,a1)={e}, f(x2,a1)={e,f,g}.
Since {e,f,g}⊇{e}, we have x2⪰a1x1, that is, x2 is at least as good as x1 with respect to a1.
Definition 3
Let S=(U,C∪{d},V,f) be a disjunctive SOIS and A⊆C.
The dominance relation in terms of A is defined as:(2)RA∨⩾={(y,x)∈U×U|y⪰Ax}={(y,x)∈U×U|maxf(y,a)⩾minf(x,a),∀a∈A}
Example 2
Table 2 illustrates a disjunctive SOIS, where U={x1,x2,x3,x4,x5,x6},C={a1,a2,a3,a4}, D={d}, VC={0,1,2} and Vd={1,2,4}.
Here, we can obtain that f(x1,a1)={1}, f(x2,a1)={0,1}.
Since maxf(x1,a1)=1⩾minf(x2,a1)=0, we have x1⪰a1x2, that is, x1 is at least as good as x2 with respect to a1.
For convenience, we denote RAΔ⩾(Δ∈{∧,∨}) as the dominance relation in SOIS, where ∧ represents the conjunctive SOIS and ∨ represents the disjunctive SOIS.
Furthermore, we denote the granules of knowledge induced by the dominance relation RAΔ⩾(Δ∈{∧,∨}) as follows:•
[x]AΔ⩾={y|(y,x)∈RAΔ⩾},(Δ=∧,∨)
•
[x]AΔ⩽={y|(x,y)∈RAΔ⩾},(Δ=∧,∨)
where [x]AΔ⩾ is called the A-dominating set, describes the objects that dominate x in terms of A.
[x]AΔ⩽ is called the A-dominated set, describes the objects that are dominated by x in terms of A, respectively.
Let U/RA∧⩾ denote a classification on the universe, which is the family set {[x]AΔ⩾|x∈U}.
Any element from U/RA∧⩾ is called a dominance class with respect to A.
Dominance classes in U/RA∧⩾ do not constitute a partition of U in general.
They constitute a covering of U.Example 3
Continuation of Examples 1 and 2
From Table 1, U/RC∧⩾={[x1]C∧⩾,[x2]C∧⩾,…,[x6]C∧⩾}, where [x1]C∧⩾={x1,x2,x3},[x2]C∧⩾={x2},[x3]C∧⩾={x2,x3},[x4]C∧⩾={x2,x4},[x5]C∧⩾={x2,x5},[x6]C∧⩾={x6}.
Analogously, U/RC∧⩽={[x1]C∧⩽,[x2]C∧⩽,…,[x6]C∧⩽}, where [x1]C∧⩽={x1},[x2]C∧⩽={x1,x2,x3,x4,x5},[x3]C∧⩽={x1,x3},[x4]C∧⩽={x4},[x5]C∧⩽={x5},[x6]C∧⩽={x6}.
From Table 2, U/RC∨⩾={[x1]C∨⩾,[x2]C∨⩾,…,[x6]C∨⩾}, where [x1]C∨⩾={x1,x5},[x2]C∨⩾={x2,x3},[x3]C∨⩾={x2,x3,x4,x5,x6},[x4]C∨⩾={x4,x6},[x5]C∨⩾={x5},[x6]C∨⩾={x4,x6}.
Analogously, U/RC∨⩽={[x1]C∨⩽,[x2]C∨⩽,…,[x6]C∨⩽}, where [x1]C∨⩽={x1,x6},[x2]C∨⩽={x2,x6},[x3]C∨⩽={x2,x3,x6},[x4]C∨⩽={x3,x4,x6},[x5]C∨⩽={x1,x3,x5},[x6]C∨⩽={x3,x4,x6}.
Assume that the decision attribute d makes a partition of U into a finite number of classes.
Let D={D1,D2,…,Dr} be a set of these classes that are ordered, that is, ∀i, j⩽r, if i⩾j, then the objects from Di are preferred to the objects from Dj.
The sets to be approximated in DRSA are upward and downward unions of classes, which are defined respectively as Di⩾=⋃i⩽jDj,Di⩽=⋃j⩽iDj,1⩽i⩽j⩽r.
The statement x∈Di⩾ means "x belongs to at least class Di", where x∈Di⩽ means "x belongs to at most class Di".Definition 4
Let S=(U,C∪{d},V,f) be a SOIS.
A⊆C,∀Di⩾(1⩽i⩽r), the lower and upper approximations of Di⩾ with respect to the dominance relation RAΔ⩾(Δ∈{∧,∨}) are defined respectively as follows:(3)RAΔ⩾̲Di⩾=x∈U|[x]AΔ⩾⊆Di⩾,(4)RAΔ⩾¯Di⩾=⋃x∈Di⩾[x]AΔ⩾.Analogously, ∀Di⩽(1⩽i⩽r), the lower and upper approximations of Di⩽ are defined as:(5)RAΔ⩾̲Di⩽=x∈U|[x]AΔ⩽⊆Di⩽,(6)RAΔ⩾¯Di⩽=⋃x∈Di⩽[x]AΔ⩽.
Example 4
Continuation of Example 3
(1)
From Table 1, we have D={D1,D2,D3}, where D1={x3,x5},D2={x1,x4}, D3={x2,x6}.
Thus, we get the unions of classes as follows: D1⩽=D1,D2⩽=D1∪D2,D2⩾=D2∪D3,D3⩾=D3.
From Definition 4, we have: RCΔ⩾̲D1⩽={x5},RCΔ⩾¯D1⩽={x1,x3,x5},RCΔ⩾̲D2⩽={x1,x3,x4,x5},RCΔ⩾¯D2⩽={x1,x3,x4,x5},RCΔ⩾̲D2⩾={x2,x4,x6},RCΔ⩾¯D2⩾={x1,x2,x3,x4,x6},RCΔ⩾̲D3⩾={x2,x6},RCΔ⩾¯D3⩾={x2,x6}.
(2)
Analogously, from Table 2, we have D={D1,D2,D3}, where D1={x4},D2={x1,x3,x6}, D3={x2,x5}.
Thus, we get the unions of classes as follows: D1⩽=D1,D2⩽=D1∪D2,D2⩾=D2∪D3,D3⩾=D3.
From Definition 4, we have: RCΔ⩾̲D1⩽=∅,RCΔ⩾¯D1⩽={x3,x4,x6},RCΔ⩾̲D2⩽={x1,x4,x6},RCΔ⩾¯D2⩽={x1,x2,x3,x4,x6},RCΔ⩾̲D2⩾={x1,x2,x5},RCΔ⩾¯D2⩾={x1,x2,x3,x4,x5,x6},RCΔ⩾̲D3⩾=∅,RCΔ⩾¯D3⩾={x2,x3,x5}.
Incremental updating approximations in SOIS when the object set varies with time
With the variation of an information system, the structure of information granules in the information system may vary over time which leads to the change of knowledge induced by RST.
For example, let us consider a practical information system from the test for foreign language ability of undergraduates in Shanxi University, the test results can be expressed as a set-valued information system where the attributes are all inclusion increasing preferences and the value of each student under each attribute is given by an evaluation expert through a set-value [43].
However, during the process of evaluating the undergraduates language ability, data in an information system does not usually remain a stable condition.
Some objects may be inserted into the original information system due to the arrival of the new students.
On the other hand, some objects will be deleted from the original information system with the graduation of the senior students.
Then the discovered knowledge may become invalid, or some new implicit information may emerge in the whole updated information system.
Rather than restarting from scratch by the non-incremental or batch learning algorithm for each update, developing an efficient incremental algorithm to avoid unnecessary computations by utilizing the previous data structures or results is thus desired.
In this section, we discuss the variation of approximations in the dynamic SOIS when the object set evolves over time while the attribute set remains constant.
For convenience, we assume the incremental learning process lasts two periods from time t to time t+1.
We denote a dynamic SOIS at time t as S=(U,C∪{d},V,f), and at time t+1, with the insertion or deletion of objects, the original SOIS will change into a new one, denoted as S′=(U′,C′∪{d′},V′,f′).
Similarly, we denote the union of classes and the A-dominating set as Di⩾ and [x]AΔ⩾, respectively at time t, which are denoted as Di⩾′ and [x]AΔ⩾′, respectively at time t+1.
According to Definition 4, the lower and upper approximations of Di⩾ with respect to A⊆C are denoted as RAΔ⩾̲Di⩾ and RAΔ⩾¯Di⩾, respectively at time t, which are denoted as RAΔ⩾̲Di⩾′ and RAΔ⩾¯Di⩾′, respectively at time t+1, respectively.
Here, we only discuss the incremental approach for updating approximations in the cases that a single object enter and go out of the information system.
The change of multiple objects can be seen as the cumulative change of a single object.
The approximations can be updated step by step through the updating principles in the case that a single object varies.
Principles for incrementally updating approximations with the deletion of a single object
Given a SOIS S=(U,C∪{d},V,f) at time t, the deletion of object x¯∈U (x¯ denotes the deleted object) will change the original information granules [x]AΔ⩾ (x∈U,A⊆C) and the union of decision classes Di⩾ (1⩽i⩽r).
The approximations of Di⩾ will change accordingly.
Here, we discuss the principles for updating approximations of Di⩾ from two cases: (1) The deleted object belongs to Di⩾, i.e., x¯∈Di⩾; (2) The deleted object does not belong to Di⩾, i.e., x¯∉Di⩾.Case 1:
The deleted object x¯ belongs to Di, i.e., x¯∈Di⩾.
Proposition 1
Let S=(U,C∪{d},V,f) be a SOIS, A⊆C.
When x¯∈Di⩾ is deleted from U, for RAΔ⩾̲Di⩾′, we have:(1)
If x¯∈RAΔ⩾̲Di⩾, then RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾-{x¯};
(2)
Otherwise, RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾.
Proof
When x¯∈Di⩾ is deleted from U, we have U′=U-{x¯},Di⩾′=Di⩾-{x¯}.
For x∈U′,[x]AΔ⩾′=[x]AΔ⩾-{x¯}.
∀x∈U′, if [x]AΔ⩾⊆Di⩾, then [x]AΔ⩾′⊆Di⩾′; Analogously, if [x]AΔ⩾⊈Di⩾, then [x]AΔ⩾′⊈Di⩾′; Thus, from the definition of lower approximation in Definition 4, we have ∀x∈U′, if x∈RAΔ⩾̲Di⩾, then x∈RAΔ⩾̲Di⩾′; If x∉RAΔ⩾̲Di⩾, then x∉RAΔ⩾̲Di⩾′.
Hence, it is easy to get if x¯∈RAΔ⩾̲Di⩾, then RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾-{x¯}; Otherwise, the lower approximation of Di⩾ will remain constant, i.e., RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾.□
Example 5
Continuation of Example 4
(1)
For Table 1, according to Proposition 1, we compute the lower approximations of D2⩾ by deleting x1 and x2 from U, respectively.•
Assume the object x1 is deleted from Table 1, and U′=U-{x1}.
We have x1∈D2⩾ and x1∉RCΔ⩾̲D2⩾.
Therefore, RAΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾={x2,x4,x6}.
•
Assume the object x2 is deleted from Table 1, and U′=U-{x2}.
We have x2∈D2⩾ and x2∈RCΔ⩾̲D2⩾.
Therefore, RAΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾-{x2}={x4,x6}.
(2)
For Table 2, according to Proposition 1, we compute the lower approximations of D2⩾ by deleting x1 and x3 from U, respectively.•
Assume the object x1 is deleted from Table 2, and U′=U-{x1}.
We have x1∈D2⩾ and x1∈RCΔ⩾̲D2⩾.
Therefore, RAΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾-{x1}={x2,x5}.
•
Assume the object x3 is deleted from Table 2, and U′=U-{x3}.
We have x3∈D2⩾ and x3∉RCΔ⩾̲D2⩾.
Therefore, RAΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾={x1,x2,x5}.
Proposition 2
Let S=(U,C∪{d},V,f) be a SOIS, A⊆C.
When x¯∈Di⩾ is deleted from U, for RAΔ⩾¯Di⩾′, we haveRAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾-[x¯]AΔ⩾∪Kwhere K={x|x∈[x¯]AΔ⩾∩K′},K′=⋃x∈Di⩾-{x¯}[x]AΔ⩾.
Proof
According to Definition 4, we have RAΔ⩾¯Di⩾=⋃x∈Di⩾[x]AΔ⩾.
Thus, when the object x¯∈Di⩾ is deleted from U, the A-dominating set [x¯]AΔ⩾ should be removed from the upper approximation RAΔ⩾¯Di⩾, i.e., RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾-[x¯]AΔ⩾.
However, ∃x∈Di⩾-{x¯} satisfies that K=[x¯]AΔ⩾∩[x]AΔ⩾≠∅, and the object x∈[y]AΔ⩾ (y∈Di⩾-{x¯}) should not be removed from RAΔ⩾¯Di⩾.
Therefore, we have RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾-[x¯]AΔ⩾∪K, where K={x|x∈[x¯]AΔ⩾∩K′},K′=⋃x∈Di⩾-{x¯}[x]AΔ⩾.□
Example 6
Continuation of Example 4
(1)
For Table 1, according to Proposition 2, we compute the upper approximation of D2⩾ by deleting x1 from U.
Assume the object x1 is deleted from Table 1, and U′=U-{x1}.
We have x1∈D2⩾,K′=⋃x∈D2⩾-{x1}[x]CΔ⩾={x2,x4,x6}.
Then K={x|x∈[x1]CΔ⩾∩K′}={x2},RCΔ⩾¯D2⩾′=RAΔ⩾¯Di⩾-[x¯]AΔ⩾∪K={x2,x4,x6}.
(2)
For Table 2, according to Proposition 2, we compute the upper approximation of D2⩾ by deleting x1 from U.
Assume the object x1 is deleted from Table 2, and U′=U-{x1}.
We have x1∈D2⩾,K′=⋃x∈D2⩾-{x1}[x]CΔ⩾={x2,x3,x4,x5,x6}.
Then K={x∣x∈[x1]CΔ⩾∩K′}={x5},RCΔ⩾¯D2⩾′=RAΔ⩾¯Di⩾-[x¯]AΔ⩾∪K={x2,x3,x4,x5,x6}.
Case 2:
The deleted object x¯ does not belong to Di, i.e. x¯∉Di⩾.
Proposition 3
Let S=(U,C∪{d},V,f) be a SOIS, A⊆C.
When x¯∉Di⩾ is deleted from U, for RAΔ⩾̲Di⩾′, we haveRAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾∪Kwhere K=x|x∈Di⩾-RAΔ⩾̲Di⩾,Di⩾⊇[x]AΔ⩾′.
If x¯∈[x]AΔ⩾, then [x]AΔ⩾′=[x]AΔ⩾-{x¯}; Otherwise, [x]AΔ⩾′=[x]AΔ⩾.
Proof
According to Definition 4, we have ∀x∈Di⩾, if x∈RAΔ⩾̲Di⩾, then Di⩾⊇[x]AΔ⩾.
When the object x¯∉Di⩾ is deleted from U, we have U′=U-{x¯},Di⩾′=Di⩾, and ∀x∈U′,[x]AΔ⩾′=[x]AΔ⩾-{x¯}.
It is easy to get if Di⩾⊇[x]AΔ⩾, then Di⩾′⊇[x]AΔ⩾′; Thus, ∀x∈RAΔ⩾̲Di⩾,x∈RAΔ⩾̲Di⩾′.
On the other hand, ∀x∈Di⩾-RAΔ⩾̲Di⩾, we know that Di⩾⊉[x]AΔ⩾.
However, it may exist that x¯∈[x]AΔ⩾, and after the deletion of x¯,Di⩾⊇([x]AΔ⩾)′.
Then x should be added to RAΔ⩾̲Di⩾′, that is, RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾∪{x}.
Therefore, we have RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾∪K, where K=x|x∈Di⩾-RAΔ⩾̲Di⩾,Di⩾⊇[x]AΔ⩾′,[x]AΔ⩾′=[x]AΔ⩾-{x¯}.□
Example 7
Continuation of Example 4
(1)
For Table 1, according to Proposition 3, we compute the lower approximation of D2⩾ by deleting x3 from U.
Assume the object x3 is deleted from Table 1, and U′=U-{x3}.
We have x3∉D2⩾,D2⩾-RCΔ⩾̲D2⩾={x1},D2⩾⊇[x1]CΔ⩾-{x3}={x1,x2}.
Therefore, K={x1} and RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾∪K={x1,x2,x4,x6}.
(2)
For Table 2, according to Proposition 3, we compute the upper approximation of D2⩾ by deleting x4 from U.
Assume the object x4 is deleted from Table 2, and U′=U-{x4}.
We have x4∉D2⩾,D2⩾-RCΔ⩾̲D2⩾={x3,x6},D2⩾⊇[x3]CΔ⩾-{x4}={x2,x3,x5,x6} and D2⩾⊇[x6]CΔ⩾-{x6}={x6}.
Therefore, K={x3,x6} and RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾∪K={x1,x2,x3,x5,x6}.
Proposition 4
Let S=(U,C∪d,V,f) be a SOIS, A⊆C.
When the object x¯∉Di⩾ is deleted from U, for RAΔ⩾¯Di⩾′, we have:(1)
If x¯∈RAΔ⩾¯Di⩾, then RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾-{x¯};
(2)
Otherwise, RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾.
Proof
According to Definition 4, we have that RAΔ⩾¯Di⩾=⋃x∈Di⩾[x]AΔ⩾.
Since the deleted object x¯∉Di⩾, there exists an object x∈Di⩾ satisfies x¯∈[x]AΔ⩾ if x¯∈RAΔ⩾¯Di⩾.
Therefore, when x¯ is deleted, we have [x]AΔ⩾′=[x]AΔ⩾-{x¯}.
Then RAΔ⩾¯Di⩾′=⋃x∈Di⩾[x]AΔ⩾′=RAΔ⩾¯Di⩾-{x¯}.
On the other hand, if x¯∉RAΔ⩾¯Di⩾, we have ∀x∈Di⩾,x¯∉[x]AΔ⩾.
Hence, the upper approximation of Di⩾ will remain constant, i.e., RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾.□
Example 8
Continuation of Example 4
(1)
For Table 1, according to Proposition 4, we compute the lower approximations of D2⩾ by deleting x3 and x5 from U, respectively.•
Assume the object x3 is deleted from Table 1, and U′=U-{x3}.
We have x3∉D2⩾ and x3∈RCΔ⩾¯D2⩾.
Therefore, RAΔ⩾¯D2⩾′=RCΔ⩾¯D2⩾-{x3}={x1,x2,x4,x6}.
•
Assume the object x5 is deleted from Table 1, and U′=U-{x5}.
We have x5∉D2⩾ and x5∉RCΔ⩾¯D2⩾.
Therefore, RAΔ⩾¯D2⩾′=RCΔ⩾¯D2⩾={x1,x2,x3,x4,x6}.
(2)
For Table 2, according to Proposition 4, we compute the upper approximation of D3⩾ by deleting x3 and x4 from U, respectively.•
Assume the object x3 is deleted from Table 2, and U′=U-{x3}.
We have x3∉D3⩾ and x3∈RCΔ⩾¯D3⩾.
Therefore, RAΔ⩾¯D2⩾′=RCΔ⩾¯D3⩾-{x3}={x2,x5}.
•
Assume the object x4 is deleted from Table 2, and U′=U-{x4}.
We have x4∉D3⩾ and x4∉RCΔ⩾¯D3⩾.
Therefore, RAΔ⩾¯D3⩾′=RCΔ⩾¯D3⩾={x2,x3,x5}.
Principles for incrementally updating approximations with the insertion of a new object
Given a SOIS (U,C∪{d},V,f) at time t, when the information system is updated by inserting a new object x̃ (x̃ denotes the inserted object) into the unverse U at time t+1, two situations may occur: (1) x̃ forms a new decision class, i.e., ∀x∈U,f(x̃,d)≠f(x,d); (2) x̃ does not form a new decision class, i.e., ∃x∈U,f(x̃,d)=f(x,d).
The difference between the two situations is: in the first situation, in addition to updating the approximations of union of the existing decision classes, we need to compute the approximations for the new decision class.
Firstly, for updating the approximations of the union of the existing decision classes Di⩾ (1⩽i⩽r) when inserting an object, we discuss the principles through two cases similar to the approach taken in the model of deletion: (1) The inserted object will belong to Di⩾, i.e., x̃⪰dx, where x∈Di; (2) The inserted object will not belong to Di⩾, i.e., x̃⪰dx, where x∈Di.
To illustrate our incremental methods for updating approximations when inserting a new object into SOIS, two tables (Tables 3 and 4) are given as follows.
We assume that the objects in Table 3 will be inserted into Table 1, and the objects in Table 4 will be inserted into Table 2.Case 1:
The inserted object x̃ will belong to Di.
Proposition 5
Let S=(U,C∪{d},V,f) be a SOIS, A⊆C.
When x̃ is inserted into U, for RAΔ⩾̲Di⩾′, we have:(1)
If Di⩾′⊇[x̃]AΔ⩾, where Di⩾′=Di⩾∪{x̃}, then RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾∪{x̃};
(2)
Otherwise, RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾.
Proof
According to Definition 4, we have ∀x∈Di⩾, if [x]AΔ⩾⊆Di⩾, then x∈RAΔ⩾̲Di⩾.
Thus, when the object x̃ is inserted into U, we have Di⩾′=Di⩾∪{x̃}; ∀x∈Di⩾, if x̃∈[x]AΔ⩾, then [x]AΔ⩾′=[x]AΔ⩾∪{x̃}.
That is, if Di⩾⊇[x]AΔ⩾, then Di⩾′⊇[x]AΔ⩾′; If Di⩾⊉[x]AΔ⩾, then Di⩾′⊉[x]AΔ⩾′.
It follows that if x∈RAΔ⩾̲Di⩾, then x∈RAΔ⩾̲Di⩾′; If x∉RAΔ⩾̲Di⩾, then x∉RAΔ⩾̲Di⩾′.
Therefore, according to Definition 4, if [x̃]AΔ⩾⊆Di⩾′, we have x̃∈RAΔ⩾̲Di⩾′, and RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾∪{x̃}.
Otherwise, RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾.□
Example 9
Continuation of Example 4
(1)
For Table 1, according to Proposition 5, we compute the lower approximations of D2⩾ when the object x7 and x8 in Table 3 insert into Table 1, respectively.•
Assume the object x7 in Table 3 is inserted into Table 1, and U′=U∪{x7}.
Since f(x7,d)=3, then D2⩾′=D2⩾∪{x7}.
Because of Di⩾′⊇[x7]CΔ⩾={x2,x7}, we have RCΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾∪{x7}={x2,x4,x6,x7}.
•
Assume the object x8 in Table 3 is inserted into Table 1, and U′=U∪{x8}.
Since f(x8,d)=3, then D2⩾′=D2⩾∪{x8}.
Because of D2⩾′⊉[x8]CΔ⩾={x4,x6,x8}, we have RCΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾={x2,x4,x6}.
(2)
For Table 2, according to Proposition 4, we compute the lower approximation of D2⩾ when the objects x7 and x8 in Table 4 insert into Table 2, respectively.•
Assume the object x7 in Table 4 is inserted into Table 2, and U′=U∪{x7}.
Since f(x7,d)=2, then D2⩾′=D2⩾∪{x7}.
Because of D2⩾′⊇[x7]CΔ⩾={x5,x7}, we have RCΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾∪{x7}={x1,x2,x5,x7}.
•
Assume the object x8 in Table 4 is inserted into Table 2, and U′=U∪{x8}.
Since f(x8,d)=2, then D2⩾′=D2⩾∪{x8}.
Because of D2⩾′⊉[x8]CΔ⩾={x4,x6,x8}, we have RCΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾={x1,x2,x5}.
Proposition 6
Let S=(U,C∪d,V,f) be a SOIS, A⊆C.
When x̃ is inserted into U, for RAΔ⩾¯Di⩾′, we haveRAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾∪[x̃]AΔ⩾
Proof
When the object x̃ is inserted into U,U′=U∪{x̃}.
According to Definition 4, we have RAΔ⩾¯Di⩾′=⋃x∈Di⩾′[x]AΔ⩾′.
Since Di⩾′=Di⩾∪{x̃}, then we have RAΔ⩾¯Di⩾′=⋃x∈Di⩾[x]AΔ⩾′∪[x̃]AΔ⩾.
Because ∀x∈U,[x]AΔ⩾′=[x]AΔ⩾∪{x̃} or [x]AΔ⩾′=[x]AΔ⩾, and x̃∈[x̃]AΔ⩾, we can obtain that RAΔ⩾¯Di⩾′=⋃x∈Di+1⩾[x]AΔ⩾∪[x̃]AΔ⩾=RAΔ⩾¯Di⩾∪[x̃]AΔ⩾.□
Example 10
Continuation of Example 4
(1)
For Table 1, according to Proposition 6, we compute the upper approximations of D2⩾ when the object x7 in Table 3 inserts into Table 1.
Assume the object x7 in Table 3 inserts into Table 1, and U′=U∪{x7}.
Since f(x7,d)=3, then D2⩾′=D2⩾∪{x7} and RCΔ⩾¯D2⩾′=RCΔ⩾¯D2⩾∪[x7]CΔ⩾={x1,x2,x3,x4,x6,x7}.
(2)
For Table 2, according to Proposition 6, we compute the upper approximations of D2⩾ when the object x7 in Table 4 inserts into Table 2.
Assume the object x7 in Table 4 inserts into Table 2, and U′=U∪{x7}.
Since f(x7,d)=2, then D2⩾′=D2⩾∪{x7} and RCΔ⩾¯D2⩾′=RCΔ⩾¯D2⩾∪[x7]CΔ⩾={x1,x2,x3,x4,x5,x6,x7}.
Case 2:
The inserted object x̃ will not belong to Di.
Proposition 7
Let S=(U,C∪d,V,f) be a SOIS, A⊆C.
When x̃ is inserted into U, for RAΔ⩾̲Di⩾′, we haveRAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾-Kwhere K=x|x∈RAΔ⩾̲Di⩾,x̃∈[x]AΔ⩾′.
Proof
When the object x̃ is inserted into U, since x̃⪰dx (x∈Di), we have U′=U∪{x̃},Di⩾′=Di⩾.
∀x∈Di⩾′,[x]AΔ⩾′=[x]AΔ⩾ or [x]AΔ⩾′=[x]AΔ⩾∪{x̃}.
We have if [x]AΔ⩾⊈Di⩾, then [x]AΔ⩾⊈Di⩾′.
That is, if x∉RAΔ⩾̲Di⩾, then x∉RAΔ⩾̲Di⩾′.
Hence, we only consider the object x∈RAΔ⩾̲Di⩾, i.e., [x]AΔ⩾⊆Di⩾.
When x̃ is deleted, there may exist that [x]AΔ⩾′=[x]AΔ⩾∪{x̃}.
Then [x]AΔ⩾′⊈Di⩾′=Di⩾, i.e., x∉RAΔ⩾̲Di⩾′.
Therefore, we have RAΔ⩾̲Di⩾′=RAΔ⩾̲Di⩾-K, where K=x|x∈RAΔ⩾̲Di⩾,x̃∈[x]AΔ⩾′.□
Example 11
Continuation of Example 4
(1)
For Table 1, according to Proposition 7, we compute the lower approximations of D2⩾ when the object x9 in Table 3 inserts into Table 1.
Assume the object x9 in Table 3 inserts into Table 1, and U′=U∪{x9}.
Since f(x9,d)=1, then D2⩾ remains unchanged.
Because of RCΔ⩾̲D2⩾={x2,x4,x6},x9⪰Cx6, that is, x9∈[x6]CΔ⩾′.
Hence, we have K={x|x∈RCΔ⩾̲D2⩾,x9∈[x]CΔ⩾′}={x6},RCΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾-K={x2,x4}.
(2)
For Table 2, according to Proposition 7, we compute the lower approximations of D2⩾ when the object x9 in Table 4 inserts into Table 2.
Assume the object x9 in Table 4 is inserted into Table 2, and U′=U∪{x9}.
Since f(x9,d)=1, then D2⩾ remains unchanged.
Because of RCΔ⩾̲D2⩾={x1,x2,x5},x9⪰Cx1, that is, x9∈[x1]C⊇′.
Hence K={x|x∈RCΔ⩾̲D2⩾,x9∈[x]CΔ⩾′}={x1},RCΔ⩾̲D2⩾′=RCΔ⩾̲D2⩾-K={x2,x5}.
Proposition 8
Let (U,C∪d,V,f) be a SOIS, A⊆C.
When x̃ is inserted into U, for RAΔ⩾¯Di⩾′, we have:(1)
If ∃x∈Di⩾,x̃∈[x]AΔ⩾, then RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾∪{x̃};
(2)
Otherwise, RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾.
Proof
When the object x̃ is inserted into U, since x̃⪰dx (x∈Di), we have U′=U∪{x̃},Di⩾′=Di⩾.
Then, ∀x∈Di⩾′, if x̃∈[x]AΔ⩾, then [x]AΔ⩾′=[x]AΔ⩾∪{x̃}.
According to Definition 4, we have x̃∈RAΔ⩾¯Di⩾′, that is, RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾∪{x̃}; Otherwise, if ∀x∈Di⩾,x̃∉[x]AΔ⩾′, that is, [x]AΔ⩾′=[x]AΔ⩾.
Then we have RAΔ⩾¯Di⩾′=RAΔ⩾¯Di⩾.□
Example 12
Continuation of Example 4
(1)
For Table 1, according to Proposition 8, we compute the lower approximations of D2⩾ when the object x9 and x10 in Table 3 insert into Table 1, respectively.•
Assume the object x9 in Table 3 inserts into Table 1, and U′=U∪{x9}.
Since f(x9,d)=1, then D2⩾ remains unchanged.
Because of D2⩾={x1,x2,x4,x6},x9⪰Cx6, that is, x9∈[x6]CΔ⩾′.
Hence RCΔ⩾¯D2⩾′=RCΔ⩾¯D2⩾∪{x9}={x1,x2,x3,x4,x6,x9}.
•
Assume the object x10 in Table 3 inserts into Table 1, and U′=U∪{x10}.
Since f(x10,d)=1, then D2⩾ remains unchanged.
Because of ∀x∈D2⩾={x1,x2,x4,x6},x9⪰Cx, that is, x9∉[x]CΔ⩾′.
Hence RCΔ⩾¯D2⩾′=RCΔ⩾¯D2⩾={x1,x2,x3,x4,x6}.
(2)
For Table 2, according to Proposition 8, we compute the upper approximations of D2⩾ when the object x9 and x10 in Table 4 insert into Table 2, respectively.•
Assume the object x9 in Table 4 inserts into Table 2, and U′=U∪{x9}.
Since f(x9,d)=1, then D2⩾ remains unchanged.
Because of D2⩾={x1,x2,x3,x5,x6},x9⪰Cx1, that is, x9∈[x1]CΔ⩾′.
Hence RCΔ⩾¯D2⩾′=RCΔ⩾¯D2⩾∪{x9}={x1,x2,x3,x5,x6,x9}.
•
Assume the object x10 in Table 4 inserts into Table 2, and U′=U∪{x10}.
Since f(x10,d)=1, then D2⩾ remains unchanged.
Because of ∀x∈D2⩾={x1,x2,x3,x5,x6},x10⪰Cx, that is, x10∉[x]CΔ⩾′.
Hence RCΔ⩾¯D2⩾′=RCΔ⩾¯D2⩾={x1,x2,x3,x5,x6}.
Based on the above analysis, we can compute the approximations of the union of existing decision classes Di⩾ (1⩽i⩽r) when inserting a new object into SOIS.
However, when a new object x̃ is inserted into the universe, it might happen that x̃ will form a new decision class, i.e., ∀x∈U,f(x̃,d)≠f(x,d).
Then the universe U′=U∪{x̃} will be divided into r+1 partitions, such as: D={D1,…,Di, Dnew, Di+1,…,Dr}, where |D|=r+1,Dnew={x̃}.
At this point, in addition to updating the approximations of the existing unions of decision classes, we need to compute the unions of new decision class Dnew: Dnew⩾=Di+1⩾∪{x̃}.Proposition 9
Let S=(U,C∪d,V,f) be a SOIS, A⊆C.
When x̃ is inserted into U, if ∀x∈U,f(x̃,d)≠f(x,d), then the lower approximation of the union of the new decision class Dnew⩾ can be computed as follows:(1)
If [x̃]AΔ⩾⊆Dnew⩾, where Dnew⩾=Di+1⩾∪{x̃}, then RAΔ⩾̲Dnew⩾=RAΔ⩾̲Di+1⩾∪{x̃};
(2)
Otherwise, RAΔ⩾̲Dnew⩾=RAΔ⩾̲Di+1⩾.
Proof
When the object x̃ is inserted into U, then U′=U∪{x̃}.
Since ∀x∈U,f(x̃,d)≠f(x,d),x̃ will form a new decision class.
U′ will be divided into r+1 partitions, such as: D={D1,…,Di,Dnew,Di+1,…,Dr}, where |D|=r+1,Dnew={x̃}.
It is easy to obtain that the union of the new decision class Dnew is: Dnew⩾=Di+1⩾∪{x̃}.
Then from Definition 4, we know that ∀x∈U′, if [x]AΔ⩾′⊆Dnew⩾, then x∈RAΔ⩾̲Dnew⩾; Furthermore, since Dnew⩾=Di+1⩾∪{x̃} and ∀x∈U,[x]AΔ⩾′=[x]AΔ⩾∪{x̃} or [x]AΔ⩾′=[x]AΔ⩾, we have if x∈RAΔ⩾̲Di+1⩾, then x∈RAΔ⩾̲Dnew⩾, and if x∉RAΔ⩾̲Di+1⩾, then x∉RAΔ⩾̲Dnew⩾.
Hence, if [x̃]AΔ⩾⊆Dnew⩾, then RAΔ⩾̲Dnew⩾=RAΔ⩾̲Di+1⩾∪{x̃}; Otherwise, RAΔ⩾̲Dnew⩾=RAΔ⩾̲Di+1⩾.□
Example 13
Continuation of Example 4
(1)
For Table 1, according to Proposition 9, we compute the lower approximations of Dnew⩾ when the object x11 and x12 in Table 3 insert into Table 1, respectively.•
Assume the object x11 in Table 3 inserts into Table 1, and U′=U∪{x11}.
Since ∀x∈U, f(x,d)≠f(x11,d)=3 and f(D2,d)<f(x11,d)<f(D3,d), then D={D1,D2,Dnew,D3},Dnew⩾=D3⩾∪{x11}={x2,x6,x11}.
Because of [x11]CΔ⩾={x2,x11},[x11]CΔ⩾⊆Dnew⩾, we have RCΔ⩾̲Dnew⩾=RCΔ⩾̲D3⩾∪{x11}={x2,x6,x11}.
•
Assume the object x12 in Table 3 inserts into Table 1, and U′=U∪{x12}.
Since ∀x∈U, f(x,d)≠f(x12,d)=3 and f(D2,d)<f(x12,d)<f(D3,d), then D={D1,D2,Dnew,D3},Dnew⩾=D3⩾∪{x11}={x2,x6,x12}.
Because of [x12]CΔ⩾={x2,x4,x12},[x11]CΔ⩾⊈Dnew⩾, we have RCΔ⩾̲Dnew⩾=RCΔ⩾̲D3⩾={x2,x6}.
(2)
For Table 2, according to Proposition 9, we compute the lower approximations of Dnew⩾ when the object x11 and x12 in Table 4 are respectively inserted into Table 2.•
Assume the object x11 in Table 4 inserts into Table 2, and U′=U∪{x11}.
Since ∀x∈U, f(x,d)≠f(x11,d)=3 and f(D2,d)<f(x11,d)<f(D3,d), then D={D1,D2,Dnew,D3},Dnew⩾=D3⩾∪{x11}={x2,x5,x11}.
Because of [x11]CΔ⩾={x5,x11},[x11]CΔ⩾⊆Dnew⩾, we have RCΔ⩾̲Dnew⩾=RCΔ⩾̲D3⩾∪{x11}={x11}.
•
Assume the object x12 in Table 4 inserts into Table 2, and U′=U∪{x12}.
Since ∀x∈U, f(x,d)≠f(x12,d)=3 and f(D2,d)<f(x12,d)<f(D3,d), then D={D1,D2,Dnew,D3},Dnew⩾=D3⩾∪{x12}={x2,x5,x12}.
Because of [x12]CΔ⩾={x3,x5,x12},[x12]CΔ⩾⊈Dnew⩾, we have RCΔ⩾̲Dnew⩾=RCΔ⩾̲D3⩾=∅.
Proposition 10
Let S=(U,C∪d,V,f) be a SOIS, A⊆C.
When x̃ is inserted into U, if ∀x∈U,f(x̃,d)≠f(x,d), then the upper approximation of the union of the new decision class Dnew⩾ can be computed as follows:RAΔ⩾¯Dnew⩾=RAΔ⩾¯Di+1⩾∪[x̃]AΔ⩾where Dnew⩾=Di+1⩾∪{x̃}.
Proof
When the object x̃ inserts into U,U′=U∪{x̃}.
According to Definition 4, we have RAΔ⩾¯Dnew⩾=⋃x∈Dnew⩾[x]AΔ⩾′=⋃x∈Di+1⩾[x]AΔ⩾′∪[x̃]AΔ⩾.
Since Dnew⩾=Di+1⩾∪{x̃}, then RAΔ⩾¯Dnew⩾=⋃x∈Di+1⩾[x]AΔ⩾′∪[x̃]AΔ⩾.
Because ∀x∈U,[x]AΔ⩾′=[x]AΔ⩾∪{x̃} or [x]AΔ⩾′=[x]AΔ⩾, and x̃∈[x̃]AΔ⩾, we can obtain that RAΔ⩾¯Dnew⩾=⋃x∈Di+1⩾[x]AΔ⩾∪[x̃]AΔ⩾=RAΔ⩾¯Di+1⩾∪[x̃]AΔ⩾.□
Example 14
Continuation of Example 4
(1)
For Table 1, according to Proposition 9, we compute the lower approximations of Dnew⩾ when the object x11 in Table 3 inserts into Table 1.
Assume the object x11 in Table 3 inserts into Table 1, and U′=U∪{x11}.
Since ∀x∈U, f(x,d)≠f(x11,d)=3 and f(D2,d)<f(x11,d)<f(D3,d), then D={D1,D2,Dnew,D3},Dnew⩾=D3⩾∪{x11}={x2,x6,x11}.
Because of [x11]CΔ⩾={x2,x11}, we have RCΔ⩾¯Dnew⩾=RCΔ⩾¯D3⩾∪[x11]AΔ⩾={x2,x6,x11}.
(2)
For Table 2, according to Proposition 9, we compute the lower approximations of Dnew⩾ when the object x11 in Table 4 inserts into Table 2.
Assume the object x11 in Table 4 inserts into Table 2, and U′=U∪{x11}.
Since ∀x∈U, f(x,d)≠f(x11,d)=3 and f(D2,d)<f(x11,d)<f(D3,d), then D={D1,D2,Dnew,D3},Dnew⩾=D3⩾∪{x11}={x2,x5,x11}.
Because of [x11]CΔ⩾={x5,x11}, we have RCΔ⩾¯Dnew⩾=RCΔ⩾¯D3⩾∪[x11]CΔ⩾={x2,x3,x5,x11}.
Static (non-incremental) and incremental algorithms for computing approximations in SOIS with the variation of the object set
In this section, we design static and incremental algorithms on the variation of the object set in SOIS corresponding to Sections 2 and 3, respectively.
The static algorithm for computing approximations in SOIS
Algorithm 1 is a static (non-incremental) algorithm for computing the lower and upper approximations in SOIS while the object set in the information system is changed.
In Step 2, we compute all the decision classes, and the set of decision classes are preference-ordered according to the increasing order of class indices.
Step 3-7 compute all the upward unions of classes based on the set of decision classes.
Step 9-11 compute all the A-dominating sets.
Step 12-21 compute the lower and upper approximations in SOIS based on Definition 4.
The incremental algorithm for updating approximations in SOIS when deleting an object from the universe
Algorithm 2 is an incremental algorithm for updating approximations in SOIS while deleting an object from the universe.
Step 3-16 update the approximations of the union of classes Di⩾, when the deleted object x¯ belongs to the union of classes Di⩾.
Step 4-8 compute the lower approximations of Di⩾ by Proposition 1.
Step 9-16 compute the upper approximations of Di⩾ by Proposition 2.
Step 18-34 update the approximations of the union of classes Di⩾, when the deleted object x¯ does not belong to the union of classes Di⩾.
Step 19-27 compute the lower approximations of Di⩾ by Proposition 3.
Step 28-33 compute the upper approximations of Di⩾ by Proposition 4.
The incremental algorithm for updating approximations in SOIS when inserting an object into the universe
Algorithm 3 is an incremental algorithm for updating approximations in SOIS while inserting an object into the universe.
Step 2 compute the A-dominating set with respect to the inserted object x̃.
Step 3-25 update the approximations of the union of classes Di⩾, when the inserted object x̃ will belong to the union of classes Di⩾.
Step 5-10 compute the lower approximations of Di⩾ by Proposition 5.
Step 11 compute the upper approximation of Di⩾ by Proposition 6.
Step 13-24 update the approximations of the union of classes Di⩾, when the inserted object x̃ will not belong to the union of classes Di⩾.
Step 13-18 compute the lower approximations of Di⩾ by Proposition 7.
Step 19-24 update the approximations of Di⩾ by Proposition 8.
Step 26-35 compute the approximation of the union of new decision class Dnew⩾, if the inserted object does not belong to any existed decision classes.
Step 29-33 compute the lower approximation of Dnew⩾ by Proposition 9.
Step 34 compute the upper approximation of Dnew⩾ by Proposition 10.
Experimental evaluations
In this section, in order to evaluate the performance of the proposed incremental algorithms, we conduct a series of experiments to compare the computational time between the non-incremental algorithm and the incremental algorithms for computing approximations based on standard data sets.
The algorithms are implemental using the JAVA programming language in Eclipse 3.5 with Java Virtual Machine (JVM) 1.6 (available at http://www.eclipse.org/platform).
Experiments are performed on a computer with 2.66GHz CPU, 4.0GB of memory and 32-bit Windows 7 OS.
We download four data sets from the machine learning data repository, University of California at Irvine [51], where the basic information of data sets is outlined in Table 5.
The data sets 1-4 in Table 5 are all incomplete information systems with missing values.
In our experiment, we represent all the missing values by the set of all possible values of each attribute.
Then this type of data sets can be regarded as a special case of the set-valued information system.
Besides, we also use the set-valued data generator to generate two artificial data sets 5-6 in order to test the efficiency of the proposed algorithms, which are also outlined in Table 5.
Generally, we perform the experimental analysis with applying the non-incremental algorithm along with our proposed incremental algorithms when the objects inserting into or deleting from the information system, respectively.
In order to present more informative comparative data and acquire more dependable results in our experiments, we compare the computational efficiency of the algorithms according to the following two aspects:(1)
Size of the data set: To compare the computational efficiency and distinguish the computational times used by the non-incremental and incremental algorithms with different-sized data sets, we divide each of the six data sets into 10 parts of equal size, respectively.
The first part is regarded as the 1st data set, the combination of the first part and the second part is viewed as the 2nd data set, the combination of the 2nd data set and the third part is regarded as the 3rd data set, and so on.
The combination of all ten parts is viewed as the 10th data set.
(2)
Update ratio of the data set: The size of updated objects which inserting into or deleting from the universe may different, that is, the update ratio, i.e., the ratio of the number of updating (deleting or inserting) data and original data, may different.
Here, in order to analyze the influence of the update ratio on the efficiency of algorithms, we compare the computational time of the static and incremental algorithms with different update ratios.
That is to say, for each data sets, we conduct the comparison experiments with the same original data size, but different update ratios, i.e., deleting ratios and inserting ratios.
A comparison of computational efficiency between static and incremental algorithms with the deletion of the objects
To compare the efficiency of static (Algorithm 1) and incremental (Algorithm 2) algorithms for computing approximations when deleting the objects from the data sets.
Firstly, we compare the two algorithms on the six data sets in Table 5 with the same updating ratio (the ratio of the number of deleting data and original data), but different sizes of the original data.
Here, we assume that the updating ratio is equal to 5%.
The experimental results are shown in Table 6.
More detailed changing trendline of each of two algorithms with the increasing size of data sets are illustrated in Fig. 1.
Secondly, we compare the computational time of the two algorithms with the same size of original data, but different updating ratios for each data sets (from 5% to 100%).
we show the experimental results in Table 7.
More detailed changing trendline of each of two algorithms with the increasing updating ratio of data sets are presented in Fig. 2.
In each sub-figures (a)-(f) of Fig. 1, the x-coordinate pertains to the size of the data set (the 10 data sets starting from the smallest one), while the y-coordinate presents the computational time.
We use the star lines to denote the computational time of the static algorithm on different sizes of data sets, and the plus lines denote the computational time of the incremental algorithm on different sizes of data sets when deleting the objects into the universe.
It is easy to see the computational time of the both algorithms usually increases with the increase of the size of data sets according to Table 6 and Fig. 1.
As the important advantage of the incremental algorithm shown in Table 6 and Fig. 1, when deleting the objets from the universe, we find that the incremental algorithm is mush faster than the static algorithm for computing the approximations.
Furthermore, the differences become larger and larger when increasing the size of data sets.
In each sub-figures (a)-(f) of Fig. 2, the x-coordinate pertains to the ratio of the number of the deleting data and original data, while the y-coordinate concerns the computational time.
According to the experimental results in Table 7 and Fig. 2, we find that, for the static algorithm, the computational time for computing approximations with deletion of the objects from the universe is decreasing monotonically along with the increase of deleting ratios.
It is because with the increase of ratios, the size of the universe decreases gradually.
On the contrary, for incremental algorithm, we can see that the computational efficiency for computing approximations is changing smoothly along with the increase of deleting ratios.
It is easy to find out the incremental algorithm always performs faster than the non-incremental algorithm for computing approximations until a threshold of the deleting ratio.
The threshold differs depending on the data sets.
For example, in Fig. 2(a), (e), and (f), the thresholds of ratios are around 85%; In Fig. 2(b) and (c), the thresholds of ratios are around 65%; In Fig. 2(d), the incremental algorithm consistently outperforms the static algorithm even in the value of 90%.
A comparison of computational efficiency between static and incremental algorithms with the insertion of the objects
In each sub-figures (a)-(f) of Fig. 3, the x-coordinate pertains to the size of the data set (the 10 data sets starting from the smallest one), while the y-coordinate presents the computational time.
We use the star lines to denote the computational time of static algorithm (Algorithm 1) on different sizes of data sets, and the plus lines denote the computational time of incremental algorithm (Algorithm 3) on different sizes of data sets when inserting the objects into the universe.
Obviously, according to Table 8 and Fig. 3, we can find that the computational time of the both algorithms usually increases with the increasing size of data sets.
However, the incremental algorithm is much faster than the static algorithm for computing the approximations when inserting the objects into the universe.
Furthermore, the differences between static and incremental algorithms are getting larger when increasing the data size.
In each sub-figures (a)-(f) of Fig. 4, the x-coordinate pertains to the ratio of the number of the inserted objects and original data, while the y-coordinate concerns the computational time.
According to the experimental results as shown in Table 9 and Fig. 4, we find that the computational time of both static (Algorithm 1) and incremental (Algorithm 3) algorithms are increasing monotonically along with the increasing of insert ratios.
It is easy to get the incremental algorithm is always faster than the static algorithm when the inserting ratio increases from 10% to 100% according to Fig. 4(a)-(e).
In Fig. 4(f), we find the incremental algorithm is mush faster than the static algorithm when the inserting ratio is less than 85%, but slower than the static algorithm when the inserting ratio is more than 85%.
Conclusions
The incremental technique is an effective way to maintain knowledge in the dynamic environment.
In this paper, we proposed incremental methods for updating approximations in SOIS when the information system is updated by inserting or deleting objects.
Through discussing the principles of updating approximations by deleting objects from the information system and inserting objects into the information system, respectively, we proposed the incremental algorithms for updating approximations based on SOIS in terms of inserting or deleting an object.
Experimental studies pertaining to four UCI data sets and two artificial data sets showed that the incremental algorithms can improve the computational efficiency for updating approximations when the object set in the information system varies over time.
In real-world applications, an information system may be updated by inserting and deleting some objects at the same time.
In our further work, we will focus on improving the incremental algorithm for updating knowledge by deleting and deleting some objects simultaneously.
Furthermore, as an information system consists of the objects, the attributes, and the domain of attributes values, all of the elements in the information system will change as time goes by under the dynamic environment.
In the future, the variation of attributes and the domain of attributes values in SOIS will also be taken into consideration in terms of incremental updating knowledge.
Acknowledgements
This work is supported by the National Science Foundation of China (Nos.
61175047, 61100117 and 71201133) and NSAF (No.
U1230117), the Youth Social Science Foundation of the Chinese Education Commission (11YJC630127) and the Fundamental Research Funds for the Central Universities (SWJTU11ZT08, SWJTU12CX091, SWJTU12CX117).

Hydrothermal synthesis of pure BaFe12O19 hexaferrite nanoplatelets under high alkaline system

BaCl2 2H2O and FeCl3 6H2O are used as initial materials. In a typical experiment, 0.487 g of FeCl3 6H2O and 0.054 g of BaCl2 2H2O (Fe/Ba mole ratio: N=8) were dissolved in 21 mL of distilled water. Then, 4.0 g of KOH was added to the solution with continuous stirring. Finally, the solution was put into a 30 mL of Teflon-lined stainless-steel autoclave and kept it in an oven at 220 degC for 24 h, followed by furnace cooling to room temperature. The as-prepared ultrafine particles were collected by a magnet and then washed with distilled water, alcohol and 10% acetic acid for several times. At last, we got the pure-phase BaFe12O19.

High efficiency hysteresis-less inverted planar heterojunction perovskite solar cells with a solution-derived NiOx hole contact layer
3.084 g nickel(II) acetylacetonate and 1.262 g diethanolamine were added into 30 ml ethanol and the mixture was continuously stirred at 60 degC for 12 h to make a clear green solution. Then, the green solution was transferred into a Teflon-lined stainless steel autoclave of 50 ml capacity and kept in an oven which was previously set to 150 degC for 10 h. The autoclave was cooled to room temperature after the reaction and the resulting dark green solution was filtered and centrifuged at 2000 rpm for 3 min successively. Following this, the upper clear green solution was collected and mixed with about 400 ml cyclohexane, which was then stirred for tens of minutes until a turbid solution was obtained. Finally, the turbid solution was centrifuged at 10000 rpm for 6 min and the precipitation was collected and dispersed again in some ethanol to make NiOx nanoparticles solutions with different concentrations.Specification of Functional Cranial Placode Derivatives from Human Pluripotent Stem Cells

Summary
Cranial placodes are embryonic structures essential for sensory and endocrine organ development.
Human placode development has remained largely inaccessible despite the serious medical conditions caused by the dysfunction of placode-derived tissues.
Here, we demonstrate the efficient derivation of cranial placodes from human pluripotent stem cells.
Timed removal of the BMP inhibitor Noggin, a component of the dual-SMAD inhibition strategy of neural induction, triggers placode induction at the expense of CNS fates.
Concomitant inhibition of fibroblast growth factor signaling disrupts placode derivation and induces surface ectoderm.
Further fate specification at the preplacode stage enables the selective generation of placode-derived trigeminal ganglia capable of in vivo engraftment, mature lens fibers, and anterior pituitary hormone-producing cells that upon transplantation produce human growth hormone and adrenocorticotropic hormone in vivo.
Our results establish a powerful experimental platform to study human cranial placode development and set the stage for the development of human cell-based therapies in sensory and endocrine disease.
Graphical Abstract
Highlights
•
Timed withdrawal of BMP inhibitor is sufficient to induce placode fates from hESCs
•
Timed FGF inhibition suppresses placode fate and induces epidermal lineage
•
Placode-derived trigeminal neurons are functional in vitro and engraft in vivo
•
Pituitary placode-derived cells are capable of hormone release in vitro and in vivo
Cranial placodes are essential for the development of sensory and endocrine organs.
Here, Studer and colleagues report the efficient derivation of cranial placodes from human pluripotent stem cells.
They demonstrate how to obtain specific placode derivatives such as trigeminal ganglia capable of in vivo engraftment, mature lens fibers, and anterior pituitary cells capable of producing human GH and ACTH in vivo.
Their results establish a platform for studying human cranial placode development and for applications of placode derivatives in regenerative medicine.

Introduction
Cranial placodes give rise to cells of the sensory organs, including the optic lens, the nasal epithelium, otic structures, the adenohypophysis, and a subset of cranial nerves such as the trigeminal ganglia.
During development, sensory placodes are formed at the interface of the nonneural ectoderm and neural plate, surrounding the anterior portion of the future CNS (Figure S1A).
Defects in placode development cause a wide spectrum of human congenital malformations ranging from blindness and deafness to hormone imbalance or loss of smell (Abdelhak et al., 1997; Baker and Bronner-Fraser, 2001; Ruf et al., 2004).
To date, cranial placode development has been characterized in model organisms, including the frog, zebrafish, chicken, and, to a lesser extent, the mouse (Baker and Bronner-Fraser, 2001; Bhattacharyya and Bronner-Fraser, 2004; Schlosser, 2006).
However, human placode development has remained largely unexplored due to lack of access to early human tissue and specific placode markers.
Human pluripotent stem cells (hPSCs), including human embryonic (hESCs), and human induced pluripotent stem cells (hiPSCs) have the potential to self-renew, while retaining a very broad differentiation potential.
Over the last few years, protocols have been developed for directing the fate of hESCs into specific cell lineages.
The derivation of CNS cells was among the first hESC differentiation protocols developed in the field (Reubinoff et al., 2001; Zhang et al., 2001).
The differentiation of hESCs into cells of the peripheral nervous system has also been achieved (Lee et al., 2007; Menendez et al., 2011).
In contrast to the successful derivation and application of defined CNS-and neural-crest (NC)-derived cell types, there has been limited success on modeling cranial placode development in hPSCs.
Recently, we developed a neural induction strategy based on the concomitant inhibition of the Bone Morphogenetic Protein (BMP) and TGF-β/Activin/Nodal signaling pathways (dual-SMAD inhibition [dSMADi] (Chambers et al., 2009).
Exposure to Noggin (N) and SB431542 (SB) leads to the synchronized, rapid and efficient differentiation of hPSCs into CNS fates.
Here, we report that derepression of endogenous BMP signaling during dSMADi is sufficient for the selective induction of human cranial placodes.
Using the placode induction protocol (PIP) >70% of all cells adopt a SIX1+ cranial placode precursor fate by day 11 of differentiation.
We further identify a preplacodal lineage competent to differentiate into selective placode fates including trigeminal sensory neurons, mature lens fibers, and hormone-producing anterior pituitary cells.
Trigeminal sensory neurons are characterized by marker expression, electrophysiology, and by transplantation into the developing chick embryo and the adult mouse CNS.
Finally, we report the derivation of human pituitary cells producing growth hormone (GH) and adrenocorticotropic hormone (ACTH) hormones in vitro and in vivo.
Results
Derepression of Endogenous BMP Signaling Induces Placode at the Expense of Neuroectoderm
To address whether the dSMADi protocol is suitable for the derivation of placodal cells, we first defined a set of appropriate placode markers.
Based on studies in model organisms, we hypothesized that members of the SIX, EYA, and DLX family of transcription factors (Baker and Bronner-Fraser, 2001; Schlosser, 2006) mark human placode fate.
Within the ectodermal lineage, SIX1 is placode specific, marking both the early preplacodal region and the various specific placodes (Schlosser, 2006).
Based on studies in the chick embryo, placode induction relies on a complex interplay of fibroblast growth factor (FGF), BMP, and WNT signals during early ectodermal patterning in vivo (Litsiou et al., 2005).
Activity of BMPs within the ectoderm is thought to be particularly critical in allocating fates.
A model has been proposed initially whereby high levels of signaling promote an epidermal fate, moderate levels induce placodes, intermediate levels specify NC, and a complete absence of BMP activity is required for neural plate formation (Wilson et al., 1997).
More recent studies have revised the original model by confirming an early role for BMP signaling in establishing placode competence (Kwon et al., 2010), whereas the subsequent stage was shown to require BMP inhibition rather than BMP activation (Ahrens and Schlosser, 2005; Kwon et al., 2010; Litsiou et al., 2005).
To test whether early BMP exposure promotes the derivation of SIX1+ placodal cells, we exposed SB (the TGF-β inhibitor) -treated hESCs to various concentrations of BMP4.
However, addition of BMP4 in the presence of SB caused a dramatic morphological change and triggered induction of CDX2 (Figures S1B and S1C), similar to the BMP-mediated induction of trophectoderm-like lineages reported previously (Xu et al., 2002).
We next tested whether timed withdrawal of the BMP inhibitor Noggin during Noggin/SB431542 (N-SB)-mediated differentiation could induce placodal fates via derepressing endogenous BMP signaling.
We performed a time-course analysis during which we removed Noggin at different time points of the N-SB protocol (Figure 1A).
Gene expression analysis at day 11 revealed a robust induction of DLX3, SIX1, and EYA1 (Figure 1B) upon withdrawal of Noggin at days 2 or 3 of differentiation.
In contrast, Noggin withdrawal at day 1 of differentiation led to the induction of EYA1 in the absence of SIX1 expression and triggered morphological changes as well as CDX2 expression, suggesting trophectodermal differentiation (though CDX2 and EYA1 can also be expressed in hESC-derived mesodermal lineages [Bernardo et al., 2011]).
Our data indicate that EYA1 is expressed in both trophectodermal and placodal lineages, and that coexpression with SIX1 is required to define placodal lineage.
Immunocytochemical analysis of hESC progeny at day 11 of differentiation demonstrated that Noggin withdrawal at day 3 (PIP conditions) induced a switch from 82% PAX6+ neuroectodermal cells under N-SB conditions to 71% SIX1+ putative placode precursor cells under PIP (Figures 1C, 1D, and S1D).
SIX1+ clusters expressed other placodal markers such as EYA1, DACH1, and FOXG1 (BF1) (Figure 1E).
DACH1 is also expressed in anterior neuroectodermal cells (Elkabetz et al., 2008) marking neural rosettes, whereas in PIP-treated cultures DACH1 marks placodal clusters (Figure S1E).
Temporal analysis of gene expression under PIP conditions revealed rapid downregulation of pluripotency markers (OCT4, NANOG), as well as markers of trophectoderm (CDX2) (Figure 1F), mesoderm (T), and endoderm (SOX17) (Figure 1G).
SIX1 expression in placode was confirmed in human primary tissue (Carnegie Stage 15, ∼5.5 weeks postconception [p.c.]; data not shown).
SIX1 is also expressed in precursors of skeletal muscle, thymus, and kidney cells.
However, we did not detect expression of skeletal muscle (MYOD), endoderm (SOX17), or mesoderm (Brachyury [T]) markers during PIP confirming that the hESC-derived SIX1+ cells are of placode identity.
We also observed very few (<1%) NC lineage cells under PIP conditions based on SOX10 expression by immunocytochemistry and SOX10::GFP (Chambers et al., 2012; Mica et al., 2013) reporter line expression (Figure S1F).
Induction of cranial placode markers was observed by day 5 with FOXG1 preceding expression of SIX1 and DLX3 (Figure 1H).
The PIP protocol was validated in multiple hESC and hiPSC lines (Figures S1G and S1H).
To further validate the identity of hESC-derived placodal precursors, we made use of a conserved Eya1 enhancer element (Ishihara et al., 2008).
We readily observed GFP expression following nucleofection of the enhancer in olfactory placode but not in age-matched midbrain cultures (Figures S1I-S1J) confirming specificity.
In hESC-derived placode, we observed an 8-fold increase in the percentage of cells with enhancer activity in PIP versus N-SB (Figure S1K).
Microarray Analysis Reveals Human Placode Progenitor Gene Expression
We next performed temporal transcriptome analysis to establish an unbiased molecular assessment of the in vitro placode induction process.
RNA was collected at five time points in triplicates (days 1, 3, 5, 7, and 11) in control N-SB- versus PIP-treated cultures (Figures 2A-2E; all raw data are available at GEO accession no.
GSE51533).
Prior to microarray analysis, the quality of each sample was verified for expression of a panel of placode markers (SIX1, DLX3, EYA1) and the absence of other lineage markers (FOXA2, endoderm; SOX17, endoderm; MYOD, skeletal muscle; CDX2, trophoblast; and T, mesoderm).
Cluster and principal component analyses showed a temporal segregation of the transcriptome data in PIP versus N-SB-treated cells by day 7 of differentiation (Figures 2A and S2A).
Transcriptome data also defined a set of genes that distinguish placodal from neuroectodermal fate (Tables S1 and S2).
To gain insight into specific genes differentially expressed during placode induction, we performed pairwise comparisons for each differentiation time point (Figures 2B-2E).
Among the most highly enriched transcripts under PIP condition were known placode markers such as GATA3, DLX5, DLX3, TFAP2A, and TFAP2C.
SIX1 was significantly upregulated in the microarray analysis by day 9 of differentiation, though it was not among the 20 most differentially regulated genes.
Differential expression for these and additional genes was verified by quantitative RT-PCR (qRT-PCR) (Figure S2B).
We also observed significant transcriptional changes in WNT and BMP pathway components such as an increase in the WNT pathway inhibitor DKK-1 and BMP antagonists, such as GREMLIN-1 and BAMBI (Figures 2B-2D), which are known transcriptional targets of BMP signaling (Grotewold et al., 2001).
Interestingly, we also noticed induction of ISL1, a well-known marker for sensory neurons, motoneurons, heart progenitors, and pancreatic islet cells (Hunter and Rhodes, 2005).
Based on the early onset of ISL1 expression and the absence of mesodermal fates, we surmised that under PIP conditions ISL1 represents an early human placode marker.
ISL1 was one of the first makers induced during PIP differentiation.
Similar to TFAP2A and SIX1 (Figure 2F), ISL1 protein remained expressed in the majority of cells by day 11 of differentiation partially colocalizing with SIX1 (Figure 2G).
Another placode marker identified by our microarray expression data is OVOL2, a member of the Ovo family of zinc-finger transcription factors.
OVOL2 was enriched during PIP by day 5 of differentiation, and placode clusters showed strong immunoreactivity for OVOL2 (Figure 2H).
In contrast, OVOL2 was downregulated under conditions promoting CNS (N-SB) or neural crest (N-SB/CHIR [Chambers et al., 2012; Mica et al., 2013]) fates (Figure 2I).
We identified additional differentially expressed genes during anterior placode specification, including FOXC1 and HAPNL1 (also known as CRTL1).
FOXC1 marks the lens placode during chick development (Bailey et al., 2006) but may also be expressed at the preplacode stage (Sasaki and Hogan, 1993).
HAPNL1 was one of the most differentially expressed genes in PIP and was previously shown to be expressed in the surface ectoderm and chick neural plate border (Colas and Schoenwolf, 2003).
Gene ontology (GO) analysis, using DAVID (http://david.abcc.ncifcrf.gov/) (Dennis et al., 2003), indicated that transcripts highly enriched in PIP versus N-SB conditions at day 7 and day 9 of differentiation are associated with sensory organ development, BMP and WNT pathways, as well as ectoderm and epidermis development (Figure S2C).
The total number of differentially expressed genes increased by day 7 of differentiation (Figures S2D and S2E) matching the proposed time frame for placode commitment.
In summary, our molecular analyses present a roadmap of human placode development and an important resource for future functional studies.
Early Treatment with FGF Inhibitor Suppresses Placode and Induces Surface Ectoderm Fate
Lineage studies in several vertebrate species indicate that cranial placode precursors originate from the surface ectoderm and are induced by signals emanating from the adjacent neuroectoderm (Pieper et al., 2012), rather than being of neural origin.
However, no such data are available for human development.
The putative timing of human placode induction (approximately day 20 p.c.) (O'Rahilly, 1987) makes this a stage largely inaccessible to experimental manipulations.
Gene expression data during PIP indicate that TFAP2A is one of the earliest upregulated and differentially expressed genes (Figures 2B-2E).
During mouse development, early expression of the orthologous gene Tcfap2a (E7.5) is restricted to the future surface ectoderm (Arkell and Beddington, 1997).
The induction of TFAP2A was first detected at day 5 of PIP, whereas absent in N-SB protocol (Figure 3A) suggesting that the period between days 3 and 5 of PIP corresponds to the commitment toward surface ectoderm.
SIX1 was induced 2 days later than TFAP2A (Figures 3A and 3B) further pointing to a surface ectoderm intermediate.
In vivo genetic studies indicate that high levels of BMPs in the absence of FGF signaling promote epidermal fate, whereas placodal cells are established at reduced levels of BMPs upon activation of FGFs (Kudoh et al., 2004; Litsiou et al., 2005).
Therefore, we tested whether blocking endogenous FGF signaling under PIP conditions disrupts placode induction and triggers epidermal fate.
Exposure to SU5402, a small molecule inhibiting FGF signaling, from day 3 to 11 of differentiation (Figure S3A) suppressed the emergence of SIX1+ clusters while maintaining TFAP2A expression (Figure 3C).
Quantification of SIX1 and TFAP2A gene expression at day 11 confirmed a complete loss of placode marker expression (Figure 3D), whereas TFAP2A expression was maintained.
The ectodermal precursor identity of TFAP2A+ cells was supported by the expression of the epidermal precursor marker KRT8 in PIP + SU5402-treated cultures (Figure 3E).
Furthermore, long-term cultures (days 42-60) showed robust induction of the mature keratinocyte marker KRT14 (Figure 3F) and formed E-CADHERIN-positive patches with KRT14-immunoreactive cells at the periphery (Figure S3B).
Analysis of the proliferative capacity of the epidermal precursors showed KI67 expression primarily in E-CADHERIN+/KRT14- cells.
Cell proliferation decreased by day 60 concomitant with an increased percentage of cells expressing KRT14 (Figures 3G and S3C).
In addition to the requirement for endogenous FGF signaling, we observed low WNT and BMP levels as other important parameters for the transition from surface ectoderm (day3) to early placode fate.
Exposure to high concentrations of CHIR or BMP4 suppressed placode at the expense of NC or putative trophectoderm, respectively (Figures S3D and S3E).
Placode Precursors Efficiently Differentiate into Trigeminal-type Sensory Neurons
Cranial placodes give rise to a broad range of specialized cell types, including hormone-producing cells of the anterior pituitary gland, structural cells such as lens fibers in the eye, and sensory neurons including trigeminal neurons (Figure 4A).
Placodes can be characterized by the expression of specific PAX genes (McCauley and Bronner-Fraser, 2002).
We observed that under standard PIP conditions most SIX1+ clusters coexpressed PAX3 (Figure 4B), suggesting ophthalmic trigeminal placode identity (McCabe et al., 2004; Stark et al., 1997).
The spontaneous generation of HNK1+ (Metcalfe et al., 1990) cells with neuronal morphologies and coexpression of ISL1 confirmed peripheral sensory neuron identity (Figure 4C).
To further ascertain the placode origin of the sensory neurons under PIP conditions, we assessed coexpression of neuronal markers with SIX1 (Figures 4D and S4A).
Lack of SOX10 expression during PIP-based sensory neuron differentiation ruled out a neural crest origin.
By day 20 of differentiation (7 days after replating), the cells formed ganglia-like structures with neurons extending long, radial processes and with nuclear expression of BRN3A (Figures 4D and 4E), a sensory neuron marker.
Most neurons retained ISL1 expression by day 42 of differentiation and acquired expression of the peripheral neuron marker peripherin (Figures 4F and S4B).
Immunocytochemical analysis for neurotransmitter phenotypes revealed expression of glutamate (Figure 4G) but lack of expression of TH and GABA (data not shown).
These data are compatible with the generation of glutamatergic trigeminal sensory neurons.
Gene expression analysis showed induction of RUNX1 (Figures 4H and S4C) and RET (Figure S4D), indicating sensory/nociceptive lineage.
Expression of TRK receptors (Figures 4I and S4E) including NTRK1 and NTRK2 points to the presence of both nociceptive and nonnociceptive sensory neurons (Figures S4E-S4G).
Diagnostic markers of nociceptive neuron identity include expression of specific sodium channels (SCN9A, SCN10A, SCN11A; Figure 4J) as well as classic pain receptors such as the capsaicin receptor (TRPV1), the receptor for cold sensation (TRPM8), and the P2X3 receptor critical for sensation of inflammatory pain mediated by ATP (Figure 4K).
Gene expression analysis in long-term trigeminal neuronal cultures (day 55) showed sustained expression of RET1 and RUNX1 (Figures S4C and S4D).
Functional analysis of placode-derived sensory neurons was performed by whole-cell patch-clamp recordings (Figure S4G).
Neurons were filled with Lucifer yellow from the recording pipette exhibiting either bipolar or tripolar morphologies (Figures 4L and 4M).
Responses were measured to a series of hyperpolarizing and depolarizing pulses (Figure 4N).
The hESC-derived neurons produced single action potentials at a threshold depolarization matching the functional properties reported for primary embryonic trigeminal neurons (Grigaliunas et al., 2002).
The average resting membrane potential (RMP) was -65.6 ± 6.7 (Figures 4L and 4M).
Passive membrane and action potential properties were comparable between bipolar and tripolar sensory neurons as summarized in (Figure 4O) suggesting that two morphologically distinct populations do not reflect functionally distinct subgroups.
We further assessed the robustness of the trigeminal sensory neuron induction protocol across multiple hESC and hiPSC lines.
We observed comparable percentages of ISL1- and BRN3A-expressing neurons across lines (Figure S4H).
In Vivo Analysis of hESC-Derived Trigeminal Neurons in the Developing Chick Embryo and Adult Mouse CNS
To assess the in vivo properties of hESC-derived trigeminal placode precursors, PIP-induced neuronal clusters, derived from a constitutively GFP-positive hESC line (Figures S5A and S5B), were injected into the developing chick embryo targeting the early trigeminal anlage at H&H stage 10-12 (Figure S5C).
Human cells were identified based on GFP expression and use of human specific antibodies against cytoplasmic antigen (hCA).
Two days after in ovo transplantation, surviving GFP+ cells were found dispersed in the area of the endogenous chick trigeminal ganglion (Figure 4P).
We observed extensive GFP+ human fiber bundles coexpressing hCA and peripherin (Figures 4Q and 4R).
In contrast, no hCA or peripherin expression was detected in the neural tube of the embryo (Figure S5D).
The in vivo fiber outgrowth 2 days after transplantation was reminiscent of the extensive in vitro fiber outgrowth of replated trigeminal neuron clusters (Figure S5A).
Peripherin expression in vivo (Figure 4S) confirmed the peripheral neuron identity of the grafted cells.
We next addressed whether hESC-derived trigeminal neurons can engraft in the adult mouse CNS and project toward their physiological target.
The trigeminal nuclei in the brainstem receive afferent innervation from the trigeminal sensory ganglion that is relayed to the contralateral thalamus.
The pons was selected as site for transplantation, because it is surgically accessible and located within proximity of the trigeminal brain stem nuclei that receive afferent input from the trigeminal ganglia.
Hence, GFP+ human trigeminal neuron clusters were injected into adult NOD/SCID mice via stereotactic surgery (see Experimental Procedures).
Histological analysis 4 weeks after transplantation showed survival of GFP+ human cell graft in the ventral pons (Figure S5E).
Although GFP+ cell bodies remained tightly clustered at injection site, GFP+ fibers showed extensive projections into the host brain (n = 6) including the endogenous trigeminal nuclei (Figure S5F).
Expression of BRN3A confirmed the sensory neuron identity of the cells (Figure S5G).
Graft-derived human fiber bundles (hNCAM+ and GFP+) were observed emanating from the graft core (Figure S5H).
These data demonstrate in vivo survival of trigeminal placode derivatives, differentiation along sensory neuron lineage, and the establishment of axonal projections toward relevant endogenous targets in the embryonic chick and adult mouse brain.
Identification of a Putative Preplacode Stage
Our results indicate that current PIP conditions efficiently induce ophthalmic trigeminal placode fates.
To investigate whether other placodal fates can be generated using modified PIP conditions, we first addressed the presence of putative preplacode cells in our culture system.
During vertebrate development, the preplacode is characterized as the developmental anlage containing precursor cells competent to respond to signals determining placode identity (Martin and Groves, 2006).
Preplacodal cells in various model organisms have been shown to express Six1 and to coexpress markers of both ectodermal and neural fate.
However, the development of a human preplacode remains unexplored.
We performed a time-course coexpression analysis for TFAP2A (early ectodermal marker) and PAX6 (early neuroectoderm marker [Zhang et al., 2010]).
During the first 3 days of differentiation, only a few sparse patches of PAX6- or TFAP2A-expressing cells were observed without evidence of coexpression (Figure 5A).
At day 5, 2 days following Noggin withdrawal (PIP), there was an increase in the number of TFAP2A+ cells (Figure 5B) under PIP conditions and a concomitant loss of TFAP2A+ cells in N-SB (Figure 5C).
At day 7, N-SB conditions yielded PAX6+ cells devoid of TFAP2A expression (Figure 5C), whereas PIP-treated cultures showed extensive coexpression of TFAP2A and PAX6, indicating preplacode identity (Figure 5B).
The emergence of TFAP2A/PAX6 double-positive cells coincided with the onset of SIX1 gene expression and the emergence of SIX1+ clusters around day 7 of differentiation (Figures 3B and S6A).
By day 11, placode clusters were negative for PAX6 but retained expression of TFAP2A (Figure 5B), suggesting that early anterior PAX6+ preplacode cells give rise to PAX6-negative posterior placode populations enriched for PAX3+.
This model was further supported by gene expression data showing a robust PAX3 increase from day 7 to 11 of PIP (Figure 5D).
Treatment with inhibitors of WNT or FGF signaling from day 7 to 11 of PIP suppressed PAX3 induction (Figure 5E) while maintaining PAX6 (Figure 5F).
Our results indicate that endogenous signals contribute to the transition from an anterior PAX6+ preplacode to a posterior PAX3+ placode lineage.
The small number of TFAP2A cells in day 11 N-SB cultures (Figure 5C) likely represents neural crest precursors (Chambers et al., 2009) that lack PAX6 expression.
Under PIP conditions the percentage of contaminating SOX10+ NC cells at day 11 was <1% (Figure S1F).
Treatment with FGF Inhibitor SU5402 at Preplacode Stage Induces Lens Fates
We next tested whether putative preplacode cells can be coaxed into specific placode fates other then trigeminal neurons.
The spontaneous appearance of lens precursors (lentoid bodies) from primate and hESCs has been previously reported (Ooto et al., 2003; Zhang et al., 2010), although the lineage origin and inducing signals remained unexplored in those studies.
We tested the impact of four developmental signaling pathways on lens placode specification using activators and inhibitors of BMP, FGF, WNT, and Hedgehog signaling.
To quantify the induction of lens placode fate, we monitored expression of the lens precursor marker PITX3 at day 16 of differentiation (Figure S6B).
PITX3 expression was significantly induced in the presence of recombinant BMP4 or upon exposure to the FGF-inhibitory molecule, SU5402 (Figure 5G).
A role for BMPs and FGFs has been previously proposed in developmental studies in the chick (Sjödal et al., 2007).
Further differentiation revealed strong induction of αβ-crystalline and the formation of mature lens fiber structures by day 57 (Figure 5H, left panel).
The characteristic layering of lens fibers (Figure 5H, right panel) mimicked the structural properties of developing lens in vivo.
Treatment with SHH at Preplacode Stage Induces Anterior Pituitary Cells
We next explored whether preplacodal cells can be differentiated into anterior pituitary placode and recreate the various pituitary precursors and hormone-producing cell types (Figure 6A).
We observed that treatment with agonists for SHH signaling (Figure S7A) at the preplacode stage (days 7-11) induced expression of the oral ectoderm marker SIX6 and PITX1 (Figure 6B), master regulators of pituitary gland development (Tremblay et al., 1998).
Induction of PITX1 and SIX6 at transcript and protein levels was dependent on SHH dose (Figures 6B and 6C).
Furthermore, SHH treatment triggered the expression of the definitive pituitary precursor marker LHX3 (Figure 6D).
Endocrine cells of the anterior pituitary gland are derived from three main precursors lineages (Scully and Rosenfeld, 2002).
Following PIP + SHH treatment, we observed robust induction of TBX19 (Figure 6E), specific to precursors giving rise to ACTH- and MSH-producing cells.
Induction of PIT1 and GATA2 precursor lineages was less efficient but was increased following treatment with the γ-secretase inhibitor DAPT (Figures 6F and 6G).
Immunocytochemistry for pituitary hormones showed expression of CGA by 16 days, whereas protein expression of FSH, ACTH, and GH was first observed by days 25-30 of differentiation (Figures 6H-6K).
The induction of ACTH+ cells was particularly efficient (Figure 6J), and in vitro release of ACTH hormone could readily be detected by ELISA (Figure 6L).
We next tested whether hESC-derived pituitary cells are capable of in vivo survival and function in mouse and rat xenograft models (Figures 6M, S7B, and S7C).
Subcutaneous injection of GFP-marked, hESC-derived pituitary precursors (day 16 of differentiation) into adult male NOD/SCID mice demonstrated survival of glycoprotein subunit (GSU) and follicle-stimulating hormone (FSH) cells in vivo (Figures S7D and S7E).
Longer-term survival studies in nude male rats (n = 8; 4-6 weeks postgrafting) showed significant increases in serum ACTH levels (Figure 6N) as measured during early morning hours (low point of endogenous ACTH expression during diurnal cycle).
We also measured levels of human GH in vivo using an ELISA assay that selectively detects human but not mouse GH (Figure 6O).
Transplantation into rat hosts allowed for repeated blood draws and showed a consistent increase of ACTH and GH levels in grafted as compared to sham injected (Matrigel-only) animals.
Finally, histological analysis demonstrated in average 0.46 ± 0.015 million surviving hNCAM+ cells (Figure 6P) at 6 weeks after transplantation.
About 10% of the surviving human cells expressed ACTH (Figure 6Q) and 6% of the cells were immunoreactive for GH (Figure 6R).
The ability to derive functional hormone-producing pituitary cells from hESCs via modified PIP conditions is particularly intriguing given previous work in mouse ESCs suggesting the need for complex coculture systems to induce pituitary lineages (Suga et al., 2011).
In vivo survival and production of graft-derived ACTH and GH suggest translational potential for patients suffering from genetic, surgical, or radiation induced hypopituitarism (Tabar, 2011).
Discussion
An Experimental Platform for the Efficient Generation of Human Cranial Placodes In Vitro
The study of human cranial placode development has been challenging due to the lack of a tractable experimental system, the inaccessibility of this transient structure during early human development, and the absence of validated human placode markers.
Our data resolve those major challenges and establish a versatile platform for the study of human placode and ectoderm development (Figure 7).
Previous studies have observed the emergence of certain placode derivatives such as lens (Ooto et al., 2003; Zhang et al., 2010) or otic placode-derived cells (Chen et al., 2012; Oshima et al., 2010).
However, the mechanisms of placode induction were not addressed, and no general model of placode specification has emerged from those studies.
Very recent studies have shown some promise in directing pluripotent stem cells toward placode fates (Leung et al., 2013; Mengarelli and Barberi, 2013; Shi et al., 2007).
However, no functional human placode derivatives have been reported under those conditions.
In contrast, studies in mouse ESCs have successfully derived functional otic (Koehler et al., 2013) and pituitary (Suga et al., 2011) placode derivatives.
A developmental question of particular interest is the origin of placode cells.
The prevalent hypothesis is that placode tissue originates from nonneural ectoderm upon response to inductive signals from the adjacent neural tissue (Schlosser, 2006).
Our data indicate that human placode development similarly originates from nonneural ectoderm based on the result that TFAP2A expression precedes the induction of SIX1, whereas TFAP2A is suppressed during N-SB induction.
BMP-based induction of TFAP2A and GATA3 may be critical in establishing nonneural ectoderm lineage competent for placode induction (Kwon et al., 2010).
The ability to block placode induction at the expense of nonneural ectoderm by inhibition of FGF signaling further supports a nonneural ectoderm origin and suggests that endogenous FGF signals may be the neural signal responsible for the placode default in PIP.
Our findings further delineate the time point of developmental commitment to placodal fate by day 7, given that treatment of cells at day 7 of differentiation (preplacode) induced a switch between various placode fates but did not affect the ratio of cells of placode versus nonneural ectoderm fate.
Previous work described FOXG1 and DACH1 as markers of anterior neuroectoderm and neural rosette stage cells during hPSC differentiation (Chambers et al., 2009; Elkabetz et al., 2008).
Our study reports a small percentage of SIX1+ cells that emerge spontaneously under N-SB conditions.
Therefore, differentiation studies aimed at generating CNS lineages should address whether contaminating placodal tissues are present in hPSC-derived neural cultures.
Those spontaneously emerging placodal cells are likely the source of lentoid bodies and other placode derivatives observed in past neural differentiation studies.
Our gene expression data define a broad set of placode markers valuable for future studies.
For example, OVOL2 has been shown to be expressed in the mouse epiblast and surface ectoderm, and loss of Ovol2 leads to early embryonic lethality (Mackay et al., 2006).
Here, we define OVOL2 as a human preplacode marker.
FOXC1 and ISL1 are additional transcription factors specifically expressed at the preplacode stage.
Our data provide a framework for defining transcriptional networks that distinguish cranial placode identity from early CNS, neural crest, and surface ectoderm identity.
Specification toward Functional hESC-Derived Placode Derivatives
The current study defines a transient preplacode population competent to adopt various specific placode identities.
The emergence of a preplacodal region has been described during Xenopus and zebrafish development (Bailey et al., 2006; Martin and Groves, 2006; Schlosser, 2006).
Our data indicate that PIP initially yields a PAX6+/SIX1+ anterior preplacode population that spontaneously adopts a more posterior, PAX3+ ophthalmic trigeminal fate upon further differentiation, likely due to the caudalizing effects of endogenous FGF and WNT signals.
Exposure to SHH or suppression of FGF signaling at day 7 of differentiation directs the putative preplacode precursors into anterior pituitary and lens placodes fates.
Lens placode has been suggested as the default state during chick placode development with FGF8 being necessary and sufficient to specify olfactory placode fate (Bailey et al., 2006).
Elegant studies in mouse ESCs (Koehler et al., 2013; Suga et al., 2011) have shown the feasibility of generating hormone-producing pituitary cells and otic sensory neurons respectively using sophisticated 3D culture systems.
Our results demonstrate that modified PIP conditions yield human pituitary precursors efficiently without the need for complex 3D culture conditions.
Although we observe a bias in generating preferentially TBX19-related pituitary lineages including ACTH-producing cells, all three major precursor lineages (GATA-2, TBX19, and PIT1 lineages, Figure 6A) could be derived.
Therefore, it is likely that further optimization of the protocol will provide selective access to individual pituitary hormone lineages.
Our current in vivo studies are limited to a maximum engraftment period of 3 months, subcutaneous injections into the flank of adult murine hosts, and use of animals (both mouse and rat) with normal pituitary function.
Future experiments should include longer-term survival studies and an in-depth analysis of graft integration into the hypothalamic-pituitary axis, possibly via orthotopic graft placement into the hypothalamus or the sella.
Finally, in vivo hormone function will need to be validated in animal models of pituitary dysfunction.
We present data on the specification of trigeminal, lens, and anterior pituitary placode lineages.
However, preliminary evidence indicates that other placode fates are accessible as well using modified PIP conditions.
For example, exposure of preplacode cells to FGF8 enriches for ASCL1 expression compatible with olfactory placode fate (Balmer and LaMantia, 2005).
Exposure to caudalizing cues such as WNT3A leads to the induction of a population of cells coexpressing SIX1 and SOX10 compatible of otic placode fates.
Those conditions, although requiring further optimization, support the notion that PIP represents a universal platform for cranial placode fate specification.
One of the most immediate applications of PIP-based differentiation is the derivation of trigeminal sensory neurons.
Given our recent success in deriving neural-crest-derived nociceptive neurons (Chambers et al., 2012), it will be intriguing to compare the developmental and functional features of neural crest versus placode-derived nociceptors.
Trigeminal neurons are involved in several pain syndromes such as trigeminal nerve palsy, trigeminal neuralgia, and migraine pain (Love and Coakham, 2001).
Therefore, the ability to generate large numbers of trigeminal neurons will be particularly useful for modeling human nociception and for the development of cell-based drug screens in pain research.
Another important application will be modeling Herpes simplex encephalitis using human iPSCs (Lafaille et al., 2012).
HSV-1 is a virus that specifically persists in a latent form within the trigeminal ganglia (Barnett et al., 1994).
The ability to derive trigeminal neurons from patient-specific iPSCs should address whether defects in the control of viral latency contributes to Herpes simplex encephalitis.
The robust in vivo survival of trigeminal placode precursors raises the possibility for developing future regenerative approaches with the goal of nerve repair following mechanical, radiation, or chemotherapy-induced damage.
Finally, the potentially largest impact on regenerative medicine may come from deriving functional hormone-producing cells.
Hypopituitarism is a common consequence of congenital defects, head injury, or therapeutic intervention in patients with pituitary tumors or patients receiving radiation therapy (Tabar, 2011).
Although replacement hormones can be given to normalize resting serum levels in patients, the financial, logistic, and medical costs for such lifelong treatments are considerable.
Furthermore, hormone replacement therapy does not allow for dynamic release in response to circadian rhythms, or rapid adjustments to physiological changes in the environment or stressful challenges.
Therefore, the ability to generate large numbers of functional ACTH- and GH-producing cells launches the possibility of long-term therapeutic cell replacement strategies in pediatric and adult patients for restoring endocrine function.
Experimental Procedures
Cell Culture
hESCs (WA-09; passages 35-45), hiPSC lines (iPS-14, iPS-27; passages 20-30), and I6 were maintained at undifferentiated state and differentiated toward CNS lineages using dual-SMAD inhibition protocol described previously.
For PIP, Noggin was removed at day 3 of differentiation.
In some experiments, BMP-4, Noggin, DKK-1, FGF8, SU5402, Wnt-3a, DAPT, CHIR99021, cyclopamine, Sonic Hedgehog (SHH), and purmorphamine were added as detailed in the Supplemental Experimental Procedures.
For differentiation toward trigeminal sensory fate placode clusters were maintained in N2 medium supplemented with ascorbic acid and BDNF.
Pituitary fate was induced by exposure to SHH and purmorphamine from day 7 to 11 of PIP followed by treatment with DAPT to promote PIT1+ fate.
Cell Characterization
qRT-PCR data were normalized to HPRT and are based on four to six technical replicates from at least three independent experiments.
Global gene expression analysis was performed by the MSKCC genomics core according to the specification of the manufacturer (Illumina Human-6 oligonucleotide arrays).
Detailed information on the use of primary antibodies for immunocytochemistry and flow analysis and on the electrophysiological analyses is presented in the Supplemental Experimental Procedures.
Animal Studies
Animal studies were done in accordance with protocols approved by our institutional Animal Care and Use Committee and following NIH guidelines.
Hormone-producing cells were injected subcutaneously into adult male NOD-SCID IL2Rgc mice and eight adult male nude rats.
Blood was collected at 4-6 weeks after the transplantation followed by ELISA analysis for determining hormone levels.
Chick transplantation studies were performed at HH stage 9-10, and embryos were harvested at HH stage 20.
Injections into pons of adult NOD-SCID IL2Rgc mice were performed by stereotactic surgery.
Statistical Analysis
Statistical analysis was performed using GraphPad Prism version 5.0b (GraphPad).
All data were derived from at least three independent experiments.
Asterisks mark experimental groups that were significantly different from control groups by a two-tailed Students t test, or by ANOVA followed by Dunnett test to compare control against multiple independent treatment groups.
Data are presented as mean ± SEM unless indicated otherwise.
Acknowledgments
This work was supported in part through grants from the Starr Foundation, NINDS grant NS072381, and NYSTEM contract C026447.
We are grateful to E. Lai for BF1 antibody, A.S.
McNeilly for GSU antibody, R.
Vinagolu for FSH antibody, and K.
Kawakami for Eya1::GFP enhancer element.
We also would like to thank H.
Ford for advice on SIX1 antibody, M. Hashimi for advice on microarray analysis, M. Tomishima and J. Tchieu for critical review of the manuscript, and M. Bronner for help in designing the chick in vivo transplantation studies.
Accession Numbers
The GEO accession number for the microarray gene expression data reported in this paper is GSE51533.
Supplemental Information
Supplemental Information includes Supplemental Experimental Procedures, seven figures, and two tables and can be found with this article online at http://dx.doi.org/10.1016/j.celrep.2013.10.048.
Supplemental Information
Document S1.
Figures S1-S7 and Supplemental Experimental ProceduresTable S1.
PIP SignaturesTable S2.
N-SB SignaturesDocument S2.
Article plus Supplemental Information

1. A method of preparing a positive electrode active material, comprising:
preparing a mixture by mixing a lithium compound, a transition metal precursor, and a metal oxide additive; and
sintering the mixture to form a lithium transition metal oxide,
wherein the sintering is performed through two-stage temperature holding sections, a temperature of a first temperature holding section is in a range of 400° C. to 650° C., and a temperature of a second temperature holding section is in a range of 700° C. to 900° C.2. The method of claim 1, wherein the metal oxide additive comprises at least one of ZrO2, ZnO, Nb2O5, MgO, Fe2O3, V2O5, WO3, SiO, SiO2, or Sn2O3.3. The method of claim 1, wherein the metal oxide additive is mixed in an amount of 0.01 wt % to 1.0 wt % based on a combined weight of the lithium compound and the transition metal precursor.4. The method of claim 1, wherein, when an average particle diameter (D50) of the transition metal precursor is in a range of 5 μm to 10 μm, the temperature of the first temperature holding section is in the range of 400° C. to 600° C.5. The method of claim 1, wherein an average particle diameter (D50) of the transition metal precursor is greater than 10 μm, the temperature of the first temperature holding section is in the range of 500° C. to 650° C.6. The method of claim 1, wherein the transition metal precursor comprises at least one of nickel (Ni), cobalt (Co), or manganese (Mn).7. The method of claim 6, wherein the transition metal precursor is Nia1Cob1Mnc1Md1(OH)2,
wherein M comprises at least one of aluminum (Al), zirconium (Zr), magnesium (Mg), zinc (Zn), yttrium (Y), iron (Fe), tungsten (W), and titanium (Ti), and 0.4≤a1≤1.0, 0≤b1≤0.6, 0≤c1≤0.6, 0≤d1≤0.2, and a1+b1+c1+d1=1.8. The method of claim 7, wherein the transition metal precursor includes Ni in an amount of 70 mol % or more based on a total transition metals.9. The method of claim 1, wherein a first heating rate before reaching the first temperature holding section is in a range of 1.0° C./min to 3.0° C./min.10. The method of claim 1, wherein a second heating rate before reaching the second temperature holding section from the first temperature holding section is in a range of 1.0° C./min to 3.0° C./min.11. The method of claim 1, wherein a holding time of the first temperature holding section is in a range of 2 hours to 8 hours.12. The method of claim 1, wherein a holding time of the second temperature holding section is in a range of 4 hours to 12 hours.13. A positive electrode comprising a positive electrode active material prepared according to claim 1.14. A lithium secondary battery comprising the positive electrode of claim 13, a negative electrode, a separator disposed between the positive electrode and the negative electrode, and an electrolyte.
Effect of alkyl chain length of imidazolium cations on the electron transport and recombination kinetics in ionic gel electrolytes based quasi-solid-state dye-sensitized solar cells
1-Methyl-3-propylimidazolium iodide (MPII), 1-Methyl-3-butylimidazolium iodide (BMII) and 1-Methyl-3-hexylimidazolium iodide (HMII) were prepared as reported previously [14] and [15]. The ionic liquid electrolytes (ILEs) for QS-DSSC were composed of 0.35 mol L-1 iodine (I2: 99%, Aldrich), 0.02 mol L-1 anhydrous lithium iodide (LiI: 99%, Aldrich) and 0.5 mol L-1 N-methylbenzimidazole (NMBI: 99%, Aldrich) in MPII, BMII and HMII, respectively. IGEs were prepared by adding 20 wt% (vs. ILE) 12-hydroxystearicacid (99%, Aldrich) into ILEs and heated under stirring until the gelators melted. After cooling to room temperature, the IGEs were formed. The IGEs based on MPII, MBII and HMII correspond to IGE P, IGE B and IGE H, respectively.
What is claimed is:
1. An active material for a battery, wherein the active material is hollow and is formed from an active material precursor comprising:
NiaMnbCocMd(OH)2Formula 1
wherein, in Formula 1, 0<an For<b≤1, 0<c≤1, 0≤d<1, a+b+c=1; and
M is at least one metal selected from the group consisting of titanium (Ti) vanadium (V), chromium (Cr), iron (Fe), copper (Cu), aluminum (Al), magnesium (Mg), zirconium (Zr), and boron (B).2. The active material for a battery of claim 1, wherein the active material is represented by Formula 3′:
xLi2MnO3-(1-x)LiyNiaMnbCocMdO2Formula 3′
wherein, in Formula 3′, 0<xn For 1.0n Formu; 0<an Formula 3′, la, 0≤0n and a+b+c+d=1; and
M is at least one metal selected from the group consisting of Ti, V, Cr, Fe, Cu, Al, Mg, Zr, and B.3. The active material of claim 1, wherein the active material is represented by Formula 4:
xLi2MnO3-(1-x)LiyNiaMnbCocO2Formula 4
wherein, in Formula 4, 0<xn Fo and 1.0d Formu; 0<ad, 0<b≤1, 0<c≤1, and a+b+c=1.4. The active material precursor of claim 1, wherein a tap density of the active material precursor is about 1.95 g/ml or lower.5. The active material of claim 1, wherein the active material is 0.2Li2MnO3-0.8LiNi0.5Co0.2Mn0.3O2.6. The active material of claim 1, wherein the active material precursor is represented by Formula 2:
NiaMnbCoc(OH)2Formula 2
wherein, in Formula 2, 0<a<1, 0.36a 2, ula 2, d, and a+b+c=1.7. The active material of claim 6, wherein, in Formula 2, a is about 0.22 to about 0.70, b is 0.36 to 0.47, and c is about 0.12 to about 0.30.8. The active material of claim 1, wherein the active material precursor comprises Ni0.30Co0.30Mn0.40(OH)2, Ni0.265Co0.265Mn0.47(OH)2, Ni0.265Co0.265Mn0.47(OH)2, Ni0.40Co0.16Mn0.44(OH)2, Ni0.45Co0.18Mn0.37(OH)2, Ni0.48Co0.16Mn0.36(OH)2.9. The active material of claim 1, wherein the active material has a singlet peak that is observed at a 2θ angle of 21ngle.10. The active material of claim 1, wherein the active material is obtained by a method comprising: mixing the active material precursor of Formula 1 with a lithium precursor to form a mixture, mixing the mixture with a lithium compound, and heat-treating the resultant at a temperature of in a range of about 700° C. to about 900° C., wherein the mixing molar ratio of the lithium precursor to the active material precursor is 2:1:
NiaMnbCocMd(OH)2Formula 1
wherein, in Formula 1, 0<a≤1, 0<b≤1, 0<c≤1, 0≤d<1, a+b+c+d=1; and
M is at least one metal selected from the group consisting of titanium (Ti) vanadium (V), chromium (Cr), iron (Fe), copper (Cu), aluminum (Al), magnesium (Mg), zirconium (Zr), and boron (B).11. The active material of claim 1, wherein the active material precursor is obtained by a method comprising:
mixing a nickel precursor, a manganese precursor, a cobalt precursor, a metal (M) precursor, and a solvent to prepare a precursor mixture; and
mixing the precursor mixture and a pH adjusting agent to adjust a pH value of the resultant to be in a range of about 11.0 to about 11.2.12. The active material of claim 11, wherein a chelating agent is added to the mixing of the precursor mixture and the pH adjusting agent.13. The active material of claim 12, wherein an amount of the chelating agent is about 0.1 mole to about 3 moles based on 1 mole of the nickel precursor.14. The active material of claim 12, wherein the chelating agent is at least one selected from the group consisting of ammonia water, acetyl acetone, ethylenediaminetetraacetic acid (EDTA), and benzoylacetone (BzAc).15. The active material of claim 11, wherein the pH adjusting agent is at least one selected from a sodium hydroxide, a potassium hydroxide, and a lithium hydroxide or an aqueous solution thereof.
Spin-state transition, magnetic, electrical and thermal transport properties of the perovskite cobalt oxide Gd0.7Sr0.3CoO3

Polycrystalline Gd0.7Sr0.3CoO3 sample was prepared by a conventional solid-state reaction method. Appropriate proportions of high purity Gd2O3, SrCO3, and Co3O4 powders were thoroughly mixed according to the desired stoichiometry, and then prefired at 800 and 1200 degC for 24 h, respectively. The powders obtained were reground, pelletized and sintered at 1200 degC for 24 h. In order to get relative homogeneous sample with less oxygen deficiency, the sample was annealed in the oxygen pressure of 165 atm at 500 degC for 48 h.


Facile synthesis of mesoporous Ge/C nanocomposite as anode material for lithium-ion battery

In a typical synthesis of Ge/OMC nanocomposite, 1.0 mL of TEOG was slowly dropped onto 1.00 g of the OMC matrix material in 5 min under stirring without adding any other solvent. Afterwards, the mixture was kept stirring for another 60 min at room temperature. The mixture was then kept in a crucible overnight for the conversion of TEOG to GeO2 and the evaporation of ethanol byproduct. The GeO2/C intermediate was heated up to 500 degC with a ramp of 2 degC/min under a H2 gas flow (100 mL/min) in a quartz tube furnace for reducing the GeO2 to Ge nanoparticles. The final product was collected after the furnace was cool down. In order to increase the loading amount of Ge, the adding amount of TEOG can be increased to 2.0, 3.0 and 4.0 mL.

1. A method for producing transition metal composite hydroxide particles serving as a precursor of a positive electrode active material for a nonaqueous electrolyte secondary battery by a crystallization reaction, comprising:
a nucleus-forming step of controlling an aqueous solution for nucleus formation containing at least a transition metal-containing metal compound and an ammonium ion donor so that the aqueous solution for nucleus formation has a pH of 12.0 to 14.0 at a liquid temperature of 25 ℃ and forming nuclei,
a particle growth step of controlling the aqueous solution for particle growth containing the nuclei obtained in the nucleus production step so that the pH of the aqueous solution for particle growth becomes lower than that in the nucleus production step at a liquid temperature of 25 ℃ and 10.5 to 12.0 to grow the nuclei,
the reaction environment in the initial stage of the nucleus generation step and the particle growth step is a non-oxidizing environment having an oxygen concentration of 5 vol% or less, and the reaction environment is switched from the non-oxidizing environment to an oxidizing environment having an oxygen concentration of more than 5 vol% and then from the oxidizing environment to a non-oxidizing environment having an oxygen concentration of 5 vol% or less by performing environmental control at least once in the particle growth step.2. The method for producing transition metal composite hydroxide particles according to claim 1, wherein in the particle growth step, the non-oxidizing atmosphere is switched to the oxidizing atmosphere within a range of 5% to 35% of the total time of the particle growth step from the start of the particle growth step.3. The method for producing transition metal composite hydroxide particles according to claim 1 or 2, wherein, in the case where the environmental control is performed only once, the crystallization reaction time in the oxidizing environment in the particle growth step is set to 3% to 20% of the total time in the particle growth step.4. The method for producing transition metal composite hydroxide particles according to claim 1 or 2, wherein in the case where the environmental control is performed 2 or more times, the total crystal reaction time in the oxidizing environment in the particle growth step is 3% to 30% of the total time in the particle growth step, and the crystal reaction time in each oxidizing environment is 1% or more of the total time in the particle growth step.5. The method for producing transition metal composite hydroxide particles according to any one of claims 1 to 4, wherein the transition metal composite hydroxide particles are represented by the general formula (A): niXMnyCozMt(OH)2+aIn the transition metal composite hydroxide particles represented by general formula (A), x + y + z + t is 1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, 0. ltoreq. a.ltoreq.0.5, and M is one or more additive elements selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.6. The method for producing transition metal composite hydroxide particles according to claim 5, further comprising a coating step of coating the transition metal composite hydroxide particles with a compound containing the additive element M after the particle growth step.7. Transition metal composite hydroxide particles which are precursors of positive electrode active materials for nonaqueous electrolyte secondary batteries, wherein,
the transition metal composite hydroxide particles are composed of a plurality of plate-like primary particles and secondary particles formed by aggregating fine primary particles smaller than the plate-like primary particles,
the secondary particles have a central portion formed by the aggregation of the plate-like primary particles, and have at least one layered structure formed by the lamination of a low-density portion formed by the aggregation of the fine primary particles and a high-density portion formed by the aggregation of the plate-like primary particles outside the central portion,
the secondary particles have an average particle diameter of 1 to 15 [ mu ] m, and the [ (d90-d 10)/average particle diameter ] as an index indicating the width of the particle size distribution is 0.65 or less.8. The transition metal composite hydroxide particles according to claim 7, wherein, in the case where only one of the layered structures is provided, the average value of the ratio of the outer diameter of the central portion to the particle diameter of the secondary particles is 30% to 80%.9. The transition metal composite hydroxide particles according to claim 7 or 8, wherein, in the case where only one of the layered structures is provided, the average value of the ratio of the thickness in the diameter direction of the high-density portion to the particle diameter of the secondary particles is 5% to 25%.10. The transition metal composite hydroxide particles according to any one of claims 7 to 9, wherein the transition metal composite hydroxide particles are represented by the general formula (A): niXMnyCozMt(OH)2+aIn the transition metal composite hydroxide particles represented by general formula (A), x + y + z + t is 1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, 0. ltoreq. a.ltoreq.0.5, and M is one or more additive elements selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.11. The transition metal composite hydroxide particles according to claim 10, wherein the additive element M is uniformly distributed in the interior of the secondary particles and/or uniformly coats the surfaces of the secondary particles.12. A method for producing a positive electrode active material for a nonaqueous electrolyte secondary battery, comprising:
a mixing step of mixing the transition metal composite hydroxide particles according to any one of claims 7 to 11 with a lithium compound to form a lithium mixture, and,
and a firing step of firing the lithium mixture formed in the mixing step in an oxidizing atmosphere at a temperature of 650 to 980 ℃.13. The method for producing a positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 12, wherein in the mixing step, the lithium mixture is adjusted so that the ratio of the sum of the numbers of atoms of the metals other than lithium contained in the lithium mixture to the number of lithium atoms is 1: 0.95 to 1.5.14. The method for producing a positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 13, further comprising a heat treatment step of heat-treating the transition metal composite hydroxide particles at a temperature of 105 to 750 ℃ before the mixing step.15. The method for producing a positive electrode active material for a nonaqueous electrolyte secondary battery according to any one of claims 12 to 15, wherein the positive electrode active material for a nonaqueous electrolyte secondary battery is represented by general formula (B): li1+uNixMnyCozMtO2The positive electrode active material for a nonaqueous electrolyte secondary battery is composed of hexagonal lithium nickel manganese composite oxide particles having a layered structure, and in the general formula (B), -0.05. ltoreq. u.ltoreq.0.50, x + y + z + t-1, 0.3. ltoreq. x.ltoreq.0.95, 0.05. ltoreq. y.ltoreq.0.55, 0. ltoreq. z.ltoreq.0.4, 0. ltoreq. t.ltoreq.0.1, and M is at least one additive element selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, and W.16. A positive electrode active material for nonaqueous electrolyte secondary battery, wherein,
the positive electrode active material for a nonaqueous electrolyte secondary battery is composed of secondary particles formed by aggregating a plurality of primary particles,
the secondary particles have a central portion of a hollow structure or a hollow structure, and have at least a space portion where the primary particles are not present and a shell portion electrically connected to the central portion outside the central portion,
the secondary particles have an average particle diameter of 1 to 15 [ mu ] m, and the [ (d90-d 10)/average particle diameter ] as an index indicating the width of the particle size distribution is 0.7 or less.17. The positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 16, wherein when the secondary particles are composed of the central portion, a space portion outside the central portion where the primary particles are not present, and an outer shell portion electrically conducting with the central portion, an average value of a ratio of an outer diameter of the central portion to a particle diameter of the secondary particles is 30% to 80%.18. The positive electrode active material for a nonaqueous electrolyte secondary battery according to claim 16 or 17, wherein when the secondary particles are composed of the central portion, a space portion outside the central portion where the primary particles are not present, and an outer shell portion electrically conducting with the central portion, an average value of a ratio of a thickness in a diameter direction of the outer shell portion to a particle diameter of the secondary particles is 5% to 25%.19. The positive electrode active material for a nonaqueous electrolyte secondary battery according to any one of claims 16 to 18, wherein the specific surface area is 0.7m2/g～3.0m2/g。20. The positive electrode active material for a nonaqueous electrolyte secondary battery according to any one of claims 16 to 19, wherein the positive electrode active material is represented by general formula (B): li1+uNixMnyCozMtO2The positive electrode active material is composed of lithium transition metal composite oxide particles having a hexagonal crystal structure and a layered structure, and in the general formula (B), 0.05 u 0.50, x + y + Z + t 1, 0.3 x 0.95, 0.05 y 0.55, 0Z 0.4, 0 t 0.1, and M is selected from Mg, Ca, Al, Ti, V, Cr, Zr, Nb, Mo, Hf, Ta, W.21. A nonaqueous electrolyte secondary battery comprising a positive electrode, a negative electrode, a separator, and a nonaqueous electrolyte, wherein the positive electrode active material for nonaqueous electrolyte secondary batteries according to any one of claims 16 to 20 is used as a positive electrode material of the positive electrode.High-temperature phase equilibria in the oxide systems SrFe1-xGaxO2.5-SrFe1-xGaxO3 (, 0.1, 0.2)

The samples used in this study were prepared by solid-state reactions. Starting materials were oxides Fe2O3 (99.92%), Ga2O3 (99.99%) and strontium carbonate SrCO3 (99.94%). The raw materials were weighed in desirable amounts and thoroughly mixed with a mortar and pestle with addition of ethanol. The mixtures were pressed into pellets and fired at 900-1250 degC in air. The materials were crushed into powder, pressed and fired several times with a gradual increase in temperature before single-phase specimens were obtained. Phase purity and determination of the lattice parameters were carried out with X-ray powder diffraction ( λ = 1.5 4 1 7 8 Å ). The lattice parameters were refined with the using of FullProf program [9]. A part of the synthesized materials was ball-milled in ethanol media and pressed into discs under 2 kbar uniaxial load. The disks were sintered in air at 1250 degC for 10 h to a density no less than 90% of theoretical. Rectangular bars 2x2x18 mm were cut from the sintered discs for the conductivity measurements.

How does wind farm performance decline with age?

Abstract
Ageing is a fact of life.
Just as with conventional forms of power generation, the energy produced by a wind farm gradually decreases over its lifetime, perhaps due to falling availability, aerodynamic performance or conversion efficiency.
Understanding these factors is however complicated by the highly variable availability of the wind.
This paper reveals the rate of ageing of a national fleet of wind turbines using free public data for the actual and theoretical ideal load factors from the UK's 282 wind farms.
Actual load factors are recorded monthly for the period of 2002-2012, covering 1686 farm-years of operation.
Ideal load factors are derived from a high resolution wind resource assessment made using NASA data to estimate the hourly wind speed at the location and hub height of each wind farm, accounting for the particular models of turbine installed.
By accounting for individual site conditions we confirm that load factors do decline with age, at a similar rate to other rotating machinery.
Wind turbines are found to lose 1.6 ± 0.2% of their output per year, with average load factors declining from 28.5% when new to 21% at age 19.
This trend is consistent for different generations of turbine design and individual wind farms.
This level of degradation reduces a wind farm's output by 12% over a twenty year lifetime, increasing the levelised cost of electricity by 9%.
Graphical abstract
Highlights:
•
The output of 282 wind farms is accurately estimated using public wind speed data.
•
Correcting for variability in the weather allows wind turbine ageing to be studied.
•
Onshore wind farm output falls 16% a decade, possibly due to availability and wear.
•
Performance decline with age is seen in all farms and all generations of turbines.
•
Decreasing output over a farm's life increases the levelised cost of electricity.

Introduction
Ageing is a fact of life.
Its effects are inevitable for all kinds of machinery, reducing the efficiency, output and availability of steam and gas turbines, solar PV modules, batteries and automobiles alike.
Previous work on wind turbines has considered the reliability of individual components and the effect of ageing on availability, but any impact on the energy production of turbines or farms has not been widely reported.
If load factors (also known as capacity factors) decrease significantly with age, wind farms will produce a lower cumulative lifetime output, increasing the levelised cost of electricity from the plants.
If the rate of degradation were too great, it could become worthwhile to prematurely replace the turbines with new models, implying that the economic life of the turbine was shorter than its technical life, further increasing its cost.
This could have significant policy implications for the desirability of investing in wind power, as argued in a recent report by Hughes for the Renewable Energy Foundation (REF) [1].
That report suggested that the load factors of wind farms in the UK have declined by 5-13% per year, normalising for month-by-month variations in wind speeds.
These findings could represent a significant hurdle for the wind industry, but they require replication.
Several factors can confound the relationship between age and observed output in a fleet of wind farms, given that a turbine's output is dependent on wind speeds at its site and the efficiency with which it captures the energy in that wind.
For example, if wind speeds have fallen slightly over time, farms would have lower load factors in recent months, when they were at their oldest, giving a spurious correlation between age and poor performance.
If improvements in design increase a turbine's output relative to capacity (its power coefficient) then newer turbines (of the improved design) will have higher load factors than old turbines, so that turbine output appears to decline with age, when really it improves with newer generations.
On the other hand, if the best (windiest) sites were occupied first, then old farms could have higher load factors than new ones built on inferior sites, so that turbines would appear to improve with age.
This paper uses public domain data to infer the hour-by-hour wind speeds at the site of every wind farm in the UK, and the power curve for each farm's model of turbine to estimate the output that they would ideally produce.
This technique corrects for the confounding factors (wind patterns, turbine model and site quality), and validates well for farms that report their half-hourly output to National Grid.
Simulated ideal outputs are compared with actual monthly load factors from a large portion of the UK's fleet over the last decade (282 wind farms, 4.5 GW, 53 TWh), yielding the normalised performance of each wind farm accounting for its wind resource availability, and a set of weather-corrected load factors which reveal the effects of ageing.
We measure the level of age-related degradation at the national level, accounting for the vintage of turbine and local site conditions at each wind farm.
We test different generations of technology and individual wind farms to confirm that specific units experienced similar declines in performance.
We find the ageing effect to be present, but much smaller than predicted by Hughes, in line with experience of other rotating machinery.
The specific causes of this performance loss and their relative contribution are not considered in this paper, although an overview of potential reasons is given in the discussion and conclusions.
Due to the amount of data and processing required for this study we provide online supplementary material which documents our sources and their validation in greater depth, along with downloadable datasets of UK wind farms and their energy output histories.
Previous studies
All machinery experiences an unrecoverable loss in performance over time.
Gas turbine efficiency suffers an unrecoverable decline of 0.3-0.6% per year despite regular washing and component replacement, or by 0.75-2.25% without [2].
Similarly, the output of solar photovoltaic panels declines by 0.5% per year on average [3].
This loss in performance is not routinely accounted for in studies of the levelised cost of electricity (LCOE) of wind power.
Recent studies by Mott MacDonald, Parsons Brinckerhoff and Arup accounted for the efficiency of conventional plants falling by 0.15-0.55% per year, but omitted any such factor for wind turbines [4-6].
Previous studies of wind turbines have focussed on availability and reliability [7-9].
There appear to be no long term fleet-level studies into loss of output from wind farms in open literature.
Regardless of technology, quantifying performance degradation is difficult because consistent and validated field data is hard to obtain [2].
The recent study by Hughes [1] is therefore significant, in that we believe it is the first to attempt to estimate the rate of decline in wind farm load factors on a national scale.
Hughes analysed over 10 years of operating data from the British and Danish fleets of turbines, finding rates of performance degradation that are much higher than for other technologies, and which vary remarkably between the UK and Denmark, and between onshore and offshore turbines.
This was based on econometric analysis of monthly load factors, using a regression which corrected for the quality of each wind farm's location, the monthly variation in national wind conditions, and the age of each farm.
Hughes argues (and shows mathematically) that accounting for monthly wind conditions with a set of 'fixed effects' determined by the regression is econometrically superior to using a measure of average wind speeds across the country, since site-specific conditions differ from the national average and the output of wind turbines depends non-linearly on the wind speed at every moment in time, which is very poorly captured by its average over a month.
We therefore use wind speed data with high temporal and spatial resolution, and measure the performance of wind farms by estimating their theoretical potential output over the course of a month and comparing this with the actual reported load factors.
While we believe we are the first researchers to assess wind farm performance with this kind of ex-post data, a number of papers present techniques to estimate output levels from time series of wind data.
Many studies have used hourly wind speed data recorded by met masts; for example investigations into wind variability by Pöyry [10] and SKM [11], and estimates of future national output by Green et al. [12,13] and Sturt and Strbac [14].
Hourly met mast speeds have been directly compared to metered wind farm load factors in Northern Spain [15] and Scotland [16], showing that accurate estimates can be made for monthly energy generation, but not for hourly power outputs.
More recent studies use reanalyses as a source of wind speed data: atmospheric boundary layer models which process physical observations from met masts and other sources into a coherent and spatially complete dataset, and are widely used to produce wind atlases.
Kiss et al. [17] were first to compare the European ERA-40 reanalysis to nacelle measurements of wind speed and power output at two turbines in Hungary, finding "surprisingly good" agreement.
Hawkins et al. [18] were able to replicate UK monthly load factors using a custom reanalysis model, while Kubik et al. [19] compared the global NASA reanalysis to half-hourly farm output in Northern Ireland, finding it to be more accurate than met mast data.
The first practical application appears to have been made by Ofgem to estimate the equivalent firm capacity of the UK's wind fleet during winter peaks in demand [20].
Both Hawkins and Ofgem noted that the reanalysis outputs need to be scaled down by a constant factor (29% and 20% respectively) in order to match actual production in the UK, a finding which we elaborate upon in this paper.
Data sources
Predicting a given wind farm's output is far from being a new science: on-site monitoring of conditions using wind turbine SCADA systems is commonplace; and software tools such as WaSP or consultancies such as GL Garrad Hassan are widely used in the field.
Data is not made publicly available, and these services come at a price of several thousand Euros.
On the other hand, national average data cannot reveal what is happening at individual wind farms.
We therefore employ farm-specific data for output and site-specific data for wind speeds, taken from free and publicly accessible datasets.
The primary data used in our main analysis are described in this section, and additional data used for validation are described in Section 4.
Further information on our data is given in the Supplementary Material.
Ofgem/REF output data
All wind farms enrolled in the UK government's incentive scheme, the Renewables Obligation, publish their monthly outputs (in MWh) in the Ofgem Renewables and CHP Register.11
www.renewablesandchp.ofgem.gov.uk.
 Hughes extracted and cleaned this data, cross-linking outputs with details about each wind farm (its capacity and date of commissioning), and ensured that each wind farm contained only the same model and vintage of turbine [1].
This cleaned dataset was published on the internet by the Renewable Energy Foundation (REF) [21].
We are very grateful to Prof.
Hughes and the REF for making this rich data source available to the community.
We further validated this dataset, corrected the commissioning date for 15 wind farms (which were incorrectly reported by Ofgem), integrated further meta-data for each farm (the geographical location, wind turbine model and hub height), and extended the time-series by 8 months, adding data from April to December 2012.
Our modified dataset is provided as Supplementary Material to this paper.
It contains 1687 farm-years of load factor data, covering onshore turbines built from 1991 onwards and spanning 11 years of operation.
The study was restricted to onshore wind farms, as only a small amount of data was available for the UK's 20 offshore farms, and these would need to be considered separately as they face a very different operating environment and maintenance issues to onshore farms.
Table 1 and Fig. 1 provide a selection of summary statistics for the data.
Figs. 2 and 3 show what has happened to the load factor of these wind farms as they get older.
Fig. 2 simply plots the distribution of all observed load factors against age, showing a steady decline of -0.44 ± 0.04 absolute percentage points per year past age one (1.69 ± 0.17% loss per year relative to the UK mean load factor).
As explained in the introduction this is not necessarily due to ageing if newer turbine models are more efficient at extracting energy from the wind.
The oldest turbines in the sample (aged 15-19 years) were built in the early- to mid-1990s; typically 300-500 kW two or three bladed machines on 25-50 m towers.
These will clearly be outperformed by the latest generation of 2-3 MW turbines which are no older than 5 years.
To control for technology effects, Fig. 3a charts the individual histories of the 53 farms which have more than ten years of data, using a 12-month moving average to smooth out seasonal variations in the wind.
Load factors tend to rise during the first year of operation while turbines are still being commissioned until the farm achieves full operation and teething problems are ironed out.
Fig. 3b summarises the annual degradation rates, estimated by running individual linear regressions on each farm's unsmoothed load factors, excluding the first year.
The areas with the darkest shading are derived from farms which are old enough to give ten years of data; lighter areas add the more recent farms which have fewer observations.
The degradation rates of individual farms are predominantly bunched around 0 to -1 percentage points of absolute load factor per year, but several outliers make the overall distribution fat-tailed.
A Cauchy (or Lorentz) distribution therefore provides a better fit than a normal distribution, centred on -0.48 with a half width at half maximum of 0.36.
The distribution becomes wider for farms with fewer observations; and would stretch all the way from -25 to +15 points per year if farms with less than five years' data were included.
With an absolute degradation rate of -0.48 ± 0.36 points per year, a typical wind farm loses 1.81 ± 1.32% of its output per year on average.
The range of degradation rates for individual farms is clearly much greater than uncertainty on the average rate for the dataset as a whole.
Neither Fig. 2 nor Fig. 3 corrects for weather effects.
To extract the true rate of degradation in the UK's wind farms, we account for variations in the weather over the last decade using a detailed wind resource assessment, and the rate of technology improvement by modelling the specific turbines installed at every wind farm.
NASA wind speed data
A database of wind speeds for the British Isles was created using NAA's MERRA dataset: a historical reanalysis of global atmospheric observations assimilated and processed using the Goddard Earth Observing System (GEOS-5) [22,23].
Wind observations with "fairly complete global coverage" are taken from weather stations, balloons, aircraft, ships, buoys and satellites, and processed by the model to give data with hourly resolution on a ½° latitude and ⅔° longitude grid (approx.
55 by 44 km), at heights of 2 and 10 m above the surface displacement height (d, the point at which a logarithmic wind profile would tend to zero) and at 50 m above ground.
We acquired data for the UK, Ireland and surrounding waters (-15⅓° to 10°E, 46.5° to 65.5°N) from 1993 to 2012, giving a database ith 1.06 billion observations (175,320 temporal × 1521 geographic × three speed variables plus displacement height).
This database was then processed using R [24] as follows:•
For each hour, the east (u) and north (v) components of wind speed were extracted at all three heights;
•
A nonparametric polynomial surface was fitted to each set of spatially gridded observations using a 2-dimensional LOESS regression.
This allowed wind speeds at any coordinates to be locally interpolated using the nearest twelve observations, as in Fig. 4;
•
The magnitude of the wind speed vector (w) was calculated at each location from w=u2+v2;
•
Wind speed was extrapolated from the three observation heights to the hub height of each farm (averaging 60 ± 14 m) using the log law described in the next section.
Data processing and validation
The aim of our method was to use NASA modelled wind speed data as a predictor for the monthly energy output of wind farms.
Two forms of validation were performed to give confidence that this technique can estimate load factors for a given wind farm that are broadly representative and free of seasonal or inter-annual bias.
The first test was whether interpolated values from the NASA GEOS-5 model accurately represent actual wind speeds measured at a particular location; and the second was that these speeds, when extrapolated to hub height and transformed using a representative power curve, accurately match the actual metered output from a wind farm.
The data processing and validation was split into five stages:•
The NASA speeds at 10 m were compared to ground-based observations from the Met Office to validate our use of the GEOS-5 model (reported in our Supplementary Material);
•
Wind speeds at each farm's location were extrapolated from 50 m to the hub height of that farm to account for wind shear;
•
These extrapolated speeds were transformed into estimated ideal load factors using the power curve of the installed turbine model;
•
The ideal hourly load factors were compared to the half-hourly metered output data for farms where this was available, to check that the preceding methods were robust;
•
Ideal load factors were aggregated for each farm monthly over the period of 2002-12, for comparison against the Ofgem/REF dataset.
The following sections briefly summarise our methods and findings.
An extended validation section which covers each topic in greater depth is provided as supplementary material.
Extrapolating wind speeds to turbine hub height
Extrapolating wind speeds from the height of measurement stations to the much higher hub height of wind turbines is "probably one of the most critical uncertainty factors affecting the wind power assessment at a site" [25].
The change in horizontal wind speeds with height (known as wind shear or the wind profile) is generated by friction from the earth's surface, and so is highly dependent on the specific site conditions: the surface roughness of the terrain, air temperature, season, atmospheric stratification, and the wind speed itself.
Many simplifications for extrapolating wind speeds exist, most notably the empirically derived power law and the theoretically derived log law (Eq.
(1)) [25,26](1)wx=Alog(hx-dz0)
The simplified log law assumes that wind speed, w, is related to the logarithm of its height, h, under the assumption of neutral atmospheric stability.
The logarithmic wind profile tends to zero at a height of the roughness length, z0, plus the surface displacement height, d, and is scaled by a constant, A, which equals the friction velocity (u*) divided by 0.4 (von Karman's constant).
The log law provides robust extrapolations of wind speeds over a range of wind speeds, locations and altitudes [25,27].
The latter is especially important as turbine hub heights range from 25 to 100 m, and the Hellman exponent used in the power law decreases non-uniformly with height.
The NASA data give simultaneous wind speeds at three heights, along with d, allowing the coefficients for wind shear to be calculated for the specific site and time of the observation.
Eq.
(1) was linearised to Eq.
(2), allowing the coefficients A and z0 to be estimated by a least-squares regression of the three NASA observations.
The coefficient values were independently estimated for each site and time period with no smoothing or prior values, but were found to be temporally and spatially stable.(2)wx=Alog(hx-d)-Alog(z0)
At the average UK hub height of 60 m, wind speeds are 6-9% larger than at 50 m and 32-41% larger than at 10 m.
The NASA data greatly reduces the extrapolation uncertainty compared with using 10 m met mast data.
Converting wind speed to power
The power curve for an 'ideal' wind farm was applied to the speed data at each site, estimating the potential output from a farm with perfect availability, perfect calibration, and no site-related performance loss (e.g. turbulence from surrounding geography or wake effects from other turbines).
We consider this to be the maximum possible attainment and name it the ideal yield.
The meta-data that we integrated into the Ofgem/REF dataset gave the model of turbine used at each of the 282 wind farms.
We compiled the power curves for 50 of these turbine models, accounting for 92% of the wind farms in the UK.
For the remaining farms, the best match was found from the known curves, based on the installed turbine's capacity and power density (peak power divided by swept area).
Supplementary Table 1 gives details of all the turbines considered.
The power curve for a single turbine is shown in Fig. 5 with the aggregate power curve for a typical farm of these turbines.
The multi-turbine power curve accounts for the fact that wind speeds at the location of each individual turbine within a farm will vary according to a normal distribution.
Following the practice of [10,20,28], the turbine power curve for each farm was convoluted by a normal distribution.
The standard deviations were determined by the estimated geographic area that each farm covers [28], based on observations that UK farms occupy 100 m2 of land per kW capacity [29].
The mean for each farm was chosen to normalise the total energy production to that of the individual turbine curve.
Comparing predicted and metered output
The Ofgem/REF output data has monthly resolution, which is too coarse to see whether the structure of hourly wind variations accurately replicates output from operating turbines.
Our second measure of wind farm energy production is the half-hourly metered output from all transmission-connected generators published on Elexon's TIBCO relay service.22
www.bmreports.com.
 Data from 2005 to 2012 covers 25 TWh of output from the 47 wind farms highlighted in Fig. 6a.
Fig. 6b and c compare the hourly metered output from Black Law wind farm with NASA wind speeds interpolated at its location at 80 m above ground, and the ideal energy yields derived from these speeds.
Each point in Fig. 6b represents one hour's operation to give the empirical power curve for that farm.
This empirical curve follows the features of the farm-aggregated Siemens SWT-2.3-82 power curve (solid line), albeit shifted to the right (as NASA speeds are the theoretical maximum, and actual speeds will be lower due to local site features) and downwards (due to downtime and sub-optimal turbine calibration), with substantial scatter (as local conditions like turbulence vary over time).
Fig. 6c shows 1000 h of metered output together with the simulated ideal output, which was scaled by a constant factor of 0.698 for reasons explained in the next section.
The simulated ideal load factors for other farms compare similarly well to their metered outputs, with plots given in Supplementary Fig. 9.
Results
Simulated wind speeds
The average monthly wind speed at UK onshore wind farms was estimated to be 7.5 ± 1.5 m/s at the location and hub height of each farm, which average 62 m above ground.
This average speed has experienced a slight decline over the last 12 years, although the trend is not statistically significant (-0.23 ± 0.37 m/s per decade).
Fig. 7 shows the correlation between the simulated monthly NASA wind speeds, averaged overall operating sites, and the national average load factors reported to Ofgem.
The correlation is very high as the simulation is able to represent the actual sites of generation and the evolution of this site population over time, reaffirming the strong linear relationship between speed and output at monthly resolution.
The correlation between monthly average speed and reported load factor is also high for individual wind farms (averaging 0.84), as shown in Supplementary Fig. 12.
If performance declines with wind farm age, we would expect the reported load factor to fall relative to the simulated wind speed over the sample period.
This is in fact the case although it is hard to see in Fig. 7, because so many new farms have been built that the average age of the UK fleet has only risen only from 4.8 to 7.2 years between 2002 and 2012.
The degradation rates derived below imply that this would reduce the fleet's average load factor by only 1 percentage point.
Ideal load factor and performance ratio
The ideal load factors derived from these wind speeds ignore a number of factors that will reduce the actual output attained by a wind farm at the given location and hub height.
Three are well-understood:1.
Machine availability: analysis of national fleets suggests 4-7% downtime for farms and the electrical infrastructure they rely upon [9,30], which translates to an 11% reduction in energy output as turbines on average fail in windier than average conditions [31];
2.
Operating efficiency: sub-optimal control systems, misaligned components and electrical losses within the farm are found to reduce output by 2% in well-performing field installations relative to the turbine's supplied power curve [30];
3.
Wake effects: wind farms suffer from power loss as interactions between neighbouring turbines increase turbulence and reduce wind speeds; for relatively small (up to 20 turbine) onshore farms estimates are in the region of 5-15% [25,32-35];
and two are less well understood:4.
Turbine ageing: based on the findings from Fig. 2 (and presented later in this paper), energy output from the UK's fleet is 7.5% lower than it would be for a fleet of the same turbines as-new, due to their average age being 5.9 years across the sample period33
In practice ageing is not a separate issue from availability and efficiency, as these likely fall over a turbine's lifetime from the as-new values listed in points 1 and 2, producing the ageing effect that we observe.
;
5.
Site conditions: imperfections in a turbine's surroundings are not considered in our model; for example: turbulence intensity, terrain slope, blockage effects, blade fouling (by dirt, ice, insects, etc.), or masking by surrounding terrain.
These impacts are highly site specific and hard to quantify with a single factor, with the only source we found estimating that they reduce output by 2-5%, plus 1% per 3% increase in turbulence intensity [36].
Combining the first four terms, we could expect the ratio of observed to ideal load factors to be 0.89·0.98·0.90·0.925 = 0.725.
We call this metric the Performance Ratio (PR), which is analogous to availability, except it deals with output rather than uptime.
Fig. 8 plots the relationship between actual and ideal load factors, showing that the performance ratio is unbiased across the range of simulated wind conditions.
Based on the simulated wind speeds and the model of turbine installed at each farm, the ideal load factor of UK onshore wind farms should average 38.4%, whereas the mean observed load factor for these farms from 2002 to 12 has been 26.3%.
The average performance ratio of the farms is therefore 68 ± 19%, confirming previous work from Ref.
[18] which found that a scale factor of 0.69 gave good correlation between load factors derived from a custom reanalysis and the Ofgem ROC data.
This result does not imply that UK wind farms produce only two-thirds of what they ought to, for the ideal yield represents a hypothetical turbine sited on perfectly flat and smooth terrain, several kilometres from other turbines, foliage or buildings.
The real-world factors 1-4 listed above suggest that a performance ratio of 0.725 should be expected, which leaves a reduction of 4 percentage points (6%) attributable to the specific site conditions for UK turbines.
Weather-corrected load factors
Combining the observed and ideal load factor data allows us to calculate a weather-corrected load factor (WCLFf,t) for every observation, giving a time series for each farm that should not be affected by wind conditions changing from month to month.
The actual load factor (LFf,t) for a given farm (f) and month (t) is divided by the ratio of its ideal load factor (ILFf,t) for that month to the farm's mean ideal load factor over the entire 2002-12 period (whether or not the farm existed throughout the whole period), as in Eq.
(3).
This can be simplified to the performance ratio (PR) for each month multiplied by the farm's average ILF.(3) WCLFf,t=LFf,t÷(ILFf,tILF¯f)=PRf,t×ILF¯f
The WCLF represents what a particular farm would have produced each month if wind conditions followed their long-term mean distribution.
As with the uncorrected load factors, the absolute value encompasses the available wind resource, the quality of the local site conditions, and the turbine model installed, but the variation over time is no longer dominated by seasonal weather patterns.
This is demonstrated in Fig. 9a: removing the weather noise reveals a gradual decline in this farm's conversion efficacy, and allows periods of low availability to be easily identified.
Fig. 9b shows the WCLF averaged across all farms of a given age (in months), revealing the aggregate level of degradation without the need for smoothing.
The reduction in scatter reduces the uncertainty on this degradation rate from 0.04 when using nominal load factors (as in Fig. 2), to 0.01 points per year.
Weather correction is notoriously difficult and has the potential to skew results as it makes large changes in the month-to-month load factor values.
We find this correction procedure to be unbiased with wind speed (as in Fig. 8b); and comparing Figs.
2 and 9b shows that it has almost no effect on the fleet-average degradation rate (a change of 0.01 points per year).
The distribution of WCLF decline rates at individual farms exhibits little change from that of the unmodified load factors presented in Fig. 3b, averaging -0.45 instead of -0.48 points per year for farms with more than ten years of data.
As we find in Supplementary Section 5.3, the weather correction process can clean up the short-term fluctuations without having a systematic effect on the long-term trends.
Technology improvement over generations
Having rejected the idea that an underlying change in national wind speeds has distorted the results, we look at the evolution of the population of turbines.
The oldest farms in our sample are the earliest to have been built, using (presumably) the worst technology, and hence are likely to have the lowest load factors.
Fig. 10 shows the individual degradation rate for each farm against the year it began operating, for all farms with more than 5 years of data.
Bubble size is proportional to capacity, bubble colour represents the number of observations, and horizontal bars depict the standard error on each decline rate.
Black lines show the best fit to the data using a capacity-weighted loess regression, showing the central estimate and the range that covers 95% of observations.
The central fit to the raw load factors in Fig. 10a has a mean of -0.50 ± 0.07 points per year of operation, and although it shows noticeable variation between start years there is no long term trend.
A few modern farms have seen increasing load factors over the first few years of their lives, but this is offset by the majority having declining outputs.
Correcting for the weather in Fig. 10b greatly reduces the uncertainty on individual farms and the scatter between them.
It also reduces the mean decline rate to -0.36 ± 0.05 points overall farms.
This decline rate appears stable until 2002, after which it reduces for more recently commissioned turbines.
Farms built before 2003 have an average decline rate of -0.49 ± 0.05 points per year, whereas those built afterwards average -0.16 ± 0.08.
The impact of weather correction is strongest on the farms commissioned most recently as they have the fewest years of data, and so the noise introduced by year-on-year variations in wind speed is strongest.
This is most evident in the group of farms commissioned after 2003 which were pulled downwards by the low wind speeds experienced in 2010, and so see a notable improvement in Fig. 10b compared to a.
Full regression of national fleet performance
Four systematic factors determine the actual output of a wind farm: wind speeds at the site, the quality of individual turbine locations (with regards to turbulence and masking), the model of turbine installed and age-related deterioration of performance.
Other factors, such as the number of turbines suffering faults in a given month or undergoing planned maintenance, are less systematic.
As demonstrated by Hughes in Ref.
[1], it is possible to separate the impact of turbine ageing from these other factors by using an error components model with fixed effects.
The observed load factor, LFf,t, of wind farm f in month t is estimated by least squares regression against the ideal load factor, ILFf,t, with fixed effects for each site, sf, and age of the turbine, Af,t, minimising the sum of the squares of the error component, εf,t, as in Eq. (4):(4)LFf,t=α+βILFf,t+sf+Af,t+εf,t
The fixed effects (sf and Af,t) are the freeform equivalent of a linear trend.
A numeric constant is determined for each site by the regression to control for any farm-specific factors which affect the actual wind speeds experienced at the site relative to the NASA estimates.
Similarly, constant modifiers are determined for each age of turbine (in years) to assess the impact of ageing.
A linear or quadratic regression against age would not capture complex non-linear behaviour, and so ageing can be better understood through fixed effects.
For it to be possible to solve this model, the fixed effect for one site (chosen at random) is held constant at zero, as is the effect for turbines aged 1, which together act as the reference point.44
In the results presented, the performance of Shooters Bottom (in Somerset) aged 1 is used as a reference.
In the model chosen by Hughes [1], the site fixed effects had to account for the model of turbine as well as local site conditions, and period fixed effects were used for each observation month to account for available wind resource (but with the same impact on every farm in that month).
In our model, the systematic effect of location-specific wind speeds and turbine model are both incorporated in the ideal load factor data, while the site dummy variables measure the extent to which each farm's surroundings and layout systematically give it more or less wind than the simulation predicts.
The error term picks up the unsystematic factors affecting each observation.
The regression produced a constant offset of α = -1.634 ± 1.529 percentage points of load factor55
This is made up from the offset relative to the reference farm (2.926) plus the average of all farm site effects (-4.560).
 and an estimated coefficient on the ideal load factor of β = 0.755 ± 0.003.
This coefficient is above our average performance ratio of 68% as it is for one-year-old wind farms, and the age effects show that performance declines over time.
This model, which uses a highly specific wind resource assessment, provides a better fit to the observed data than using period fixed effects, giving an R2 of 0.802 compared to 0.657 attained in Ref.
[1].
The full regression results are given in Supplementary Section 5.6, along with alternative model formulations which we tested.
Fig. 11 plots the fixed effects produced by this regression, re-centred to give actual load factors as opposed to deviations from the reference point.
Fig. 11a shows the impact of turbine age on the load factor, taking account of spatial and temporal differences in the available wind resource, technology installed and local site conditions.
From Fig. 11a it can be seen that the uncertainty on the age fixed effects is small enough to be confident that there is indeed degradation, but is too large to be able to discern whether the trend beyond age 1 is linear, exponential, or some more complex function.
For simplicity's sake we fit a linear trend to these effects, which falls from 28.5% at age 1 to the national average load factor of 26.3% for farms at the national average age (5.9 years), reaching 21.0% for the oldest farms aged 19.
Fig. 11b plots the site fixed effects added to the regression offset and the individual contributions from the ideal load factor at each farm (i.e. α+βILF¯f+sf).
This combination yields what we call the individual farm effects, which are the model's estimate of each farm's load factor at age 1, accounting for location (estimated wind resource), technology (turbine model and its hub height), and surroundings (local site quality).
Regressing these farm effects against the year each farm was built yields no significant trend, implying that any technical improvement has been masked by diminishing site quality.
Many variations on this model can be considered; for example, using an exponential rather than linear fit, substituting the expected load factor with wind speeds (wf,t), or estimating the farms' performance ratio rather than load factor.
These models were found to give very similar results to the linear model in Eq.
(2), with annual degradation rates that range from 1.5 to 1.9% per year.
Supplementary Section 5.6 details these results and shows that our extension of the original REF dataset did not skew the rate of decline.
Discussion
The finding that wind turbines lose around 1.6% of their output each year poses three questions:1.
What are the reasons for this deterioration?
2.
Can we expect it to continue in future?
3.
What are its wider impacts?
Reasons for declining output
The degradation rate we observe is perhaps to be expected, as it lies in the middle of the range experienced by gas turbine technologies: 0.75-2.25% per year [2].
As with gas turbines and other aerodynamic rotating machinery, a portion of the unrecoverable loss could be attributed to gradual deterioration, such as fouling of the blades (which will impede the aerodynamic performance) and a gradual reduction in component efficiencies (gearbox, bearings, generator).
These may not be recoverable by maintenance procedures, but only by component replacement.
A (potentially larger) contribution could come from availability declining with age, either because older turbines fail more frequently or because they take longer to bring back online.
Possible reasons for the latter are the likelihood that older machines suffer more serious failures, difficulty in obtaining components for obsolete models, and operators being less likely to hold comprehensive maintenance contracts.
Availability will depend on the amount of effort the owner is willing to invest in maintenance, which may naturally fall over time as the asset is paid down, and will depend on electricity prices and O&M costs.
The manufacturer's availability warranty provided with a new turbine (which may for example guarantee 97% uptime) is also likely to exceed the standard provided by third party O&M providers in later life.
Early turbine death is a third contributing reason.
If one turbine in a farm of four fails completely at age 17, the farm will continue operating at a maximum of 75% of its original load factor, which would translate to an annualised degradation rate of around 1.6%.
Fig. 9a highlights these first two reasons as a gradual downwards slope in the bulk of weather-corrected load factor observations, and isolated periods of very low output due to downtime which are concentrated towards higher ages.
Fig. 3a shows an example of the third: Blyth Harbour stands out at the lower-right of the chart as its load factor declined from 12% to just 2% between the ages of 12 and 17.
By the end of its life only one of the nine turbines was generating, giving it the worst degradation rate of the farms we observed.
Future trends
What of the future? Although our main calculations are based on linear rates of decline, we cannot yet know if this assumption is correct.
Figs.
9b and 11a suggest a slightly lower rate of ageing in the first six or seven years of a farm's life.
This early period is typically covered by comprehensive warranties which guarantee near-maximal output, whereas older farms may be less intensively maintained and thus deteriorate more rapidly.
The pattern seen in these Figures could be produced if all turbines (of whatever cohort) had an initial period of slow decline, followed by accelerating degradation as they become older.
Alternatively, it could be that advances in technology mean that recent turbines experience relatively little decline and will continue to do so in the future, whereas earlier models have declined relatively faster throughout their lives.
As those early turbines were new in the 1990s their output is not available in our dataset, and so the experience of those years does not show up in Figs.
9b and 11a.
If those early turbines had declined rapidly when new, the kink visible in these Figures would have been lessened or even removed.
Although the improvement in WCLF decline rates seen in farms commissioned after 2003 is statistically significant, that is no guarantee that this result will still hold if we were to repeat this study in a few years' time.
Based on this, and further analysis in Supplementary Section 5.5, we must conclude that even though the initial signs are good, more data are required before we could say for sure that modern turbines are declining less rapidly than earlier cohorts.
Wider impacts
The cumulative lifetime output of a 100 MW wind farm with a 28.5% load factor would be 4.99 TWh over 20 years.
If this farm suffers a linear annual deterioration of -0.41 points after the first year, its lifetime output reduces to 4.37 TWh, a fall of 12.5%.
This will increase the cost of electricity from wind generators, as less electricity is produced to recover the costs of construction.
The economic value of the lost output is relatively low as it mostly occurs in the far future.
With a discount rate of 10%, degradation increases the levelised cost of electricity by 9%, from approximately £90 [4,5] to £98 per MWh.
This impact becomes greater if the economic lifetime increases or the discount rate decreases.
A second impact is that more capacity will need to be installed to produce a given level of output.
The UK has a target for energy production from renewable sources (15% of all final energy by 2020), as opposed to a target for peak capacity.
If turbine build rates peak in the coming years, the average age of the UK's wind farms will creep upwards, and so the output from a fixed capacity can be expected to decline.
For every year the fleet ages, an additional 435 MW (4 large farms) would need to be brought online to maintain the original capacity of the UK's anticipated 30 GW fleet.
Conclusions
This paper demonstrates a generic and broadly applicable method for predicting a wind farm's monthly load factor, accounting for its location, hub height and the particular model of turbine installed.
We use this to estimate the ideal monthly load factors for 282 of the UK's wind farms over the last decade, and compare these to the actual outputs over this period.
This allows us to correct for the rapid improvement in wind turbine technology over the last two decades and the huge seasonal variability in wind speeds, thus revealing the subtle rate of degradation.
We find evidence of important, but not disastrous, performance degradation over time in a large sample of UK wind farms.
When variations in the weather and improvement in turbine design are accounted for, we find that the load factors of UK wind farms fall by 1.57% (0.41 percentage points) per year.
This degradation rate appears consistent for different vintages of turbines and for individual wind farms, ranging from those built in the early 1990s to early 2010s.
We use six methods of increasing complexity to find the following rates of degradation in absolute percentage points per year; and relative to the UK mean wind farm:•
Simple regression of all load factors against age (-0.44 ± 0.04 absolute) (-1.69 ± 0.17% relative);
•
Average trend in load factors for individual farms (-0.48 ± 0.36 absolute) (-1.81 ± 1.32% relative);
•
Correct for wind resource and regress weather-corrected load factor against age (-0.45 ± 0.01 absolute) (-1.68 ± 0.05% relative);
•
Trend in weather-corrected load factors for individual farms against their age (-0.45 ± 0.22 absolute) (-1.70 ± 0.82% relative);
•
Capacity-weighted fit to individual farms against their year of commissioning (-0.50 nominal, -0.36 weather-corrected) (-1.90% and -1.24% relative);
•
Full fixed effects regression, accounting for site-specific wind speeds, turbine model and site quality (-0.41 ± 0.01 absolute) (-1.57 ± 0.06% relative).
The combined average of these measures is -0.43 ± 0.05 percentage points per year, giving -1.6 ± 0.2% annual degradation.
The similarity of results from different methods gives us confidence that the underlying trend is robust: the decline in load factor with age is neither an artefact of systematic variation in wind speeds nor of the continual improvement in technology.
Questions do however remain as to the exact form of this degradation, for example whether it is linear, quadratic or logarithmic with age; or how degradation rates are changing over time and whether they will be lower in the future.
Access to data from more farms, and a more detailed wind resource assessment for each site will be fundamental to furthering our understanding of these issues.
The level of degradation we find is not insignificant, yet it is not unusual compared to conventional generation technologies.
The fact that it has been omitted from calculations of the levelised cost of electricity from wind means that these estimates are around 9% below the true value (depending on assumed discount rate and economic lifetime).
This is unlikely to be large enough to change the business case for wind power, but nonetheless it needs to be accounted for to give an accurate picture of its cost.
Acknowledgements
The authors would like to thank:
• Gordon Hughes, John Constable and the Renewable Energy Foundation, who enabled this research by generously making their data publicly available;
• Christopher Crabtree, Andrew Garrad and Paul Gardner (GL Garrad Hassan), Matthew Hanson (Pöyry), David MacKay (DECC), David Newbery, Jim Oswald (Oswald Consulting), Jim Platts and Peter Tavner for insightful discussions and feedback on our earlier drafts;
• The British Atmospheric Data Centre and NASA Goddard Flight Centre for the excellent services they provide;
• Anabelle Guillory from the Centre for Environmental Data Archival (CEDA) for help in acquiring the MIDAS marine data;
• Alasdair Skea and Simon Vosper from the UK Met Office for improving our understanding of the MIDAS equipment and data;
• The Alan Howard Charitable Trust and the EPSRC Grand Challenge Project 'Transforming the Top and Tail of the Energy Networks' (EP/I031707/1) for funding this work.
Supplementary data
Supplementary data
Supplementary data related to this article can be found at http://dx.doi.org/10.1016/j.renene.2013.10.041.

Magneto-orbital ordering in the divalent A -site quadruple perovskite manganites AMn7O12 ( A=Sr , Cd, and Pb)

Powder samples of SrMn7O12, CdMn7O12, and PbMn7O12 were prepared from stoichiometric mixtures of Mn2O3, MnO1.839 (Alpha Aesar MnO2 99.997% with the precise oxygen content determined by thermogravimetric analysis), PbO (99.999%), CdO (99.99%), and 4H-SrMnO3. The mixtures were placed in Au capsules and treated at 6 GPa and 1373 K for 2 h for CdMn7O12 and PbMn7O12 and at 6 GPa and 1573 K for 2 h for SrMn7O12 (the duration of heating to the desired temperatures was 10 min) in a belt-type high-pressure apparatus. After the heat treatments, the samples were quenched to room temperature, and the pressure was slowly released. Data on CaMn7O12 presented in this paper have been reproduced from previous studies [3,22], in which single crystals were grown at ambient pressure by the flux method [3], and then ground and sieved through a 35-μm mesh. All samples were characterized by laboratory based x-ray powder-diffraction, and heat capacity and magnetization measurements using a Quantum Design physical property measurement system and magnetic property measurement system, respectively.


Oxidation resistance and structural evolution of (TiVCrZrHf)N coatings

The (TiVCrZrHf)N coatings were deposited on p-Si (100) wafers by a reactive RF magnetron sputtering system using equimolar TiVCrZrHf targets (75 mm in diameter). Before deposition, the Si substrates were cleaned and rinsed with ethanol and distilled water in an ultrasonic bath. Then, 1.4 μm-thick (TiVCrZrHf)N coatings were deposited at a plasma power of 350 W and a substrate bias of - 100 V in an Ar + N2 mixed atmosphere under a working pressure of 6.67 x 10- 1 Pa at 450 degC. The flow rates of Ar and N2 were maintained at 100 and 4 sccm, respectively. To determine the oxidation resistance of the as-deposited (TiVCrZrHf)N coatings, they were annealed at different temperatures (300-700 degC) for 2 h in air using a furnace.
Resistance switching properties of epitaxial Pr 0.7 Ca 0.3 MnO 3 thin films with different electrodes

A polycrystalline PCMO target was prepared by standard ceramics technique as follows. Accurate amounts of high-purity (99.9%) Pr6O11, MnO and CaCO3 powders were weighed and mixed by ball milling. After calcination at 1250 degC for 12 h, the powder was grounded, pressed and sintered in air for another 12 h at 1400 degC. Synthesis of PCMO target with x = 0.3 was confirmed by X-ray diffractometry (XRD) and electron diffraction spectroscopy (EDX).
Role of oxygen functional groups in reduced graphene oxide for lubrication
The sample was prepared by Hummer's method44,45. Graphite flakes (10 g) and NaNO3 (10 g) were mixed in 500 mL of H2SO4 (98%) at 5 degC with continuous stirring for 2 h, and KMnO4 (30 g) was gradually added to the suspension at a controlled reaction temperature lower than 15 degC. The ice bath was then removed, and the mixture was stirred at 35 degC until it became pasty brownish; stirring continued for two days, after which the mixture was diluted with a slow addition of 300 ml of water. The reaction temperature was rapidly increased to 98 degC, and the color changed to dark brown. This solution was further diluted by adding an additional 400 ml of water and stirring continuously. The solution was finally treated with 150 ml of H2O2 to terminate the reaction, indicated by the appearance of a yellow color. For purification, the mixture was washed by rinsing and centrifugation with 10% HCl and then with deionized water (DI) several times. After filtration and drying under a vacuum at room temperature, the GO1 was obtained as powder. Reduction was performed to reduce the functional group density, towards decreasing the inter-plane spacing and the flake dimensions and improving the friction properties. Reduction was done via treating 100 mg of GO1 in 30 mL of water with 3 mL of hydrazine and refluxing at 95 degC for 24 h, followed by filtering, washing and drying at 60 degC under vacuum to obtain the rGO1 powder.
Graphite flakes (10 g) and NaNO3 (10 g) were mixed in 600 mL of H2SO4 (98%) at 5 degC with continuous stirring for 6 h, and KMnO4 (60 g) was gradually added to the suspension while maintaining the temperature at 15 degC. The mixture was diluted with a slow addition of 400 mL of water and with stirring maintained for 2 h. The ice bath was then removed, and the mixture was stirred at 35 degC for 2 h and then refluxed at 98 degC for 30 min. The solution was finally treated with 200 ml of H2O2 and the color changed to bright yellow. The resulting mixture was washed by centrifugation with 10% HCl and then with DI water several times until it became gel-like. After centrifugation, the gel was vacuum dried at 80 degC for 8 h to obtain the GO2 powder. Reduction was performed similarly to rGO1.Direct synthesis of palladium nanoparticles on Mn3O4 modified multi-walled carbon nanotubes: A highly active catalyst for methanol electro-oxidation in alkaline media
All chemical reagents used in this experiment were of analytical grade. Sulfuric acid, nitric acid, hydrochloric acid, ethanol, methanol, NaOH, H2PdCl4, KMnO4, NaBH4 and polyethlyleneglycol 20,000 (PEG 20000) were procured commercially and used without further purification. The raw MWCNTs were purchased from Shenzhen Nanotechnologies Port Co. Ltd. (Shenzhen, China) with the diameter of 40-60 nm, length of 5-15 μm, and purity of 98%. Nafion (perfluorosulfonic acid-PTFE copolymer) was purchased from Alfa Aesar (A Johnson Matthey Company) with the concentration of 5% w/w solution.

Raw-MWCNTs were refluxed in a concentrated H2SO4-HNO3 mixture (8.0 M for each acid) at a bath temperature of 80 degC with duration of 2 h to remove the impurities. The acid-treated MWCNTs (AO-MWCNTs) were washed for several times with deionized water and dried in a vacuum oven at 70 degC for 12 h for further use.

For preparing the Mn3O4/MWCNT composites, the AO-MWCNTs, hydrochloric acid (HCl) and potassium permanganate (KMnO4) were used as the starting material, PEG 20000 as the surfactant and reducing agent. A typical synthesis route was as follows: 5 mL of 0.1 M KMnO4 and 2.5 mL of 50 g L-1 PEG 20000 was mixed in 100 mL beaker, stirred for 30 min. 30 mg AO-MWCNTs and 10 mL of H2O was dispersed in 50 mL beaker after 30 min sonication and this suspension was added into the above solution with continuous stirring for 2 h. Then 1.0 M hydrochloric acid was added to the solution until its pH dropped to about 2.0, and then stirred about 2 h at room temperature until the color of the KMnO4 faded. Finally, the resulting product was centrifuged, washed with deionized water and ethanol respectively, dried overnight at 70 degC to obtain high purified Mn3O4/MWCNTs.

Pd-Mn3O4/MWCNTs (20 wt.% metal content) nano-sized catalysts were synthesized using NaBH4 as reducing agent by wet impregnation method: 20 mg of Mn3O4/MWCNTs powder was dispersed in 20 mL ethanol/water (1:1, v/v ratio) solution in 100 mL beaker, mixed with 9.4 mL of 5 mM H2PdCl4 solution and stirred for 10 h. A freshly prepared solution of 50 mg NaBH4 in 10 mL water was added dropwise into the above solution under vigorous stirring. After stirred for an additional 2 h, the black solid was centrifuged and washed with deionized water for several times, and then dried overnight in oven at 70 degC. For comparison, Pd nanoparticles supported on MWCNTs and Vulcan XC-72 catalysts were also obtained by the same process. The Mn3O4 content of prepared Pd-Mn3O4/MWCNTs was 40 wt. % and the palladium loading of every catalysts was 20 wt. %.1. A positive active material for a lithium secondary battery, the positive active material comprising:
a core part and a shell part that both comprise a nickel-based composite oxide represented by Formula (1) below:
Lia[NixCoyMnz]O2(1)
where 0.8≤a≤1.2, 0.05≤x≤0.9, 0.1≤y≤0.8, 0.1≤z≤0.8, and x+y+z=1,
wherein the content of nickel in the core part is larger than the content of nickel in the shell part, and wherein the core part comprises needle-like particles.2. The positive active material according to claim 1, wherein the core part precursor has open pores.7. The positive active material according to any one of claims 1 to 6, wherein a content of nickel in the core part is in a range of 50 to 90 mole%, and a content of nickel in the shell part is in a range of 5 to 49 mole%.8. The positive active material according to any one of claims 1 to 7, wherein the core part and the shell part are substantially spherical, and wherein a diameter of the core part is in a range of 1 to 10 µm, and a diameter of the shell part is in a range of 5 to 20 µm.9. A method of preparing a positive active material for a lithium secondary battery, the method comprising:
a first process for mixing a first precursor solution comprising a nickel salt, a cobalt salt, and a manganese salt, at a molar ratio of x:y:z satisfying 0.05≤x≤0.9, 0.1≤y≤0.8, 0.1≤z≤0.8, and a first base, to prepare a first mixture and inducing a reaction in the first mixture to obtain a precipitate;
a second process for adding to the precipitate a second precursor solution comprising a nickel salt, a cobalt salt, and a manganese salt, at a molar ratio of x':y':z' satisfying 0.05≤x'≤0.9, 0.1≤y'≤0.8, 0.1≤z'≤0.8, and x'+y'+z'=1, and a second base, to obtain a second mixture and inducing a reaction in the second mixture to obtain a nickel-based composite hydroxide; and
mixing the composite metal hydroxide with a lithium salt and heat treating the mixed composite metal hydroxide to prepare the positive active material according to any one of claims 1 to 8,
wherein the content of nickel in the first precursor solution is adjusted to be larger than the content of the second precursor solution.10. The method of claim 9, wherein the reaction time of the second mixture in the second process is adjusted to be longer than the reaction time of the first mixture in the first process.11. The method according to any one of claims 9 and 10, wherein a reaction time of the first mixture in the first process is in a range of 5 to 7 hours, and a reaction time of the second mixture in the second process is in a range of 8 to 10 hours.12. The method according to any one of claims 9 to 11, wherein an amount of the nickel salt in the first process is in a range of 1 to 1.2 moles based on 1 mole of the cobalt salt.13. The method according to any one of claims 9 to 12, wherein a pH of the first mixture in the first process is adjusted to be in a range of 10 to 11, and a pH of the second mixture in the second process is adjusted to be in a range of 11.5 to 12.0.14. A lithium secondary battery (30) comprising a positive electrode (23), a negative electrode (22), a separator (24) and an electrolyte, wherein the positive electrode (23) comprises the positive active material according to any one of claims 1 to 8.15. A positive active material precursor for the positive active material according to any one of claims 1 to 8, the positive active material precursor comprising:
a nickel-based composite hydroxide represented by Formula (2) below
NixCoyMnzOH     (2) wherein 0.05≤x≤0.9, 0.1≤y≤0.8, 0.1≤z≤0.8, and x+y+z=1,
preferably Ni0.6Co0.2Mn0.2OH or Ni1/3Co1/3Mn1/3OH.2. The carbonate precursor compound of claim 1, having the general formula MCO3, wherein M= NixMnyCozAv, A being a dopant, wherein 0.20≤x≤0.90, 0.10≤y≤0.67, and 0.10≤z≤0.40, v≤0.05, and x+y+z+v= 1.3. The carbonate precursor compound of claim 2, wherein A is either one or more of Mg, Al, Ti, Zr, Ca, Ce, Cr, Nb, Sn, Zn and B.4. A carbonate precursor compound for manufacturing a lithium metal (M)-oxide powder usable as an active positive electrode material in lithium-ion batteries, having the general formula MCO<sub>3</sub>, wherein M= Ni<sub>x</sub>Mn<sub>y</sub>Co<sub>z</sub>A<sub>v</sub>, A being a dopant, wherein 0.10≤x<0.30, 0.55≤y≤0.80, and 0<z≤0.30, v≤0.05, and x+y+z+v=1, the precursor further comprising a sodium and sulfur impurity, wherein the sodium to sulfur molar ratio (Na/S) is 0.4<Na/S<2.5. The carbonate precursor compound of any one of claims 1 to 4, wherein the sodium content is between 0.1 and 0.7 wt%, and the sulfur content is between 0.2 and 0.9 wt%.6. A lithium metal oxide powder for a positive electrode material in a rechargeable battery, having the general formula Li<sub>1+a</sub>M<sub>1-a</sub>O<sub>2</sub> where M= Ni<sub>x</sub>Mn<sub>y</sub>Co<sub>z</sub>A<sub>v</sub>, A being a dopant, wherein -0.05≤a≤0.25, 0.20≤x≤0.90, 0.10≤y≤0.67, and 0.10≤z≤0.40, v≤0.05, and x+y+z+v=1, the powder having a particle size distribution with 10µm≤D50≤20µm), a specific surface area with 0.9≤BET≤5, the BET being expressed in m<sup>2</sup>/g, the powder further comprising a sodium and sulfur impurity, wherein the sum (2* Na<sub>wt</sub>)+ S<sub>wt</sub> of the sodium (Na<sub>wt</sub>) and sulfur (S<sub>wt</sub>) content expressed in wt% is more than 0.4 wt% and less than 1.6 wt%, and wherein the sodium to sulfur molar ratio (Na/S) is 0.4<Na/S<2.7. The lithium metal oxide powder of claim 6, comprising a secondary LiNaSO4phase.8. The lithium metal oxide powder of claim 6 or 7, wherein either:
0.4<Na/S<1, and the powder further comprises Na<sub>2</sub>SO<sub>4</sub>; or 1<Na/S<2, and the powder further comprises Li<sub>2</sub>SO<sub>4</sub>.9. The lithium metal oxide powder of any one of claims 6 to 8, wherein A is either one or more of Mg, Al, Ti, Zr, Ca, Ce, Cr, Nb, Sn, Zn and B.10. A lithium metal oxide powder for a positive electrode material in a rechargeable battery, having the general formula Li<sub>1+a</sub>M<sub>1-a</sub>O<sub>2</sub> where M= Ni<sub>x</sub>Mn<sub>y</sub>Co<sub>z</sub>A<sub>v</sub>, A being a dopant, wherein 0.10≤a≤0.25, 0.10≤x≤0.30, 0.55≤y≤0.80, and 0<z≤0.30, v≤0.05, and x+y+z+v=1, the powder having a particle size distribution with 10µm≤D50≤20µm, a specific surface area with 0.9≤BET≤5, the BET being expressed in m<sup>2</sup>/g, the powder further comprising a sodium and sulfur impurity, wherein the sum (2* Na<sub>wt</sub>)+ S<sub>wt</sub> of the sodium (Na<sub>wt</sub>) and sulfur (S<sub>wt</sub>) content expressed in wt% is more than 0.4 wt% and less than 1.6 wt%, and wherein the sodium to sulfur molar ratio (Na/S) is 0.4<Na/S<2.11. A method for preparing a carbonate precursor compound according to claim 2 or 3, <u>the</u> precursor compound having a sodium to sulfur molar ratio (Na/S) of 0.4<Na/S<2, and having a sum (2* Na<sub>wt</sub>)+ S<sub>wt</sub> of the sodium (Na<sub>wt</sub>) and sulfur (S<sub>wt</sub>) content expressed in wt% of more than 0.4 wt% and less than 1.6 wt%, comprising the steps of:
- providing a feed solution comprising Ni-, Mn- and Co-ions, and a source of A, wherein the Ni-, Mn-, Co- and A-ions are present in a water soluble sulfate compound,
- providing an ionic solution comprising a carbonate solution and Na-ions, wherein the CO3/SO4ratio is selected so as to obtain the Na/S molar ratio of the precursor compound,
- providing a slurry comprising seeds comprising M'-ions, wherein M' = Nix'Mny'Coz'A'n',
A' being a dopant, with 0≤x'≤1, 0≤y'≤1, 0≤z'≤1, 0≤n'≤1 and x'+y'+z'+n'=1,
- mixing the feed solution, the ionic solution and the slurry in a reactor, thereby obtaining a reactive liquid mixture,
- precipitating a carbonate onto the seeds in the reactive liquid mixture, thereby obtaining a reacted liquid mixture and the carbonate precursor, and
- separating the carbonate precursor from the reacted liquid mixture.12. The method according to claim 11, wherein the molar ratio (M'seeds/Mfeed) of the metal content in the seed slurry to the metal content in the feed solution is between 0.001 and 0.1, and wherein the median particle size of the carbonate precursor is determined by the ratio M'seeds/Mfeed.13. The method according to claim 11 or 12, wherein the concentration of NH3in the reactor is less than 5.0 g/L.14. The method according to any one of claims 11 to 13, wherein M=M'.15. The method according to any one of claims 11 to 14, wherein the ionic solution further comprises either one or both of a hydroxide and a bicarbonate solution, and the ratio OH/CO3, or OH/HCO3, or both these ratios are less than 1/10.16. The method according to any one of claims 11 to 15, wherein the seeds have a median particle size D50 between 0.1 and 3 µm.
Mesoscopic architectures of Co(OH)2 spheres with an extended array of microporous threads as pseudocapacitor electrode materials

All the chemicals used in the present study were of analytical grade obtained from Nice Chemicals, India. Firstly, 1 M of Co(NO3)2 was completely dissolved in 32 mL of methanol under constant stirring condition. To this solution, 1 M of ammonia solution was added followed by 32 mL addition of water. This solution was stirred for 30 min. The obtained precipitate solution was autoclaved in a teflon lined container at 90 degC for different time intervals ranging from 3 to 24 h. The resultant solution was centrifuged and filtered for 30 min. The obtained precipitate was thoroughly washed with de-ionized water and oven dried for 12 h in air at 50 degC. Based on the processing time the powders were designated as C-3, C-6, C-12 and C-24 where C and number represent cobalt hydroxide and time, respectively.

Highly porous TiO2 hollow microspheres constructed by radially oriented nanorods chains for high capacity, high rate and long cycle capability lithium battery

Titanium tetrachloride (TiCl4), titanium sulfate (Ti(SO4)2), isopropanol and anhydrous ethanol were purchased from Sinopharm Chemical Reagent Ltd. Co. and used as received.

In a typical synthesis, 3 mL of titanium tetrachloride (TiCl4) was added to 84 mL of isopropanol. Then the solution was still magnetically stirred for 30 min. Subsequently, the precursor solution was then transferred into a 150 mL Teflon-lined autoclave and kept at 200 degC for 24 h. After this hydrothermal reaction, the reaction was naturally cooled to room temperature. The recovered white precipitate was washed thoroughly with ethanol and dried at 60 degC for 12 h.
Silicon oxide-on-graphite planar composite synthesized using a microwave-assisted coating method for use as a fast-charging lithium-ion battery anode

Si oxide-coated graphite flake (SGF) is synthesized by a microwave-heating method.

To prepare the Si-containing liquid precursor, 19 g of polymethylhydrosiloxane (Aldrich) was mixed with 1 g of divinyl polydimethylsiloxane (Aldrich) and 0.25 g of a catalyst solution (1wt.% hydrogen hexachloroplatinate in isopropyl alcohol), and the resulting mixture was heated at 70 degC for 2 h. Graphitic flake (GF; KS6, Timcal) powder, which consisted of flakes having an average particle size of 3 μm (vender's value) and thicknesses ranging from 50 to 100 nm, was added into the liquid precursor and stirred for 1 h. The suspension was then filtered to collect solution-coated GFs. The wet GF powder was placed in an evacuated glass vessel and heated in a commercial microwave oven (Panasonic, NN-ST651; power setting: low-300 W) for 15 min. After the heating process, the powder was washed with hexane to remove residual precursors and then finally calcined at 1000 degC under vacuum for 1 h in a quartz tube furnace to obtain the SGFs.
1-15. (canceled)16. A heterogeneous metal bearing material, comprising a host material composed of primary particles agglomerated into secondary particles, and a particulate dopant material, wherein the particulate dopant material is homogeneously distributed within the secondary particles of said host material.17. The heterogeneous metal bearing material of claim 16, wherein said host material comprises one or more of a metal hydroxide, oxyhydroxide, oxide, oxycarbonate, carbonate, or oxalate.18. The heterogeneous metal bearing material of claim 17, wherein said heterogeneous metal bearing material has the general formula (dopant material)<sub>a </sub>(host material)<sub>b</sub>, where a and b are weight fractions, with 0<a<0.4 and b=1−a.19. The heterogeneous metal bearing material of claim 16, wherein said dopant material is selected from the group consisting of MgO, Cr2O3, ZrO2, Al2O3, TiO2, and mixtures thereof, and is in the form of nanoparticles.20. The heterogeneous metal bearing material of claim 18, wherein the dopant material is TiO2and the host material is selected from the group consisting of NixMnyCozhydroxide, oxyhydroxide, oxide, and mixtures thereof, where x, y, z are atomic fractions, with 0≦x≦1, 0≦y≦1, 0≦z≦1, and x+y+z=1.21. The heterogeneous metal bearing material of claim 16, wherein said secondary particles are spherical.22. The heterogeneous metal bearing material of claim 16, wherein said dopant material comprises MgF2, CaF2, or another water insoluble metal halide, and is in the form of nanoparticles.23. The heterogeneous metal bearing material of claim 16, wherein said dopant material has a size range of 5 nm to 200 nm.24. A process for homogeneously distributing a particulate dopant material in a host material composed of primary particles agglomerated into secondary particles, thereby obtaining a heterogeneous metal bearing composite material, comprising:
providing a first flow comprising a solution of a precursor of the host material,
providing a second flow comprising a precipitation agent,
providing a third flow comprising a complexing agent,
providing a quantity of insoluble particulate dopant material, either in one or more of said first, second and third flows, or in a fourth flow consisting of a suspension of said particulate dopant material, and
mixing said first, second and third flows, and, if present, said fourth flow, thereby precipitating said host material and said dopant and obtaining a heterogeneous metal bearing composite material.25. The process of claim 24, wherein said solution of the precursor is an aqueous metal salt solution, and said suspension of the dopant material is a suspension in water with a suspension stabilizing agent.26. The process of claim 24, wherein the particulate dopant material consists of stabilized nanoparticles and the precursor is selected from the group consisting of a metal nitrate, chloride, halide, sulphate powder, and mixtures thereof.27. The process of claim 24, wherein said dopant material is selected from the group consisting of MgO, Cr2O3, ZrO2, AlO3, TiO2, and mixtures thereof, and has a size range of 5 to 200 nm.28. The heterogeneous metal bearing material of claim 18, wherein 0.001<a<0.4.29. The heterogenous metal bearing material of claim 28, wherein 0.001<a<0.02.30. The heterogenous metal bearing material of claim 23, wherein said dopant material has a size range of 10 nm to 50 nm.31. The process of claim 26, wherein the particulate dopant material consists of stabilized nanoparticles of metals or metal oxides.32. The heterogeneous metal bearing material of claim 20, wherein the host material is further doped with Mg.
1. Li-Ni composite oxide particles having a composition of Li<sub>x</sub>Ni<sub>1-y-a-b</sub>CO<sub>y</sub>M1<sub>a</sub>M2<sub>b</sub>O<sub>2</sub> wherein x, y, a and b represent 1.00 ≤ x ≤ 1.10; 0 < y ≤ 0.25; 0 < a ≤ 0.25; and 0 ≤ b ≤ 0.10, respectively; M1 is at least one element selected from the group consisting of Al and Mn; and M2 is at least one element selected from the group consisting of Zr and Mg, a product of a metal occupancy (%) of lithium sites of the Li-Ni composite oxide as determined by Rietveld analysis of X-ray diffraction thereof and a crystallite size (nm) of the Li-Ni composite oxide as determined by the Rietveld analysis being not less than 700 and not more than 1400, the metal occupancy of lithium sites of the Li-Ni composite oxide as determined by the Rietveld analysis being not less than 2% and not more than 7%, and the crystallite size of the Li-Ni composite oxide as determined by the Rietveld analysis is not more than 500 nm.2. The Li-Ni composite oxide particles according to claim 1, wherein the Li-Ni composite oxide particles have an average particle diameter of 1 to 20 µm.3. The Li-Ni composite oxide particles according to claim 1 or 2, wherein the Li-Ni composite oxide particles have a BET specific surface area of 0.1 to 1.6 m2/g.4. A process for producing the Li-Ni composite oxide particles as claimed in any one of claims 1 to 3, comprising the steps of:
producing Ni-Co hydroxide particles by mixing an aqueous solution of a sulfuric acid salt of each metal element, an ammonia aqueous solution and a sodium hydroxide aqueous solution such that an ammonia concentration in a reaction vessel is controlled to not more than 1.4 mol/L, and a ratio of the ammonia concentration in the reaction vessel to a surplus hydroxyl group concentration in the reaction vessel is controlled to not less than 6,
mixing lithium compound particles and the Ni-Co hydroxide particles, and
calcining the resulting mixture, the mixing ratio of the particles being controlled such that the molar ratio of Li/(Ni+Co+M1+M2) is 1.00 to 1.10.5. A process for producing the Li-Ni composite oxide particles according to claim 4, which comprises the steps of mixing lithium compound particles, Ni-Co hydroxide particles, and aluminum compound particles and/or zirconium compound particles, and calcining the resulting mixture.6. A process according to either claim 4 or claim 5, wherein the lithium compound is lithium hydroxide.7. A process according to any one of claims 4 to 6, wherein the lithium compound has a lithium carbonate content of less than 5%.8. A process according to any one of claims 4 to 7, wherein the lithium compound particles have an average particle diameter of not more than 50 µm.9. A process according to any one of claims 4 to 8, wherein the Ni-Co hydroxide particles have an average particle diameter of 2 to 30 µm and BET specific surface area of 1 to 20 m2/g.10. A process according to any one of claims 4 to 9, wherein the calcination takes place at a temperature of 650 to 950°C in an oxidative gas atmosphere.11. A non-aqueous electrolyte secondary battery using a positive electrode comprising a positive electrode active substance comprising the Li-Ni composite oxide particles as claimed in any one of claims 1 to 3.12. Use of the Li-Ni composite oxide particles according to any one of claims 1 to 3 as a positive electrode active substance for a non-aqueous electrolyte secondary battery.Promotional effect of HZSM-5 on the catalytic oxidation of toluene over MnOx/HZSM-5 catalysts
Firstly, the HZSM-5 (Si/Al = 50) zeolite was prepared by a conventional hydrothermal method. Tetraethyl orthosilicate (TEOS, Sinopharm AR, 15.6 g) was added into a solution containing water (7 g), sodium aluminate (NaAlO2, Sinopharm AR, 0.123 g) and tetrapropylammonium hydroxide (TPAOH, Aladdin 25 wt% in water, 12.18 g). After being stirred at 30 degC for 5 h, the resulting mixture was transferred into a Teflon-lined autoclave with hydrothermal treatment at 120 degC for 2 h and 170 degC for 24 h in an electric oven. Then, the solid product was collected by centrifugation and calcined at 550 degC for 8 h to remove the TPA+ species, resulting in the formation of Na-type ZSM-5 (NaZSM-5). Finally, H-type ZSM-5 (HZSM-5) was obtained by three consecutive ion exchanges in 1 M NH4NO3 aqueous solution and subsequent calcination in static air at 550 degC for 8 h.
Grain boundary excess volume and defect annealing of copper after high-pressure torsion

Abstract
The release of excess volume upon recrystallization of ultrafine-grained Cu deformed by high-pressure torsion (HPT) was studied by means of the direct technique of high-precision difference dilatometry in combination with differential scanning calorimetry (DSC) and scanning electron microscopy.
From the length change associated with the removal of grain boundaries in the wake of crystallite growth, a structural key quantity of grain boundaries, the grain boundary excess volume or expansion eGB=(0.46±0.11)×10-10 m was directly determined.
The value is quite similar to that measured by dilatometry for grain boundaries in HPT-deformed Ni.
Activation energies for crystallite growth of 0.99±0.11 and 0.96±0.06eV are derived by Kissinger analysis from dilatometry and DSC data, respectively.
In contrast to Ni, substantial length change proceeds in Cu at elevated temperatures beyond the regime of dominant crystallite growth.
In the light of recent findings from tracer diffusion and permeation experiments, this is associated with the shrinkage of nanovoids at high temperatures.

Introduction
Processing by severe plastic deformation (SPD) has been established as one of the most promising routes to produce bulk ultrafine or even nanocrystalline materials.
These materials made by SPD exhibit exceptional mechanical properties (see e.g.
Refs.
[1-4]).
A comprehensive understanding of these enhanced properties and of the process of grain refinement during severe plastic deformation is currently a major research topic in materials science.
In particular, the roles of the various types of defects produced during deformation are widely studied.
Furthermore, from a basic materials physics point of view, SPD metals offer the opportunity to study different types of deformation-induced defects and their mutual interaction.
It has already been shown that athermally produced excess vacancies are present in high concentrations that are otherwise found only close to the melting temperature [5,6].
A complex defect annealing kinetics is suggested from volume and grain boundary diffusion studies in SPD-processed Cu and Ni [7,8].
During annealing, abundant and highly mobile vacancies may, for example, agglomerate or form a percolating porosity network in combination with triple junctions of grain boundaries.
In addition, ultrafine-grained SPD materials give access to basic physical key parameters and processes such as thermally activated grain boundary relaxation prior to grain growth, as well as to the grain boundary excess volume, often also denoted as grain boundary expansion, of relaxed high-angle grain boundaries [9].
For this purpose, the direct experimental method of difference dilatometry is applied in the present work in combination with differential scanning calorimetry (DSC) and scanning electron microscopy (SEM).
The subjects of the present study are high-purity samples of the face-centered cubic (fcc) metals Cu and Ni.
Novel results obtained from difference dilatometry for copper are compared with results obtained by the other above-mentioned methods, as well as with results previously obtained for pure nickel.
Experimental
A Cu disk with a purity of 99.995wt.% was deformed by high-pressure torsion (HPT) at room temperature, with six revolutions being applied at 2.2GPa (for details see Refs.
[10,11]).
Samples were cut from this HPT-deformed disk (30mm in diameter and 7mm in height) at distances of at least 7.3mm from the center.
This corresponds to a von Mises equivalent strain of ε>23, ensuring a regime, where the deformation is in saturation, i.e. where further deformation will not lead to further grain refinement.
For the dilatometric measurements, a total of nine prism-shaped specimens with the dimension of 3×3×7mm3 were prepared.
The direction of the length change measurement is defined with regard to the HPT deformation axis (see Fig. 1, axial, tangential and radial directions).
Here, seven samples were prepared in the axial direction and one each in the tangential and radial directions.
Experiments were performed with a high-precision, vertical double-dilatometer (Linseis, L75VD500 LT), which allows the simultaneous measurement of two samples under an argon (5N) gas flow.
One of the two samples served as a reference and was made from the same Cu material, which was well annealed and coarse grained (grain size >100μm).
The experimental data, plotted as a dilatometric length change curve Δl/l0, represents the difference signal between the specimen and the reference (so-called difference dilatometry).
The length change is directly related to the defect volume via the relation 3×Δl/l0=ΔV/V0, assuming an isotropic distribution and annealing of defects [12].
The temperature of the maximum defect release rates can be determined from the minima of the derivative d(Δl/l0)/dT of the length change curve with respect to temperature.
For the case of a constant linear heating rate, dT/dt, the temperature, T, is directly proportional to the time, t.
Crystallite sizes were determined by a scanning electron microscope (LEO 1525 field emission scanning electron microscope, with a nominal resolution of 1.5nm at 20kV) equipped with a backscattering detector.
For correlating the dilatometric length changes with modifications of microstructure, microscopy samples were prepared from the same part of the HPT disk as the dilatometric samples and subsequently annealed under identical conditions in the dilatometer up to predefined temperatures at a heating rate of 5K/min, followed by rapid cooling to ambient temperature at a rate of about 30K/min.
The crystallite size was determined from the SEM images by analyzing the area of the grains using the software program ImageJ [14].
A spherical grain shape was assumed for the analysis, and the arithmetic mean value of the diameters was calculated.
The number of grains evaluated in each micrograph was about 380 except for the totally recrystallized sample annealed at 673K, for which about 130 grains were considered.
The error in the 1% range is due to uncertainties of assigning contrast features to grain boundaries.
Results
Correlation between microstructure and free volumes
Fig. 2 shows a typical dilatometric length contraction Δl/l0, indicating the annealing out of defects associated with the release of free volume upon linear heating of the HPT-deformed Cu sample.
Three substages, A, B and C, can clearly be discerned.
In order to correlate these substages with annealing of specific types of lattice defects, a microstructural characterization by means of scanning electron microscopy was performed at the onset and at the end of each substage, as indicated by arrows in Fig. 2.
In stage A, only a minor increase in the crystallite size from 209±4nm (as-deformed, Fig. 3a) to 222±4nm (end of stage A, Ta=413K,5K/min-1, Fig. 3b) is observed.
Substantial crystallite growth up to 764±4nm (Ta=468K, Fig. 3c) takes place in the subsequent distinct annealing stage B.
The broad annealing stage C is accompanied by further crystallite growth up to 12.09±0.04μm (Ta=673K, Fig. 3d).
In contrast to HPT-deformed Ni (see below), both the crystallite shape and the dilatometric length change are isotropic.
Quite similar dilatometric curves were obtained for the three measuring directions (axial, tangential and radial, Fig. 1).
From the measurements of nine dilatometric samples, a total length change Δl/ltotal=(7.02±1.92)×10-4 is deduced that comprises a fraction (Δl/l0)Stage A=(1.32±0.46)×10-4 for stage A, (Δl/l0)Stage B=(1.85±0.37)×10-4 for stage B and (Δl/l0)Stage C=(3.77±1.33)×10-4 for stage C.
The results and uncertainties represent the mean value and standard deviation of the nine dilatometric measurements.
Since, during annealing in stage A, the crystallite size and, therefore, the fraction of grain boundaries remain nearly constant, the length contraction in this stage has to be attributed to the annealing out of crystal lattice defects as well as to a relaxation of grain boundaries (see Section 4).
In the clearly defined subsequent stage B, the length change is predominantly caused by the removal of relaxed grain boundaries, as evidenced by the increase in the crystallite size by more than a factor of 3 in this narrow temperature regime.
This allows one to determine the grain boundary excess volume eGB from the relative length change (Δl/l0)Stage B in this stage according to(1)Δll0Stage B=eGB1dini-1dfinwhere dini=222nm and dfin=764nm denote the crystallite diameter at the beginning and at the end of stage B, respectively.
From the mean value (Δl/l0)Stage B=(1.85±0.37)×10-4 of nine samples, an apparent grain boundary excess volume eGB′ of (0.58±0.13)×10-10m can be derived.
However, for a more detailed analysis, it should be taken into account that the onset of the annealing processes of stage C already occurs during stage B.
Subtracting this contribution by means of extrapolation of stage C to lower temperatures into the stage B regime yields a reduced effective value (Δl/l0)Stage B, eff=(1.48±0.29)×10-4 associated with the removal of grain boundaries, resulting in a corrected value of the grain boundary excess volume of eGB=(0.46±0.11)×10-10m.
Kinetics and comparison with DSC
The shift of the dilatometric length change in stage B with heating rate (see the inset in Fig. 2) provides insight into the kinetics of crystallite growth.
Analyzing this shift for the nine dilatometric measuring runs according to Kissinger (for details of the method see Ref.
[15]) yields an activation energy Q of crystallite growth of 0.99±0.11eV (see the Kissinger plot, Fig. 4).
For the sake of comparison, a series of 14 DSC measurements with different heating rates were taken on samples prepared from the same HPT disk.
As shown in Fig. 5, the dilatometric stage B due to crystallite growth is accompanied by a pronounced heat release.
Kissinger analysis of the shift of this DSC peak with heating rate yields an activation energy Q of 0.96±0.06eV, which is in excellent agreement with the dilatometric measurements (see Fig. 5).
From the heat release upon linear heating through stage B, a mean value of the enthalpy of ΔH=-0.92±0.06Jg-1 for the exothermic process is deduced from the various measuring runs.
Attributing this enthalpy release exclusively to the removal of grain boundaries in stage B, a specific grain boundary energy(2)γ=Hρ3dini-1-dfin-1=0.85±0.08Jm-2is estimated using the initial and final crystallite diameters of stage B, as given above (dini=222nm, dfin=764nm), as well as the Cu bulk value of 8.92gcm-3 for the mass density ρ.
This value of γ is typical of relaxed grain boundaries in Cu (see Ref.
[16] and references therein).
The results for Cu of the present work are summarized in Table 1, together with data previously obtained for Ni [9,15] for comparison.
Discussion
For the discussion of the present dilatometric studies of free volumes in HPT-deformed Cu, first of all a comparison with recent results on HPT-deformed Ni [9,15,17] is instructive.
The Ni samples were HPT-deformed under identical conditions as in the present case for Cu, and the sample purity (99.99+wt.%) was also similar.
As shown in Fig. 6, the dilatometric change in HPT-Ni exhibits a qualitatively similar three-stage behavior.
In particular, a well-defined narrow stage B also occurs in HPT-Ni due the removal of grain boundaries in the wake of pronounced crystallite growth in this stage [9], similar to Cu.
However, a number of distinct differences between HPT-Ni and Cu should be noted:(i)
Stage B in Cu is shifted to lower temperatures by about 40K compared to Ni for similar heating rates.
(ii)
The crystallites of HPT-deformed Cu exhibit an isotropic shape, in contrast to HPT-Ni, where a pronounced elongation of the crystallites in the direction tangential to the HPT disk occurs, giving rise to a strong variation in the dilatometric length change with measuring direction, unlike in Cu.
(iii)
More than 50% of the total length change in HPT-Cu actually occurs in stage C, whereas for HPT-Ni the regime beyond stage B makes only a minor contribution.
(iv)
Stage A, on the other hand, is slightly more pronounced in HPT-Ni ((Δℓ/ℓ0)Stage A=1.57×10-4, mean value of 14 samples) than in Cu ((Δℓ/ℓ0)Stage A=(1.32±0.46)×10-4).
The shift of the crystal growth-associated stage B towards lower temperatures in Cu compared to Ni (item i; Fig. 6) reflects the different melting temperatures.
The same is the case for the activation energies of Q=0.99 and 1.20eV determined for Cu and Ni, respectively, from Kissinger analysis (Cu, present studies; Ni [15]) and from Johnson-Mehl-Avrami-Kolmogorov analysis (Ni [15]).
The value of Q=0.99eV for Cu is identical to that reported by Číček et al. [18] (1.0eV) for HPT-Cu.
Lower values were reported by Setman et al. [13] (0.48-0.78eV), Cao et al. [19] (0.8eV), and by Molodova et al. [20] (0.68eV), depending on the applied shear strain.
A remarkably higher value of 1.68eV, caused by impurities of oxygen and phosphorus segregating at grain boundaries, was observed by Amouyal et al. [21].
In addition to this issue of kinetics, one should point out the major novel aspect in characterizing SPD materials by means of dilatometry: namely, the access to the absolute concentration of free volumes, and in particular to the grain boundary excess volume eGB, the structural key parameter of grain boundaries.
For Cu, a value (0.46±0.10)×10-10m was deduced from the present measurements, which is slightly higher than recently reported results for Ni [9].
In the case of Ni with elongated crystallites (item ii), nearly identical values of eGB=0.35×10-10 and 0.32×10-10m were found for the dilatometric measuring directions perpendicular to and parallel to the crystallite elongation, respectively, which confirms the attribution of the length change in stage B to grain boundaries [9].
As any relaxation processes of the SPD-generated grain boundaries should be finished at the elevated temperatures of stage B, these values of the excess volume can be considered as characteristic values for grain boundaries of polycrystalline Cu and Ni in general.
It should be noted that the grain boundary excess volume eGB represents the GB expansion with respect to a perfect crystal lattice and should not be intermixed with the grain boundary width δ, which is usually in the range of 0.5nm, i.e. much larger than eGB.
Only a few experimental data are available in the literature for grain boundary expansion, primarily for isolated grain boundaries with a distinct orientation relation.
From high-resolution transmission electron microscopy, values for Au of eGB=(0.04-0.10)×10-10m (Ref.
[22]) or eGB=0.12×10-10m (Ref.
[23]) are reported.
From measurements of the grain boundary contact angle in an Al tricrystal, a value eGB=0.64×10-10m is reported by Shvindlerman et al. [24] applying a thermodynamic model.
For nanocrystalline Pd [25] and Fe [26], values of eGB=0.23×10-10m and 0.19×10-10m were determined from density measurements and modeling of grain growth kinetics, respectively.
A number of computer simulations of grain boundaries [27-30] deal with the issue of grain boundary expansion.
Here, however, the choice of the interatomic potentials was found to have a substantial influence on the numerical results [27].
Most recently, grain boundary expansion data have been reported from molecular dynamics simulations on Ni, eGB=(0.28-0.42)×10-10m for random high-angle grain boundaries [30] and eGB=(0.39-0.41)×10-10m (at T=1200K) for Σ5 grain boundaries [29].
Here, the matching of the data values with the data of the dilatometric studies of Cu and Ni is remarkable.
It is worthwhile mentioning a model proposed by Estrin et al. [31], according to which the annealing out of grain boundary excess volume gives rise to linear grain growth instead of a parabolic behavior.
From this point of view, it would be interesting to extend the dilatometry to isothermal measurements and to measure the crystallite size in stage B in more detail, in order to derive experimental information on the correlation between the grain growth kinetics and the free volume release.
The different dilatometric characteristics of Cu and Ni with respect to stage A and, particularly, stage C (items iii and iv) is considered to arise primarily from the different behaviors of lattice vacancies in both fcc metals.
Lattice vacancies generated in highly abundant concentrations by HPT deformation become mobile at about 360K in Ni [32], i.e. in the regime of stage A, whereas in Cu lattice vacancies are already mobile below ambient temperature [33].
Therefore, lattice vacancies in Cu may anneal out or form more stable vacancy agglomerates during deformation.
This may explain the reduced amplitude of stage A in Cu compared to Ni.
For a discussion of the substantial release of free volumes (Δl/l0)Stage C=(3.77±1.33)×10-4 in stage C, one must first note that the removal of remnant grain boundaries in this temperature range contributes to only a minor extent.
Taking into account the grain boundary excess free volume derived from stage B, a length change (Δl/l0)Stage C, GB=0.56×10-4 due to the removal of grain boundaries in stage C is derived from the mean crystallite sizes at the onset and the end of this stage, which corresponds to only 15% of the total length change (Δl/l0)Stage C.
Thus, the major part of stage C is considered to arise from the shrinkage of nanovoids at high temperatures.
Indeed, the shrinkage of nanovoids in Cu by self-diffusion in this temperature range is well documented from early studies of transmission electron microscopy of coarse-grained Cu in which nanovoids were generated by precipitation of quenched-in vacancies (see Bowden and Balluffi [34]).
Annealing of vacancy agglomerates in Cu at these elevated temperatures is also deduced from residual resistivity measurements after quenching [33].
Nanovoids in equal-channel angular pressing (ECAP)-prepared Ti and HPT-prepared Cu were detected by small-angle neutron scattering [35] and positron annihilation [18], respectively.
Most recently, evidence of percolating porosity in HPT-prepared Cu with a high volume fraction ΔV/V0 in the range of 2-3×10-3 was deduced from radiotracer diffusion and permeation experiments [36].
Assuming isotropic annealing, the dilatometric length change in stage C minus the contribution from removal of grain boundaries corresponds to a change volume ΔV/V0=3×Δℓ/ℓ0 in the range of 1×10-3, which is smaller than that from the radiotracer experiments by a factor of 2-3.
The different amounts of porosity estimated from the tracer technique and in the present dilatometric annealing experiments therefore indicate that only part of the porosity is annealed out up to the maximum annealing temperature of 673K - especially since the percolating porosity detected by the tracer method may be considered as the lower limit of the total porosity.
Indeed, the dilatometric annealing curve shows that the absolute value of the derivative d/dT(ΔV/V0)=3×Δl/l0 is still increasing at the maximum annealing temperature, i.e. the maximum reaction rate of stage C is not yet attained (Fig. 2).
Dilatometry experiments up to higher temperatures may clarify to what extent porosity can be further removed by annealing.
The much less pronounced length change in stage C for HPT-deformed Ni (cf.
Fig. 6) may indicate a substantially reduced amount of porosity or at least a higher thermal stability of such porosity for Ni compared to Cu.
Tracer diffusion and permeation data for ECAP-prepared metals reveal a higher receptivity for percolating porosity in the case of Cu [37] compared to Ni [8].
This may be related to the aforementioned different vacancy characteristics, which in the case of Cu may favor the formation of stable vacancy agglomerates during deformation.
In conclusion, the direct and specific method of high-precision dilatometry has proven to be a powerful tool for the study of free volume in bulk nanocrystalline metals.
In addition to issues of defect kinetics, the absolute value of free volumes, such as the grain boundary excess volume, can be measured directly.
Acknowledgement
Financial support by the Austrian Science Fund (FWF) by Grants P21009-N20, P25628-N20 and T512-N20 is appreciated.

Binding SnO 2 Nanocrystals in Nitrogen-Doped Graphene Sheets as Anode Materials for Lithium-Ion Batteries
Fabrication of graphite oxide: The graphite oxide was produced from natural graphite flakes (Alfa Aesar, 325 mesh) by a modified Hummers method.57
Synthesis of SnO2 nanocrystal aqueous suspension: 2.0 g SnCl4*5H2O (Sinopharm Chemical Reagent Co., Ltd.) was first dissolved in 100 mL deionized (DI) water. The resulting solution was then transferred to two 100 mL poly(tetrafluoroethylene) (Teflon)-lined stainless steel autoclaves and heated in an oven at 120 degC for 28 h to produce a white precipitate, which was harvested by centrifugation and dispersed in 35 mL DI water to achieve a SnO2 nanocrystal aqueous suspension with a concentration of 20 mg mL-1.
Synthesis of SnO2 nanocrystal/nitrogen-doped RGO hybrid (SnO2NC@N-RGO): Typically, 10 mL of graphite oxide aqueous suspension (10.0 mg mL-1) was dispersed in DI water (90 mL), followed by addition of the above SnO2 nanocrystal aqueous suspension (10 mL). The mixture was continuously sonicated to form a homogeneous suspension. After lyophilization (freeze-drying), 300 mg of the obtained gray powder (SnO2 nanocrystals/graphene oxide hybrid) was put into a small glass beaker, which was further placed in a glass bottle containing 6 mL of hydrazine monohydrate (Alfa Aesar, 99%), without direct contact between the powder and the hydrazine monohydrate. After sealing, the glass bottle was maintained at 120 degC for 2 h to reduce the graphene oxide in situ. After cooling, the product was washed with water six times to remove the adsorbed hydrazine monohydrate and byproducts, followed by drying at 120 degC under vacuum for 20 h to obtain the final hybrid SnO2NC@N-RGO.
Synthesis of SnO2 nanocrystal/RGO hybrid (SnO2NC@ RGO): SnO2NC@RGO was synthesized following the same procedure as for SnO2NC@N-RGO except that 300 mg of the gray powder was reduced by thermal reduction in a tube furnace at 500 degC for 2 h under Ar flow with a heating rate of 2 degC min-1 and no washing and subsequent drying were needed.
Synthesis of nitrogen-doped RGO (N-RGO): N-RGO was produced using the same procedure as for SnO2NC@N-RGO except that SnO2 nanocrystals were not added.
Synthesis of a physical mixture of SnO2 nanocrystals and N-RGO (SnO2NC + N-RGO mixture): 30 mg N-RGO was dispersed in 100 mL DI water, followed by addition of the above SnO2 nanocrystal aqueous suspension (3.5 mL). The mixture was sonicated for 1 h and subsequently freeze-dried to generate SnO2NC + N-RGO mixture.
Synthesis of SnO2 nanocrystal/N-RGO composite by means of hydrazine monohydrate aqueous solution reduction (SnO2NC/N-RGO-HS): SnO2NC/N-RGO-HS was prepared following the same procedure as for SnO2NC@N-RGO except that the gray powder was reduced by hydrazine aqueous solution. 150 mg of the gray powder was mixed with 200 mL DI water and 200 μL hydrazine monohydrate (Alfa Aesar, 99%) in a 500 mL flask. After the mixture had been vigorously stirred, the flask was heated in an oil bath (100 degC) for 1 h, followed by freeze-drying to obtain the product SnO2NC/N-RGO-HS.Synthesis and lithium ion conductivity of cubic deficient perovskites SrLi?TiTaO and the La-doped compounds

Samples were prepared by a conventional solid-state reaction. Starting materials were SrCO3 (3 N), Li2CO3 (3 N), La2O3 (4 N), TiO2 (3 N), and Ta2O5 (3 N). The metal content in the La2O3 reagent was determined by quantitative chelatometry analysis using EDTA (etylenediaminetetraacetic acid). For Li2CO3, a 5-30% stoichiometric excess amount was used due to the problem of vaporization. Starting materials were mixed in an agate mortar with ethanol. The mixed powder was calcined at 1373 K for 12 h in air with intermediate grindings. The calcined powder was pressed into pellets 7 mm in diameter and 2 mm in thickness under a uniaxial pressure of 130 MPa, then fired at 1523-1623 K for 10 h in air. The identification of phases and the determination of lattice constants were carried out by a powder X-ray diffraction method with CuKa radiation using a Rigaku X-ray diffractometer (RINT 2100) equipped with a graphite monochrometer. In the determination of the lattice parameters, Si powder (5 N) was used as an internal standard.

Synthesis and characterization of anisotropically expanded graphite oxide compounds derived from spherical graphite

Graphite, mesocarbon microbeads (MCMBs, average particle size: 15 μm), was purchased from Hosen Co., Japan. Potassium permanganate, sodium nitrate, concentrated sulfuric acid and hydrogen peroxide were from Wako Corporation. Quaternary ammonium salts and tetraalkyl ammonium hydroxide solutions were purchased from Wako Corporation. The intercalants were displayed in Table 1.

GO was synthesized from MCMB according to the traditional Hummers method [31]. Typically, MCMB (10 g) and NaNO3 (5 g) together with H2SO4 (220 ml) were mixed in a flask placed into the water ice bath. To avoid vigorous reaction, KMnO4 (20 g) was gradually added into the mixture at 0-10 degC. After the addition of KMnO4, the mixture was kept at 35 degC for 1 h. Subsequently, the mixture was transferred to 460 ml H2O and hold strictly at 98 degC for 30 min. The reaction was terminated by adding 1400 ml H2O and 10 ml H2O2. Then the diluted solution was kept for 24 h followed by filtering and drying at 80 degC for 12 h.

New insights into how Pd nanoparticles influence the photocatalytic oxidation and reduction ability of g-C3N4 nanosheets
All chemicals used in this study were of analytical grade (Sigma-Aldrich). Typically, 16 g of urea was placed into a 50 mL alumina crucible with a cover and was heated in air at a rate of 15 degC min-1 to reach 550 degC. After being heated at 550 degC for 2 h, the obtained g-C3N4 was well ground in an agate mortar and was denoted as C3N4 in various characteristic figures below. To prepare C3N4-Pd composites, 1.0 g of the as-prepared g-C3N4 was added into 100 mL of ethanol and was sonicated for 2 h to obtain thin g-C3N4 nanosheets. The resultant was dried at 60 degC. Then, 1 g of the dried thin g-C3N4 nanosheets was added into 100 mL of H2O and was kept stirring for 30 min. Then, an appropriate amount of Pd(NO3)2 dissolved in 30 mL of H2O was added dropwise into the above suspension. Next, a certain amount of NaBH4 (according to the molar ratio of n(Pd(NO3)2):n(NaBH4) = 1:5) dissolved in 30 mL of H2O was added into the above suspension. After stirring for 10 min, the resulted suspension was aged for 2 h. Finally, the resulted grey products were collected by filtration, washed with water and ethanol four times and dried at 60 degC to obtain the final products. Depending on the ratio of n(Pd):n(C3N4) = 1%, 3%, 5% and 10%, the products were labelled as C3N4-Pd-1%, C3N4-Pd-3%, C3N4-Pd-5%, and C3N4-Pd-10%, respectively.1. A transition metal precursor for preparation of a lithium transition metal oxide, in which a ratio of tap density to average particle diameter D50 of the precursor satisfies a condition represented by Equation 1 below:
<maths id="math0003" num="(1)."><math display="block"><mrow><mn>0</mn><mo><</mo><mfrac><mrow><mi>Tap density</mi></mrow><mrow><mi>Average particle diameter D</mi><mo>⁢</mo><mn>50</mn><mspace width="1em"/><mi>of transition metal precursor</mi></mrow></mfrac><mo><</mo><mn>3500</mn><mfenced separators=""><mi mathvariant="normal">g</mi><mo>/</mo><mi>cc</mi><mo>⋅</mo><mi>cm</mi></mfenced></mrow></math><image height="10mm" id="ib0003" img-content="math" key="EP2902364" src="imgb0003.tif" width="131mm"/></maths>6. The transition metal precursor according to claim 5, wherein the transition metal hydroxide particles are a compound represented by Formula 2 below:
M(OH1-x)2(2)   wherein M is at least two selected from Ni, Co, Mn, Al, Cu, Fe, Mg, B, Cr, and period 2 transition metals; and 0≤x≤0.5.8. The transition metal precursor according to claim 1, wherein the transition metal precursor has an average particle diameter D50 of 1 to 30 µm.9. A lithium transition metal oxide in which a ratio of average particle diameter D50 of the lithium transition metal oxide to average particle diameter D50 of a transition metal precursor for preparation of the lithium transition metal oxide satisfies the condition represented by Equation 3 below:
<maths id="math0004" num="(3)."><math display="block"><mrow><mn>0</mn><mo><</mo><mfrac><mrow><mi>Average particle diameter D</mi><mo>⁢</mo><mn>50</mn><mspace width="1em"/><mi>of lithium transition metal oxide</mi></mrow><mrow><mi>Average particle diameter D</mi><mo>⁢</mo><mn>50</mn><mspace width="1em"/><mi>of transition metal precursor</mi></mrow></mfrac><mo><</mo><mn>1.2</mn></mrow></math><image height="10mm" id="ib0004" img-content="math" key="EP2902364" src="imgb0004.tif" width="120mm"/></maths>11. The lithium transition metal oxide according to claim 10, wherein the lithium transition metal oxide is a compound represented by Formula 4 below:
LiaNixMnyCozMwO2-tAt(4)
wherein 0<a≤1.2, 0≤x≤0.9, 0≤y≤0.9, 0≤z≤0.9, 0≤w≤0.3, 2≤a+x+y+z+w≤2.3, and 0≤t<0.2;
M is at least one metal cation selected from the group consisting of Al, Cu, Fe, Mg, B, Cr, and period 2 transition metals; and
A is at least one monovalent or divalent anion.14. A lithium secondary battery in which a unit cell comprising a positive electrode comprising the lithium transition metal oxide according to any one of claims 9 to 13, a negative electrode, and a polymer membrane disposed between the positive electrode and the negative electrode is accommodated in a battery case.Defect chemistry and enhancement of thermoelectric performance in Ag-doped Sn1+δ-xAgxTe

The crystalline ingots of Sn1-xAgxTe and Sn1+δ-xAgxTe (x = 0.01, 0.03, 0.05 and 0.07) were synthesized by a melting and slow cooling process. The high purity elements of Ag (99.999%), Sn (99.999%), and Te (99.999%) were placed in quartz tubes according to the stoichiometric molar ratios. The quartz tubes were evacuated, sealed, and melted at 1000 degC for 12 h. After cooling down to 150 degC with a cooling rate of 10 degC h-1, they were kept at 150 degC for 12 h followed by furnace cooling. The ingots were ground in an agate mortar and the powders were sintered by hot press sintering at 400 degC with a uniaxial pressure of 70 MPa for 1 hour.
1. Li-Ni composite oxide particles having a composition of Li x Ni1-y-a-b Co y M1a M2b O2 wherein x, y, a and b represent 1.00 <= x <= 1.10; 0 < y <= 0.25; 0 < a <= 0.25; and 0 <= b <= 0.10, respectively; M1 is at least one element selected from the group consisting of Al and Mn; and M2 is at least one element selected from the group consisting of Zr and Mg, a product of a metal occupancy (%) of lithium sites of the Li- Ni composite oxide as determined by Rietveld analysis of X- ray diffraction thereof and a crystallite size (nm) of the Li-Ni composite oxide as determined by the Rietveld analysis being not less than 700 and not more than 1400.Graphene-based nitrogen-doped carbon sandwich nanosheets: a new capacitive process controlled anode material for high-performance sodium-ion batteries
Graphene oxide (GO) was synthesized from graphite via a modified Hummers method32,33 and was dispersed into water by ultrasonication (1 mg mL-1, ~300 min). 25 mL of the above GO suspension was further dispersed into 175 mL of H2O by ultrasonication for 5 min. Then, the solution was put into a refrigerator and cooled down to 0-5 degC, which was followed by the addition of pyrrole (Sigma-Aldrich 1 mL). After vigorous stirring for 5 min, the oxidizer ((NH4)2S2O8 1.2 g/10 mL) was added to initiate the polymerization reaction of pyrrole which continued for 120 min under stirring. After that, the suspension was concentrated to approximately 30 mL by filtration, which was further dried by vacuum freeze-drying. Then, the as-prepared product (Go-Ppy) was pyrolysed at 500 degC, 700 degC, and 900 degC for 1 h under an Ar atmosphere, yielding the final products (designated as G-NCs-5, G-NCs, and G-NCs-9). Graphene (G) and nitrogen-doped carbon (NC) were prepared via the same process only without the addition of pyrrole and graphene, respectively.1. A precursor for preparing a lithium transition metal oxide for use in preparing a lithium transition metal oxide as a cathode active material in a lithium secondary battery by reacting with a lithium-containing compound, wherein the precursor contains two or more transition metals, and the precursor further contains sulfate ions derived from a transition metal sulfate used for preparing the precursor in an amount of 0.1 to 0.7% by weight based on the total weight of the precursor, the precursor being prepared by a co-precipitation method,
wherein the composition of the transition metal is NixCoyMn1-(x+y)Wherein x is more than or equal to 0.3 and less than or equal to 0.9, y is more than or equal to 0.1 and less than or equal to 0.6, and x + y is less than or equal to 1.2. The precursor of claim 1, wherein 0.1 mole or less of the transition metal is replaced with one or more elements selected from the group consisting of Al, Mg, Cr, Ti and Si.3. The precursor of claim 1, wherein the sulfate ion is present in an amount of 0.2 to 0.6 wt%, based on the total weight of the precursor.4. A lithium transition metal oxide prepared by sintering the precursor for preparing a lithium transition metal oxide according to claim 1 and a lithium-containing material.5. The oxide of claim 4, wherein the lithium-containing material is lithium carbonate and/or lithium hydroxide.6. A cathode active material for a lithium secondary battery comprising the lithium transition metal oxide of claim 4.7. A lithium secondary battery comprising the cathode active material according to claim 6.1. Li—Ni composite oxide particles having a composition of Li<sub>x</sub>Ni<sub>1-y-a-b</sub>Co<sub>y</sub>M1<sub>a</sub>M2<sub>b</sub>O<sub>2 </sub>wherein x, y, a and b represent 1.00≦x≦1.10; 0<y≦0.25; 0<a≦0.25; and 0≦b≦0.10, respectively; M1 is at least one element selected from the group consisting of Al and Mn; and M2 is at least one element selected from the group consisting of Zr and Mg, a product of a metal occupancy (%) of lithium sites of the Li—Ni composite oxide as determined by Rietveld analysis of X-ray diffraction thereof and a crystallite size (nm) of the Li—Ni composite oxide as determined by the Rietveld analysis being not less than 700 and not more than 1400.2. The Li—Ni composite oxide particles according to claim 1, wherein the metal occupancy of lithium sites of the Li—Ni composite oxide as determined by the Rietveld analysis is not less than 2% and not more than 7%.3. The Li—Ni composite oxide particles according to claim 1, wherein the crystallite size of the Li—Ni composite oxide as determined by the Rietveld analysis is not more than 500 nm.4. The Li—Ni composite oxide particles according to claim 1, wherein the Li—Ni composite oxide particles have an average particle diameter of 1 to 20 μm and a BET specific surface area of 0.1 to 1.6 m2/g.5. A process for producing the Li—Ni composite oxide particles as claimed in claim 1, comprising the steps of mixing lithium compound particles and Ni—Co hydroxide particles, and calcining the resulting mixture, the Ni—Co hydroxide particles being produced by mixing an aqueous solution of a sulfuric acid salt of each metal element, an ammonia aqueous solution and a sodium hydroxide aqueous solution such that an ammonia concentration in a reaction vessel is controlled to not more than 1.4 mol/L, and a ratio of the ammonia concentration in the reaction vessel to a surplus hydroxyl group concentration in the reaction vessel is controlled to not less than 6.6. A process for producing the Li—Ni composite oxide particles as claimed in claim 1, comprising the steps of mixing lithium compound particles, Ni—Co hydroxide particles, and aluminum compound particles and/or zirconium compound particles, and calcining the resulting mixture, the Ni—Co hydroxide particles being produced by mixing an aqueous solution of a sulfuric acid salt of each metal element, an ammonia aqueous solution and a sodium hydroxide aqueous solution such that an ammonia concentration in a reaction vessel is controlled to not more than 1.4 mol/L, and a ratio of the ammonia concentration in the reaction vessel to a surplus hydroxyl group concentration in the reaction vessel is controlled to not less than 6.7. A non-aqueous electrolyte secondary battery using a positive electrode comprising a positive electrode active substance comprising the Li—Ni composite oxide particles as claimed in claim 1.1. Represented by the formula of the mixed metal oxide hydroxide precursor material:
(1-z) (NiaCobMnc(OH)2(a+b+c)·Cob'Mnc'(OOH)b'+c'·Mnc"O2c")·
Z (NidCoeMnf(OH)2(d+e+f)·Coe'Mnf'(OOH)e'+f'·Mnf"O2f").2. Precursor material according to claim 1, wherein
0 z 0.1;
A=a, B=b + b ', C=c + c' + c ", A + B + C=1 and is 0 A 1, 0 B 1, 0 C 1;
D=d, E=e + e ', F=f + f' + f", and the D + E + F=1 0 D 1, 0 E 1, 0 F 1;
And to
A D, B E, C F.3. Precursor material according to claim 1, wherein the precursor material comprises a spherical particles and non-spherical particles having a surface and interior.4. Precursor material according to claim 3, wherein the particles have a gradient structure, wherein Co and the Mn as compared with, a majority of the molar ratio of Ni at the surface, and the particles from the surface of the particles having a metal mole ratio changes toward the inside.5. Claim 4 of the precursor material. Wherein the particle has a surface with about 8: 1:1 of the Ni: Co: Mn ratio.6. Precursor material according to claim 3, wherein the particles are doped with at least one element selected from Mg, Al, Zr, Ti, Ni, Co and the Mn metal ions.7. Precursor material according to claim 1, wherein the precursor material has an average particle diameter (D50) in the 3-30 microns.8. Precursor material according to claim 7, wherein the precursor material has an average particle diameter (D50) at 7-13 microns.9. Precursor material according to claim 1, wherein the precursor material has a tap density is 0.8-2.8 g/cm3range.10. Precursor material according to claim 9, wherein the precursor material has a tap density of 1.8-2.3 g/cm3range.11. Precursor material according to claim 1, wherein the precursor material in the surface area of 2-20m2/g range.12. Precursor material according to claim 11, wherein the precursor material in the surface area of 2-8m2/g range.13. Precursor material according to claim 1, wherein the precursor materials within the sodium level is less than 500 ppm.14. Precursor material according to claim 13, wherein the precursor materials within the sodium level is less than 300 ppm.15. Claim 1 of the precursor material prepared by lithiation of the cathode active material for lithium ion batteries.16. Preparing a mixed metal oxide hydroxide precursor material of the method, comprising the steps of:
Co-precipitated with an alkaline hydroxide solution and ammonia to form a precipitate comprising a plurality of metal salt solution, wherein the metal salt of a metal selected from nickel, cobalt, manganese and combinations thereof;
The precipitate was filtered;
The precipitate was washed; and
Drying the precipitate to form a mixed metal oxide hydroxide precursor material, wherein the precursor material represented by the formula:
(1-z) (NiaCobMnc(OH)2(a+b+c)·Cob'Mnc'(OOH)b'+c'·Mnc"O2c")·
Z (NidCoeMnf(OH)2(d+e+f)·Coe'Mnf'(OOH)e'+f'·Mnf"O2f").17. Method according to claim 16, wherein
0 z 0.1;
A=a, B=b + b ', C=c + c' + c ", A + B + C=1 and is 0 A 1, 0 B 1, 0 C 1;
D=d, E=e + e ', F=f + f' + f", and the D + E + F=1 0 D 1, 0 E 1, 0 F 1;
And to
A D, B E, C F.18. Method according to claim 16, wherein in the 1st order through 2nd order to perform the method precipitation reactor and reactor.19. Method according to claim 18, wherein at least 90% in the reactor to bring the 1st, but less than 100% precipitation of the metals.20. Method according to claim 18, wherein the at least one selected from the group 1st order of adding Mg, Al, Ti/Zr and a 2nd order or in a metal ion and at least one element selected from Mg added to the reactor, Al, Zr, Ti, Ni, Co and the Mn to change the composition of the metal ions precipitate.21. Method according to claim 16, wherein the alkaline hydroxide pH of the solution was maintained at about 11-13 within a range.22. Method according to claim 16, wherein the precursor material comprises a spherical shaped particles and spherical particles having a surface and an interior.23. Method according to claim 22, wherein the particles have a gradient structure, wherein the Co and the Mn pared, a majority of the molar ratio of Ni at the surface, and the particles from the particle surface of a metal having a molar composition ratio changes toward the inside.24. Method according to claim 23, wherein the particle has a surface with about 8: 1:1 of the Ni: Co: Mn ratio.25. Method according to claim 22, wherein the ammonia is stable particle growth of the complexing agent.26. Method according to claim 16, wherein the solution of ammonia: metal molar ratio of between about 0.1-3.0 range.27. Method according to claim 26, wherein the solution of ammonia: metal molar ratio of between about 0.5-1.5 range.28. Method according to claim 16, wherein in the reactor at about 50-70 °C performed at a temperature in the range of co-precipitation.29. Method according to claim 16, wherein the precursor material has an average particle diameter (D50) in the 3-30 microns.30. Method according to claim 27, wherein the precursor material has an average particle diameter (D50) at 7-13 microns.31. Method according to claim 16, wherein the precursor material has a tap density is 0.8-2.8 g/cm3range.32. Method according to claim 31, wherein the precursor material has a tap density of 1.8-2.3 g/cm3range.33. Method according to claim 16, wherein the precursor material in the surface area of 2-20m2/g range.34. Method according to claim 33, wherein the precursor material in the surface area of 2-8m2/g range.35. Method according to claim 16, wherein the precursor materials within the sodium level is less than 500 ppm.36. Method according to claim 35, wherein the precursor materials within the sodium level is less than 300 ppm.37. With precursor material for lithium ion batteries prepared by lithiation of the cathode active material, the precursor material is produced by a process according to claim 16.Surfactant-templated sol-gel silica thin films bearing 5-mercapto-1-methyl-tetrazole on carbon electrode for Hg(II) detection
Tetraethoxysilane (TEOS, 98%) and cetyltrimethylammonium bromide (CTAB, 98%) were purchased from Fluka. The MTTZ-siloxy derivative (triethoxysilyl-propyl-5-mercapto-1-methyl-tetrazole) was synthesized as previously described [38] and its structure (Scheme 1) confirmed by 13C and 1H NMR. Ethanol (95-96%, Merck) and HCl (37%, Aldrich) were used for precursor hydrolysis and film deposition, as well as for extraction of the surfactant template. Ru(NH3)6Cl3 (Aldrich), K3Fe(CN)6 (Fluka) and Fc(MeOH)2 (Alfa Aesar) have been used as redox probes, usually in solutions containing potassium hydrogen phthalate (KHP, Merck) as supporting electrolyte. Analytical grade Hg(NO3)2 (Prolabo) and HNO3 (65%, Merck) were used for the preparation of diluted mercury solutions; their concentration was checked using a certified standard solution (Hg2+ 1.001 +- 0.002 g L-1 in HNO3 3 M, Merck). All solutions were prepared with high-purity water (18 MΩ cm) from a millipore milli-Q water purification system.

MTTZ-modified silica films were deposited by spin-coating on freshly polished surfaces of glassy carbon electrode (GCE, purchased from EG&G, Princeton Applied Research), according to a procedure adapted from Ref. [43]. The sol solutions were typically prepared by heating a mixture of 2.08 g TEOS (including selected equivalent fractions of MTTZ derivative, from 0 to 20% relative to TEOS), 5.5 g EtOH, 0.5 g water and 0.4 g of a 0.1 M HCl solution, at 70 degC for 1 h. 0.78 g of CTAB dissolved in 10 g of EtOH was then added to this mixture, which was stirred again for 1 h at room temperature. GCE surfaces have been carefully polished before deposition of the film, using successively alumina suspensions made of Al2O3 powders (Buehler) of 1, 0.3 and 0.05 μm particle sizes; they were then washed with ethanol and water. Hundred microliters of sol (prepared as above) was deposited on the electrode surface, and left 30 s before the electrode was rotated at 7000 rpm for 30 s (optimized values). This was performed using a disk electrode rotator (model 636, EG&G, Princeton Applied Research). The spin-coated films were then treated for 20 min at 60 degC. The extraction of the surfactant was performed in an ethanol solution containing 0.1 M HCl under stirring conditions for 15 min. The film electrodes were used in the electrochemical experiments without further treatment.Real-time oxide evolution of copper protected by graphene and boron nitride barriers

The graphene samples were grown using a commercial Annealsys AS-ONE cold-wall chemical vapour deposition (CVD) reactor on 25 μm-thick electropolished copper foils. First, the Cu foils were annealed in argon at 1035 degC in atmospheric pressure for 10 minutes, then graphene was synthesised at the same temperature at a pressure of 25 mbar using a mixture of 900sccm Ar, 60sccm H2 and 2sccm CH4 for 15 minutes. hBN coatings were grown using a quartz tube furnace on the same batch of electropolished copper foils used for growing graphene. The growth was done at 900 degC at a total pressure of 60 mbar for 15 minutes with a mixture of 300sccm Ar, 15sccm H2 and 3sccm of borazine (B3H6N3, from Fluorochem).

In situ spectroscopic investigations of MoOx/Fe2O3 catalysts for the selective oxidation of methanol
MoOx/Fe2O3 was prepared using the incipient wetness technique. Commercial Fe2O3 (Sigma Aldrich, <50 nm) was first calcined at 500 degC for 3 hours, before adding the desired amount of aqueous ammonium heptamolybdate for 1, 3 or 6 monolayers (MLS) MoOx/Fe2O3. The samples were dried at 120 degC for 24 hours, before being calcined at 500 degC for 24 hours in a muffle furnace. For subsequent reduction studies, the calcined materials were reduced in a tube furnace under a methanol/He flow at 350 degC for three hours. Bulk phase samples were synthesised to be used as reference standards in the analysis. Stoichiometric Fe(NO3)3*9H2O (Fe(III), Sigma Aldrich >98%) was added drop wise to a stirring solution of (NH4)6Mo7O24*4H2O (Fluka Analytical, >99%) previously acidified to pH 2 with dilute HNO3. The resulting mixture was heated to 90 degC for ~1 hour until a yellow sludge remained. This was left to air dry overnight, after which it was dried at 120 degC for 24 hours, followed by calcination at 500 degC for 48 hours.1. A method for making a lithium transition metal oxide precursor, which is used for a lithium-containing compound may be prepared by neutralization with a lithium secondary battery of the lithium transition metal oxide as cathode active material, wherein the precursor contains two or more transition metal, and the precursor further containing a transition metal salt for the preparation of the precursor from, containing sulfate ion (SO4) ions of the salt, based on the total weight of the ion content of 0.1 to 0.7 by weight of the precursor %.2. Precursor according to claim 1, wherein the transition metal is two or more kinds selected from Group VB to VIIIB of the Periodic Table of the Elements.3. Precursor according to claim 1, wherein the transition metal is two or more kinds selected from Ni, Co and the Mn of the element.4. Precursor according to claim 1, wherein the transition metal has a composition of NixCoyMn1-(x+y), wherein 0.3 ≤ x ≤ 0.9, 0.1 ≤ y ≤ 0.6, and the x + y ≤ 1.5. Precursor according to claim 4, wherein the transition metal is 0.1 mol or less of one or more selected from Al, Mg, Cr, Si Ti and the substituted with elements of.6. Precursor according to claim 1, wherein the transition metal salt is a sulfate.7. Precursor according to claim 6, wherein one or more of the sulfate salt is selected from the group consisting of nickel sulfate, cobalt sulfate and manganese sulfate.8. Precursor according to claim 1, wherein the salt ions include nitrate ion (NO3).9. Precursor according to claim 1, wherein the salt ion content of 0.2 to 0.6 by weight based on the total weight of the precursor %.10. A method for making a lithium transition metal oxide precursor, wherein the precursor acid is added to the precursor dissolved followed by chromatographic assay, based on the total weight of 0.1 to 0.7 detected % by weight of the precursor having a sulfate ion (SO4).11. A lithium transition metal oxide, by for making a lithium transition metal oxide claim 1 of the precursor and a lithium-containing material prepared by sintering.12. Oxide according to claim 11, wherein the lithium-containing material is lithium carbonate (Li2CO3)/ or lithium hydroxide and the (LiOH).13. For a lithium secondary battery cathode active materials, comprising lithium transition metal oxide claim 11.14. A lithium secondary battery, comprising a cathode active material claim 13.Synthesis of C2+ hydrocarbons by CO2 hydrogenation over the composite catalyst of Cu-Zn-Al oxide and HB zeolite using two-stage reactor system under low pressure
Among a number of Cu-Zn based methanol synthesis catalysts, we selected a Cu-Zn-Al oxide catalyst with an atomic composition of 6:3:1 in this study, which is analogous to the catalyst actually employed in commercial industrial processes for methanol production [10] and [35]. This Cu-Zn-Al oxide catalyst was prepared by a co-precipitation method using an aqueous solution of the corresponding metal nitrates and Na2CO3. The mixed 1 M solution of Cu, Zn and Al nitrates (6:3:1 in molar ratio) was added at once into the 1 M solution of Na2CO3 at 80 degC. The amount of Na2CO3 was 1.2 equivalents to the total stoichiometric quantities of the nitrates. The precipitate was aged for 2 h at the same temperature, then filtered and washed 5 times with fresh distilled H2O. The gel was dried at 120 degC for 12 h and finally calcined at 500 degC for 4 h or at 350 degC for 3 h in air. The resulting powder, Cu-Zn-Al (6:3:1) oxide, was physically mixed with HB zeolite [Tosoh corporation; HSZ-931HOA (SiO2/Al2O3 = 28.5) or HSZ-940HOA (SiO2/Al2O3 = 41.6)] by grinding about 1000 times in various weight ratios to give the composite catalysts. The Cu-Zn (3:7) oxide catalyst and its composite catalyst were prepared by the procedures similar to those described above.Controlled Synthesis and High Oxidation Stability of Cobalt Nanoparticles Encapsulated in Mesoporous Silica using a Modified Stober Approach and a Pseudomorphic Transformation
Materials: Cobalt(II) nitrate hexahydrate (VWR Prolabo, 98 %), sodium borohydride (Sigma-Aldrich, 98 %), citric acid monohydrate (Sigma-Aldrich, 99.5 %), 3-aminopropyltriethoxysilane (APTES, Sigma-Aldrich, 99 %), tetraethylorthosilicate (TEOS, VWR Prolabo, 99.9 %), hexadecyltrimethylammonium bromide (CTAB, C19H42BrN, Sigma, >=98 %), poly(vinylpyrrolidone) PVP (Sigma-Aldrich, average mol. wt 40000), NaOH (Sigma-Aldrich, >=98 %), and EtOH (VWR, absolute 99.9 %) were used as received. Deionized water (18 MΩ cm) was used for all the preparations after deoxygenation.
Synthesis of the Co NPs: Citrate-stabilized Co NPs were prepared from classical NaBH4 reduction.56 In a typical synthesis, cobalt(II) nitrate hexahydrate (115.5 mg, 0.397 mmol) and citric acid monohydrate (83-167 mg, 0.397-0.793 mmol, 1-2 equiv.) were added to deoxygenated water (175 mL) under mechanical stirring. After 1 h under nitrogen stripping, NaBH4 (150 mg, 3.97 mmol, 10 equiv.) previously solubilized in cold water (5 mL) was quickly added to the vigorously stirred solution. The mixture turned black, which is indicative of reduced cobalt NP formation. In certain cases, PVP (600 mg) was added immediately after the introduction of NaBH4 to form PVP-stabilized Co NPs. Complete dissolution of the polymer occurred rapidly (<1 min).
Encapsulation of the Co NPs in Silica, Co@SiO2: Co@SiO2 were prepared by controlled growth of the silica shell on the Co NPs in an ethanol/water mixture according to a modified Stober method.16 After the in situ synthesis of the Co NPs, an ethanol solution (390-780 mL) containing APTES (77 μL, 0.331 mmol, 0.8 equiv.) and TEOS (881 μL, 3.97 mmol, 10 equiv.) was added to the NP solution. This one-pot reaction was allowed to proceed for 20 h before centrifuging and drying to isolate the silica-coated particles.
Synthesis of Cobalt-Mesoporous Silica NPs (Co@m-SiO2): Co@SiO2 were used as a silica source for the synthesis of Co@m-SiO2 by a PMT.40 The appropriate quantity of Co@SiO2 was dispersed into an alkaline solution containing CTAB. The molar composition of the system was 1:0.11:0.24:395:36 SiO2/CTAB/NaOH/H2O/EtOH. After 1 h of stirring at room temperature, the gel was put in an autoclave and heated at 100 degC for 24 h under static conditions. The final solid was recovered by centrifugation, washed with water, and dried at 80 degC for 12 h. The CTAB rod-like template was removed by thermal treatment under hydrogen in a programmed heated tubular oven before use. The temperature was initially increased from room temperature to 400 degC at a rate of 5 degC min-1, and the temperature kept for 4 h to remove the organic phase.
Synthesis of Co/SiO2: The cobalt-based reference catalyst Co/SiO2 was prepared by incipient wetness impregnation over a silica-based carrier (SBET = 241 m2 g-1, Vp = 0.41 mL g-1, and Dp = 5.7 nm) starting from an aqueous solution of cobalt nitrate hexahydrate. Subsequently, the impregnated catalyst was dried by direct heating of the sample from room temperature to 85 degC for one night. Finally, the catalyst was calcined at 420 degC in dry air for 4 h and activated by a reduction treatment under pure hydrogen. To achieve a Co loading of 12.7 wt.-%, two successive incipient wetness impregnations were realized (see the Supporting Information, Table S1 and Figures S1 and S2, for characterization).A new insight into the LiTiOPO4 as an anode material for lithium ion batteries

LiTiOPO4 was synthesized via a solution route. A stoichiometric aqueous mixture of Li2CO3 and (NH4)2HPO4 was prepared at the room temperature, while corresponded stoichiometric amount of TiCl4 was diluted in ethanol. The TiCl4 solution was added dropwisely into the aqueous mixture with stirring at room temperature, which induced a gelatinous precipitate. The resulting solution was stirred overnight and then evaporated the solvent at 80 degC under vacuum. The dried white precipitate was calcinated in air at 400 degC for 5 h and then at 850 degC for 12 h with a heating rate of 1 degC min-1.

A measured amount (0.4 g) of PVDF was dissolved into 10 mL of N-methylpyrrolidone (NMP) and then 2.0 g of LiTiOPO4 was dispersed into the formed solution with stirring for 2 h. The mixture was dried to remove solvent at 80 degC under vacuum and subsequently calcinated in Ar atmosphere at 650 degC for 3 h with a heating rate of 1 degC min-1.
3. A heterogeneous metal bearing material according to claim 2, wherein said heterogeneous metal bearing material has the general formula (dopant material)a(host material)b, where a and b are weight fractions, with 0 < a < 0.4, preferably 0.001 < a < 0.4, and more preferably 0.001 < a < 0.02, and where b = 1 - a.4. A heterogeneous metal bearing material according to any one of claims 1 to 3, wherein said dopant material is either one or more of MgO, Cr2O3, ZrO2, A12O3, and TiO2, and is in the form of nanoparticles.5. A heterogeneous metal bearing material according to claim 3, wherein the dopant material is TiO2 and the host material is either one or a mixture of Ni x Mn y Co2 hydroxide, oxyhydroxide, and oxide, where x, y, z are atomic fractions, with 0 <= x <= 1, 0 <= y <= 1, 0 <= z <= 1, and x+y+z= 1.8. A heterogeneous metal bearing material according to any one of claims 1 to 7, wherein said dopant material has a size range of >= 5 nm and <= 200 nm, and preferably between 10 and 50 nm.12. Process according to any one of claims 9 to 11, wherein said dopant material is either one or more of MgO, Cr2O3, ZrO2, A1 2O3, and TiO2, and has a size range of > 5 nm and >= 200 nm.14. Use according to claim 13, wherein the dopant material is either one of MgO, Cr2O3, ZrO2, A1 2O3, and TiO2, and the cathode material is a lithium transition metal oxide.15. Use according to claim 14, wherein the dopant material is A1203, and the cathode material is LiNiO2.Influence of non-stoichiometry on solid-state reactive sintering of YAG transparent ceramics

Y3(1+x)Al5O12 ceramics with x changed from -4.3% to 4.7% were fabricated by a solid-state reaction method from commercial α-Al2O3 (99.98%, Alfa Aesar, Tianjin, China) and Y2O3 (99.999%, Alfa Aesar, Tianjin, China) powders. The powders were mixed in ethanol and ball-milled with high-purity corundum balls in Al2O3 jars for 10 h with 0.08 wt% MgO powder (99.998%, Alfa Aesar) and 0.8 wt% tetraethoxysilane (TEOS, >99.999%, Alfa Aesar) as sintering aids. The slurry with the solid loading of 1.8 g/ml was dried at 80 degC for 4 h in an oven and then sieved through a 200-mesh screen with the screen pore size of ~74 μm, the powder mixture was calcined at 600 degC for 4 h to remove the organic components. After that the powder mixture was uniaxially pressed into 20 mm diameter pellets and then the green bodies were compacted by cold isostatic pressing (CIP) at 250 MPa.

The compacted pellets were then sintered at 1500-1750 degC under vacuum (10-3 Pa) for up to 50 h in a tungsten mesh-heated vacuum furnace. The heating rate was 5 degC/min, and the cooling rate was 10 degC/min. After sintering, the specimens were annealed at 1450 degC for 10 h in air to remove oxygen vacancies. The samples were mirror polished on both surfaces with different grade of the diamond slurries. The polished specimens were thermally etched at 1400 degC or 1500 degC for 3 h to expose the grain boundaries.
Claims
1. A heterogeneous metal bearing material, comprising a host material composed of primary particles agglomerated into secondary particles, and a particulate dopant material, characterised in that the particulate dopant material is homogeneously distributed within the secondary particles of said host material.2. A heterogeneous metal bearing material according to claim 1, wherein said host material is either one or a mixture of a metal hydroxide, oxyhydroxide, oxide, oxycarbonate, carbonate, or oxalate.3. A heterogeneous metal bearing material according to claim 2, wherein said heterogeneous metal bearing material has the general formula (dopant material)<sub>a</sub>(host material)b, where a and b are weight fractions, with 0 < a < 0.4, preferably 0.001 < a < 0.4, and more preferably 0.001 < a < 0.02, and where b = 1 - a.4. A heterogeneous metal bearing material according to any one of claims 1 to 3, wherein said dopant material is either one or more of MgO, Cr2O3, ZrO2, Al2O3, and TiO2, and is in the form of nanoparticles.5. A heterogeneous metal bearing material according to claim 3, wherein the dopant material is TiO<sub>2</sub>and the host material is either one or a mixture of Ni<sub>x</sub>Mn<sub>x</sub>Co<sub>2</sub>hydroxide, oxyhydroxide, and oxide, where x, y, z are atomic fractions, with 0 < x < 1 , 0 < y < l, 0 < z < l, and x+y+z = 1.6. A heterogeneous metal bearing material according to any one of claims 1 to 5, wherein said secondary particles of the heterogeneous metal bearing material are spherical.7. A heterogeneous metal bearing material according to any one of claims 1 to 6, wherein said dopant material is either one of MgF2and CaF2, or another water insoluble metal halide, and is in the form of nanoparticles.8. A heterogeneous metal bearing material according to any one of claims 1 to 7, wherein said dopant material has a size range of > 5 nm and < 200 nm, and preferably between 10 and 50 nm.9. Process for homogeneously distributing a particulate dopant material in a host material composed of primary particles agglomerated into secondary particles, thereby obtaining a heterogeneous metal bearing composite material, comprising the steps of: - providing a first flow comprising a solution of a precursor of the host material, - providing a second flow comprising a precipitation agent, - providing a third flow comprising a complexing agent, - providing a quantity of insoluble particulate dopant material, either in one or more of said first, second and third flows, or in a fourth flow consisting of a suspension of said particulate dopant material, and - mixing said first, second and third flow, and, if present, said fourth flow, thereby precipitating said host material and said dopant.10. Process according to claim 9, wherein said solution of the precursor is an aqueous metal salt solution, and said suspension of the dopant material is a suspension in water and a suspension stabilizing agent.11. Process according to claims 9 or 10, wherein the particulate dopant material consists of stabilized nanoparticles, preferably of metals or metal oxides, and the precursor is either one or a mixture of a metal nitrate, chloride, halide, and sulphate powder.12. Process according to any one of claims 9 to 11, wherein said dopant material is either one or more of MgO, Cr<sub>2</sub>O<sub>3</sub>, ZrO<sub>2</sub>, Al<sub>2</sub>O<sub>3</sub>, and TiO<sub>2</sub>, and has a size range of > 5 nm and < 200 nm.13. Use of the material according to any one of claims 1 to 8 for manufacturing a cathode material for a secondary battery, by firing said material with a lithium source.14. Use according to claim 13, wherein the dopant material is either one of MgO, Cr2O3, ZrO2, Al2O3, and TiO2, and the cathode material is a lithium transition metal oxide.15. Use according to claim 14, wherein the dopant material is Al2O3, and the cathode material is LiNiO2
.Leaving flatland: Diagnostics for Lagrangian coherent structures in three-dimensional flows

Abstract
Finite-time Lyapunov exponents (FTLE) are often used to identify Lagrangian Coherent Structures (LCS).
Most applications are confined to flows on two-dimensional (2D) surfaces where the LCS are characterized as curves.
The extension to three-dimensional (3D) flows, whose LCS are 2D structures embedded in a 3D volume, is theoretically straightforward.
However, in geophysical flows at regional scales, full prognostic computation of the evolving 3D velocity field is not computationally feasible.
The vertical or diabatic velocity, then, is either ignored or estimated as a diagnostic quantity with questionable accuracy.
Even in cases with reliable 3D velocities, it may prove advantageous to minimize the computational burden by calculating trajectories from velocities on carefully chosen surfaces only.
When reliable 3D velocity information is unavailable or one velocity component is explicitly ignored, a reduced FTLE form to approximate 2D LCS surfaces in a 3D volume is necessary.
The accuracy of two reduced FTLE formulations is assessed here using the ABC flow and a 3D quadrupole flow as test models.
One is the standard approach of knitting together FTLE patterns obtained on adjacent surfaces.
The other is a new approximation accounting for the dispersion due to vertical (u,v) shear.
The results are compared with those obtained from the full 3D velocity field.
We introduce two diagnostic quantities to identify situations when a fully 3D computation is required for an accurate determination of the 2D LCS.
For the ABC flow, we found the full 3D calculation to be necessary unless the vertical (u,v) shear is sufficiently small.
However, both methods compare favorably with the 3D calculation for the quadrupole model scaled to typical open ocean conditions.
Highlights
•
Approximations of 2D Lagrangian coherent structures in 3D flows are studied.
•
Vertical shear is shown to be important in the three-dimensional flows.
•
A new approximate Cauchy-Green tensor without using vertical velocities is proposed.
•
Two velocity gradient diagnostics help to decide when the full 3D velocity is needed.

Introduction
Hassan Aref's 1984 seminal paper on chaotic advection [1] initiated a new direction for fluid mechanics research.
Dynamical systems theory (DST) has provided a theoretical and computational basis for much of the subsequent developments.
DST methods are now widely used to identify critical material structures that govern transport patterns.
In geophysical flows, these structures persist from a few hours to weeks.
They are precisely identified by evolving distinguished invariant manifolds.
Jones and Winkler [2] provide a widely used algorithm for computing such manifolds in two-dimensional (2D) flows.
The methodology has been extended to three-dimensional (3D) dynamical systems by Branicki and Wiggins [3].
Although theoretically appealing, invariant manifold calculations are tedious and not easily automated.
Consequently, a much simpler, easily automated approach for determining approximate transport barriers from finite-time Lyapunov exponents (FTLE) has been developed [4-6].
The patterns in FTLE and related diagnostic fields are now known in the literature as Lagrangian Coherent Structures (LCS).
It is well known that LCS delineated by invariant manifolds and those derived from FTLE are equivalent for simple analytically prescribed flows.
However, this appears not to be true generally.
Branicki and Wiggins [7] made an extensive comparison of both methods and established useful criteria for identifying when they are in agreement.
Here, we study FTLE, with the expectation that our conclusions apply to the identification of transport barriers, within the provisos established by Branicki and Wiggins [7].
It is noteworthy that other diagnostics of LCS have been proposed [8-14].
All LCS diagnostics have attributes that are appropriate for particular situations.
Moreover, our impression is that all diagnostics agree when there are clear and robust LCS signatures.
Since all diagnostics are derived from trajectories, the effects of the approximations discussed here are likely to carry over to approaches other than FTLE as well.
Most studies that use DST methods to identify transport barriers have considered Hamiltonian-type flows and been restricted to 2D ([15], and references therein).
This restriction, however, is not a theoretical limitation, since the mathematical constructs have no dimensional requirements [16].
Indeed, several applications of DST methods to idealized 3D flows have been published[17-21,7].
Haller [22] developed a theoretical framework for calculating LCS in 3D flows and applied it to steady and unsteady Arnold-Beltrami-Childress (ABC) flows.
Lekien et al. [23] extended LCS theory to n-dimensional systems.
These and other studies have provided considerable theoretical insight into the topology of stirring and mixing in 2D and 3D flows.
However, they are largely based on analytically prescribed flows or simple numerical models.
Extensions of 2D methods to realistic 3D flows present a number of challenges that are not addressed in these studies.
Some computational fluid dynamics studies require millions of particle trajectories to adequately resolve the complexity of the 2D LCS in 3D flows.
To reduce the prohibitive associated computational cost, alternative algorithms that rely on reduced representations of the Cauchy-Green tensor have been proposed [24].
The approach of using a reduced Cauchy-Green tensor is attractive in the context of 3D geophysical flows, since the computation of 3D trajectories is hampered by a lack of reliable information about diabatic (atmosphere) or vertical (ocean) velocities.
These velocities are typically orders of magnitude smaller than the other two velocity components, and in many cases they are diagnostic variables recovered outside the prognostic model update cycle.
The value of including the effects of approximate diabatic or vertical velocities in LCS computations remains unclear.
As most geophysical flows are nearly 2D, one might anticipate vertical particle excursions to be negligible.
However, small vertical excursions can lead to significant horizontal end point differences due to vertical (u,v) shear [25].
These considerations lead to the basic question addressed here: Under what conditions can 2D velocity fields satisfactorily generate 2D LCS in 3D flows?
Due to the unreliability of the third velocity component in many situations, the answer to this question should ideally be based on a diagnostic readily available from 2D model flows.
Models for geophysical flows frequently solve the momentum equations for horizontal (respectively adiabatic) velocities in horizontal (respectively adiabatic) layers, leaving the normal velocity to be derived from the incompressibility condition.
Thus, information on two of the three velocity components and six of the nine velocity gradient components is provided directly.
This suggests a metric based on the magnitude of the components of the 2D velocity gradient tensor that are normal to the surfaces, since that information is currently not utilized in typical 2D analyses.
The importance of this quantity was also highlighted by Branicki et al. [26], who show that invariant manifolds are nearly vertical within a layer whose thickness is much smaller than the ratio of the horizontal velocities to their vertical gradients.
Also missing from the standard 2D analysis is any information on the normal velocity and its gradients, leading to the consideration of a second diagnostic capturing these effects.
To assess the impacts of these neglected velocity gradient components, 2D LCS derived from FTLE ridges are considered.
The structures obtained from 2D components of prescribed 3D velocity fields are compared with those from the full 3D fields.
For clarity in presentation, the geometry of the prescribed flows is here taken to be Cartesian, and the 2D surfaces are specified as horizontal planes.
Two approaches for approximating 2D LCS from the 2D velocity field are considered.
The first treats each vertical level independently, with the 2D FTLE computed from 2D trajectories on each layer within the volume of interest.
This is the "business as usual" approach adopted by Branicki and Kirwan [27] in their assessment of the lobe structure of a large anticyclonic ring in the Gulf of Mexico.
See also Bettencourt et al. [28] for a variation of this approach.
The second approach again uses 2D trajectories along each level surface, but accounts for the additional 3D dispersion due to horizontal separations between particle pairs in adjacent levels.
See Sulman et al. [29] for examples of this approach.
When computing FTLE in a 3D volume, the additional computational cost of the second approximation is minimal, since both approximations require 2D trajectories computed along each vertical level.
There is an additional approach that might be considered for approximating 2D LCS in cases where the full 3D velocity is known: using 3D trajectories, but restricting the analysis to a plane embedded in the flow.
Along this plane, the 3D FTLE can be approximated with a reduced Cauchy-Green tensor using only trajectories initialized on the plane, effectively neglecting normal gradients.
Garth et al. [24] compared this planar approximation with the full 3D rendering of FTLE fields and reported that the fields agreed quite well.
Since this approach requires vertical velocities that are normally not available in geophysical models, and since the computational savings vis-à-vis the full solution are minimal in this context, we chose not to discuss this approach further.
Two test flows are used in the analysis.
The first is the well-studied ABC flow.
The second, which is more relevant to geophysical flows, is a quadrupole with the velocities scaled to match typical open ocean conditions.
We anticipate that the results presented here will have some relevance to atmospheric flows when the diabatic velocities are small relative to the horizontal velocities.
We also discuss only steady flow.
We found that adding prescribed time periodicity yielded little additional insight for these two test flows.
There are fundamental issues with more general time dependence, the most crucial being interaction between the integration period for evaluating the Cauchy-Green tensor and the intrinsic time scales of the flow.
A thorough investigation of this matter is ongoing.
The report is organized as follows.
Section 2 reviews FTLE theory.
Section 3 provides a useful paradigm for studying arbitrary incompressible flows and describes the two test models.
Section 4 assesses the impact of the two FTLE approximations in a steady ABC flow and analyzes the critical parameters in the problem.
A steady quadrupole model is used to assess situations with more geophysical relevance in Section 5.
A summary of the results and some additional remarks are given in Section 6.
Review of 3D FTLE theory
Consider the flow (1)dxdt=v(x,t),x∈Ω⊂R3,t∈[t0,tf] where v is a smooth velocity field and Ω is an open subset of R3.
Let x(t;t0,x0) denote the trajectory solution of (1) at time t with x(t0;t0,x0)=x0.
This solution is given by (2)x=x0+∫t0tv(x,τ)dτ.
The right Cauchy-Green tensor is defined as (3)G=FTF, where the superscript T signifies the matrix transpose and F is the strain tensor (4)F=∂x∂x0=I+∫t0t∂v∂x⋅∂x∂x0dτ with I the identity matrix.
The FTLE is then given by (5)Λ(tf;t0,x0)=log(λmax(G))tf-t0, where λmax(G) is the maximum eigenvalue of the matrix G.
Repelling LCS are typically defined as ridges of local maxima in the FTLE field computed from trajectories in forward time (tf>t0).
Similarly, attracting LCS are defined as ridges in the backward-time (tf<t0) FTLE field [4,22].
Where the peak values of these ridges are high, they have been shown to be good surrogates for the material manifolds that more precisely delineate LCS [6].
In the full 3D formulation, x=(x,y,z),v=(u,v,w)∈R3, and (6)F0=∂x∂x0=[∂x∂x0∂x∂y0∂x∂z0∂y∂x0∂y∂y0∂y∂z0∂z∂x0∂z∂y0∂z∂z0].
Our first FTLE approximation treats 2D horizontal cross-sections of Ω individually, so that x=(x,y) and v=(u,v).
In this case, tensor G is second order, with (7)F1=∂x∂x0=[∂x∂x0∂x∂y0∂y∂x0∂y∂y0].
While this approximation follows standard practice, it inherently neglects the fact that ocean flows are essentially 3D.
A more appropriate reduction of the full matrix (6) for 2D cross-sections would be (8)F1̃=[∂x∂x0∂x∂y00∂y∂x0∂y∂y00001].
The singular values of F1 (i.e. the eigenvalues of G1) are identical to two of the singular values of F1̃, whose additional singular value is 1.
For incompressible flows, det(G0)=det(F0)=1 so that volumes are conserved under the flow map.
Since G0 is symmetric and positive definite (with three positive eigenvalues), λmax(G0)≥1.
In the following, we will restrict our discussion to such flows.
It is likely then (although not guaranteed) that (7) and (8) will yield equivalent results.
They will differ only where λmax(G1)<1, leading to a negative FTLE.
We did not encounter this problem in any of the cases reported here.
Our second FTLE approximation also ignores the vertical velocity w, which can be problematic in many geophysical fluid dynamics models, but considers vertical gradients, i.e. x∈R3 but ∂z/∂t=0∀x,t.
Consequently, (9)F2=∂x∂x0=[∂x∂x0∂x∂y0∂x∂z0∂y∂x0∂y∂y0∂y∂z0001].
Unlike approximation F1,F2 accounts for the effects of vertical shear in the horizontal velocity components.
To summarize, we will compare three different FTLE formulations: 1.
3D form of tensor G with trajectories from 3D velocities(10)FTLE3d3d=log(λmax(G0))tf-t0with G0=F0TF0
2.
2D form of tensor G with trajectories from 2D velocities(11)FTLE2d2d=log(λmax(G1))tf-t0with G1=F1TF1
3.
3D form of tensor G with trajectories from 2D velocities(12)FTLE3d2d=log(λmax(G2))tf-t0with G2=F2TF2.
It follows from these definitions (see the Appendix) that (13)FTLE3d2d≥FTLE2d2d∀tf,to,x0.
However, there is no similar general relationship between FTLE3d3d and either of the two approximations.
It follows directly from the above that FTLE2d2d=FTLE3d2d if ∂x/∂z0=∂y/∂z0=0.
Due to (4), this condition is satisfied if ∂u/∂z=∂v/∂z=0∀x,t.
Define (14)Sv=(∂u∂z)2+(∂v∂z)2 as the measure of the vertical shear in the horizontal velocities.
When Sv=0∀x,t,FTLE2d2d=FTLE3d2d.
We propose to determine threshold Sv values for which FTLE2d2d and FTLE3d2d differ significantly.
Similarly, (10) and (12) imply that FTLE3d3d=FTLE3d2d if ∂z/∂x0=∂z/∂y0=0 and ∂z/∂z0=1.
Due to (4), this condition is satisfied if ∂w/∂x=∂w/∂y=∂w/∂z=0∀x,t.
To identify cases when FTLE3d2d is a poor approximation for FTLE3d3d, the relevant diagnostic is therefore the magnitude of the vertical velocity gradient: (15)Sw=(∂w∂x)2+(∂w∂y)2+(∂w∂z)2.
Generally, both Sv and Sw are functions of t and x.
However, below only steady flows are analyzed.
Furthermore, whenever there is x-dependence, we will focus on the maximum values achieved.
The notation max(Sv) and max(Sw) will imply maxima taken over space x.
In Sections 4 and 5, FTLE3d3d, defined in (10) and derived from the full 3D formulation, is the benchmark against which the two approximations FTLE2d2d, defined in (11), and FTLE3d2d, defined in (12), are evaluated.
To quantify the roles of Sv and Sw, the root-mean-square (RMS) differences between FTLE3d3d and each of the approximations are computed.
To simplify the notation, we define (16)Δ1=|FTLE3d3d-FTLE2d2d|(17)Δ2=|FTLE3d3d-FTLE3d2d| and use the notation RMS(Δi) for the respective RMS values.
Admittedly, this criterion is somewhat crude, as it does not focus singularly on the ridges that define the LCS but includes off-ridge data.
It does capture differences in ridge sharpness.
Ridge position shifts are considered separately, but an adequate metric to quantify these differences eludes us.
For the computation of the FTLE, trajectories are determined by integrating the differential equation (1) using DLSODA from the Fortran ODEPACK library [30] with an absolute tolerance of 1×10-6 and a relative tolerance of 1×10-6.
The derivatives in the Cauchy-Green tensors are then approximated using second-order centered finite-differences.
Eigenvalues are calculated analytically using algebraic expressions.
Test models for 3D incompressible flows
We study the impact of the vertical velocity and the vertical shear of the horizontal velocity components on the FTLE approximations in two analytic 3D flows.
The first is the well known ABC flow [31], in which the scales of the vertical velocities and the vertical shear of horizontal velocities are commensurate with the corresponding horizontal scales.
Results of the analysis of this model are presented in Section 4.
In many geophysical fluid flows, however, the vertical velocities are small relative to the horizontal velocities, and there may be a disparity between the magnitudes of the horizontal and vertical shears.
Since the ABC model cannot adequately represent these flows, we also study a single mode quadrupole model.
Results and analysis for this model are described in Section 5.
Even though our analyses are purely kinematic, we stress that both of these models have a dynamical basis.
The ABC flow is an exact periodic solution to the nonlinear Euler equations in a non-rotating frame, while the quadrupole model solves the linear Euler equations on an f-plane in a periodic domain.
Both model flows can be expressed in the vector potential representation of a general 3D incompressible flow [32] with the 3D velocity v given in terms of two scalar potentials Ψ and Φ as (18)v=∇×[-Ψk+∇×(Φk)].
Generally, k can be the unit vector normal to any preferred surface.
We take k to be the vertical unit vector, so the velocity components are (19a)u=-∂Ψ∂y+∂2Φ∂z∂x,(19b)v=∂Ψ∂x+∂2Φ∂z∂y,(19c)w=-∇h2Φ, where ∇h2 is the Laplacian with respect to the horizontal coordinates, i.e., (20)∇h2=∂∂x2+∂∂y2.Ψ and Φ are solutions to Helmholtz equations with Dirichlet and Neumann boundary conditions, respectively.
Each can be expressed in terms of sine and cosine functions for our two model domains, both of which are rectangular and periodic.
Steady ABC flow
The steady ABC flow can be derived from (19) by defining Ψ and Φ as (21a)Ψ=-[Csin(y)+Bcos(x)],(21b)Φ=A[-xcos(z)+ysin(z)]-Ψ, where x,y,z∈R.
The velocity components are then (22a)u=Asin(z)+Ccos(y),(22b)v=Bsin(x)+Acos(z),(22c)w=Csin(y)+Bcos(x).Fig. 1 shows example cross-sections of this velocity field with A=1 and B=C=0.8.
We focus only on cases where B2+C2≥A2≥|B2-C2|.
Note that this assures the existence of stagnation points in the flow (see [33] for more details).
We also choose B=C.
With this choice, the two free parameters independently control the two diagnostics.
Sv=A and is constant throughout the domain, while Sw=B2sin2(x)+C2cos2(y)=|B|sin2(x)+cos2(y), with values ranging from 0 to |B|2.
The left inequality of the stagnation point condition above is therefore equivalent to max(Sw)≥Sv.
The computations presented in Section 4 are performed on a grid of size 201×201×201 for x,y,z∈[0,2π], from time t0=0 to tf=10.
Steady quadrupole flow
The steady quadrupole flow is obtained from (19) by defining Ψ and Φ as (23a)Ψ=A(z)sin(πx/Lx)sin(πy/Ly),(23b)Φ=B(z)cos(πx/Lx)cos(πy/Ly), where x∈(-Lx,Lx),y∈(-Ly,Ly), and z∈(-H,0).
In this case, the velocity components are (24a)u=-(αyA(z)+αxBz(z))sin(αxx)cos(αyy),(24b)v=(αxA(z)-αyBz(z))cos(αxx)sin(αyy),(24c)w=(αx2+αy2)B(z)cos(αxx)cos(αyy), where (25)Bz=∂B∂z(26)αx=π/Lx(27)αy=π/Ly.
We will restrict our analysis to a domain with Lx=Ly, so that we can define α=αx=αy.
Sulman et al. [29] used this model to illustrate manifolds and associated flow properties for a prescribed free surface with non-zero velocity.
Here the focus is on investigating the parameter space of Sv and Sw.
For clarity, it is useful to further restrict the analysis to functions for A and B that are at most linear in z, i.e., (28a)A(z)=(1/α)[A0+A1z](28b)B(z)=(1/2α2)[B0+B1z].
Horizontal and vertical length and velocity parameters can be scaled to be appropriate for the atmosphere or ocean.
We choose typical oceanic values, setting the horizontal domain dimensions to Lx=Ly=200km.
The domain depth is allowed to be infinite.
This is a computational convenience necessitated by the linear vertical profiles of w, which lead in extreme cases to a handful of particles in the domain reaching depths below 20 km.
Various boundary conditions could be imposed at a more realistic oceanic depth.
We have examined several and found no impact on our conclusions.
At any depth z in this flow, the LCS (ridges in the FTLE field) are straight lines at x=[-Lx,0,Lx] and y=[-Ly,0,Ly].
Computations are performed on a horizontal grid of size 257×257 from time t0=0 to time tf=8days.
The results presented here focus on a single horizontal layer at z=-0.3km.
RMS differences as well as maxima are computed over this single layer only.
Trajectories in adjacent vertical layers at z=-0.29km and z=-0.31km are also computed to allow for finite-difference approximation of strain tensor terms involving vertical gradients, using a 10 m vertical spacing representative of many ocean models.
Parameter values in Section 5 are reported so that all quadrupole velocities have units of m s-1.
With z in units of kilometers, this implies that A0 and B0 have units of m s-1, and A1 and B1 have units of m s-1 km-1.
Note that B0 is the peak w magnitude at z=0, and when Bz=0,A0 is the peak horizontal velocity magnitude at z=0.
LCS in steady ABC flows
The role of Sv and Sw in determining the accuracy of the two FTLE approximations given in Section 2 is first tested on the ABC flow.
The range of values for the parameters was chosen to sample a reasonably large section of the parameter space, within the constraints of permitting stagnation points within the flow.
B and C are always set to be equal.
This condition implies that there is no scaling difference between the two horizontal velocities.
As discussed in Section 3.1, the two ABC parameters independently control the two diagnostics, with Sv=A and Sw depending only on B, with values ranging from 0 to |B|2.
We proceed by systematically varying one of the parameters while keeping the other fixed, resulting in two series of experiments.
As noted in Section 3.1, the condition guaranteeing stagnation points in the flow also implies that max(Sw)≥Sv.
This is the reverse of the situation studied in the quadrupole flow (Section 5).
Fixed max(Sw) with increasing Sv
We first consider a series of cases with Sv=A=[0.1,0.2,…,1.1], keeping B=C fixed at a value of 0.8 for max(Sw)=1.13.
Two examples of the three FTLE fields and their differences (Δ1=|FTLE3d3d-FTLE2d2d| and Δ2=|FTLE3d3d-FTLE3d2d|) are shown in Figs.
3 and 4 for Sv=0.1 and Sv=1.1, respectively.
Fig. 5(a)-(e) show cross-sections of the same five fields at z=2π for an intermediate Sv value (Sv=0.5).
When Sv is small, both FTLE approximations (FTLE3d2d and FTLE2d2d) adequately capture much of the larger-scale LCS structure (Fig. 3).
As Sv increases, however, both approximate FTLEs miss important smaller scale structure (Fig. 5), and for large Sv (Fig. 4), the differences are particularly striking, as both approximate FTLEs miss important vertical structure (Fig. 4(b)-(c)) when compared with the benchmark (Fig. 4(a)).
Fig. 5 suggests that accuracy of the two FTLE approximations degrades steadily as Sv increases.
Plots of RMS FTLE differences (computed over the entire domain) as a function of Sv (Fig. 5(f)) confirm this steady degradation in both approximations over the range of Sv considered here.
Curiously, both RMS difference curves exhibit inflection points near Sv=0.8.
Although A=B=C when Sv=0.8, a simple explanation for this feature in the difference growth rate eludes us.
Fig. 5(f) shows that FTLE3d2d is a better approximation than FTLE2d2d for all Sv values considered.
Note also that as they degrade, the spatial patterns in the two FTLE approximations remain quite similar to each other (Figs.
4(b)-(c) and 5(b)-(c)).
RMS FTLE differences (Fig. 5(f)) are convenient metrics for demonstrating the relationship between increasing Sv and decreasing accuracy in FTLE approximations.
However, because these scalar metrics represent average differences computed over the entire 3D domain, they cannot easily be used to distinguish cases with differences due mostly to errors in FTLE magnitude (as in Fig. 3) from cases with differences due to significant errors in FTLE ridge positions (as in Fig. 4).
The accuracy of approximate FTLE ridge positions can be explored by examining one-dimensional (1D) cross-sections of FTLE2d2d,FTLE3d2d, and FTLE3d3d.
Fig. 6 provides an example along the x-direction at z=2π,y=π/2 for Sv=0.1, 0.5, and 1.1.
These plots demonstrate that ridges in both FTLE2d2d and FTLE3d2d align well with those for FTLE3d3d when Sv=0.1 (Fig. 6(a)).
For larger Sv values (Fig. 6(b)-(c)), FTLE2d2d and FTLE3d2d ridge positions are shifted significantly along the x-axis.
Additionally, when Sv=1.1, peak FTLE values tend to be underestimated and FTLE3d2d overestimates FTLE values near local minima.
These profile plots indicate that neither FTLE2d2d nor FTLE3d2d is a good approximation for FTLE3d3d when the magnitude of Sv approaches that ofmax(Sw).
Fixed Sv with increasing max(Sw)
We now consider the effects of increasing max(Sw) on approximate FTLEs computed for the ABC flow.
Here, Sv is fixed by specifying A=0.1 and Sw is increased by choosing B=C=[0.5,0.6,…,1.5].
Examples of the three FTLE fields and their differences Δ1 and Δ2 are shown in Figs.
7 and 8 for B=C=0.5 and B=C=1.5, respectively.
Fig. 9(a)-(e) show cross-sections of the same five fields at z=2π for an intermediate value (B=C=1.0).
Plots of RMS FTLE differences (computed over the entire domain) as a function of max(Sw) are shown in Fig. 9(f).
Figs. 7 and 8 show that, even for large max(Sw), both approximations capture the LCS structure adequately.
RMS differences increase as max(Sw) increases (Fig. 9(f)), although the differences remain relatively small over the range of max(Sw) values considered here.
At all max(Sw) values considered, the RMS differences show that FTLE3d2d is a better approximation than FTLE2d2d.
Fig. 10 displays 1D cross-sections of FTLE3d3d, FTLE3d2d, and FTLE2d2d along the x-direction at z=2π and y=π/2 for B=C=0.5,1.0,1.5.
These curves show that both FTLE approximations accurately describe peak FTLE values and ridge positions, with only small position differences along the x-axis.
LCS in a steady quadrupole
Analysis of the ABC flow (Section 4) identified the important role that vertical shear of the horizontal velocity plays in 3D LCS.
That analysis also demonstrated that two potentially appealing FTLE approximations can be significantly degraded when this shear is ignored.
In ABC flows, however, all three velocity components have similar scales since w is constrained by the same parameters that control the (u,v) structure.
Consequently, ABC flows are not representative of most geophysical flows which have a natural disparity-typically three orders of magnitude or more-between the vertical and horizontal velocity scales.
To assess the validity of the two FTLE approximations defined in Section 2 for flows with representative geophysical velocity scalings, we now appeal to the quadrupole model described in Section 3.2.
This model allows for a natural separation between the horizontal and vertical velocity scalings.
Like the ABC flow, it also allows Sv and Sw to be varied independently, so that the relative importance of vertical velocities and vertical (u,v) shear can be examined separately.
Given the choices of the model formulation specified in Section 3.2, the two diagnostics take on the following values: (29)Sv=|A1|sin2(αx)cos2(αy)+cos2(αx)sin2(αy) and (30)Sw2=α2(B0+B1z)2[sin2(αx)cos2(αy)+cos2(αx)sin2(αy)]⋯+B12[cos2(αx)cos2(αy)].
For ocean flows, the Richardson shear criterion provides an approximate upper bound on Sv.
From this criterion, we expect N2/Sv2≥0.25 where N is the stratification (Brunt-Väisälä) frequency.
Flows with Sv values exceeding this limit will likely be unstable.
Here, we consider Nmax=1cycle/h (1.75×10-3s-1) as an upper bound for ocean flows and we examine a range of A0 and A1 values corresponding to maximum Sv values ranging from 0.2Nmax to 2Nmax.
An ocean limit on Sw is harder to establish.
To allow for more direct comparisons between quadrupole experiments with w≠0, we choose to fix the maximum vertical velocity magnitude (|w|max) at the analysis depth z=-0.3km at 10-4ms-1 in all cases.
From Eq.
(24c), note that both ∂w/∂x and ∂w/∂y scale like α|w|max=π|w|max/L, where L=Lx=Ly.
For our choices of |w|max=10-4ms-1 and L=200km, these horizontal w gradients do not exceed 1.57×10-9s-1.
For the incompressible ocean flows considered here, the remaining contributor to Sw,∂w/∂z, is equal to the horizontal divergence.
Estimates of ocean mesoscale horizontal divergence from drifter trajectories indicate that a conservative upper bound on its magnitude is 0.1f, where f is the Coriolis parameter [34-36].
Consequently, we choose a representative mid-latitude f value of 1×10-4s-1 and examine a range of B0 and B1 values corresponding to maximum ∂w/∂z magnitudes between 0.01f and 0.1f.
For these parameter choices, since ∂w/∂z is always at least two orders of magnitude larger than either ∂w/∂x or ∂w/∂y,Sw∼|∂w/∂z|.
In Sections 5.1 through 5.3, we consider three quadrupole cases: 1.
Zero vertical (u,v) shear with nonzero vertical velocity gradients (Sv=0,max(Sw)≠0).
2.
Nonzero vertical (u,v) shear with zero vertical velocity (max(Sv)≠0,w=0,Sw=0).
3.
Nonzero vertical (u,v) shear with nonzero vertical velocity (max(Sv)≠0,w≠0,max(Sw)≠0).
These three cases permit the investigation of the effects of Sv and Sw first separately and then jointly.
Zero vertical (u,v) shear with nonzero w gradient
Here we specify Sv=0 and max(Sw)≠0.
This is achieved by setting A1=0.
A0=0.5ms-1, and max(Sw) is varied by choosing (31)B0=-[2,5,8,11,14,17,20,23,26,29]×10-4 in m s-1 and (32)B1=-[1,2,3,4,5,6,7,8,9,10]×10-4 in m s-1 km-1.
For these parameter choices, the peak w magnitude at z=-0.3km is fixed to 10-4ms-1.
In the analysis layer, max(Sw) values, range from 0.0864 to 0.8640days-1.
Note that, since Sv=0,FTLE3d2d and FTLE2d2d are derived from identical Cauchy-Green tensors.
Thus, for this case only FTLE3d3d and FTLE2d2d are compared.
Fig. 11((a)-(c)) show plots of FTLE3d3d,FTLE2d2d, and their absolute difference at z=-0.3km with B0=-2×10-4ms-1 and B1=-10-3ms-1km-1.
The small absolute differences (Fig. 11(c)) demonstrate that FTLE2d2d is an excellent approximation in this case.
Fig. 11(d) shows a plot of RMS(Δ1) versus max(Sw).
This plot indicates that the RMS difference is insignificant (on the order of 10-6days-1) for max(Sw) values up to about 0.43days-1.
For higher values of max(Sw),RMS(Δ1) increases rapidly by at least five orders of magnitude.
Since the peak values of FTLE2d2d are O(1), these differences are significant.
Nonzero vertical (u,v) shear with no vertical velocity
Next we explore how vertical (u,v) shear impacts the FTLE approximations when Sw=0.
From Eq.
(30), this is obtained by specifying B0=B1=0.
The following values are chosen for A0 and A1 to achieve a range of max(Sv): (33)A0=[0.53,0.59,0.65,0.80,0.95,1.10,1.25,1.40,1.55] in m s-1, and (34)A1=[0.10,0.30,0.50,1.00,1.50,2.00,2.50,3.00,3.50] in m s-1 km-1.
These nine (A0,A1) pairs were chosen so that the (u,v) velocities at z=-0.3km remain constant, with a peak value of 0.5 m s-1.
Over this range of A1 values, max(Sv) increases from 1×10-4 s-1 to 3.5×10-3s-1 (0.2Nmax to 2Nmax).
Since w=0 here, the 3D and 2D trajectories are identical.
Hence for this case, FTLE3d2d=FTLE3d3d, and we need only compare FTLE3d3d with FTLE2d2d.
Fig. 12(a)-(c) show the two FTLE fields and their differences at z=-0.3km, with A0=0.53ms-1 and A1=0.10ms-1km-1.
Note that the magnitude of the peak FTLE differences is comparable to the peak FTLE values.
FTLE differences are small at the eddy centers and along the LCS.
Panel (d) of this figure shows the RMS(Δ1) versus max(Sv) for the plane at z=-0.3km.
The RMS differences rise rapidly with max(Sv) at first but increase more slowly after about 50days-1.
Nonzero vertical (u,v) shear, nonzero w gradient
In Sections 5.1 and 5.2, the effects of Sv and Sw were examined separately.
Here, the general case where both max(Sv) and max(Sw) are nonzero is considered.
Ninety separate cases are examined.
These cases pair each of the ten values of max(Sw) explored in Section 5.1 (range of B0 and B1 values shown in Eqs.
(31) and (32)) with the nine values of max(Sv) explored in Section 5.2 (range of A0 and A1 values shown in Eqs.
(33) and (34)).
Fig. 13(a)-(e) show the three FTLE fields and their differences at z=-0.3km, with A0=0.53ms-1,A1=0.10ms-1km-1,B0=-1.1×10-3ms-1, and B1=-4×10-3ms-1km-1.
A comparison of panels (d) and (e) shows that Δ2 is much smaller than Δ1.
In particular, the magnitude of max(Δ1) is comparable to the peak FTLE values, whereas max(Δ2) is an order of magnitude smaller.
Also noteworthy are the uniformly small FTLE differences at the eddy centers and along the LCS, as in the previous section.
At eddy centers, Sv=0 and Sw=0, so that small differences are expected.
The LCS, on the other hand, pass through both maxima and minima in the Sv and Sw fields.
We hypothesize that here the primary direction of dispersion lies in the horizontal plane, resulting in similar FTLE values for the approximations.
Fig. 13(f) shows RMS(Δ1) and RMS(Δ2) versus max(Sv) for the case with max(Sw)=0.3456days-1.
After an initial rise, RMS(Δ2) quickly levels off at a relatively low value, while RMS(Δ1) continues to grow.
Fig. 14 shows contour plots of RMS(Δ1) and RMS(Δ2) as a function of both max(Sw) and max(Sv).
For RMS(Δ2), the controlling diagnostic is max(Sw): For any nonzero value of max(Sv),RMS(Δ2) increases rapidly as max(Sw) increases.
The exception occurs at very low values of max(Sv).
Note that for max(Sv)=0,RMS(Δ2) is small and noticeably lower than for max(Sv)>0.
Sw is therefore important only in the presence of vertical (u,v) shear.
Even though changes in max(Sw) have a greater impact on RMS(Δ2) than changes in max(Sv), an increase in max(Sv) is associated with larger RMS differences for almost all values of max(Sw).
For large max(Sw) (greater than about 0.6days-1), this general rule no longer strictly holds, leading to the counter-intuitive result that increasing max(Sv) may actually improve the accuracy of the FTLE estimate slightly.
This phenomenon may result from aliasing that arises when the integration time approximates an integer multiple of the eddy orbit period at some critical locations in the flow.
Fig. 14(b) shows that, for RMS(Δ1), the situation is reversed: Especially at low values of max(Sv), increasing max(Sw) has little effect.
The errors are dominated by the vertical (u,v) shear effects.
As max(Sv) rises above about 100days-1,max(Sw) becomes more relevant, until it becomes the controlling factor (max(Sv)≳175days-1 and max(Sw)≳0.5days-1).
It is worth noting that for most of the explored parameter space, RMS(Δ1) is close to an order of magnitude larger than RMS(Δ2).
This suggests that the additional computational cost for FTLE3d2d is often justified by the improvement in the FTLE approximation.
The metric used so far in the quadrupole analysis is an RMS difference between the exact FTLE field and FTLE approximations taken over the analysis layer at z=-0.3km.
As for the ABC flow, identification of LCS positions as ridges and valleys may be less error-prone than the exact FTLE magnitudes over the entire field.
Fig. 15 compares FTLE ridge positions, showing 1D cross-sections of FTLE3d3d,FTLE3d2d, and FTLE2d2d at z=-0.3km along an east-west transect at y=100km.
The plots clearly show that both FTLE approximations capture the ridge locations nearly perfectly.
FTLE3d2d also accurately reproduces the steep FTLE drop within the eddies.
Discussion
In baroclinic geophysical flows one expects that LCS are 2D structures embedded in a 3D flow field.
In many models, only 2D velocities are calculated from the primitive equations, with the third component obtained from a diagnostic computation.
Typically that component is small, and since the diagnostic calculation contains numerical uncertainty, it may be preferable to estimate the LCS characterizing a 3D flow with more reliable 2D velocities.
The LCS intersect the corresponding surfaces along 1D curves.
Here we investigated the conditions when 2D velocities are sufficient for delineating these curves.
As discussed in Section 1, the answer has consequences: If these conditions are satisfied, the computational cost can be reduced, in the case of FTLE2d2d cutting the number of required trajectories by a factor of three.
Moreover, the investigation here could validate, or negate, a substantial body of research in oceanography where only 2D velocities are readily available and in atmospheric sciences where diabatic velocities are often neglected.
Our approach was to study this issue with two simple models, the ABC flow and a quadrupole model scaled for open ocean conditions.
In Section 2, we proposed two reduced representations of the Cauchy-Green tensor that utilized just 2D velocity information.
One was the standard 2×2 matrix widely used in oceanography.
The other was a reduced form of the 3×3 matrix that included the vertical shears of the horizontal positions but none of the terms involving gradients of vertical positions.
Two diagnostics were also proposed.
One, Sv, involves the components of the 2D velocity gradient normal to the surfaces.
This is readily computed.
The other metric, Sw, involves the gradients of the vertical/diabatic velocity component.
This is not generally available in general circulation models, but was specified in the simple models used here and employed as a guide to the impact these terms might have on the FTLE estimates.
Comparisons of the FTLE obtained from the two approximations were made with that obtained from the complete 3D velocity fields.
For the ABC model, when Sv was small, the analysis indicated that both approximations adequately described both LCS positions and peak FTLE values regardless of Sw (Section 4.2).
For moderate to large Sv values, neither approximation adequately delineated the 2D LCS (Section 4.1).
In these cases, magnitudes of the approximate FTLEs deviated significantly from the exact value and the approximations produced spurious LCS ridges.
Analysis of the FTLE approximations for the quadrupole were more encouraging.
In this case, the vertical and horizontal velocities along with their gradients were constrained to be consistent with open ocean conditions.
Both FTLE approximations yielded magnitudes comparable to the exact values.
More importantly, no spurious LCS ridges or LCS position errors were discovered.
This suggests that, for geophysical models, approximate FTLEs computed using only 2D velocities may be accurate enough to reliably characterize important mixing structures.
Recent studies have used FTLEs to explore how LCS change with depth in the vicinity of ocean mesoscale eddies.
Branicki and Kirwan [27] and Bettencourt et al. [28] used 2D velocities to show that LCS surfaces near eddies are aligned almost vertically, like "curtains", in the water column.
Our analysis of approximate FTLEs in the idealized quadrupole does not invalidate these studies.
However, a thorough assessment of approximate FTLEs in realistic ocean models is needed.
Such an investigation should be guided by the diagnostics developed here.
In particular, the diagnostic Sv was found useful for determining the accuracy of approximate FTLEs.
Its utility is related to the Richardson criterion noted in Section 5.
This criterion is similar to that identified by Branicki et al. [26], but replaces a scale velocity with N2 and gives a non-dimensional numerical threshold applicable to geophysical flows.
It is used somewhat differently here, in that it characterizes the accuracy of the FTLE approximations and not the verticality of the invariant manifolds.
Our results suggest that large values of Sv identify regions where FTLE2d2d approximations become problematic.
Finally, we recommend the new approximation FTLE3d2d for geophysical flows with no reliable information about vertical or diabatic velocities.
FTLE3d2d accounts for additional dispersion due to vertical (u,v) shear using an approximate Cauchy-Green tensor based on the strain tensor shown in Eq.
(9).
For the quadrupole studied here, this additional dispersion was often significant.
Fig. 15 shows the substantial improvement FTLE3d2d provides when compared with the FTLE2d2d approximation most often used in prior work.
Importantly, the necessary vertical (u,v) shear information is readily available in existing geophysical models.
Acknowledgments
Support for this research came from the Office of Naval Research through grants N00014-10-1-0522 and N00014-11-10081 to the University of Delaware, the Office of Naval Research MURI OCEAN 3D + 1 grant N00014-11-1-0087, a grant from BP/The Gulf of Mexico Research Initiative, and the Mary A.S.
Lighthipe endowment to the University of Delaware.
Below a derivation of the inequality in (13) is given.
Proposition 1
(35)FTLE3d2d≥FTLE2d2d∀tf,to,x0.
Proof
Recall the definitions (36)FTLE2d2d=log(λmax(G1))tf-t0with G1=F1TF1 and (37)FTLE3d2d=log(λmax(G2))tf-t0with G2=F2TF2, where (38)F1=[∂x∂x0∂x∂y0∂y∂x0∂y∂y0] and (39)F2=[].
Note that λmax(Gi)=σmax(Fi), where σmax is the largest singular value.
Define S to be the subspace of R3 spanned by {[1,0,0]T,[0,1,0]T}.
There is an obvious bijection from R2 to S, namely γ([x,y]T)=[x,y,0]T.
Note that γ preserves the L2-norm.
Furthermore, for any vector r∈R2,γ(F1r)=F2(γ(r)).
It is well established (e.g. [37]) that ‖M‖2=σmax(M), where ‖M‖2 denotes the matrix 2-norm, which is defined for an m by n matrix M as (40)‖M‖2=supr∈Rn,‖r‖2=1‖Mr‖2.
In particular, (41)σmax(F1)=supr∈R2,‖r‖2=1‖F1r‖2, and (42)σmax(F2)=supν∈R3,‖ν‖2=1‖F2ν‖2(43)≥supν∈S,‖ν‖2=1‖F2ν‖2(44)=supr∈R2,‖r‖2=1‖F2γ(r)‖2(45)=supr∈R2,‖r‖2=1‖γ(F1(r))‖2(46)=supr∈R2,‖r‖2=1‖F1(r)‖2(47)=σmax(F1).
Consequently, σmax(F2)≥σmax(F1), which implies that FTLE3d2d≥FTLE2d2d.□

Electronically coupled hybrid structures by graphene oxide directed self-assembly of Cu 2-x S nanocrystals
GO was synthesized from graphite powder using a modified Hummers method, and dispersed in water.11 In a typical macroemulsion preparation, 150 μL of the GO solution in water (initial concentration ~ 4 mg mL-1) was diluted with 150 μL of deionized water (1:1 v/v). The solution was sonicated for about 20 minutes in order to exfoliate the GO sheets that are initially stacked in the dispersion. To this, 50 μL of diluted hydrochloric acid (HCl, 1 M) were added dropwise and mixed with the solution. The addition of HCl is needed for the later stabilization of the macroemulsion. Thereafter, a toluene solution containing the Cu2-xS NCs (200 μL) was added to the diluted GO solution. Upon addition of the toluene solution an emulsion was formed in the vial by manual shaking. Subsequently, the emulsion was sonicated in order to decrease droplet size, leading to the formation of a macroemulsion having a milky appearance. Pure toluene/GO macroemulsions (in the absence of Cu2-xS NCs) were prepared in an analogous way, but using pure toluene instead of Cu2-xS NCs in toluene. The reference Cu2-xS NCs samples were prepared by diluting the original NC's solution two times, in order to obtain similar NC's concentration.
The synthesis of 5 nm Cu2-xS NCs was performed according to a previous report.8 Typically, the synthesis was performed by mixing 1.25 mmol of ammonium diethyldithiocarbamate, 10 mL of dodecanethiol and 17 mL of oleic acid in a three-neck flask. The solution was degassed at 120 degC for 1 h. Under nitrogen a suspension composed of 1 mmol of copper(II) acetylacetonate and 3 mL of oleic acid was injected followed by heating up to 180 degC, and kept at this temperature for 10-20 min. After cooling to room temperature the NC solution was transferred to a glove box and washed twice with anhydrous solvents before being redispersed in toluene. The NC dispersion was kept in the glovebox until utilization for the formation of hybrid GO/Cu2-xS NCs structures.Operando Raman spectroscopy and kinetic study of low-temperature CO oxidation on an α-Mn2O3 nanocatalyst
Mn3O4 nanoparticles wrapped with oleic acid were prepared by a one-pot synthesis method as precursors. First, manganese acetate tetrahydrate (14.7 g) and oleic acid (8.0 ml) were dissolved in methanol (100 ml) under magnetic stirring for 1 h. The solution was then sealed in a Teflon-lined stainless steel autoclave (200 ml) and heated to 180 degC for 24 h. Next, the product was dissolved in hexane and Mn3O4 nanoparticles were extracted with ethanol. The brown powder product was washed using deionized water and ethanol and then dried in air for 5 h. The prepared Mn3O4 was calcined in a tubular furnace in an airflow of 50 ml/min at 500 degC with a ramp of 1 degC /min. After 10 h, pure α-Mn2O3 nanoparticles were obtained. It is noted that all chemicals (C.P. grade) are supplied by Shanghai Lingfeng Co. Ltd.Frontal and superior temporal auditory processing abnormalities in schizophrenia

Abstract
Background
Although magnetoencephalography (MEG) studies show superior temporal gyrus (STG) auditory processing abnormalities in schizophrenia at 50 and 100ms, EEG and corticography studies suggest involvement of additional brain areas (e.g., frontal areas) during this interval.
Study goals were to identify 30 to 130ms auditory encoding processes in schizophrenia (SZ) and healthy controls (HC) and group differences throughout the cortex.
Methods
The standard paired-click task was administered to 19 SZ and 21 HC subjects during MEG recording.
Vector-based Spatial-temporal Analysis using L1-minimum-norm (VESTAL) provided 4D maps of activity from 30 to 130ms.
Within-group t-tests compared post-stimulus 50ms and 100ms activity to baseline.
Between-group t-tests examined 50 and 100ms group differences.
Results
Bilateral 50 and 100ms STG activity was observed in both groups.
HC had stronger bilateral 50 and 100ms STG activity than SZ.
In addition to the STG group difference, non-STG activity was also observed in both groups.
For example, whereas HC had stronger left and right inferior frontal gyrus activity than SZ, SZ had stronger right superior frontal gyrus and left supramarginal gyrus activity than HC.
Conclusions
Less STG activity was observed in SZ than HC, indicating encoding problems in SZ.
Yet auditory encoding abnormalities are not specific to STG, as group differences were observed in frontal and SMG areas.
Thus, present findings indicate that individuals with SZ show abnormalities in multiple nodes of a concurrently activated auditory network.
Highlights
•
Auditory encoding in schizophrenia (SZ) and healthy controls (HC) was examined.
•
Distributed source localization provided whole-brain measures from 30 to 130ms.
•
Abnormalities were observed in superior temporal gyrus (STG) auditory areas in SZ.
•
Encoding abnormalities were also observed in frontal and supramarginal gyrus areas.
•
SZ shows abnormalities in multiple nodes of a distributed auditory network.

Introduction
Using electroencephalography (EEG) and magnetoencephalography (MEG), a now large number of studies show smaller 100ms auditory amplitudes in individuals with schizophrenia (SZ) than healthy controls (HC).
In a review of studies examining N1 and M100 in schizophrenia, Rosburg et al. (2008) concluded that 100ms auditory abnormalities are most commonly observed in studies using interstimulus intervals greater than 1s and that an increase in N1 amplitude by allocation of attention is often lacking in individuals with SZ.
Several large-sample studies provide examples.
Examining N1 activity in the standard paired-click paradigm, Turetsky et al. (2008) observed a small first and a normal second N1 click response in SZ (N=142) relative to HC (N=221).
Reduced N1 was also observed in the unaffected first-degree relatives of individuals with SZ without co-morbid psychiatric or substance use conditions, and N1 amplitude was observed to be a heritable measure and a better endophenotype than N1 gating.
In another recent large-N study, Smith et al. (2010) used simultaneous EEG and MEG to examine 100ms auditory processes in individuals with SZ (N=79) and HC (N=73) during a paired-click task.
Patients had larger N1 Cz and left and right superior temporal gyrus (STG) M100 ratio scores (second-click/first-click), with EEG and MEG ratio score group differences due to a smaller first click (S1) response in patients, suggesting a deficit in encoding auditory information rather than a deficit in filtering redundant information.
N1 (EEG) and M100 (MEG) are the most prominent deflections of the adult auditory event-related potential (ERP) or field (ERF) (Hari, 1990).
In an early study, Naatanen and Picton (1987) argued that the electric N1 reflects contributions from up to 6 distinct cortical areas: dipoles in or near the primary auditory cortex as well as prefrontal cortex (PFC) sources.
Later studies showed connections between STG and PFC.
For example, several studies have demonstrated bidirectional connections between STG and PFC in the rhesus monkey (Knight et al., 1999).
Combined tracing and immunohistochemistry studies have revealed that projections from PFC pyramidal neurons make synaptic contact with a subset of calbindin-positive GABAergic interneurons in auditory areas (Barbas et al., 2005).
Through these connections, PFC pyramidal neurons may modulate the excitability of microcircuits within the monkey's auditory belt and parabelt (Barbas et al., 2005).
Neural tracers infused into auditory cortex have also been found to emerge in PFC axonal terminal (Romanski, 2004; Romanski et al., 1999).
Finally, anatomical studies in humans have identified white-matter tracks connecting auditory cortex with lateral and medial PFC.
These observations have been corroborated via in vivo imaging (Catani et al., 2002).
Taken together, monkey and human studies support the hypothesis that PFC pyramidal neurons modulate the flow of information in auditory cortices by controlling the activity of GABAergic interneurons, which in turn modulate the excitability of STG pyramidal neurons (Barbas et al., 2005).
With regard to individuals with SZ, there is evidence of aberrant fronto-temporal connectivity: in a diffusion tensor imaging (DTI) study, Abdul-Rahman et al. (2012) showed that disruption of fronto-temporal white-matter tracks involving arcuate fasciculus may be associated with psychotic features and auditory hallucinations in SZ.
Although equivalent current dipole source localization techniques work well to examine 50 and 100ms STG activity (Edgar et al., 2003, 2008; Huang et al., 2003; Smith et al., 2010), equivalent current dipole techniques are likely less optimal in terms of localizing auditory activity in non-STG areas, because activity in non-STG areas is often distributed and thus non-dipolar.
The present study reports findings using a lead-field-based source localization method, Vector-based Spatio-temporal Analysis using L1-minimum norm (VESTAL; Huang et al., 2006), to examine auditory processes throughout the brain in HC and in individuals with SZ.
Given that our and others previous paired-click findings indicated group differences for the S1 but not the second click (S2)11
Within-group and between-group VESTAL group statistics for S2 are provided in Supplementary Figs. 1-3.
 (Smith et al., 2010; Turetsky et al., 2008), the present study focused on examining early S1 activity at 50ms and 100ms.
The following predictions were made:Hypothesis 1
STG activity would be observed in both groups, and VESTAL STG group differences would be analogous to those reported previous studies.
In particular, 100ms STG group differences would be observed bilaterally.
If 50ms group S1 differences were observed, the 50ms group differences would be left lateralized.
Hypothesis 2
Given studies indicating prefrontal activation during simple auditory tasks, frontal activation was expected in both groups.
Although prior literature does not provide evidence for making strong predictions about group differences in frontal activity, it was hypothesized that the spatial pattern of frontal activity would be different in patients and controls.
Although equivalent current dipole source localization techniques work well to examine 50 and 100ms STG activity (Edgar et al., 2003, 2008; Huang et al., 2003; Smith et al., 2010), equivalent current dipole techniques are likely less optimal in terms of localizing auditory activity in non-STG areas, because activity in non-STG areas is often and thus non-dipolar.
The present study reports findings using a lead-field-based source localization method, Vector-based Spatio-temporal Analysis using L1-minimum norm (VESTAL; Huang et al., 2006), to examine auditory processes throughout the brain in HC and in individuals with SZ.
Given that our and others previous paired-click findings indicated group differences for the S1 but not the second click (S2)1 (Smith et al., 2010; Turetsky et al., 2008), the present study focused on examining early S1 activity at 50ms and 100ms.
The following predictions were made:Hypothesis 1
STG activity would be observed in both groups, and VESTAL STG group differences would be analogous to those reported previous studies.
In particular, 100ms STG group differences would be observed bilaterally.
If 50ms group S1 differences were observed, the 50ms group differences would be left lateralized.
Hypothesis 2
Given studies indicating prefrontal activation during simple auditory tasks, frontal activation was expected in both groups.
Although prior literature does not provide evidence for making strong predictions about group differences in frontal activity, it was hypothesized that the spatial pattern of frontal activity would be different in patients and controls.
Methods and materials
Subjects
Nineteen patients with chronic SZ (14 males, mean age 40.31±11.7years) and 22 age-matched HC (15 males; mean age 34.95±10.2years) were recruited.
Selection criteria were (1) diagnosis of schizophrenia with no other Axis I diagnosis, determined by the Structured Clinical Interview for DSM-IV-Patient Edition (SCID-DSM-IV; American Psychiatric Association, 1994); (2) stable, continuous treatment with one antipsychotic medication for at least 3months; (3) no history of substance dependence (determined during the SCID-DSM-IV interview); (4) no history of alcohol or other substance abuse in the past 3months (determined during the SCID-DSM-IV interview); (5) no history of head injury with loss of consciousness for more than 5minutes; and (6) no psychiatric hospitalization in the last 3months.
As shown in Table 1, groups did not differ in age, education, or parental socioeconomic status (SES, Oakes and Rossi, 2003; scores derived from individual's income, education, and occupation information, with lower SES score indicating higher socioeconomic status).
Patients' SES was significantly lower than controls'.
Mean total scores of the Positive and Negative Syndrome Scale (PANSS) (Kay et al., 1987) were 20.00 for positive symptoms and 17.46 for negative symptoms (N=13; PANSS scores were not available in 6 subjects).
Additional recruitment procedures and additional information on inclusion and exclusion criteria are reported in Smith et al. (2010).
Five HC and 2 SZ were left-handed as assessed by the Waterloo Handedness Questionnaire (Bryden, 1977).
Patients with SZ were medicated and clinically stable without change in medications for at least one month before MEG.
In the patient group, 14 participants were treated with 2nd generation antipsychotics: 2 on aripiprazole, 5 on olanzapine, 3 on risperidone, 3 on quetiapine and 1 on ziprasidone.
Two participants were treated with 1st generation antipsychotic halperiodol.
Finally, 2 subjects were treated with both aripiprazole and clozapine and 1 with aripiprazole, clozapine, and halperiodol.
The average of Chlorpromazine equivalent dosage for all patients was 587mg/day (1 patient did not have medication dosage information).
Six patients with SZ and 2 HC were smokers.
Paired-click paradigm
The paired-click paradigm followed the protocol of Adler et al. (1993), in which 3ms binaural clicks were presented in pairs (S1 and S2) with 500ms inter-stimulus interval and with inter-trial interval jitter between 7 and 11s, averaging 9s.
Clicks were delivered through earphones placed in each ear canal.
The peak intensity of the click was presented 35dB above each subject's hearing threshold.
Presenting 150 click trials, the duration of the task was approximately 25minutes.
As previously noted, the present study examined only S1 activity at 50 and 100ms.
MEG and MRI data acquisition and coregistration
MEG data were recorded in a magnetically shielded room (Vacuumschmelze, Germany) using a 306-channel Vector-View MEG system (Elekta-Neuromag, Helsinki, Finland).
After a band-pass (0.1-330Hz) and 60Hz notch filter, MEG signals were digitized at 1000Hz.
Electro-oculogram (EOG) (vertical EOG on the upper and lower left sides) and electrocardiogram (ECG) (at the collarbone) were also obtained.
The subjects' head position was monitored using four HPI coils attached to the scalp.
Participants were asked to refrain from smoking for at least 1h before the recording session.
To ensure compliance, they were asked to report to the facility an hour before recording commenced, during which time participants were familiarized with equipment and procedures.
After the MEG session, structural magnetic resonance imaging (sMRI) provided T1-weighted, 3-D anatomical images using a 3T Siemens Trio scanner (voxel size 1×1×1mm3).
To coregister MEG and sMRI data, three anatomical landmarks (nasion and right and left preauriculars) as well as an additional 150+ points on the scalp and face were digitized for each subject using the Probe Position Identification (PPI) System (Polhemus, Colchester, VT).
The three fiducials were identified in the subject's sMRI, and a transformation matrix that involved rotation and translation between the MEG and sMRI coordinate systems was obtained by matching the 150+ points from the PPI measurements to the surfaces of the scalp and face from the sMRI.
Magnetic source analysis
MEG raw signals were first processed with Signal Space Separation (SSS; Taulu et al., 2004) using Maxfilter (Elekta MaxfilterTM; Elekta Oy).
SSS separates neuronal magnetic signals arising from inside the MEG sensor array from external magnetic signals arising from the surrounding environment to effectively reduce environmental noise and artifacts.
After SSS, S1 epochs 500ms pre-stimulus to 500ms post-stimulus were averaged.
Trials containing eye-blinks and large eye-movements were excluded.
On average, 103 trials were obtained for each subject, and there were no group differences in number of accepted trials (t(39)=0.30, p=0.77).
To calculate MEG forward solutions, a realistically shaped Boundary Element Method (BEM) head model was created from each subject's inner skull (Hamalainen and Sarvas, 1989), with the BEM mesh obtained from tessellating the inner skull surface from the MRI into ~6000 triangular elements with ~5mm size.
Vector-based Spatio-temporal Analysis using L1-minimum norm (VESTAL; Huang et al., 2006) provided source images for each subject.
VESTAL selects the source configuration that minimizes the absolute value of the source strength.
Both magnetometers and planar gradiometers were used in the source localization.
Whereas L1-minimum norm methods have been used in previous MEG studies (Auranen et al., 2005; Osipova et al., 2005; Pulvermuller and Shtyrov, 2003), a major limitation with previous L1-minimum norm routines has been instability in spatial localization and poor smoothness in reconstructed source time-courses (i.e., the time-course of one specific grid point can show substantial spiky-looking discontinuities).
This problem is also encountered in other focal localization methods using lead-field approaches.
In VESTAL, the temporal information in the data is used to enhance the stability of the reconstructed solution.
Since this approach makes no assumptions about the temporal dynamics of the sources, the approach can handle sources that are 100% correlated.
VESTAL also effectively obtains source strength and dipole orientation without iteration or choosing a pre-fixed dipole orientation for each grid node.
The technical details of VESTAL, in which VESTAL was tested with computer simulations and human data, are presented in Appendix A and in Huang et al. (2006).
Results show that VESTAL provides high spatial stability and continuous temporal dynamics, without compromising spatial or temporal resolution.
For group analyses, the following procedures were applied.
(1) T1-weighted sMRIs from each subject (Fig. 1A) were registered to MNI space (Montreal Neurological Institute, MNI-152 atlas as in Fig. 1B) using an affine transformation (FLIRT-FMRIB's Linear Image Registration Tool) (Jenkinson and Smith, 2001) in FSL (www.fmrib.ox.ac.uk/fsl/).
(2) The cortical (Fig. 1C) and subcortical masks with pre-defined brain regions from the standard atlas were transferred to the individual's headspace (Fig. 1D), using the inverse of the transformation obtained in the first step: the Harvard-Oxford Atlas, part of the FSL software with masks of 96 cortical gray-matter regions (48 regions in each hemisphere), 21 sub-cortical regions, and cerebellum, was used.
(3) The regional masks were down-sampled to a cubic source grid with voxels of 5mm per side (Fig. 1E).
(4) VESTAL MEG source imaging used the source grid from step 3.
This step permits group-based analyses.
In the shown example, MEG responses evoked by S1 localized to left and right Heschl's gyri (Fig. 1F).
(5) Finally, for regions of interest (ROIs), the source time course was obtained by summing activity from all ROI voxels.
Fig. 1H shows the time course from left Heschl's gyrus (dark blue region in Fig. 1C and D).
Prior to VESTAL analyses, a 5-55Hz bandpass filter was applied.
VESTAL analyses examined activity 30-130ms post-stimulus producing a 4D activation map (3D volumes across time) as well as a 2D source time-course matrix.
The average percent variance explained for gradiometer data using VESTAL was 95.81% for HC and 94.38% for SZ.
The average percent variance explained for magnetometer data using VESTAL program was 96.24% for HC and 93.17% for SZ.
There were no group differences in percent variance explained for gradiometer data (t(39)=1.16, p=0.25) or magnetometer data (t(39)=1.31, p=0.20).
Statistics
The present analysis examined 50 to 100ms S1 activity only in cortical regions using source strength from VESTAL volumes summed from 30 to 80ms and from 80 to 130ms.
Within-group t-tests compared 50 and 100ms activity to baseline, and between-group t-tests examined group differences.
For within-group analyses, spatial smoothing of sigma 2mm was applied.
Given that between group differences are likely smaller than within-group differences for pre- versus post-stimulus activity, spatial smoothing of sigma 5mm was applied for between-group analyses.
To control for multiple comparisons, false discovery rates (FDR; Benjamini, 2010; Benjamini and Hochberg, 1995) were computed for within- and between-group VESTAL statistical images.
An FDR of <1% was set for within-group analysis (FDR q<0.01), and an FDR of <5% was set for between-group analysis (FDR q<0.05), and only voxels that survived a threshold according to q<0.01 or q<0.05 were retained for statistical inferences.
Results
Within-group analyses
Fig. 2 shows pre- to post-stimulus 50 and 100ms maps for each group (FDR q<0.01).
STG and frontal activity was observed bilaterally in the HC and SZ groups at both 50 and 100ms.
In addition, activity in posterior superior frontal gyrus (SFG_p)/supplementary motor area (SMA) was observed in the right hemisphere for HC and in the left hemisphere for SZ.
Interestingly, right superior frontal gyrus (R-SFG) activity was observed only in SZ.
Between-group analyses
Group contrast maps at 50ms (Fig. 3) showed group differences in STG and non-STG regions (FDR q<0.05).
HC had stronger left and right STG (L-STG, R-STG), right inferior temporal gyrus (R-ITG), and left and right IFG (L-IFG, R-IFG) 50-ms activity than SZ.
SZ had stronger right superior frontal gyrus (R-SFG) and left supramarginal gyrus (L-SMG) 50ms activity than HC.
Similar to the 50ms group maps, group contrast maps at 100ms (Fig. 4) showed group differences in STG and non-STG regions (FDR q<0.05).
HC had stronger activity in L-STG, R-STG, R-ITG, R-IFG, and the posterior part of right SFG (R-SFG_p)/SMA than SZ at 100ms.
SZ had stronger activity in R-SFG, posterior part of left SFG (L-SFG_p)/SMA, and L-SMG than HC at 100ms.
Discussion
Present STG findings are consistent with theories postulating basic sensory processing abnormalities as central to schizophrenia (for a review, see Javitt, 2009).
As also detailed in Javitt (2009), other studies indicate a downstream consequence of early auditory abnormalities, such that basic auditory abnormalities in schizophrenia are associated with impaired performance on tests of attention (Smith et al., 2010) and tests of prosody (Leitman et al., 2005, 2006).
Finally, studies observing that 100ms responses are associated with decreased STG gray matter (Edgar et al., 2012), as well as N-methyl-d-asparate (NMDA) dysfunction (Javitt et al., 2000), suggest a biological mechanism for encoding abnormalities in schizophrenia.
As noted in the Introduction, Naatanen and Picton (1987) argued that the electric N1 reflects contributions from frontal sources as well as from primary auditory cortex.
Present results provide confirmation of these findings, with frontal activity observed in both groups (Fig. 2).
Findings are also consistent with imaging studies that have observed frontal activity during auditory tasks.22
Although many paired-click studies have examined the association between frontal activity and gating measures (e.g., second click divided by first click), studies examining associations between paired-click ratio scores and frontal activity but not also associations between first click activity and frontal activity are not discussed as findings between ratio scores and frontal activity are outside the scope of this study (e.g., EEG studies such as Williams et al. (2011) and fMRI studies such as Tregellas et al. (2007) and Mayer et al. (2009)).
 For example, examining auditory responses in patients with epilepsy using intracranial microelectrode grids, Korzyukov et al. (2007) detected 50ms temporal and frontal auditory activity, findings consistent with earlier corticography studies suggesting frontal contributions to P50 (Grunwald et al., 2003).
Boutros et al. (2011) also detected 100ms temporal and frontal auditory activity using subdural electrodes.
Similar to present findings in controls, Boutros et al. observed activity in the posterior part of STG and in left ventral prefrontal cortex (more exact comparisons between the two studies in terms of frontal activity are difficult, as many subjects in Boutros et al. did not have electrodes placed in anterior frontal regions).
Boutros et al. (2011) also observed 100ms auditory activity in middle temporal gyrus, parietal, cingulate, and occipital regions.
Activity observed in similar but not identical locations in these other areas in the present study may be due to the significant latency variability Boutros et al. (2011) observed in several regions.
Variability across subjects in the location of activation may also account for study differences.
In the present study, two abnormalities in frontal activity were observed in SZ.
First, stronger right 50 and 100ms IFG activity was observed in HC than SZ.
Second, stronger right SFG 50 and 100ms activity was observed in SZ than HC (Figs.
3 and 4).
This pattern of decreased inferior frontal activity but increased superior frontal activity in SZ suggests abnormal activation of fronto-temporal auditory networks in SZ.
Recent studies have shown functionally distinct auditory pathways in humans in particular distinct 'what/where' auditory system pathways analogous to the 'what/where' visual system pathways.
For example, Romanski et al. (1999), combining microelectrode recordings with neural tracers infused in auditory cortex of rhesus monkeys, found paths from posterior and anterior auditory cortex that differentially targeted non-spatial (ventral) and spatial (dorsal) frontal areas.
Romanski et al. (1999) hypothesized these to be analogous to the 'what' and 'where' visual pathways.
These non-human primate findings are consistent with language processing models, with a ventral stream mapping acoustic speech to conceptual and semantic representations and a dorsal stream mapping phonological information within the frontal articulatory system (Hickok and Poeppel, 2007; McClelland and Rogers, 2003).
Present results suggest that HC activate the ventral 'what' auditory pathway more strongly than SZ.
Ventral PFC (orbitofrontal cortex) recordings indicate that cells in this region are responsive to the features of complex sounds (Romanski, 2004).
In addition, combining fMRI and DTI to identify anatomical pathways associated with language, Saur et al. (2008) found that linguistic processing of sound to meaning requires interaction between temporal lobe and ventrolateral PFC via the ventral route, whereas the dorsal route is involved primarily in the sensory-motor mapping of sound to articulation.
These findings are consistent with a role for ventrolateral PFC auditory neurons in analyzing the features of auditory objects.
Although the paired-click task is passive, an explanation of the present findings is that the 'what' stream (i.e., the auditory STG to IFG pathway) is activated in HC even when passively encoding auditory stimuli, with decreased activation of this pathway in SZ consistent with a deficit in encoding auditory stimuli (resulting in decreased STG auditory response), with such encoding deficits possibly leading to impairments in higher-order cognitive processes (Smith et al., 2010).
In contrast, given abnormal activation in individuals with SZ in right medial SFG areas, individuals with SZ may abnormally activate the dorsal 'where' pathway (or perhaps language pathways involved in mapping phonological information with the frontal articulatory system).
Decreased STG and PFC gray matter in SZ (Gur et al., 2000; Mitelman and Buchsbaum, 2007; Olabi et al., 2011; Shenton et al., 2001; Thoma et al., 2004) may account for the decreased STG and PFC activity observed in the present SZ sample.
Gray-matter reductions in SZ are thought to be due to elimination of the neuropil between neuron bodies (the reduced neuropil hypothesis) (Selemon and Goldman-Rakic, 1999).
Sweet et al. (2003) found that, within auditory cortex, mean somal volumes of deep layer 3 pyramidal cells in BA 41 and 42 were reduced in SZ, and Sweet et al. (2007) observed reduced axon terminal densities in feed-forward auditory pathways.
A combined MEG and proton magnetic resonance spectroscopy study showed that the number and integrity of neurons (assessed via auditory cortex N-acetylaspartate) and the density and functional integrity of cell membranes (assessed via auditory cortex choline-containing compounds) are associated with M100 source strength (Soros et al., 2006).
Such abnormalities, perhaps leading to an abnormal spread of activity within cortical auditory areas after auditory stimulation, may explain the reduced STG and PFC activity observed in SZ.
In addition to STG and frontal group differences, the present study found group differences in left SMG (Figs.
3 and 4).
SMG is located at the temporoparietal junction and links posterior auditory cortex with parietal regions via an auditory dorsal pathway (Hickok and Poeppel, 2000), a finding that again provides evidence that individuals with SZ abnormally engage fronto-temporal auditory dorsal pathways when processing auditory information.
Previous studies have shown that enhanced SMG activity is associated with auditory hallucinations.
Lewis-Hanna et al. (2011) found that healthy controls who experienced auditory hallucinations showed greater left SMG activity than controls not experiencing auditory hallucinations.
Elevated left SMG activity has been observed during auditory hallucinations in SZ (Diederen et al., 2012; Sommer et al., 2008), with the severity of auditory hallucinations in SZ associated with volume loss in left SMG as well as in left Heschl's gyrus and right IFG (Gaser et al., 2004).
In the present study, post hoc analyses showed no associations between SMG activity and PANSS measures of auditory hallucinations.
As the individuals with SZ recruited for this study were stable and mostly likely not hallucinating at the time of the MEG scan, this may explain the failure to observe SMG and PANSS associations.
Finally, and unexpectedly, posterior SFG/SMA activity was observed in both groups.
A review of the literature indicates, however, that SMA areas are part of the auditory-motor integration network, important for speech production (Hickok and Poeppel, 2000).
Although previous studies suggest involvement of SMA during auditory tasks, replication of this finding is needed.
A limitation of the present study is that most subjects were chronic patients and all patients were on medication.
As such, it is not possible to determine whether the observed abnormalities are observed only in chronic patients or whether group differences were due to medication.
Several studies, however, show that 100ms abnormalities in schizophrenia are present at first onset as well as in first-degree relatives (Turetsky et al., 2008), suggesting that auditory abnormalities in SZ are not due to medication.
In addition, in a review of 100ms auditory studies Rosburg et al. (2008) concluded that medication did not seem to account for group differences in N100 activity.
In sum, present findings indicate that early auditory encoding abnormalities in SZ are not limited to STG and that there are abnormalities in multiple nodes of a concurrently activated auditory network.
Present findings suggest that individuals with SZ show decreased activation in a temporal to frontal ventral pathway (a possible auditory 'what' pathway) as well as abnormally increased activation in a temporal to frontal dorsal pathway (a possible auditory 'where' pathway).
As detailed earlier in the Discussion, although the paired-click task is passive, an explanation of the present findings is that the 'what' stream (the auditory STG to IFG pathway) is activated in HC even when passively encoding auditory stimuli, with decreased activation of this pathway in SZ consistent with a deficit in encoding auditory stimuli (resulting in decreased STG auditory response).
The following are the supplementary data related to this article.Supplementary Fig. 1
Within-group VESTAL statistics for 50 (left panel) and 100ms (right panel) S2 activity showing left and right STG (L-STG, R-STG) and frontal regions (L-Frontal, R-Frontal) activated areas for HC (top panel) and SZ (bottom panel) (thresholded at FDR q<0.01).
Supplementary Fig. 2
Between-group analyses for 50ms S2 activity.
Activation clusters in yellow/red (thresholded at FDR q<0.05) show stronger L-STG, R-STG, and frontal medial cortex (FMC) activity in HC than SZ (HC>SZ).
Activation clusters (thresholded at FDR q<0.05) in blue show stronger activity in left middle frontal gyrus (L-MFG), L-SMG, right lateral occipital cortex (R-LOC), right frontal pole (R-FP), and right postcentral gyrus (R-PG) in SZ than HC (SZ>HC).
Supplementary Fig. 3
Between-group analyses for 100ms S2 activity.
Activation clusters in yellow/red show stronger activity in L-STG, R-STG, and FMC in HC than SZ (HC>SZ).
Activation clusters in blue show stronger activity in L-SMG, L-MFG, R-LOC, R-PG in SZ than HC (SZ>HC).
Supplementary data to this article can be found online at http://dx.doi.org/10.1016/j.nicl.2013.05.002.
Acknowledgments
This research was supported by grants from the National Institute of Mental Health (R01 MH65304 to Dr. José M. Cañive, K08 MH085100 to Dr. J.
Christopher Edgar), a VA Merit grant (VA Merit CSR&D: IIR-04-212-3 to Dr. José M. Cañive), and University of California at San Diego, Merit Review Grant from the Department of Veterans Affairs to Dr. Mingxiong Huang.
The authors would like to thank the participants who enrolled into this study and to Megan Schendel and Garrett Hosack, who helped with the data collection; and Lawrence Calais, Gloria Fuldauer, and Nickolas Lemke for their help with subject recruitment and administrative support related to this project.
Declaration of interest
The authors have no conflicts of interest to report.
In VESTAL (Huang et al., 2006), we take the lead-field based MEG source imaging approach and divide the source space (the brain volume or just the cortex) into a grid of a large number of dipole locations.
The M×N sensor waveform matrix B(t)=[b(t1),b(t2),…,b(tN)] contains MEG data where m is the number of MEG sensors and s is the number of time points, b(ti) is an M×1 vector of the MEG measurements at given time point.
For each column of B, we have:(1)Bt=GQt+Noisetwhere G is an M×2P gain (lead-field) matrix calculated from MEG forward modeling for the pre-defined source grid with P dipole locations, with each dipole location having two orthogonal orientations (i.e., θ and ϕ).
Q(t)=[q(t1),q(t2),…,q(tN)] is a 2P×N source time-course matrix.
In the spherical MEG forward head model, θ and ϕ represent the two tangential orientations for each dipole location.
In the present study, a realistic MEG forward model using the boundary element method (BEM) is used to calculate G (Huang et al., 2007; Mosher et al., 1999), and in BEM the θ and ϕ-orientations are obtained as the two dominant orientations from the singular-value decomposition (SVD) of the M×3 lead-field matrix for each dipole, as previously documented (Huang et al., 2006).
The BEM mesh was obtained from tessellating the inner skull surface from the MRI into ~6000 triangular elements with ~5mm size.
The noise term in Eq.
(1) is assumed to be Gaussian white noise.
If correlated noise exists, pre-whitening procedures can be applied (Huang et al., 2006; Mosher et al., 1999; Sekihara et al., 1997).
The inverse solution in Eq.
(1) obtains the source time-courses Q(t) for given MEG sensor wave-forms B(t).
In general, for each time-sample, since the number of unknown parameters is far greater than the number of sensor measurements (i.e. 2P≫M), MEG source imaging is dealing with a highly under-determined problem, and there are a large number of solutions that fit the data.
To reduce the ambiguity, additional constraints (source models) are needed.
The conventional minimum L1-norm solution selects the source configuration that minimizes the absolute value of the source strength.
Let G=USVT be the singular value decomposition of the gain matrix, the minimum L1-norm q that meets the following condition (Sekihara et al., 1999):(2)minwTq,subjecttoconstraintsSngVngTq≃UngTbwhere Sng, Ung, and Vng contain the ng largest singular values and the associated singular vectors, respectively.
In Eq.
(2), w is an 2P×1 optional weighting vector chosen to remove potential bias towards grid nodes at the superficial layer and it is usually taken to be the column norm of the G matrix (Matsuura and Okabe, 1997; Uutela et al., 1999) or a Gaussian function (Ioannides et al., 1993).
A major limitation with previous L1-minimum norm routines has been instability in spatial localization and poor smoothness in reconstructed source time-courses.
The time-course of one specific grid point can show substantial spiky-looking discontinuities.
This problem is also encountered in other focal localization methods using lead-field approaches.
To increase spatial and temporal stability, we developed a spatial-temporal vector-based minimum L1-norm solution (i.e., VESTAL).
The idea was based on a principle of MEG physics, which states that the magnetic waveforms in the sensor-space are linear functions of the dipole time-courses in the source-space.
If we perform singular value decomposition for the M×N MEG sensor waveform data matrix:(3)B=UBSBVBTone can see that all temporal information in the MEG sensor waveform can be represented as a linear combination of the singular vectors in the matrix VB.
Since MEG sensor waveforms are linear functions of the underlying neuronal source time-courses, the same signal sub-space that expands the temporal dimension of B should also expand the temporal dimension of the 2P×N source time-course matrix Q=[q(t1),q(t2),…,q(tN)] estimated from the vector-based minimum L1-norm solution for s time points.
In the present study, Q was solved using SeDuMi package (http://sedumi.ie.lehigh.edu/).
By projecting Q towards VB we can ensure that source time-courses matrix Q and sensor waveform matrix B share the same temporal information as requested by the MEG physics:(3)QVESTAL=QP||where the projection matrix P||=VBVBT is constructed using the dominant (signal-related) temporal singular vectors (subspace) of the sensor waveforms.
We called QVESTAL the Vector-based Spatial-temporal Analysis using L1-minimum-norm (VESTAL).
In VESTAL, the temporal information in the data is used to enhance the stability of the reconstructed solution.
Since this approach makes no assumptions about the temporal dynamics of the sources, the approach can handle sources that are 100% correlated.
More technical details of VESTAL, in which VESTAL was tested with computer simulations and human data, are presented in Huang et al. (2006).
Results show that VESTAL provides high spatial stability and continuous temporal dynamics, without compromising spatial or temporal resolution.

The mechanical properties and toughening mechanisms of an epoxy polymer modified with polysiloxane-based core-shell particles

Abstract
An epoxy resin, cured using an anhydride hardener, has been modified by the addition of pre-formed polysiloxane core-shell rubber (S-CSR) particles with a mean diameter of 0.18 μm.
The glass transition temperature, Tg, of the cured unmodified epoxy polymer was 148 °C, and this was unchanged after the addition of the S-CSR particles.
The polysiloxane rubber particles had a Tg of about -100 °C.
Atomic force microscopy showed that the S-CSR particles were well-dispersed in the epoxy polymer.
The addition of the S-CSR particles reduced the Young's modulus and tensile strength of the epoxy polymer, but at 20 °C the fracture energy, GIc, increased from 117 J/m2 for the unmodified epoxy to 947 J/m2 when 20 wt% of the S-CSR particles were incorporated.
Fracture tests were also performed at -55 °C, -80 °C, and -109 °C.
The results showed that the measured fracture energy of the S-CSR-modified epoxy polymers decreased significantly below room temperature.
For example, at -109 °C, a fracture energy of 481 J/m2 was measured using 20 wt% of S-CSR particles.
Nevertheless, this value of toughness still represented a major increase compared with the unmodified epoxy polymer, which possessed a value of GIc of 174 J/m2 at this very low test temperature.
Thus, a clear fact that emerged was that the addition to the epoxy polymer of the S-CSR particles may indeed lead to significant toughening of the epoxy, even at temperatures as low as about -100 °C.
The toughening mechanisms induced by the S-CSR particles were identified as (a) localised plastic shear-band yielding around the particles and (b) cavitation of the particles followed by plastic void growth of the epoxy polymer.
These mechanisms were modelled using the Hsieh et al. approach [33,49] and the values of GIc of the S-CSR-modified epoxy polymers at the different test temperatures were calculated.
Excellent agreement was found between the predictions and the experimentally measured fracture energies.
Further, the experimental and modelling results of the present study indicated that the extent of plastic void growth was suppressed at low temperatures for the S-CSR-modified epoxy polymers, but that the localised shear-band yielding mechanism was relatively insensitive to the test temperature.
Graphical abstract
Highlights
•
An epoxy polymer has been modified with polysiloxane core-shell rubber particles.
•
The epoxy had Tg of about 148 °C and the particles had a Tg of about -100 °C.
•
AFM showed that the particles were well-dispersed in the epoxy polymer.
•
The addition of 20 wt% particles increased GIc from about 120 J/m2 to 950 J/m2.
•
The particles also significantly toughened the epoxy polymer even at about -100 °C.

Introduction
Epoxy polymers are a class of high-performance thermosetting polymers which are widely used for the matrices of fibre-reinforced composite materials and as adhesives.
They are known for their excellent engineering properties, such as high modulus, low creep, high strength, and good thermal and dimensional stabilities.
However, epoxy polymers have inherently low toughness and impact resistance due to their highly crosslinked structure.
This structure leads to brittle behaviour and causes the polymers to suffer from relatively poor resistance to crack initiation and growth.
To improve the toughness of epoxy polymers, it has been established that the incorporation of a second micro-phase of a dispersed rubber, e.g.
Refs.
[1-5], or a thermoplastic polymer, e.g.
Ref.
[6-8], can increase the toughness.
Here the rubber or thermoplastic particles are typically about 0.1-5 μm in diameter with a volume fraction of about 5-20%.
The particles are typically well-dispersed, and formed by reaction-induced phase-separation.
However, the particle size is difficult to control as it is dependent on the curing conditions, and hence cannot be varied systematically without changing the properties of the epoxy polymer.
An alternative route to increase the toughness of epoxy polymers is to use core-shell rubber (CSR) particles.
These particles comprise a soft rubbery core within a harder shell.
The particles are typically formed by emulsion polymerisation, and then dispersed in the epoxy resin.
Hence, it is readily possible to produce particles with a controlled particle size, unlike with phase-separating rubbers.
A range of core and shell materials may be used, and multilayer particles are common [9].
The shell is chosen to be compatible with the epoxy polymer, and poly(methylmethacrylate), which is sometimes functionalised, is often used.
Typical core materials include polybutadiene [10] and acrylate-polyurethane rubbers [11].
They have been shown to increase the toughness of both bulk polymers and fibre composites, e.g.
Refs.
[12-15].
Hayes and Seferis [16] have reviewed the use of CSR particles in thermoset polymers and composites; this review also discusses some of the other properties that can be affected by the incorporation of CSR particles.
However, although the properties of these rubber-toughened epoxy polymers have been investigated extensively, only a few studies have reported the low-temperature performance of these rubber-toughened epoxies.
In general, the literature [5,17-19] reports that the fracture toughness of rubber-toughened epoxy polymers decreases as the temperature decreases, except at very low temperatures (possibly having passed the β-transition of the polymer) where the fracture toughness appears to increase somewhat again.
It is generally accepted that the major toughening mechanism of rubber-toughened epoxies is based on a series of deformation processes, namely (a) localised plastic shear-band yielding around the rubber particles and (b) cavitation of the S-CSR particles followed by plastic void growth of the epoxy polymer.
It has been noted [1,3,5] that this second mechanism needs to be operative in order to typically achieve major increases in the toughness of the rubber-particle-modified epoxy polymer.
This suggests that using rubber particles with a relatively very low glass transition temperature, Tg, may enable cavitation of the rubber particles at very low test temperatures, and as a result allow plastic void growth of the epoxy polymer even at relatively low temperatures.
Hence, the present work used pre-formed polysiloxane core-shell rubber (S-CSR) particles which possess a relatively very low Tg of -100 °C to toughen an anhydride-cured epoxy polymer.
The room-temperature and low-temperature mechanical properties and fracture energy of these S-CSR particle-toughened epoxy polymers were determined.
Also, the blend morphology, structure/property relationship, and thermal-mechanical behaviour of the modified epoxy polymers were ascertained.
Further, the toughening mechanisms involved were also identified, and analytical models were used to predict the modulus, yield stress and fracture energy.
Experimental
Materials
An anhydride-cured epoxy polymer was used.
The epoxy resin was a standard diglycidyl ether of bisphenol-A (DGEBA, Araldite LY556) with an epoxide equivalent weight (EEW) of 185 g/eq, supplied by Huntsman, UK.
The curing agent was an accelerated methylhexahydrophthalic acid anhydride (Albidur HE600) with an anhydride equivalent weight (AEW) of 170 g/eq, supplied by Evonik Hanse, Germany.
The polysiloxane core-shell rubber (S-CSR) particles used were supplied as a masterbatch of particles pre-dispersed at 40 wt% in a DGEBA also by Evonik Hanse.
The rubber particles possessed a crosslinked polysiloxane core with a glass transition temperature, Tg, of about -100 °C.
The shell consisted of a very thin skin of epoxy-functional molecules which had been grafted onto the crosslinked core.
Up to 20 wt% of the S-CSR particles was used.
Bulk plates of the unmodified and S-CSR-modified epoxy polymers were prepared.
To vary the particle content, the S-CSR-modified resins were mixed with DGEBA to give the required concentration of S-CSR particles.
The value of the EEW of the blend was calculated and a stoichiometric amount of the curing agent was added.
The mixture was stirred, degassed and poured into release agent-coated (Frekote 700NC, Henkel, UK) steel moulds.
The plates of the epoxy polymers were cured for 1 h at 120 °C followed by 2 h at 160 °C.
Dynamic-mechanical thermal analysis
The glass transition temperature, Tg, of all the bulk samples was measured using dynamic-mechanical thermal analysis (DMTA) with a Q800 DMA from TA Instruments, UK.
A double-cantilever mode at 1 Hz was employed using test specimens 60 × 10 × 3 mm3 in size.
The temperature range used was -100 °C to 200 °C with a heating rate of 4 °C/min.
The value of Tg was determined at the peak value of tan δ.
The number average molecular weight between cross-links, Mnc, was also calculated from the equilibrium modulus in the rubbery region, Er, using [20](1)Mnc=qρRT/Erwhere T is the temperature in K at which the value of Er was taken, ρ is the density of the epoxy at the temperature T, the term R is the universal gas constant, and q is the front factor.
As the density of the epoxy was only measured at room temperature, the value of the front factor, q, was taken to be 0.725, as in previous work [21].
The density, ρ, of the epoxy was measured at room temperature according to BS ISO 1183-1 Method A [22] to be 1.20 g/m3 at 20 °C.
Mechanical properties
Dumbbell specimens with a gauge length of 25 mm were machined from the bulk plates of the epoxy polymers.
Uniaxial tensile tests were conducted in accordance with the BS ISO 527 Standard [23,24], using an Instron, UK, 5584 universal testing machine.
A displacement rate of 1 mm/min and a test temperature of 20 °C were used.
The displacement over the gauge length of the samples was measured using an Instron, UK, 2620-601 dynamic extensometer.
The maximum tensile stress for each sample was recorded, and the elastic modulus, E, was calculated between strains of 0.05% and 0.25%.
At least five replicate specimens were tested for each formulation.
Plane-strain compression (PSC) tests were performed using bulk samples at a range of temperatures from -109 °C to 20 °C to obtain the yield stress and the high-strain behaviour, after Williams and Ford [25].
An Instron 5585H universal testing machine was used with a constant displacement rate of 0.1 mm/min.
This displacement rate was adopted to match the strain rate from the tensile tests.
Samples with a size of 40 × 40 × 3 mm3 were used, and loaded in compression between two parallel 12 mm wide dies.
A minimum of two specimens were tested for each formulation at temperatures of 20 °C and -55 °C, but only one sample was tested for each formulation at temperatures lower than -55 °C, due to the difficulties with performing these tests at such low temperatures.
The results were corrected by subtracting the compliance of the testing machine and compression rig.
Based on the von Mises criterion, the true compressive stress, σc, was calculated using [25](2)σc=(32)σEwhere σE is the engineering stress.
The compressive true strain, γf, at failure was calculated using(3)γf=(23)ln(BcB)where Bc is the compressed thickness and B is the initial thickness.
One of the specimens of each formulation tested at room temperature was only loaded up to the yield point and then sectioned to study the possible formation of plastic shear-bands during the test.
Fracture tests
Single-edge notch three-point bending (SENB) tests were conducted in accordance with BS ISO 13586 [26], using an Instron 3369 universal testing machine equipped with an Instron 2620-601 dynamic extensometer.
The SENB samples were machined from the bulk plates, and the pre-cracks were introduced by tapping a liquid nitrogen chilled razor blade into the notch.
The lengths of the pre-cracks were measured using a Nikon, UK, SMZ800 stereo-optical microscope.
The tests were conducted at temperatures from -109 °C to 20 °C with a constant displacement rate of 1 mm/min.
All the specimens failed by unstable crack growth.
The fracture toughness, KIc, values of the samples were calculated using(4)KIc=pBW1/2f(α/W)where p is the critical load, B is the sample thickness, W is the specimen width, α is the average pre-crack length, and f(α/W) is the non-dimensional shape factor [26].
The fracture energy, GIc, was calculated using [26](5)GIc=KIc2E(1-ν2)where E is the tensile modulus of the polymer, and ν is the Poisson's ratio.
A value of ν = 0.35 was used, which is typical for epoxy polymers [27].
Double-notched four-point bending (DN-4PB) tests were conducted to investigate the plastic deformation zone ahead of the sub-critically loaded crack tip.
This method allows the contribution and the sequence of the toughening mechanisms to be observed.
The DN-4PB tests were performed as described by Sue and Yee [28].
The samples were machined from the bulk plates and the pre-cracks in the samples were produced by tapping a liquid nitrogen chilled razor blade into the notches.
An Instron 5584 universal testing machine was used to load the specimens in four-point bending, at a constant displacement rate of 1 mm/min and a temperature of 20 °C.
Care was taken that the four loading-points contacted the specimens simultaneously in the tests.
After fracture occurred, i.e. from one of the two pre-cracks, the plastic zone at the tip of the other sub-critically loaded crack was sectioned and examined using optical microscopy.
Microscopy studies
Atomic force microscopy (AFM) was performed using a MultiMode scanning probe microscope from Veeco, UK, equipped with a NanoScope IV controller and an 'E' scanner, to obtain the polymer morphology.
The smooth surface of the samples was prepared using a PowerTome XL ultramicrotome from RMC, UK.
Silicon probes were used in the tapping mode.
Both height and phase images were captured at 512 × 512 pixel resolution, at a scan speed of 1 Hz.
For the phase images, which are sensitive to viscoelastic properties, the apparent hardness of the material is shown by the colour, where the harder phases are brighter [29].
A Leo 1525 (Zeiss, Germany) scanning electron microscope equipped with a field-emission gun (FEG-SEM) was used to obtain high resolution images of the fracture surfaces, using an accelerating voltage of 5 kV.
All the samples were sputter-coated with a thin layer of chromium to prevent charging.
The plastic deformation zone ahead of the crack tip in the DN-4PB specimens was investigated using transmission optical microscopy, using an AXIO microscope (Zeiss, Germany).
Samples were cut using an Accutom-5 precision cutter (Struers, UK), equipped with an E0D15 diamond blade, from the central, plane-strain, region perpendicular to the fracture plane and parallel to the crack direction.
The samples were mounted onto glass microscope slides using a transparent adhesive (Araldite 2020, Huntsman, UK).
The samples were then ground and polished to a nominal thickness of 100 μm for microscopic observation between crossed polarisers.
Results and discussion
Microstructure studies
Atomic force microscopy of the unmodified epoxy polymer showed that a homogeneous thermoset was formed, see Fig. 1(a).
AFM of the S-CSR-modified polymers showed that the S-CSR particles were well-dispersed in the epoxy polymer, see Fig. 1(b-d).
The particles are spherical, although the microtoming process can cause residual compression in the samples so that they can appear oval in the micrographs.
The range of diameters of the S-CSR particles was measured, using image analysis software, to be between 0.12 μm and 1.40 μm, with a standard deviation of ±0.03 and ±0.15 μm in the lower and upper bounds, respectively.
The mean diameter of the S-CSR particles was 0.18 μm.
Fig. 1(b) also shows a ring of lighter colour surrounding the darker soft cores.
This would appear to be the shell of the S-CSR particles, and the phase image indicates that this is slightly harder than the epoxy polymer.
This shell was measured to have a mean thickness of between 20 nm and 40 nm.
Glass transition temperature and viscoelastic properties
The glass transition temperatures, Tg, of the unmodified and the S-CSR-modified epoxy polymers were measured using DMTA, and the data are summarised in Table 1.
The value of the Tg of the unmodified epoxy polymer was 148 °C, and the storage modulus, G′, was 2.73 GPa at 20 °C, see Fig. 2.
The epoxy was calculated, using equation (1), to have a number average molecular weight, Mnc, between 277 g/mol and 417 g/mol, which indicates that this polymer has an intermediate crosslink density for epoxy polymers.
The addition of the S-CSR particles was found to have no significant effect on Tg of the epoxy polymer when the experimental uncertainty of about ±2 °C is considered, see Table 1.
For example, the peaks of tan δ for the unmodified epoxy polymer and the 10 wt% S-CSR-modified epoxy polymer in Fig. 2 are very close to each other and almost overlap.
The storage modulus was found to decrease linearly with the addition of S-CSR particles as expected, since the presence of the soft rubbery particles will reduce the stiffness of the relatively rigid epoxy polymer [12,30,31].
These results confirm that the S-CSR particles remain phase-separated and do not plasticise the epoxy polymer.
The presence of the soft polysiloxane rubber particles decreases the stiffness of the modified epoxy polymers by an approximately constant amount over the whole temperature range below the Tg of the epoxy as shown in Fig. 2.
Tensile properties
A tensile modulus of 3.19 ± 0.10 GPa was measured for the unmodified epoxy polymer.
The modulus decreased approximately linearly with increasing CSR content to 1.96 ± 0.08 GPa when 20 wt% of S-CSR particles were added, see Table 1.
Similar results were reported by Giannakopoulos et al. [30] using the same formulation of epoxy polymer but with different CSR particles.
The values of the measured moduli of the S-CSR-modified epoxy polymers may be compared to existing theoretical models.
Various models have been developed to predict the modulus of a particulate-filled polymer but in the present study only the Halpin-Tsai and the Lewis-Nielsen models were used, as these have been found to be the more representative models in previous work [30,32,33].
The Halpin-Tsai model [34] predicts the modulus of a reinforced polymer as a function of the modulus of the bulk polymer, Em, and of the fillers, Ef.
The modulus of the S-CSR particle-modified epoxy polymers may be predicted using(6)E=1+ζηVf1-ηVfEmwhere ζ is the shape factor, Vf is the volume fraction of the particles, and(7)η=(EfEm-1)(EfEm+ζ)
The shape factor of the Halpin-Tsai model is a function of the aspect ratio (w/t) of the particles, where w and t are the length and thickness of the particles respectively.
Halpin and Kardos [34] recommended that a shape factor of ζ = 2w/t should be used for calculating the modulus with filler particles aligned with the loading direction, and ζ = 2 with fillers perpendicular to the loading direction.
In the present study, the polysiloxane CSR particles are spherical with w/t = 1, so ζ = 2 was used for the calculation of the modulus.
A value of Ef = 2.5 MPa was used for the modulus of the polysiloxane rubber [35].
The basic Lewis-Nielsen model, using the work of McGee and McCullough [36], takes into account the effect of the adhesion between the polymer and the fillers.
This model gives the predicted modulus, E, of the S-CSR particle-modified epoxy polymers using(8)E=1+(kE-1)βVf1-βμVfEmwhere kE is the generalised Einstein coefficient, and β and μ are constants.
The constant β depends on the relative modulus of the polymeric matrix, Em, and the filler, Ef, and is given by(9)β=(EfEm-1)(EfEm+(kE-1)).
Note that β is identical to η in the Halpin-Tsai model if a shape factor of ζ = (kE - 1) is used.
The constant μ depends on the maximum volume fraction of the filler, Vmax, and is given by(10)μ=1+(1-Vf)Vmax[VmaxVf+(1-Vmax)(1-Vf)].
Nielsen and Landel [37] have tabulated values of Vmax for a range of particle shapes and types of packing.
The AFM studies showed that the S-CSR particles are non-agglomerated and randomly dispersed, so the value of Vmax = 0.632 for random close-packed and non-agglomerated spheres is suitable [37].
The value of the generalised Einstein coefficient, kE, varies with the Poisson's ratio of the polymeric matrix and the degree of the adhesion of the polymer to the particles.
Hence, in the present study, for a polymeric matrix with v = 0.35 and no slippage at the interface between the polymeric matrix and the particles (as no debonding was observed), a value of kE = 2.167 was used [37].
The predictions of the Halpin-Tsai and the Lewis-Nielsen models are compared with the experimental data in Fig. 3, and the agreement is very good.
(The volume fraction of the S-CSR particles in the epoxy polymer was measured using image analysis from the atomic force micrographs).
The experimental data generally lie between the Halpin-Tsai and Lewis-Nielsen predictions, where the Halpin-Tsai model gives the upper bound and the Lewis-Nielsen model gives the lower bound.
However, the data lie close to and just above the Lewis-Nielsen predictions, confirming that slippage does not occur at the interface.
Similar results were observed by Giannakopoulos et al. [30].
The tensile strength of the unmodified epoxy polymer was measured to be 41 MPa.
This is surprisingly low, as tensile strengths of approximately 81-83 MPa are typically measured for this epoxy polymer [30,38].
However, such unmodified thermoset polymers are extremely sensitive to the presence of surface defects, and it is likely that such imperfections caused these relatively low values to be measured.
Indeed, for the formulation containing 2 wt% of polysiloxane CSR particles, a mean tensile strength of 85 MPa was measured, which is close to the value reported by Giannakopoulos et al. [30] using different CSR particles in the same formulation of epoxy polymer.
The addition of the S-CSR particles reduced the tensile strength approximately linearly with increasing particle content.
It is well known that the addition of particles can reduce the tensile strength of thermoset polymers [12,30] due to the stress concentration effect of the particles.
The lowest tensile strength measured was 48 MPa for the 20 wt% S-CSR particle-modified epoxy polymer.
Compressive properties
Room-temperature tests (20 °C)
The mean room-temperature values of the compressive true yield stress, σyc, compressive true fracture stress, σfc, and compressive true fracture strain, γf, are summarised in Table 2.
The tensile yield stress is calculated from the measured compressive yield stress [39].
The addition of S-CSR particles reduces the compressive true yield stress, as expected, due to the relative softness of the polysiloxane rubber.
The values decreased approximately linearly with increasing S-CSR particle content, see Fig. 4.
At 20 °C, a value of 111 MPa was measured for the unmodified epoxy polymer, which also reveals that the unmodified epoxy should have the highest strength, if the effect of defects is excluded, when the test is conducted in uniaxial tension.
The lowest value of the compressive true yield stress was measured to be 63 MPa for the 20 wt% S-CSR-modified epoxy polymer.
Plots of representative compressive true stress-strain curves of the unmodified epoxy polymer and the modified epoxy polymers with 6 wt% and 20 wt% S-CSR particles are shown in Fig. 5.
The plane-strain compression tests demonstrate that the addition of the S-CSR particles suppressed the strain-softening that occurred after yield of the epoxy.
Indeed, the strain-softening zone in the compressive true stress-strain diagram of the modified epoxy polymers reduced gradually with increasing S-CSR particle content and disappeared completely for the addition of 20 wt% of S-CSR, see Fig. 5.
This can be explained by the S-CSR particles suppressing macroscopic inhomogeneous shear-band deformation and strain-softening by the promotion of the initiation and growth of highly localised plastic shear-bands.
This arises from the relatively high stress concentrations that develop around the equatorial plane of the relatively soft rubbery particles [39].
Such localised shear-bands have been observed experimentally [12] and in finite-element analysis modelling studies [39,40].
Due to the good dispersion of the S-CSR particles, these stress concentrations lead to highly localised shear-bands, and therefore the macroscopic shear-bands and the associated strain-softening gradually disappear with increasing S-CSR content as the localised shear-bands merge to give a diffuse deformation zone, as shown in Fig. 6.
Cryogenic temperature tests
At low test temperatures, the unmodified and the S-CSR particle-modified epoxy polymers became harder to deform, as expected.
The compressive true yield stress increased linearly from 20 °C to -80 °C, see Fig. 7, and then increased more rapidly between -80 °C and -109 °C.
The acceleration of the increase in the yield stress below -80 °C may be due to the epoxy polymers being below their β-relaxation, which means nearly all the local motions in the epoxy network cease, so a significantly higher stress is needed to achieve yield.
(It should be noted that the β-relaxation of this epoxy occurs at -56 °C from the DMTA results and, as the test rate is significantly higher for DMTA tests than for the compression tests, the transition would be expected to occur at a lower temperature for the compression data.)
Plots of representative compressive true stress-strain curves of the epoxy with 10 wt% of S-CSR particles showing the shape of the curves at different temperatures is given in Fig. 8.
The low-temperature compressive true stress-strain curves of the S-CSR particle-modified epoxy polymers are similar to the room-temperature curves down to -80 °C, while below -80 °C the curves are more linear due to the passing of the β-transition.
The fracture strains of the unmodified and the S-CSR particle-modified epoxy polymers were relatively insensitive to temperature, see Table 2.
The slight increase of the fracture strain at the relatively low temperatures, and the somewhat erratic shape of the stress-strain curve at -109 °C, may be due to errors caused by accumulation of frost and ice on the compression dies and test samples in the temperature chamber.
The temperature dependence of the compressive yield stress of the unmodified epoxy polymer may be predicted from existing theoretical models.
A number of models are available, including those by Eyring [41,53], Robertson [42], Argon [43,44], and Bowden and Raha [45].
In the present study, Argon's model was selected as this model is relatively simple and has been found to give more accurate predictions when compared to the other models.
Argon's model [43,44] suggests that pairs of molecular kinks are formed in the epoxy polymer, so that the external work applied to the polymer is opposed by the intermolecular resistance to chain flexing.
The temperature dependence of the compressive yield stress, σyc, is predicted to follow the relationship:(11)[2σyc(1+v)3E]5/6=A-B[2T(1+v)E]where E is the modulus and T is the corresponding temperature in K.
The constants A and B are given by(12)A=(0.0771-ν)5/6(13)B=A16(1-ν)k3πω2α3ln(γ0γ)where ν is the Poisson's ratio, ω is the angular rotation (in rad) between the initial and activated positions, α is the mean molecular radius, k is the Boltzmann constant, γ˙ is the net shear strain rate, and γ˙0 is the pre-exponential factor.
The values of these parameters were taken from Cook et al. [46] for a similar epoxy polymer to that used in the present study, and their values are ν = 0.35, ω = 2 rad, α = 0.475 nm and γ˙0=1013s-1.
The modulus values as a function of temperature were taken from the present DMTA studies.
The predictions using Argon's model are compared to the experimental values in Fig. 9, and the agreement is good.
Indeed, the model correctly predicts the trend of increasing yield stress with decreasing temperatures.
However, the prediction is for a relatively linear increase whereas the experimental data follow a rather more curved relationship.
Fracture properties
Room-temperature tests (20 °C)
The values of the fracture energy, GIc, and fracture toughness, KIc, for the unmodified and the S-CSR particle-modified epoxy polymers are summarised in Table 3.
At a test temperature of 20 °C, the fracture toughness, KIc, of the epoxy polymer increased with the addition of the S-CSR particles, from 0.70 MPam1/2 for the unmodified epoxy polymer to 1.46 MPam1/2 for the polymer containing 20 wt% of the S-CSR particles.
A mean fracture energy of 117 J/m2 was measured for the unmodified epoxy polymer, which is not significantly different from the value reported by Giannakopoulos et al. [30] using a similar epoxy system and curing schedule.
The fracture energy increased steadily with the S-CSR content, see Fig. 10.
Thus, a very significant improvement in fracture energy, GIc, was observed, with a maximum value of 947 J/m2 being measured upon the addition of 20 wt% of the S-CSR particles.
This value of toughness is approximately 800% higher than that for the unmodified epoxy polymer.
Cryogenic temperature tests
Values of the fracture energy and the fracture toughness at temperatures from -109 °C to 20 °C are summarised in Table 3.
A graph of GIc versus the concentration of the S-CSR at the various test temperatures is shown in Fig. 10.
The increase of the fracture energy at low temperatures was almost linear with the S-CSR particle content, but of a lower magnitude than at room temperature, see Fig. 10.
However, it is very noteworthy that the addition of the S-CSR particles can significantly toughen the epoxy polymer even at the lowest test temperatures.
For example, a fracture energy of 481 J/m2 was measured at -109 °C upon the addition of 20 wt% of S-CSR.
This represents an increase of approximately 175% compared with the unmodified epoxy-polymer value at this temperature, and more than 50% of the room-temperature value.
A graph of GIc versus the testing temperature for the various S-CSR contents is given in Fig. 11.
The fracture energy of the unmodified epoxy polymer is independent of the test temperature, as any variation in the values is not significant when the associated standard deviations are considered.
The toughening performance of the S-CSR particles was found to decrease linearly with the decreasing temperature, with a minimum at -80 °C.
However, when the standard deviations are considered, then there is no significant difference between the values at between -55 °C and -109 °C for epoxy polymers containing the same concentration of S-CSR particles, see Fig. 11.
The reduced toughening performance at low temperatures is partly due to the increased yield stress of the epoxy, so the plastic deformability of the epoxy decreases as the temperature decreases.
Further, the stiffness of the polysiloxane rubber will increase, which leads to an increase of the cavitational resistance of the particles; and without cavitation of the particle no plastic void growth of the epoxy polymer can occur.
The fracture energy might thus be expected to continue to decrease with temperature, rather than to reach a plateau as seen experimentally.
However, Pearson and Yee [12] have speculated that a larger cavitational resistance should cause the build-up of a larger strain-energy prior to shear yielding of the epoxy polymer, and thus lead to a faster growth of shear-bands and a larger plastic deformation zone.
This is supported by the recent finite-element analysis by Guild et al. [40], which showed that cavitation at higher applied strains causes more complex shear-band growth and enhanced plastic deformation.
These enhanced plastic deformation processes dissipate more energy and thus increase the toughness of the modified epoxy polymers at low temperatures.
Finally, a clear feature that emerges is that the addition to the epoxy polymer of the S-CSR particles with a mean diameter of 0.18 μm and a very low glass transition temperature, Tg, of about -100 °C may indeed lead to significant toughening of the epoxy, even at temperatures as low as about -100 °C.
Toughening micromechanisms
The unmodified epoxy
The fracture surfaces of the unmodified epoxy samples were found to be smooth and glassy, which is typical for a brittle thermoset polymer [33].
This shows that no large-scale plastic deformation occurred during fracture, see Fig. 12, hence giving the low fracture energies.
Feather markings are present on the fracture surfaces, which are caused by the crack forking due to the excess of energy associated with the relatively fast crack growth.
Such repeated forking and the multi-planar nature of the fracture surface absorb the excess energy during fracture of brittle materials [47].
Cavitation and void growth
At room temperature, the fracture surfaces of the S-CSR particle-modified polymers also showed crack forking and feather markings.
However, these fracture surfaces are rougher than those of the unmodified epoxy, and scanning electron micrographs of the deformation zone for the 10 wt% and 20 wt% S-CSR-modified epoxy polymers are shown in Fig. 13.
The fracture surfaces are covered with cavitated S-CSR particles, which can be identified as the circular features in Fig. 13.
The cavitation process causes the originally solid rubber particles to deform into a rubbery shell surrounding a void.
The mean diameter of these cavities was measured to be 0.296 μm.
This is significantly larger than the mean diameter of the S-CSR particles measured from the AFM images, which was 0.18 μm.
This observation clearly reveals that plastic void growth of the epoxy polymer has followed cavitation of the S-CSR particles.
This is one of the main toughening mechanisms for such thermoset polymers toughened by the presence of well-dispersed rubber particles.
Essentially, the cavitation of the particle creates voids which relieve the triaxial stress-state ahead of the crack tip and so enable plastic void growth to occur far more readily in the epoxy polymer.
Cavitation, as opposed to particle debonding, will occur when the rubber particle is strongly bonded to the surrounding polymer.
Indeed, based on the FEG-SEM observations, the core to shell adhesion must also be relatively high for the S-CSR particles, as no debonding is observed.
For the low-temperature results, the fracture surfaces of the particle-modified polymers are very similar to the samples tested at room temperature, see Figs.
14 and 15.
Indeed, all of the S-CSR particles cavitated, even at -109 °C, although the size of the cavities is reduced at low temperatures, which indicates a lesser extent of plastic void growth in the epoxy polymer.
Shear-band yielding
Shear-band yielding has previously been reported for the present epoxy-polymer formulation when particle modified [33], and was observed during the present plane-strain compression tests.
The DN-4PB tests of the S-CSR-modified epoxy polymers were performed to investigate the process further.
After fracture at 20 °C, the plastic zone at the tip of the sub-critically loaded crack was sectioned and observed using transmission optical microscopy.
This showed that a large feather-like deformation zone was formed, see Fig. 16.
This feather-like zone comprises highly plastically dilated cavities and localised shear-bands [12,48].
Transmission optical micrographs of the subsurface damage zone of the S-CSR particle-modified epoxy polymers tested at different temperatures revealed that the size of the subsurface damage zone decreased as the test temperature decreased, see Fig. 17.
This reduction in the size of the deformation zone ahead of the crack tip is due to the increase of the yield stress of the epoxy polymer at low temperatures.
Summary
The toughening events of the S-CSR particle-modified epoxy polymers can be summarised as localised shear-banding of the epoxy polymer initiated by the particles and internal cavitation of the S-CSR particles (which relieves the triaxial stress-state in the vicinity of the crack tip) followed by void growth.
These events contribute to the relatively high toughness measured from the S-CSR-modified epoxy polymers.
The differences observed at low temperatures indicate that the increase of the cavitational resistance of the S-CSR particles delays the cavitation process and enhances the localised shear-band yielding process.
The increase of the yield stress of the epoxy polymer attenuates the deformation of the polymer and reduces the size of the deformation zone ahead of the crack tip, but this effect is compensated for by the increased energy absorption from the enhanced shear-band yielding.
Hence, there is a reduction of the toughening performance of the S-CSR particles at low temperatures, but the competition between these effects results in the fracture energy being independent of the test temperature between -55 °C and -109 °C.
Modelling studies
Introduction
Localised shear-band yielding and cavitation enabling the subsequent plastic void growth have been identified as the main toughening mechanisms.
Such mechanisms have been modelled by Hsieh et al. [33,49], based on the earlier Huang and Kinloch [50] model.
This model has been found to accurately predict the fracture energy of particle-modified epoxy polymers.
Hsieh et al. [33,49] proposed that the fracture energy can be expressed as(14)Gc=GCU+ψwhere GCU is the fracture energy of the unmodified epoxy.
The toughening contribution of the S-CSR particles, ψ, is a combination of the two mechanisms of (i) plastic localised shear-band yielding, ΔGs, and (ii) plastic void growth of the epoxy polymer, ΔGv, as(15)ψ=ΔGs+ΔGv(It should be noted that the process of cavitation itself does not absorb a significant amount of energy, and so its contribution can be ignored.)
Modelling of the ΔGs contribution
The energy contribution from plastic shear-band yielding, ΔGs, initiated by the S-CSR particles is related to the size of the plastic zone [33,49] as(16)ΔGs=0.5VfσycγfF′(ry)where Vf is the volume fraction of S-CSR particles, σyc is the plane-strain compressive true yield stress, γf is the true fracture strain for the unmodified epoxy polymer, and F′(ry) is a geometric term that may be expressed as(17)F′(ry)=ry[(4π3Vfr)1/3(1-rpry)3-85(1-rpry)(rpry)52-1635(rpry)7/2-2(1-rpry)2+1635]
The value of ry may be defined as(18)ry=Kvm2(1+μm31/2)2ryuwhere rp is the particle radius, Kvm is the maximum stress concentration factor of the von Mises stresses in the epoxy polymer, μm is a material constant which allows for the pressure-dependency of the yield stress and its value is taken as 0.2 [51].
The value of Kvm is dependent on the volume fraction of particles, and was calculated by fitting to the data of Huang and Kinloch [39] who modelled rubber particles with a Poisson's ratio of 0.4999 in an epoxy polymer.
The value of ryu, the Irwin prediction of the plane-strain plastic zone radius for the unmodified epoxy polymer at fracture, was calculated [52] using(19)ryu=16π(Kcuσyt)2where KCU is the fracture toughness and σyt is the tensile true yield stress of the unmodified epoxy.
Modelling the ΔGv contribution
The contribution of ΔGv from the plastic void growth mechanism may be calculated [33] as(20)ΔGv=(1-μm23)(Vfv-Vfr)σycryuKvm2where μm is a material constant (see above), Vfv and Vfr are the volume fraction of voids and the volume fraction of S-CSR particles measured from the FEG-SEM and AFM micrographs, respectively.
The value of ψ can now be calculated by combining Equations (16) and (20) into (15) to give(21)ψ=0.5VfσycγfF'(ry)+(1-μm23)(Vfu-Vfr)σycryuKvm2
The material properties used for the modelling are given in Table 4.
The main temperature-dependent terms which significantly affect the results of the proposed model are the plane-strain compressive true yield stress, σyc, and compressive true fracture strain, γf, the uniaxial tensile true yield stress, σyt, and the fracture energy, GCU, and fracture toughness, KCU, of the unmodified epoxy.
The values for these parameters were experimentally measured as a function of the test temperature and their values are given in Tables 2 and 5.
Examination of the fracture surfaces showed that 100% of the particles cavitated, and the mean void radius was measured.
Hence, all the S-CSR particles were assumed to initiate shear-bands and to undergo cavitation with subsequent plastic void growth of the epoxy polymer.
Application of the model
The values of the fracture energy of the S-CSR particle-modified epoxy polymers may be predicted over the range of temperatures concerned, and these predicted values are compared with the experimental results in Figs.
10 and 11 and Table 5.
As may be seen, the agreement is generally good over the complete range of test temperatures.
Although, the Huang and Kinloch model tends to somewhat over-predict the fracture energy compared with the experimental results for the -55 °C and -80 °C temperature tests.
This finding may be due to the over-estimation of the plastic deformation zone of the modified epoxies at these relatively low test temperatures.
The individual contribution of the two toughening mechanisms to the total increase of the fracture energy of the modified epoxy polymers in the temperature range concerned may also be calculated by the above model, again see Table 5.
Localised shear-banding was found to be the main toughening mechanism throughout the temperatures concerned in the present study.
Indeed, over 60% of the toughness increase was from the localised shear-banding mechanism.
Plastic void growth, following cavitation of the rubber particles, only contributed approximately 35% of the toughening effect at room temperature, and its contribution to the toughness decreased rapidly at low test temperatures, especially for the epoxy polymers containing relatively high concentrations of the S-CSR particles.
This is due to the difficulty of initiating cavitation in the particles and the increase of the yield stress of the epoxy polymer, which greatly reduces the plastic deformability of the epoxy polymer, at low temperatures.
Conclusions
An epoxy resin cured with an anhydride was modified by the addition of polysiloxane core-shell rubber (S-CSR) particles.
These rubber particles possessed a mean diameter of 0.18 μm and a glass transition temperature, Tg, of about -100 °C.
From atomic force microscopy studies, they were observed to be well-dispersed throughout the epoxy polymer.
The glass transition temperature of the unmodified epoxy polymer was 148 °C.
The addition of the S-CSR particles did not alter this value, but the elastic modulus and the yield stress of the epoxy were reduced in value.
The fracture energies of the S-CSR particle-modified epoxy polymers were measured at a range of temperatures from 20 °C to -109 °C.
At room temperature, a value of the fracture energy, GIc, of 117 J/m2 was measured for the unmodified epoxy.
Addition of the S-CSR particles linearly increased the fracture energy, and a maximum value of 947 J/m2 was measured using 20 wt% of S-CSR particles at 20 °C.
Further, the S-CSR particles significantly toughened the epoxy polymer even at cryogenic temperatures.
For example, a GIc value of 481 J/m2 was measured for the epoxy containing 20 wt% S-CSR particles at -109 °C.
The toughening mechanisms of the S-CSR-modified epoxy polymers were identified, and they were observed to be the same throughout the temperature range used.
Firstly, plastic shear-band yielding, occurring as localised shear-bands, in the epoxy polymer were observed in the plastic deformation zone ahead of the crack tip in the transmission optical micrographs of the double-notched four-point bending tests.
Secondly, the results of the atomic force microscopy and the scanning electron microscopy studies revealed that cavitation and subsequent plastic void growth of the epoxy polymer occurred.
A theoretical model was used to predict the toughness increment due to these two mechanisms over the entire temperature range.
The predicted fracture energies were compared with the experimental data, and good agreement was found, which reinforces the suggested toughening micromechanisms which have been proposed.
Further, as found previously [39,50], the model predicts that at the relatively low test temperatures the former micromechanism based upon plastic localised shear-band yielding in the epoxy polymer dominates, especially for the epoxy polymers containing relatively high concentrations of the S-CSR particles.
This arises because, firstly, the S-CSR particles still act as points of stress concentration for such deformation to initiate, due to their lower modulus compared to the epoxy polymer over the complete temperate range.
In contrast, secondly, at the relatively low test temperatures the S-CSR particles are now longer truly rubbery in nature and, therefore, cavitation of the particles, which then enables subsequent plastic void growth of the epoxy polymer, is significantly inhibited.
Finally, a clear feature that emerges is that the addition to the epoxy polymer of the S-CSR particles with a mean diameter of 0.18 μm and a very low glass transition temperature, Tg, of about -100 °C may indeed lead to significant toughening of the epoxy, even at temperatures as low as about -100 °C.
Acknowledgements
Some of the equipment used was provided by ACT's Royal Society Mercer Junior Award for Innovation.

Aerodynamic and electrical evaluation of a VAWT farm control system with passive rectifiers and mutual DC-bus

Abstract
A wind farm with a simple electrical topology based on passive rectifiers and a single inverter (mutual topology) is compared to a more complex topology where each turbine has a separate inverter (separate topology).
In both cases, the turbines are controlled electrically by varying the extracted power with the rotational velocity as control signal.
These two electrical topologies are evaluated with respect to the absorbed power for a farm of four turbines placed either on a line or in a square configuration.
The evaluation is done with a vortex model for the aerodynamics, coupled with a model of the electromechanical system.
Simulations predict that individual control is beneficial for aerodynamically independent turbines if the wind speeds differ significantly between the turbines.
If the differences in wind speed are caused by one turbine operating in the wake of another, the deviations in power output between the topologies are less prominent.
The mutual topology can even deliver more power than the separate topology when one turbine is in the wake of another turbine if the wind speed changes rapidly.
Highlights
•
A VAWT farm is simulated with vortex model coupled with electric model.
•
An electric system with passive rectifiers and a mutual DC-bus has been evaluated.
•
The proposed electric system is viable for a vertical axis wind turbine farm.
•
The aerodynamic interactions between the turbines is important for the evaluation.
•
The inter turbine distance is far more important than the electrical topology.

Introduction
Wind turbines are typically placed in farms where the turbines are in close proximity to each other.
This can have economical benefits due to synergy effects such as common grid connection and infrastructure.
The most common type of wind turbine is the horizontal axis turbine with active pitch control [1].
This work is devoted to the straight bladed vertical axis turbine without pitch control, whereby the rotational velocity is the only parameter left for control of the turbine.
A comparison between the different design concepts is found in Ref.
[2].
Pitch regulation for vertical axis turbines has previously been studied, see e.g.
Refs.
[3-5].
However, pitch is not considered here since it increases turbine complexity.
The number of constructed wind farms with straight bladed vertical axis turbines is quite limited, hence limiting the amount of empirical results.
Simulations of vertical axis turbine farms have indicated that such turbines can be placed relatively close to each other with good performance for quite a wide range of incoming wind directions [6-8].
The interaction between two turbines has been tested experimentally in a towing tank [9].
These simulations and experiments are limited to turbines at constant rotational velocity, while for a control system study, variations in wind speeds and rotational velocities for the different turbines are of interest.
In Ref.
[10], a control strategy where the extracted power is proportional to the cube of the wind speed was applied to a horizontal axis turbine.
The simulations in Ref.
[11] predict that a similar strategy should work for vertical axis turbines as well and this work is an extension of that work to turbine farms.
Although a rectifier and an inverter for each turbine can be used, a less complex approach is to connect several turbines to the same inverter.
This electrical system has been studied in Refs.
[12-14] and several different electrical topologies have also been studied in Refs.
[15-17].
All these studies focus on the electrical system of the farm, while the aerodynamic modelling is limited to prescribed power coefficients of the turbines.
Further, all these studies only consider horizontal axis turbines.
The present work considers vertical axis turbines and provides a more elaborate aerodynamic description.
Time dependent aerodynamic simulations of the turbines are coupled with simulations of the electrical control system.
The method includes the fluctuations in torque, which are characteristic to straight bladed vertical axis turbines.
These torque fluctuations cause small variations in the rotational velocity of the turbines.
The method also includes the mutual aerodynamic interaction of the turbines, making it possible to study the control system behaviour when one turbine is in the wake of another.
The vertical axis wind turbine in this study is similar to the 200 kW turbine built by Vertical Wind AB [18], which has a turbine height of 24 m, diameter of 26 m and uses a permanent magnet (PM) synchronous generator.
Control strategy
The tip speed ratio of a turbine is defined as(1)λ=rωV∞,where r is the turbine radius, ω is the rotational velocity and V∞ is the asymptotic wind speed.
Maximal power absorption occurs at roughly the same tip speed ratio regardless of wind speed.
Consequently, many control strategies strive to maintain the correct tip speed ratio by adjusting the rotational velocity.
Accurate real time wind speed measurements can be used to determine the proper rotational velocity.
A weakness of this approach is that this data is typically not available.
A method to control a single turbine without knowledge of the wind speed is presented in Ref.
[10] and a brief overview is given here: For the given control strategy, there will be an equilibrium where the extracted power equals the turbine power (Pe=P), see Fig. 1.
For maximum power capture, this equilibrium should occur at the tip speed ratio λmax that gives the highest power coefficient CPmax:(2)Pe=P=12CPmaxρAV∞3=12CPmaxρAr3ω3λmax3=k2ω3.
As can be seen in Eq.
(2), the extracted power increases as the cube of the rotational velocity as long as the tip speed ratio and the power coefficient are constant.
If the extracted power is higher than the turbine power, the rotational velocity decreases, while it increases if the extracted power is lower.
For low tip speed ratios, this control strategy may cause the turbine to stop, see Fig. 1.
In this work a more stable strategy is implemented:(3){Pe=0ω≤ω0,Pe=k1ω2(ω-ω0)ω0<ω≤ω1,Pe=k2ω3ω1<ω≤ω2,Pe=k3ω2(ω-ω2)+k2ω2ω2ω2<ω,where(4)k1=ω1ω1-ω0k2due to continuity at ω=ω1.
This latter strategy (Fig. 2) maintains a higher rotational velocity at low wind speeds, i.e. the turbine is prevented from stopping at the cost of a slightly lower power coefficient at low and steady wind speeds.
The purpose of the ω2<ω case is to decrease the tip speed ratio at high wind speeds in order to reduce power absorption and mechanical loads.
The control system is optimised for 6 m/s wind speed.
The control system starts to extract power for a tip speed ratio of 4 at 3 m/s wind speed, uses the cubical behaviour between wind speeds 4.5 m/s and 9 m/s, and starts to decrease tip speed ratio above.
For simplicity, k1=k3.
These are the same control system parameters as in Ref. [11].
Farm control system
The most direct approach to farm control is to control each turbine separately as described in Section 2.
This is referred to as the separate topology.
Individual control of a permanent magnet (PM) generator can be implemented with one rectifier and inverter for each turbine.
Alternatively individual turbine control can be achieved with active rectifiers and/or DC-DC converters, which allow the entire farm to share the grid side inverter.
For the separate topology, all turbines are operated according to Eq. (3).
The complexity of the electrical system is reduced if all generators are connected to a mutual DC-bus through passive rectifiers.
This is referred to as the mutual topology, see Fig. 3.
The separate and mutual topologies are identical if the "farm" consists of a single turbine.
The only active component in the mutual topology is the grid side inverter, thereby limiting the control parameters to the total extracted power from the farm.
An outline of the system benefits of the mutual topology is found in Ref.
[19].
This topology tends to set an upper limit for the turbine rotational velocities, where the limit is proportional to the DC-bus voltage.
This prevents all turbines from operating at optimal tip speed ratio if wind speed variations exist between the turbines.
The total power extracted from the turbines is chosen as(5)Pe,tot=∑i=1NPe(ωi),where Pe(ωi) is given in Eq.
(3), N is the number of turbines in the farm and ωi is the rotational velocity of turbine i.
With this choice, both topologies yield identical behaviour if all turbines experience equal wind speed.
For different wind speeds, the average rotational velocity is higher for the mutual topology, compared to the separate topology, due to the cubic behaviour of the extracted power.
This causes the turbines that experience high wind speeds to have tip speed ratios closer to the optimal values than the turbines that experience low wind speeds.
This is beneficial, as it is the turbines experiencing the higher wind speeds that deliver most of the power.
Optimising each turbine individually works well if all turbines are aerodynamically independent.
For an aerodynamically coupled problem, the characteristics of the control strategy will change.
If the reason for the lower wind speed of one turbine is that it is in the wake of another, decreasing the rotational velocity of the upstream turbine will decrease its power absorption (if it was running at optimal rotational velocity before), but will at the same time leave more energy for the turbine downstream.
More information regarding the flow in a horizontal axis wind turbine farm can be found in Refs.
[20,21] and farm control systems are presented in e.g.
Refs. [22,23].
The vortex method
The aerodynamic simulations are performed with the same two-dimensional vortex method as described in Ref.
[11], and only the basic working principles are described here.
The vortex model is used to calculate the velocity field, while an empirical model is used to determine lift and drag coefficients.
This choice was made to substantially increase the speed of the code, as solving the boundary layer with the vortex method is computationally demanding.
The empirical force coefficients are calculated from the data in Ref.
[24] combined with the dynamic stall model developed by Gormont [25] and later improved by Massé [26] using the parameters suggested by Berg [27].
The code uses a panel to calculate the angle of attack from the velocity [28].
This angle of attack, combined with the Reynolds number, is used by the empirical force model to obtain the force coefficients.
The lift coefficient is later used to calculate the circulation according to the Kutta Joukowski lift formula (as suggested in Ref.
[29]) and the vortex released from a blade is obtained as the change in circulation for the blade.
Tip vortex corrections are applied using finite wing theory.
By combining the empirical model with the vortex model, only one vortex is released from each blade at each time step, which enables fast simulations.
To further increase the computational speed, vortices far away from the turbines are merged to reduce the total number of vortices.
Electrical system
This paper considers a simple and robust turbine design described in Ref.
[30].
Each turbine is connected to a direct driven cable wound PM generator.
PM generators are rugged and have low losses since no power or additional components are required to magnetise the rotor.
The current of each generator is rectified with a passive diode rectifier.
Passive diode rectifiers are reliable since no control system is required and the switching frequency is low.
The rectified current charges a capacitance which, in turn, supplies power to an AC grid via an inverter.
A drawback of the mutual topology is that the turbines are not controlled individually.
The electromotive force of a permanent magnet synchronous machine is proportional to the angular velocity of the rotor.
A generator with a diode rectifier does not deliver current to the DC-bus if the greatest line to line voltage of the generator is smaller than the DC-bus voltage.
Therefore, no power will be provided by a generator that operates below a threshold speed that is proportional to the DC-bus voltage.
With the mutual topology, all generators are connected to the same DC-bus and start delivering power at the same rotational speed.
The separate topology does not have this limitation since each turbine has a separate DC-bus.
Grid losses and the efficiency of active power electronic devices are outside the scope of this paper.
Instead only internal losses of the diode rectifiers and the generators are considered.
The individual control is implemented with one rectifier and inverter for each turbine.
Separate and mutual topology
In both the mutual and separate topology, the DC-bus voltage of bus n changes according to(6)dUDC,ndt=-IL,n-∑i∈xnIr,iCn,where Ir,i is the rectified current from generator i, xn is the set of turbines connected to DC-bus n, Cn is the bus capacitance and IL,n is the current drawn by the inverter.
Here, the bus capacitance per generator is equal for both topologies.
The expression for the total extracted power from the farm is the same for both topologies,(7)Pfarm=∑n(Pe(ωn)-Ploss,n),where Ploss,n compensates for internal power losses.
Eq.
(7) is satisfied on a per turbine basis with load currents of(8)IL,i=Pgen,iUDC,iwith the separate topology.
With the mutual topology, Eq.
(7) is satisfied by drawing the total load current(9)IL,1=PfarmUDC,1.
Further details on the implementation of the electrical system model, including parameter values, are found in Ref. [11].
Losses
The electrical system has three principal loss mechanisms.
The resistive loss in the armature of a generator is(10)Pr=R∑k=a,c,bIk2,where R is the per phase resistance and Ik are the phase currents.
The rectified current from the generator is(11)Ir=12∑k=a,b,c|Ik|.
The rectified current is roughly proportional to the generator torque.
The loss due to the forward voltage drop of the diode rectifier is(12)PD=2IrUD.
The power loss in the stator core due to hysteresis and eddy currents in the stator sheets is(13)Pc=khω+keω3/2+kcω2,where kh, ke and kc are constants related to hysteresis loss, excess loss and classical eddy current loss respectively [31].
Bearing friction and rotor windage are both neglected since they are typically small compared to the core loss in slowly moving machines.
Simulations
The first part of the simulations evaluate the differences between the two topologies for approximately static wind conditions and aerodynamically independent turbines, where different asymptotic wind speeds are set for different turbines.
The second part studies aerodynamically coupled turbines in static wind fields.
Several farm configurations are studied at various incident wind directions.
In this part, all variations in wind speed between the turbines originate from aerodynamic interactions between the turbines.
The third part investigates the performance of the control systems for aerodynamically coupled turbines and rapid variations in wind speed.
All simulation cases are performed with four turbines.
This relatively small number of turbines was chosen to keep the simulation times reasonable.
Aerodynamically independent turbines
The first simulations concern turbines without mutual aerodynamic interaction.
This corresponds to turbines which are spaced very far apart.
In the simulations, the asymptotic wind speeds are slowly changed according to Fig. 4.
All turbines are allowed a settling time of approximately 100 revolutions before the wind speed variations are initiated.
The wind speed variations are performed during a time interval that is 10 times longer than the settling time.
Fig. 5 shows that the rotational velocities with the mutual topology are within a narrower span than the velocities under the separate topology.
The significant change in behaviour for the mutual topology at time 1800 s occurs when the turbine that experiences the lowest wind speed (turbine 4) stops delivering power and all turbine power is used to overcome the generator core loss.
The mutual topology is unable to extract power from turbine 4 since the higher rotational velocity causes both a reduced CP (57% of the separate topology at 1800 s) and an increased core loss.
The separate topology is able to extract power from turbine 4 until 2700 s due to the lower rotational velocity.
At the time when the mutual topology stops extracting power from turbine 4, the rest of the turbines increase their rotational velocities faster, as the increases in delivered power from turbines 1 and 2 no longer are compensated for by a decrease in power from turbine 4, see Eq.
(5).
This is most beneficial for turbine 1, which obtains a better tip speed ratio and hence increases its efficiency.
This is seen in Fig. 6, where the ratios between the mutual topology and the separate topology are displayed for each turbine.
At the same point, turbine 3 starts to decrease its power coefficient at a higher rate, as the tip speed ratio for this turbine instead becomes less beneficial.
Even though the reduced turbine power absorption for the mutual topology appears significant for both turbines 3 and 4 (see Fig. 6), those two turbines are the ones that deliver the least energy.
A small increase in efficiency for turbine 1 can easily compensate for lower efficiency of turbines 3 and 4, see Fig. 7.
At the worst case at time 1800 s, the delivered power for the mutual topology is slightly below 93% of the delivered power for the separate topology.
This corresponds to a speed difference of around 5.4 m/s between the highest and the lowest wind speeds.
The different losses in the electrical system with different topologies can be seen by comparing the turbine power and the delivered power in Fig. 7.
The higher average rotational speed with the mutual topology increases the core loss, which dominates the total losses for this system at the studied wind speeds, see Fig. 8.
The resistive losses with the mutual topology are slightly higher than for the separate topology.
This originates from turbine 1 which rotates at a lower speed than the corresponding turbine with the separate topology, see Eqs. (8)-(10).
In the case with significant differences in wind speed between different turbines, it is investigated if the total power for the mutual topology can be increased through modification of the extracted power according to(14)Pe,k=k∑i=1NPe(ωi).
At the time with the largest differences, i.e. at 1800 s, the overall power is increased by less than one percent with k in the range 0.8-0.9, which would cause a slight increase in rotational velocity.
The small increase indicates that Eq.
(5) is a reasonable choice also in the case of large wind speed differences between turbines.
Turbine farm
Two different farm configurations with 4 turbines each are tested, one where all turbines are located on a line, which according to Refs.
[7,8] should be a good layout if the wind mainly comes from one direction.
The second configuration is a square configuration, which better illustrates the behaviour of the control systems when turbines operate in the wakes of others.
Both configurations are simulated with turbine spacings of 3 and 6 turbine diameters (see Fig. 9).
The simulations are performed as several separate simulations with different incoming wind directions, all with wind speed 7 m/s.
The extracted power is given in Fig. 10 for the line configuration and in Fig. 11 for the square configuration.
The line configuration performs well for a wide range of incident wind angles.
With 3D turbine spacing, the extracted power starts to decrease significantly for an incident angle of 70°, while the 6D spacing performs well up to 80°.
With higher angles of incident wind, the performance of the downwind turbines is reduced, since they are inside the wake of the upwind turbines.
The square configuration does in general show lower performance than the line configuration, except for incident angles close to 90°, as only two turbines in the square configuration are located downwind here.
The small asymmetry around 45° for the square configuration is explained by the small asymmetry in the turbines themselves, as the rotational direction of the turbine slightly affects the wake behind it.
The square configuration is more sensitive to the turbine spacing as a wider range of incidence angles causes a performance loss, compare Figs.
10 and 11.
It is therefore advisable to have a larger turbine spacing for the square configuration than for the line configuration.
The differences between the mutual and the separate topologies are generally very small.
For the line configuration at low incident wind angles, both topologies can be expected to work well, as the mutual interactions between the turbines are small.
The trends do however continue over the entire interval of angles of incidence.
When a turbine operates in the wake of another, the mutual topology causes the upstream turbine to extract less energy due to the lower rotational velocity.
This leaves more energy for the downstream turbine which gives more even energy absorption than with the separate topology.
Therefore, the topology is not as important if the differences in wind speed are caused by mutual turbine interaction, compared to the independent turbines in Section 6.1.
The most significant difference is the 45° wind angle for the square configuration.
Here, two of the turbines are operating without any turbine behind them and should therefore operate at the tip speed ratio of independent turbines.
For the other two turbines (middle turbines), one is in the wake of the other and it should therefore be a viable strategy to reduce the extraction for the first turbine for a higher power extraction on the second turbine.
The main issue here is that the two relatively independent turbines cause the mutual rotational velocity to increase, hence over-rotating the two middle turbines, thereby decreasing their efficiency.
Further, the rotational velocities of the two independent turbines are too low as the two middle turbines absorb less energy than the two independent turbines.
This causes the independent turbines to operate at a too low tip speed ratio, hence reducing their performance.
The conclusion is that if all turbines are coupled to each other (or coupled in even pairs as for the square configuration at 0° and 90°), the difference in extracted power is very small between the topologies.
However, if some turbines are coupled together while others are approximately independent, more energy will be absorbed with the separate topology.
Dynamic simulations
The final simulations are performed with varying wind velocities for different turbines in the same simulation.
To create a varying velocity field within the given model, several additional vortices are inserted from the start of the simulation.
The additional vortices are given byxi=-R(150.5+i),i=0,1,2,…,599,yi=-2Rsin(π10xi),Γi=180sin(0.03πxi),where (xi,yi) are the vortex positions and Γi are the vortex strengths.
The particular values for these positions and strengths are chosen to get quite large fluctuations in the wind speed, while still originating from a simple expression.
The vortices will in time form a pattern similar to a Karman street.
The wind speeds at the turbine positions in the square configuration with 3 turbine diameters are found in Fig. 12.
The shape is similar for the line configuration, although not identical as the turbine positions are different.
The wind speeds in Fig. 12 are obtained from the case with the separate topology by calculating the velocity contribution from only these additional vortices at the center position of each turbine.
The extracted powers as functions of time are presented in Fig. 13 for the line configuration and in Fig. 14 for the square configuration.
In both cases, a low-pass filter has been applied to remove the effects of the power fluctuations during the revolution due to the torque ripple.
The sinusoidal behaviour of the power can be related to the Karman-street behaviour of the velocity field.
In total, the line configuration with the separate topology extracts 1.7% more energy than the mutual topology in the time interval from 200 s to 1400 s.
For the square configuration, the mutual topology extracts 6.2% more energy in the same time interval.
The mutual topology tends to keep all turbines at more similar rotational velocities than the separate topology.
For the line configuration, where the turbines are nearly independent, this is disadvantageous as it does not allow each turbine to fully adapt to the changes in wind speed.
It was seen in Section 6.1 that for large variations in wind speed between the turbines, the separate topology should perform better, which is seen here.
Ref.
[11] predicts that for rapid wind speed changes, it can be advantageous to keep the rotational velocity high to extract power at a high power coefficient for high wind speeds if the wind speed fluctuations are faster that the turbine response time.
This is a possible explanation of the relatively good performance of the mutual topology.
As the additional vortices are inserted at the initialization of the simulation and drift with the flow velocity, the wakes of the turbines will affect the motion of these vortices.
The wind speeds in Fig. 12 are therefore not identical to the wind speeds for the mutual topology, and some of the differences seen in Figs.
13-15, such as the low rotational velocity for the separate topology at time 1130 s, can be explained by these differences in wind speed.
Discussion
The aim in this work is not to find the strategy that maximises power production, as this would be an extensive task and likely very turbine specific, hence requiring a much more advanced aerodynamic model.
The aim is instead to show that a simple topology, such as the mutual DC-bus, can be a viable solution for the control of a vertical axis turbine farm as it can obtain relatively good performance without the need for wind speed measurements and additional electrical components.
The control system used with the separate topology works well for independent turbines, but it is not optimised for turbine farms.
The dynamic simulation case, where the mutual topology performed better than the separate topology only shows that it worked better than this particular implementation of individually controlled turbines.
As individual turbine control has more parameters to control, maximum obtainable aerodynamic performance should at least be as high as the mutual topology, as individual control could copy the rotational velocities from the mutual topology.
Active rectification both allows for individual turbine control and a mutual DC-bus [15].
Active rectification should also decrease the power loss in the generator winding, since currents can be drawn at unity power factor, although the possible increase of the generator efficiency with active rectification is only 0.1-0.5% in a similar case with the same turbine and generator [11].
However, active rectifiers are more complex and have higher internal losses than passive diode rectifiers.
The aerodynamic model is two-dimensional and some issues regarding the two-dimensional approximation have been discussed in Ref.
[11], such as over-estimation of power coefficients due to neglected aerodynamic losses etc.
For the wind farm case, the two-dimensional approximation may be less accurate, as the distance between the turbines generally is larger than the turbine height.
Further, the two-dimensional approximation should overestimate the mutual interaction as no flow expansion occurs in the vertical direction.
The model does not include dissipation of the wake due to turbulence, velocity gradients, viscosity etc.
The extracted power from a turbine in another turbines wake is therefore not as accurate as a more realistic full three-dimensional model, especially when the distances between the turbines are large.
The model has problems with the accuracy at low tip speed ratios due to the high angles of incidence as the Gormont model only handles light stall.
This should not be a major concern at tip speed ratio 4, where most simulations are performed.
This can however affect the simulations in some of the cases where all turbines are operated at similar rotational velocities, but with different wind speeds.
Conclusions
Simulations with aerodynamically independent turbines have indicated that a mutual DC-bus with passive rectification is a viable design choice unless the variations in wind speed are very large.
If the wind speed differences among the turbines are due to the turbines' mutual interaction, the mutual and the separate topologies are almost equal in performance.
This illustrates the importance of including the aerodynamic interaction between the turbines when the viability of a control system is analysed.
It is furthermore shown that the array configuration and the distance between the turbines are far more important than the electrical topology for the energy capture.
According to the dynamic simulations, the separate topology is only slightly more efficient for the line configuration, while the mutual topology obtained better results for the square configuration.
Power loss in the cores of the generators is larger when the mutual topology is used.
Acknowledgements
This work was financially supported by the Swedish Energy Agency, Statkraft AS, Vinnova and Uppsala University within the Swedish Centre for Renewable Electric Energy Conversion.

1. An apparatus for producing a positive electrode active material precursor, comprising:a reactor to which a reaction solution is introduced;a stirrer being inserted into the reactorand stirring the reaction solution; anda filter type baffle being inserted into the reactor, wherein the filter type baffle includes a filter.
a reactor to which a reaction solution is introduced;
a stirrer being inserted into the reactorand stirring the reaction solution; and
a filter type baffle being inserted into the reactor, wherein the filter type baffle includes a filter.2. The apparatus for producing a positive electrode active material precursor of claim 1, further comprising: an additional filter being inserted into the reactor.3. The apparatus for producing a positive electrode active material precursor of claim 1, wherein the filter type baffle is disposed on an outer wall of the reactor.4. The apparatus for producing a positive electrode active material precursor of claim 1, whereinthe filter type baffle is present in plural to form a plurality of filter type baffles, andthe plurality of filter type baffles are disposed along the outer wall of the reactor.5. The apparatus for producing a positive electrode active material precursor of claim 1, wherein the filter type baffle include a plurality of filters which are operated independently of each other.6. The apparatus for producing a positive electrode active material precursor of claim 1, wherein the filter type baffle includes an upper filter and a lower filter which are operated independently of each other.7. The apparatus for producing a positive electrode active material precursor of claim 6, whereinthe lower filter is operated when the reaction solution reaches a first position of the reactor,the upper filter is operated when the reaction solution reaches a second position of the reactor, andthe second position is higher than the first position.8. The apparatus for producing a positive electrode active material precursor of claim 1, wherein the filter type baffle includes a filter made of a metal.9. The apparatus for producing a positive electrode active material precursor of claim 8, wherein the metal includes at least one selected from the group consisting of a stainless steel and a carbon steel.10. The apparatus for producing a positive electrode active material precursor of claim 1, wherein the filter includes pores having a size of 0.01 µm to 50 µm.11. A method for producing a positive electrode active material precursor, the method comprising:forming a positive electrode active material precursor while introducing a reaction solution including a transition metal-containing solution, an ammonium ion-containing solution, and a basic aqueous solution to the apparatus for producing a positive electrode active material precursor of claim 1, andperforming a continuous concentration process of introducing an additional reaction solution into the apparatus while operating the filter to continuously discharge a part of the reaction solution inside the apparatus to an outside of the apparatus.
forming a positive electrode active material precursor while introducing a reaction solution including a transition metal-containing solution, an ammonium ion-containing solution, and a basic aqueous solution to the apparatus for producing a positive electrode active material precursor of claim 1, and
performing a continuous concentration process of introducing an additional reaction solution into the apparatus while operating the filter to continuously discharge a part of the reaction solution inside the apparatus to an outside of the apparatus.12. The method for producing a positive electrode active material precursor of claim 11, wherein the filter type baffle includes an upper filter and a lower filter which are operated independently of each other,when an amount of the reaction solution inside the apparatus reaches a first position of the reactor, the lower filter is operated,when an amount of the reaction solution inside the apparatus reaches a second position of the reactor, the lower filter and the upper filter are operated, andthe second position is higher than the first position.13. The method for producing a positive electrode active material precursor of claim 11, wherein a solid content concentration of the positive electrode active material precursor inside the apparatus is increased at a specific rate.Generation of Multipotent Foregut Stem Cells from Human Pluripotent Stem Cells

Summary
Human pluripotent stem cells (hPSCs) could provide an infinite source of clinically relevant cells with potential applications in regenerative medicine.
However, hPSC lines vary in their capacity to generate specialized cells, and the development of universal protocols for the production of tissue-specific cells remains a major challenge.
Here, we have addressed this limitation for the endodermal lineage by developing a defined culture system to expand and differentiate human foregut stem cells (hFSCs) derived from hPSCs. hFSCs can self-renew while maintaining their capacity to differentiate into pancreatic and hepatic cells.
Furthermore, near-homogenous populations of hFSCs can be obtained from hPSC lines which are normally refractory to endodermal differentiation.
Therefore, hFSCs provide a unique approach to bypass variability between pluripotent lines in order to obtain a sustainable source of multipotent endoderm stem cells for basic studies and to produce a diversity of endodermal derivatives with a clinical value.
Highlights
•
Multipotent foregut stem cells are derived from pluripotent cells
•
Foregut stem cells can be expanded and retain their differentiation potential
•
Foregut stem cells differentiate into liver and pancreatic cells
•
Foregut stem cells decrease differentiation variability between cell lines
Hannan, Vallier, and colleagues have developed a defined culture system to differentiate and expand human foregut stem cells (hFSCs) from human pluripotent cells.
hFSCs can self-renew while maintaining their capacity to differentiate into pancreatic and hepatic cells.
hFSCs represent a sustainable source of multipotent endodermal cells for basic studies and can produce a diversity of endodermal derivatives with clinical value.

Introduction
The development of a universal protocol to differentiate any hPSC line into a homogenous population of a specific cell type has been rendered difficult by the inherent variability that exists between lines.
Epigenetic memory, inconsistent reprogramming, and genetic background are likely to be the main cause of this variability, which represents a major challenge for the development of personalized medicines (Lund et al., 2012) and for modeling diseases with a low-penetrance phenotype.
The expansion of intermediate stages of differentiation could represent an attractive alternative to address this issue, especially if these cell types can be isolated from a heterogeneous population.
For example, neuronal stem cells can be easily expanded from human induced pluripotent stem cell (hIPSC) lines differentiated toward the neuroectoderm lineage and then differentiated into a diversity of neurons, thereby bypassing the need to continuously grow pluripotent cells (Falk et al., 2012).
However, the same approach with endoderm differentiation has been more problematic because the complex combination of inductive signals controlling the specification and patterning of this germ layer can be difficult to mimic in vitro (Sneddon et al., 2012).
Following gastrulation, definitive endoderm migrates to form the primitive gut, which initially consists of a single flattened sheet of cells surrounded by mesoderm.
Simultaneously, as migration is occurring, the primitive gut is becoming regionalized along the dorsal-ventral and anterior-posterior axes into foregut, midgut, and hindgut domains (Wells and Melton, 1999).
This patterning is induced by the surrounding mesoderm, which secretes a variety of growth factors not limited to activin, Wnt, fibroblast growth factor (FGF), and bone morphogenetic protein (BMP) signal pathways (Deutsch et al., 2001; Zaret, 2002).
The effects of these signaling pathways can be observed at the molecular level with distinct expression patterns of genes such as SOX2 in the foregut and CDX2 in the hindgut, while early PDX1 expression marks the midgut (Zorn and Wells, 2009).
It is from the foregut region that many of the major organ systems develop such as the liver, thyroid, lungs, the upper airways, the billary system, stomach, and pancreas.
The induction of these organs is orchestrated by a complex combination of morphogen gradients secreted by other developing organ systems such as the cardiac mesoderm (Lemaigre and Zaret, 2004).
The exact nature of these concentration gradients remains difficult to reproduce faithfully in vitro, but several groups (including our own) have made significant progress in understanding how different signaling pathways impact the foregut endoderm cells (Cho et al., 2012).
To date, no one has described a foregut cell type that possesses the proliferative and differentiation potential observed within the early mammalian gut tube.
To address this challenge, we have developed a defined culture system to derive human foregut stem cells (hFSCs) from hPSCs.
These stem cells can self-renew in vitro and resemble multipotent cells of the anterior primitive gut tube by their capacity to differentiate into pancreatic and hepatic cells.
Furthermore, hFSCs can be easily derived from hIPSC lines resistant to endoderm differentiation, thereby enabling the production of endodermal derivatives from a broad number of hPSC lines.
Results
Anteroposterior Patterning of Definitive Endoderm Cells Generated from hPSCs Is Controlled by Activin/TGF-β and WNT Signaling Pathways
Our group has developed a defined culture system to direct the differentiation of hPSCs into a near-homogenous population of definitive endoderm (DE) cells that have the capacity to differentiate into hepatocytes and pancreatic progenitors (Brown et al., 2011; Cho et al., 2012; Rashid et al., 2010; Touboul et al., 2010; Vallier et al., 2009a; Yusa et al., 2011).
Cells grown in these culture conditions successively express primitive streak markers (T and Mixl1), downregulate pluripotency markers (NANOG, SOX2, and POU5F1) and progressively upregulate definitive endoderm markers (CXCR4, FOXA2, GATA4, CERB, and SOX17) (Figures S1A-S1C available online).
Flow cytometry analyses showed that 80% of the resulting DE population coexpresses CXCR4 and SOX17 (Figure S1D).
Interestingly, the resulting population of DE cells is negative for genes marking the foregut (SOX2), the midgut/hindgut (CDX2), the pancreas (PDX1), the liver (AFP), and the lungs (HOXA1) (Figures S1E and S1F) This confirms that DE cells generated in vitro could correspond to early endoderm progenitor cells prior to anteroposterior patterning or organogenesis.
We next examined the capacity of DE cells to differentiate into representatives of the anterior and posterior domains of the primitive gut tube.
We screened various growth factors (data not shown) and found that activin-A blocks CDX2 expression while inducing expression of anterior gut markers such as SOX2, HHEX, and HOXA3 (Figures 1A and 1B; data not shown).
On the other hand, DE cells grown in the presence of the GSK3β inhibitor CHIR99021 express midgut/hindgut markers such as CDX2 and HOXC5 (Figures 1C and 1D) and show no expression of anterior markers.
During both activin-A treatment and CHIR treatment, cells express high levels of the primitive gut markers GATA4, HNF4a, EpCAM, and HOXA2, demonstrating that cells retain their endodermal identity under these conditions (data not shown).
Of note, flow cytometry analyses revealed that 90% of the cells express SOX2 after activin-A treatment while 85% of the cells were positive for CDX2 after CHIR99021 treatment (Figures 1E and 1F).
Similar results were obtained using two hIPSC lines (BBHX8 and A1ATD.1) (Figures S2A-S2H).
Taken together, these data show that activin-A and GSK3beta signaling direct the anteroposterior patterning of human DE in vitro.
Hindgut/Midgut CDX2-Expressing Cells Generated from hPSCs Can Differentiate into Gut-like Organoids
To further validate the identity of the cells generated in the presence of activin-A or CHIR99021, we decided to test their capacity to differentiate into intestinal cells.
SOX2+ cells and CDX2+ cells were grown into 3D organoid culture conditions (Spence et al., 2011) known to promote posterior gut differentiation.
SOX2+ cells grown under these conditions ceased to proliferate and could not be expanded (data not shown), whereas CDX2+ cells formed spheroids with highly folded structures resembling intestinal epithelium (Figures 2A and 2B).
Gut organoids expressed intestinal markers (mucin, villin, and chromagranin A) and could be expanded for more than 2 months while displaying a progressive increase in the markers of adult intestinal epithelium (Figure 2C).
Finally, comparative immunocytochemistry analysis of CDX2+ cell-derived organoids with primary mouse intestinal organoids demonstrated a polarized epithelium with apical villin expression in both types of organoids (Figure 2D).
These data confirm that CDX2+ cells generated in the presence of CHIR99021 have the differentiation potential to form midgut/hindgut progenitors, while activin-A-induced SOX2+ cells have lost this capacity.
These SOX2+ cells could consequently be equivalent to foregut progenitors (Arnold et al., 2011).
Foregut Cells Generated from hPSCs Can Be Expanded for a Prolonged Period of Time in Defined Culture Medium
We then tested the capacity of foregut SOX2+ cells to self-renew in vitro.
DE cells derived from human embryonic stem cells (hESCs) were differentiated for 4 days in the presence of activin-A and were subsequently cultured in the presence of a diverse combination of growth factors (data not shown).
Following this approach, we identified that the combination of activin-A, basic fibroblast growth factor (bFGF), BMP4, hepatocyte growth factor (HGF), and epidermal growth factor (EGF) was sufficient to expand foregut SOX2+ cells for more than 20 passages (Figure 3A).
DE cells grown under these conditions form a simple epithelium (Figure 3B) that can be serially passaged at a plating density of 40-50 × 106 cells/cm2 of plate surface area.
Foregut cells grown in these culture conditions express homogenously the proliferation marker Ki67 (Figures 3C and 3D), retained a normal karyotype (Figure 3E), and after 20 passages have an expansion potential of more than 4.2 × 108-fold (Figure 3F).
Importantly, these culture conditions were only supportive of foregut cells, because significant cell death was observed with neuronal-like and fibroblast-like cells.
This selection could explain how a near-homogenous population of foregut cells could be easily obtained only after few passages (Figure S3A).
After more than ten passages in these culture conditions, foregut SOX2+ cells did not express transcripts or proteins of pluripotency (POU5f1 and NANOG), lung (NKX2.1), hepatic (AFP), or pancreatic (PDX1) markers while maintaining the expression of foregut markers (HNF4α, SOX17, CXCR4, EpCAM, HNF1β, GATA4, Cer, SOX2, HNF6, and HNF1beta; Figures 3G, 3H, and S3B-S3D).
Flow cytometry analyses showed that SOX17 and CXCR4 were coexpressed near homogenously (Figure S3A) along with other foregut markers such as CXCR4/HNF4a (>90% double positive), CXCR4/FOXA2 (>90% double positive), and SOX17/GATA4 (>90% double positive; Figure 3I).
Importantly, similar results were obtained with two hIPSC lines (BBHX8 and A1ATD.1; Figures S3A-S3D) (Brown et al., 2011; Rashid et al., 2010; Vallier et al., 2009a).
Finally, we observed that foregut cells could be frozen after five passages and then thawed to be expanded for at least five additional passages without any loss in gene expression profiles or observable loss of proliferation capacity (data not shown and Figure S4A).
Together, these data demonstrate that our culture system captures a homogenous population of foregut cells that can self-renew in vitro and are lineage restricted to endodermal tissue and thus could be representative of an endodermal stem cell (referred thereafter as human foregut stem cells or hFSCs).
hFSCs Can Differentiate into Cells Expressing Hepatic, Pancreatic, and Lung/Thyroid Markers
To further characterize hFSCs, we decided to investigate their spontaneous capacity to differentiate in endodermal derivatives.
For that, 1.0 × 105 hFSCs were transplanted under the kidney capsule of nonobese diabetic severe combined immunodeficiency (NOD-SCID) mice and incubated in vivo for 10 weeks.
Both hESC- and hIPSC-derived hFSCs produced very large cysts with very limited solid outgrowth (Figure 4A).
Histological examination revealed that hFSCs did not produce teratomas, as there were no tissues representative of the ectoderm and mesoderm lineages.
Instead, the outgrowths were formed mainly of cyst resembling glandular tissue and lung epithelium (Figure 4B).
Accordingly, immunostaining analyses indicated that a large majority of cells contained in hFSCs-derived outgrowth expressed the lung/thyroid marker Nkx2.1, while fewer pockets of cells expressing liver (AFP/EpCAM) and pancreatic marker (PDX1) could also be detected (Figure 4C).
However, cells expressing mesoderm (Brachyury) or neuroectoderm (N-CAM) markers were never observed and the hindgut marker CDX2 was also absent (Figure 4C) Together, these data suggest that hFSCs have the capacity to differentiate into liver, pancreas, and lung/thyroid cells and thus could be considered as multipotent stem cells.
To confirm this hypothesis, we tested the ability of hFSCs to differentiate into liver and pancreatic cells using culture systems recently developed by our group to produce these cells types directly from hPSCs (Cho et al., 2012; Rashid et al., 2010).
hFSCs grown in culture conditions inductive for pancreatic specification (Cho et al., 2012) (Figure 5A) sequentially expressed early pancreatic markers (HLXB9 and PDX1), then endocrine progenitor marker (NGN3), and finally beta cell marker (c-peptide/insulin) (Figure 5B).
As reported previously, more than 90% of cells were PDX1 positive at stage 4 of the pancreatic differentiation protocol (Figures S4B and S4C) (Cho et al., 2012).
After 25 days of differentiation, cells expressing c-peptide, PDX1, glucagon (GCG), and somatostatin (SST) could be observed by immunocytochemistry and c-peptide release was detected upon glucose stimulation (Figures 5C and 5D).
Importantly, approximately 15%-20% of pancreatic cells were positive for C-peptide and 10%-20% of these C-peptide-positive cells were polyhormonal (i.e., coexpressing both C-peptide and either GCG [20%] or SST [10%]) (Figure S4D-S4G).
While surprising, these result are in agreement with our previous study showing that monohormonal/glucose-responsive cells could be generated from hPSCs using a defined culture system (Cho et al., 2012).
Importantly, further characterization beyond the scope of the current study will be necessary to validate the real functionality of the insulin-expressing cells generated under our culture conditions.
Similar analyses were performed using our liver differentiation protocol and hFSCs described previously (Hannan et al., 2013; Rashid et al., 2010; Yusa et al., 2011) (Figure 5E).
Under these conditions, hFSCs rapidly differentiate into a near-homogenous population of hepatocyte-like cells expressing liver markers (AFP, ALB, A1AT, HNF4α, and CK18; Figure 5F and 5G).
In addition, a vast majority of hepatocyte-like cells (90%) coexpressed albumin and ASGPR and alpha-1-antitrypsin as reported previously (Figures S4H-S4K) (Rashid et al., 2010; Hannan et al., 2013).
Further functional characterization also showed that these cells could uptake low-density lipoprotein (Figure 5H) and cardiogreen from tissue culture medium (Figure S6) and secrete AAT and albumin (data not shown and Figure 5I).
Importantly, multiple hFSC lines derived from different hIPSC lines displayed similar differentiation efficiency (Figures S5 and S6), while frozen/thawed hFSCs could also generate liver and pancreatic cells.
Indeed, defrosted hFSCs could be expanded for five passages and then differentiate efficiently into cells expressing pancreatic (PDX1 > 90%) and hepatic (AFP > 90%) markers (Figures S4J-S4M).
Taken together, these data demonstrate that hFSCs have the capacity to differentiate into foregut derivatives including lung/thyroid, pancreatic, and hepatic cells, thereby confirming that they are multipotent.
Clonal Differentiation of hFSCs Confirms Their Multipotency
To further reinforce these results, we decided to confirm that single hFSC are multipotent.
hFSCs grow as an epithelium and single-cell isolation systematically resulted in cell death, thereby excluding conventional clonal assays.
To bypass this limitation, we generated GFP-expressing hFSCs that were dissociated into single cells and individually placed on a well of a 24-well plate containing non-GFP-expressing hFSCs (Figures 6A and 6B).
The following day, wells containing a single GFP-positive hFSC were marked for expansion, and after five passages, the resulting cells were differentiated into hepatic and pancreatic cells.
Of note, such an approach, while not strictly clonal, did allow us to follow the expansion of a single GFP cell and assess the differentiation potential of its progeny.
Indeed, GFP-positive hFSC cells grown in these respective culture conditions expressed hepatic (ALB, A1AT, AFP, and HNF4a and low-density lipoprotein [LDL] uptake) and pancreatic (PDX1, INS, NGN3, and SST) markers, thereby providing the necessary evidence that hFSCs generated from hESCs and hIPSCs (data not shown; Figures 6C and 6D) are multipotent stem cells.
Generation of hFSCs Bypasses Variability between hIPSC Lines
hIPSC lines can be easily generated from a broad number of patients, and this has opened the possibility to model a diversity of disorders for basic studies and drug screening.
However, recent reports have shown that hIPSC lines can vary in their capacity to generate cell types with clinical value.
The precise mechanisms responsible for this variability remain unclear but are likely to combine several origins, including epigenetic memory, genetic background, and abnormal reprogramming.
This variability also presents a major challenge for personalized cell-based therapy and large-scale genetic studies, because each hIPSC line might require protocol optimization.
Accordingly, we recently analyzed the capacity of 48 hIPSC lines derived from 16 individuals to differentiate into endoderm, and while a majority of hIPSC lines could differentiate in our culture system, we identified three endoderm-resistant hIPSC lines (COXS8, COXV3, and Line4).
These lines produced less than 30% of SOX17-positive DE cells (Figure 7A).
Importantly, this heterogeneity precluded further differentiation toward the pancreatic/hepatic lineages.
We therefore decided to isolate hFSCs from these endoderm-resistant hIPSC lines.
For that, heterogeneous endoderm cells generated from the hIPSC lines COXS8, COXV3, and Line4 were grown for 3 additional days in the presence of activin-A to promote foregut specification and the resulting cells were transferred into culture conditions supporting hFSCs expansion.
Interestingly, contaminating cells of nonendodermal origin stopped proliferating and progressively disappeared upon passaging.
Flow cytometry analyses show that cells grown for five passages homogenously expressed SOX17 and CXCR4 (99%) similarly to hFSCs generated from hIPSCs proficient for endoderm production (BBHX8) (Figure 5A).
Therefore, our culture system selectively amplifies hFSCs even when they originate from a heterogeneous population of DE cells.
The resulting population of hFSCs was expanded for two additional passages and then transferred into culture conditions inductive for pancreas and liver differentiation.
Cells differentiated toward the liver lineage expressed hepatic markers (AAT, ALB, AFP, and HNF4α) at a level similar to hepatocyte-like cells generated from control hFSCs (Figure 5B).
Similarly, cells differentiated toward the pancreatic lineage cells expressed PDX1, INS, and NGN3 (Figure 5C).
Together, these results show that hFSCs can be easily generated from hIPSCs with reduced endoderm differentiation capacity, enabling the production of hepatic and pancreatic cells.
Therefore, derivation of hFSCs could be achieved from numerous hIPSC lines and enable us to bypass in part the variability of hIPSC lines to generate hepatic/pancreatic cells.
Discussion
Derivation of foregut stem cells with a strong proliferative capacity as well as the ability to self-renew represents an attractive cell source for multiple applications within the regenerative medicine field, including disease modeling, developmental studies, and cell-based therapies.
Accordingly, our results describe a stepwise method to differentiate hPSCs into a multipotent population of foregut stem cells.
Importantly, production of foregut cells has been reported previously (Green et al., 2011), but our study provides a culture system allowing the isolation, expansion, and differentiation of multipotent self-renewing foregut stem cells.
Similarly, a recent study has shown that multipotent DE cells could be expanded in vitro (Cheng et al., 2012; Sneddon et al., 2012), yet these cells express a broad diversity of markers that render their embryonic identity difficult to establish.
Furthermore, these studies relied on feeders, Matrigel, 3D culture conditions, or serum, any of which is not compatible with large-scale or clinical applications.
Nevertheless, hFSCs resemble mouse anterior definitive endoderm cells isolated from mouse embryonic stem cells using the combination of reporter gene for HEX expression and the cell surface marker CXCR4 (Morrison et al., 2008).
In agreement, hFSCs display a similar gene expression profile including the expression of HHEX and CXCR4 and share a similar capacity of differentiation toward liver and pancreas.
However, hFSCs could also have the unique capacity to generate lung/thyroid cells while they have lost their capacity to generate gut cells.
More importantly, hFSCs can be easily isolated from a diversity of hPSCs lines without the need for cell sorting and complex genetic modifications, thereby allowing the production of a near-homogenous population of cells with clinical value.
Overall, the hFSC culture system addresses several important limitations associated with current methods available to isolate and to expand endodermal stem cells.
hFSCs also share fundamental characteristics with their in vivo counterpart, including the expression of key markers such as FOXA2, CXCR4, HHEX, SOX17, and CERB.
Nevertheless, the exact type of foregut cell described here is yet to be fully defined, as lineage tracing experiments have shown that foregut may contain only bipotential progenitors able to differentiate toward the hepatic and pancreatic lineages (Deutsch et al., 2001).
However, the property of in vivo progenitors is likely to be dictated by their localization within the foregut and thus their surrounding environment.
Moreover, the gut tube initially possesses a high degree of plasticity.
Indeed, the hindgut domain, if taken at an early time point, is capable of producing liver and pancreatic bud structures when either juxtaposed against foregut cardiac mesoderm or placed in culture conditions with BMP and FGF (Bossard and Zaret, 2000; Wells and Melton, 2000).
This suggests that during the early stages of gut formation, the entire gut epithelial sheet could be multipotent.
Thus, the culture system described here could be less restrictive, enabling hFSCs to display the full range of their developmental plasticity.
Self-renewing and multipotent adult stem cells could represent an advantageous source for the generation of large quantity of "safer" differentiated cells required for cellular therapy, because they could strongly reduce the risk of teratomas associated with pluripotent stem cells.
However, it is important to underline that the isolation and serial passage of hFSCs did not improve the overall maturity of the end-stage population after differentiation, whether this be liver, lung, or pancreatic cells.
Indeed, hepatocytes or pancreatic cells generated from either freshly derived foregut cells or P10 hFSCs still display a combination of adult and fetal characteristics and are not fully functional with regard to cytochrome P450 activity or insulin secretion, respectively.
These results are in agreement with a broad number of studies that have demonstrated that fetal-like pancreatic/hepatic cells can be efficiently generated from hPSCs (Cho et al., 2012; Hannan et al., 2013; Rashid et al., 2010; Touboul et al., 2010) or from organ-specific progenitors (Cardinale et al., 2011; D'Amour et al., 2006; Deutsch et al., 2001; Morrison et al., 2008; Si-Tayeb et al., 2010; Sneddon et al., 2012; Sullivan et al., 2010; Van Haute et al., 2009; Wang et al., 2013; Zhao et al., 2009).
Thus, the current report does not claim to solve this major challenge or even to improve existing protocol of differentiation.
Our study only establishes that hFSCs are able to produce liver and pancreas cells similar to those generated from endodermal cells directly produced from hESCs (Cho et al., 2012; D'Amour et al., 2006; Hannan et al., 2013; Rashid et al., 2010; Si-Tayeb et al., 2010; Sneddon et al., 2012; Sullivan et al., 2010; Touboul et al., 2010; Van Haute et al., 2009; Zhao et al., 2009).
The generation of fully functional cells from hPSCs remains a distant goal that will require the development of innovative approaches far beyond the scope of this work (Shan et al., 2013; Takebe et al., 2013).
To conclude, expansion of a multipotent foregut progenitor population is of considerable interest with regard to clinical applications.
Indeed, our culture system is compatible with large-scale production of a near-homogenous population of endodermal cells that could greatly simplify the production of cells for cell-based therapy.
Furthermore, derivation of hFSCs allowed for differentiation of all the tested hIPSC lines without the need to establish individual protocols.
Therefore, hFSCs not only provide a unique in vitro model of human development but also represent an important tool to deliver the clinical promises of hIPSCs in the field of personalized medicine.
Experimental Procedures
Generation of hIPSCs
hIPSCs (BBHX8 and A1TATD) were derived using retrovirus-mediated reprogramming of human skin fibroblasts using the Yamanaka factors as described elsewhere (Rashid et al., 2010).
Approval for the collection of human tissue samples was obtained from the North West Ethics Committee (13/NW/0205).
All animal work was carried out with full home-office approval (project license 80/2397).
Generation of GFP hPSCs and Clonal Analyses
GFP-expressing H9, BBHX8, and A1ATD-1 cells were generated by stable transfection using lipofectamine 2000 (Invitrogen) as described previously (Vallier et al., 2001).
GFP-positive cells were differentiated into foregut cells and then dissociated into single cells.
An individually isolated GFP cell was then transferred into a well containing non-GFP-positive hFSCs.
Wells were visually inspected 12 hr after plating, and wells containing a single GFP-positive hFSC were selected for clonal expansion.
Human Embryonic Stem and Induced Pluripotent Stem Cell Culture
hESCs (H9) and hIPSCs (BBHX8, A1ATD-1, COXV3, COXS8, Line4, and IPS40) were cultured in a chemically defined, feeder-free culture system as described previously using activin-A (10 ng/ml) and bFGF (12 ng/ml) (Brown et al., 2011; Rashid et al., 2010; Teo et al., 2011; Vallier et al., 2009a; Vallier et al., 2009b).
Cells were passaged every 7 days using a mixture of collagenase IV or collagenase and dispase at a ratio of 1:1.
Differentiation of hPSCs into Endoderm
Cells were differentiated into definitive endoderm using CDM-PVA and activin-A (100 ng/ml), BMP4 (10 ng/ml), bFGF (20 ng/ml), and LY294002 (10 μM) for 3 days as described previously (Cho et al., 2012; Rashid et al., 2010; Vallier et al., 2009a; Yusa et al., 2011).
Patterning of Definitive Endoderm
DE cells were cultured in RPMI+B27 medium with activin-A (50 ng/ml) for 3-4 days to generate foregut cells.
DE cells were cultured in RPMI+B27 medium with CHIR99021 (6 μM) for 4 days to generate posterior endoderm.
Differentiation of Posterior Endoderm into 3D Gut Organoids
Posteriorized endodermal cells were embedded in growth-factor-reduced Matrigel (BD Biosciences) containing B27 supplement (RA depleted) (Invitrogen), human R-spondin (500 ng/ml) (R&D), human Noggin (100 ng/ml) (R&D), human EGF (100 ng/ml) (R&D), and Jagged-1 peptide (1μM) (AnaSpec).
Cell/Matrigel mix was overlayed with Advanced Dulbecco's modified Eagle's medium (DMEM)/F12 (Gibco) supplemented with 2 mM GlutaMax (Invitrogen), 10 mM HEPES (Invitrogen), and 100 U/ml penicillin per 100 μg/ml streptomycin containing B27 supplement (RA depleted) (Invitrogen), Y-27632 (10 μM) (Sigma Aldrich), Noggin (100 ng/ml) (R&D), human EGF (100 ng/ml) (R&D), human R-spondin (1 μg/ml) (R&D), and human Wnt3a (100 ng/ml) (R&D).
Passaging and Maintenance of hFSCs
hFSCs were cultured on gelatine coated plates prepared as described for hPSC maintenance in RPMI medium containing B27 Supplement, NEAA, penicillin/streptomycin, activin-A (10 ng/ml), bFGF (20 ng/ml), BMP (10 ng/ml), HGF (20 ng/ml), and EGF (50 ng/ml).
Cells were passaged every 4-7 days using cell dissociation buffer (CDB).
Cells were washed once with PBS then incubated in CDB at 37°C for 10-15 min.
Cells were scraped as small clumps and transferred to a 15 ml tube and centrifuged at 800 rpm for 2 min.
Cells were washed once with RPMI medium and then resuspended in RPMI medium containing the cocktails of growth factors described above and ROCK inhibitor Y-27632 (10 μM).
Cells were plated at a density of 40-60 × 103 cells/cm2 of plate surface area.
ROCK inhibitor was not used during subsequent days of culture.
Medium was changed the following day and every subsequent day until cells were 80%-90% confluent.
Differentiation of hFSCs into Hepatic Endoderm
Hepatic differentiation has been described previously (Cho et al., 2012; Rashid et al., 2010; Yusa et al., 2011).
Briefly, hFSCs were cultured in RPMI + B27 containing BMP4 (10 ng/ml) and FGF10 (10 ng/ml) for 4 days.
Cells were then cultured in hepatocyte basal medium (Lonza) containing oncostatin M (50 ng/ml) and HGF (50 ng/ml) for at least an additional 20 days.
Differentiation of hFSCs into Pancreatic Endoderm
hFSCs were differentiated into pancreatic endoderm using a five-step process as described previously (Cho et al., 2012).
During stage 1 (S1), hFSCs were cultured in Advanced DMEM (Invitrogen) supplemented with SB-431542 (10 μM; Tocris), FGF10 (50 ng/ml; AutogenBioclear), all-trans retinoic acid (RA, 2 μM; Sigma) and Noggin (50 ng/ml; R&D Systems) for 3 days.
For stage 2 (S2), the cells were cultured in Advanced DMEM supplemented with human FGF10 (50 ng/ml; AutogenBioclear), all-trans retinoic acid (RA, 2 uM; Sigma), KAAD-cyclopamine (0.25 uM; Toronto Research Chemicals), and Noggin (50 ng/ml; R&D Systems) for 3 days.
For stage 3 (S3), the cells were cultured in human FGF10 (50 ng/ml; R&D Systems) for 3 days.
For maturation of pancreatic progenitors (stage 4 [S4]), cells were grown in Advanced DMEM + 1% vol/vol B27 and DAPT (1 mM) for 3 days and for 3 additional days in Advanced DMEM + 1% vol/vol B27 (stage 5 [S5]).
During S4 and the final stage (S5) of differentiation, medium devoid of insulin was used so as not to interfere with immunocytochemistry and ELISA assays.
Additionally, only antibodies raised against C-peptide were used to avoid potential false-positive results.
Immunocytochemistry
For a complete list of primary and secondary antibodies, please refer to Table S1.
hPSCs or their differentiated progenitors were fixed for 20 min at 4°C in 4% paraformaldehyde and then washed three times in PBS.
Cells were incubated for 20 min at room temperature in PBST (0.1% Triton X-100; Sigma; in PBS) containing 10% donkey serum (Serotec) and subsequently incubated overnight at 4°C with primary antibody diluted in 1% donkey serum in PBST.
Cells were then washed three times in PBS and incubated with secondary antibodies in 1% donkey serum in PBST for 2 hr at room temperature.
Unbound secondary antibody was removed by three 5 min washes in PBS.
Hoechst 33258 was added to the first wash.
For immunocytochemistry on 3D organoids, organoids were removed from Matrigel, fixed in 4% paraformaldehyde, and embedded in 4% agarose before processing for paraffin sections.
Following antigen retrieval, samples were permeabilized with 0.5% Triton X-100 and blocked in 10% fetal bovine serum before overnight incubation in primary antibody.
Samples were washed with PBS and incubated with secondary antibodies for 1 hr at room temperature before being counterstained using DAPI.
Samples were imaged using a Zeiss Imager M.2, equipped with AxioCam MRm and MRc cameras and AxioVision software for image capture.
Immunocytochemistry on cryosections was performed by drying sections at 37°C for 30 min before antigen retrieval at 95°C.
Cells were then fixed in 4% paraformaldehyde for 20 min before they were blocked and permeabilized using a solution of 10% donkey serum 0.01% Triton X-100 for 30 min.
Primary and secondary antibodies were applied using a 1% donkey serum 0.001% Triton X-100 solution.
Flow Cytometry
For a complete list of primary and secondary antibodies used for flow cytometry, please refer to Table S1.
Adherent cells were washed twice in PBS and then incubated for 20 min at 37°C in cell dissociation buffer (Invitrogen).
Cells were dissociated by gentle pipetting and resuspended at approximately 0.1-1 × 106 cells/ml in PBS.
Cells were pelleted and fixed by resuspending cells in 4% paraformaldehyde solution at 4°C for 20 min.
Cells were washed in PBS and then blocked and permeabilized in PBS containing 10% donkey serum and 0.01% Triton X-100.
Cells were then incubated in a solution of 1% donkey serum 0.001% Triton X-100 containing the primary antibody.
Cell were incubated for at least 2 hr at room temperature or overnight at 4°C.
Cells were then washed three times in PBS 1% donkey serum and incubated with secondary antibodies in for 2 hr at room temperature.
Unbound secondary antibody was removed by three to five washes in PBS.
Cells were then analyzed using a FACS Calibur machine (BD Biosciences).
All flow cytometry experiments were gated first using unstained cells and then cells containing the secondary antibody only.
On all flow cytometry plots, the secondary-only population is shown in gray.
All gates shown on scatterplots and histogram plots were set to the secondary-only control.
All flow cytometry was validated with immunocytochemistry to ensure that false-positive or false-negative results were not recorded.
ELISA
hESCs grown for 25 days in culture conditions inductive for pancreatic specification were cultured in differentiation medium without insulin for 24 hr prior to glucose stimulation.
Cells were washed three times with PBS and preincubated in DMEM supplemented with 2.2 mM glucose referred to as "low glucose" conditions (Invitrogen) for 60 min at 37°C.
Preincubated cells were grown in DMEM containing 22 mM glucose, referred to as "high glucose conditions" for 5, 10, or 30 min.
Supernatants were collected for determination of C-peptide release.
ELISA analyses were performed using the Mercodia Ultrasensitive C-peptide ELISA kit (Mercodia).
For albumin secretion assays, high binding surface COSTAR 96-well plates (Corning) were coated overnight with affinity-purified rabbit polyclonal antibodies against albumin (Abcam 87564) at 2 μg/ml in carbonate/bicarbonate buffer (Na2CO3/NAHCO3, pH 9.5).
After washing (0.9% w/v NaCl, 0.05% v/v Tween 20), the plates were blocked for 2 hr in blocking buffer (PBS, 0.25% w/v BSA, 0.05% v/v Tween 20).
Culture medium was diluted in blocking buffer, and 50 μl was added to each well and then incubated for 2 hr.
After washing, the wells were incubated with corresponding monoclonal antibodies (1 μg/ml diluted in blocking buffer) and incubated for 2 hr.
Bound monoclonal antibodies were detected with rabbit anti-mouse immunoglobulin G horseradish peroxidase-labeled antibody (Sigma Aldrich, 1:20,000) for 1 hr.
The reaction was developed with TMB liquid substrate (Sigma Aldrich) for 10 min in the dark and the reaction was stopped with 1 M H2SO4.
Absorbance was read at 450 nm on a Thermo-max microplate reader (Molecular Devices).
Uptake of LDL
The Dil-LDL staining kit was purchased from Cayman Chemicals and the assay was performed according to the manufacturer's instructions.
Statistics
All experiments were carried out at a minimum of three technical triplicates.
Data represent biological triplicates.
All values are expressed as the mean ± SEM.
Differences between means were assessed by t test using Graph Pad Prism 6 software.
p < 0.05 was considered significant.
All p values are indicated in figure legends.
Author Contributions
N.R.F.H. conceived of and designed the study, performed experiments, developed protocols and validation, collected and interpreted data, produced figures, and wrote and edited the manuscript.
R.P.F. performed 3D gut organoid differentiation experiments, collected and interpreted data, and edited the manuscript.
Y.A.S. performed cryosectioning and staining experiments and interpreted data.
V.M. collected data and edited the manuscript.
R.B. performed CGH analysis and interpreted data.
N.A.H. and A.B. obtained human fetal primary tissue samples.
K.J. interpreted gut organoid data and edited the manuscript.
L.V. conceived of and designed the study, interpreted data, and edited the manuscript.
All authors read and approved the final version of the manuscript.
Acknowledgments
This work was funded by a MRC senior nonclinical fellowship (L.V.), the Cambridge Hospitals National Institute for Health Research Biomedical Research Center (L.V., N.R.F.H.), the EU grant LivES (N.R.F.H.), the Evelyn trust (N.R.F.H.), the Medical Research Council (R.P.F., V.M.), and the Wellcome Trust (K.J.).
N.A.H. is a Wellcome Trust senior clinical fellow (WT088566).
The authors would like to thank Dr. William Mansfield and Charles-Etienne Duneau from the Cambridge Stem Cell Institute for performing tissue potency assays and Peter Humphries for confocal imaging of tissue outgrowths.
Supplemental Information
Supplemental Information includes Supplemental Experimental Procedures, six figures, and two tables and can be found with this article online at http://dx.doi.org/10.1016/j.stemcr.2013.09.003.
Supplemental Information
Document S1.
Supplemental Experimental Procedures and Figures S1-S6Table S1.
List of Primary and Secondary Antibodies Used, Related to Figures 1, 2, 3, 4, 5, 6, and 7Table S2.
List of qPCR Primer Sequences Used, Related to Figures 1, 3, 5, and 7


Polycrystalline SnO2 nanowires coated with amorphous carbon nanotube as anode material for lithium ion batteries

The preparation process for the nanowire arrays of the polycrystalline SnO2 coated with amorphous carbon nanotube is shown in Scheme 1. Firstly, The AAO templates were prepared as described [12]. High-purity aluminum (0.2 mm in thickness, 99.99%) was washed sequentially with acetone, ethanol, and distilled water. After annealing in Ar flow at 430 degC for 4 h, aluminum plates were anodized in 0.3 M oxalic acid under direct current voltage of 40 V at 15 degC. The remaining aluminum was etched in 1 M CuCl2 solution. The bottom part of the AAO was removed and the pore diameter was adjusted in 5 wt.% H3PO4 solution at 30 degC for 80 min. SnCl2*2H2O (1.13 g) was dissolved in 1.5 ml concentrated hydrochloric acid and kept at rest for 3 h to get a clear solution. Then 1.16 g citric acid and 1 ml H2O were added into the solution and dissolved. Ammonia was dropwise added to the clear mixture to adjust pH value to 2. Then the mixture was continuously stirred at 50 degC. At last, a 0.5 M SnO2 sol was obtained after the volume was adjusted. AAO templates were dipped into the clear sol for 1 h. Then the sol-filled AAO were kept at 80 degC for 10 h to complete the sol-gel transition. The gel-filled membranes were heated in air in a muffle furnace at 450 degC for 4 h and then cooled to room temperature. After the annealing, the sample was immersed in 1 M NaOH solution for 4 h to move the AAO template at room temperature, through which free-standing SnO2 nanowires were obtained. SnO2 powder was synthesized as a control. It was prepared by directly drying the above-mentioned 0.5 M SnO2 sol following the annealing at 450 degC for 4 h and grinding.
1. A method for preparing a precursor of a lithium transition metal oxide, a lithium-containing compound for reacting with one of a lithium secondary battery cathode active material of the lithium transition metal oxide as, wherein the precursor contains two or more kinds of transition metal, and the precursor further comprises a transition metal salt used in the preparation of a precursor thereof from, a sulfuric acid ion (SO4) salt ions, based on the total weight of the ion content of 0.1 to 0.7 weight precursor %.2. A precursor according to claim 1, wherein the transition metal is selected from the group of two or more Group VIIIB of the Periodic Table of the plurality of elements of Group VB to.3. A precursor according to claim 1, wherein the transition metal is selected from the group of two or more of Ni, Co or Mn of the element.4. A precursor according to claim 1, wherein the transition metal content of NixCoyMn1-(x+y), wherein 0.3 ≤ x ≤ 0.9, 0.1 ≤ y ≤ 0.6, and is x + y ≤ 1.5. A precursor according to claim 4, wherein 0.1 mol or less of the transition metal is one or more selected from the group Al, Mg, Cr, Ti and a Si element in place.6. A precursor according to claim 1, wherein the transition metal salt is a sulfate salt.7. A precursor according to claim 6, wherein the sulfate salt is one or more selected from the group consisting of nickel sulfate, cobalt sulfate and manganese sulfate.8. A precursor according to claim 1, wherein the salt ions include nitrate ion (NO3).9. A precursor according to claim 1, wherein the salt precursor thereof based on the total weight of the content of 0.2 to ion 0.6 wt %.10. A method for preparing a precursor of a lithium transition metal oxide, wherein the assay is carried out after addition of the acid precursor dissolvers the precursor, the precursor based on the total weight of 0.1 to 0.7 of the detected weight % of a sulfate ion (SO4).11. A lithium transition metal oxide, for producing by the lithium transition metal oxide according to claim 1 and a lithium-containing precursor to prepare a sintered material.12. An oxide according to claim 11, wherein the lithium-containing material is a lithium carbonate (Li2CO3)/ or lithium hydroxide and the (LiOH).13. One of the cathode active material for a lithium secondary battery, a lithium-transition metal oxide includes claim 11.14. A lithium secondary battery, comprising a cathode active material according to claim 13.Li3V2(PO4)3/graphene nanocomposites with superior cycling performance as cathode materials for lithium ion batteries
In a typical synthesis, graphene oxide (GO) was prepared from natural graphite powder via a modified Hummers method [26] and [27].
Pure LVP was synthesized by using a sol-gel method reported in our previous paper [28]. First, oxalic acid (3.02 g) and V2O5 (1.46 g) in a stoichiometric ratio were dissolved in deionized water with magnetic stirring at 70 degC. After a clear blue solution formed, a mixture of stoichiometric NH4H2PO4 (1.84 g) and CH3COOLi*H2O (1.68 g) was added to the solution whilst stirring for 4 h, and then a gel formed in an air oven at 100 degC. The gel was decomposed at 350 degC in an argon atmosphere for 4 h, and the obtained product was ground, pressed into pellets, and sintered at 750 degC for 4 h in flowing argon. The sample was divided into two parts: one was further heated at 750 degC for 2 h in an argon atmosphere; the other was used to prepare LVP/G nanocomposites via a simple precipitation procedure, as described in detail as follows. The as-prepared LVP (1.0 g) was transferred to a suspension of 50 mg graphene oxide dissolved in 500 mL deionized water. The mixture was stirred for 12 h at room temperature after ultrasonic treatment for 2 h. Subsequently, the mixture was dried by freeze drying. Finally the dried sample was also sintered at 750 degC for 2 h in flowing argon. The procedure for the formation of LVP/G nanocomposites is shown in Scheme 1.1. A preparation method of porous nickel-cobalt-manganese composite hydroxide is characterized by comprising the following steps: mixing a nickel salt, a cobalt salt, a manganese salt aqueous solution, a complexing agent and a precipitating agent, adding a loosening agent, adjusting the pH of the system to 10-12, and heating, stirring and reacting to obtain a porous nickel-cobalt-manganese composite hydroxide precipitate; and washing and drying to obtain the porous nickel-cobalt-manganese composite hydroxide.2. The method for preparing a porous nickel-cobalt-manganese composite hydroxide according to claim 1, characterized in that: in the nickel salt, cobalt salt and manganese salt aqueous solution, the molar ratio of nickel element, cobalt element and manganese element is (2-3) to 1 (2-1).3. The method for preparing a porous nickel-cobalt-manganese composite hydroxide according to claim 1, characterized in that: the loosening agent is at least one of oxygen, ozone, air, hydrogen peroxide and chlorine.4. The method for preparing a porous nickel-cobalt-manganese composite hydroxide according to claim 1, characterized in that: the loosening agent is at least one of oxygen, ozone and air.5. The method for preparing a porous nickel-cobalt-manganese composite hydroxide according to claim 1, characterized in that: the dosage of the loosening agent is based on the oxidation potential in the system, and the oxidation potential of the system is controlled to be 0.0-1.5V.6. The method for preparing a porous nickel-cobalt-manganese composite hydroxide according to claim 1, characterized in that: the dosage of the loosening agent is based on the oxidation potential in the system, and the oxidation potential of the system is controlled to be 0.3-0.7V.7. The method for preparing a porous nickel-cobalt-manganese composite hydroxide according to claim 1, characterized in that: the complexing agent is ammonia water; the precipitant is sodium hydroxide.8. The method for preparing a porous nickel-cobalt-manganese composite hydroxide according to claim 1, characterized in that: the dosage of the complexing agent is 1-20 g/L; the total molar ratio of the used precipitator to the nickel element, the cobalt element and the manganese element is 2: 1; the heating and stirring reaction is carried out at a rotating speed of 100-300 rpm and a temperature of 40-80 ℃ for 20-200 h.9. A porous nickel-cobalt-manganese composite hydroxide, characterized by being obtained by the production method according to any one of claims 1 to 8.10. Use of the porous nickel cobalt manganese composite hydroxide according to claim 9 in a lithium ion positive electrode material.CeO2-modified Au@SBA-15 nanocatalysts for liquid-phase selective oxidation of benzyl alcohol
The Au-CeO2@SBA-15 catalyst was prepared using the impregnation method. A required amount of Ce(NO3)3*6H2O was dissolved in 7 mL ethanol and added into the resulting Au@NH2-SBA-15 sample. The mixture underwent ultrasound for 0.5 h and was dried in air at 60 degC for 12 h to generate an Au-xCeO2@SBA-15 precursor. The Au-CeO2@SBA-15 precursor was calcined in air at 500 degC for 4 h to obtain the final catalyst labeled as the Au-xCeO2@SBA-15 catalyst (x denote the CeO2 loading; CeO2 loading = mCeO2/mSBA-15).The Baeyer-Villiger oxidation of ketones with Oxone(r) in the presence of ionic liquids as solvents
Oxone(r) (4.0 mmol) was added to a solution of ketone (4.0 mmol) in ionic liquid (3.0 g) and stirred at 40 degC for 2.5-20 h (depending on the reaction rate). The progress of the reaction was monitored by GC or HPLC. After this time, the post reaction mixture was dissolved in CH2Cl2 and filtered. Next, the filtrate was concentrated and extracted with the appropriate solvent of: ethyl acetate, diethyl or dibutyl ether (6x5 mL) and concentrated. The yields of lactones after the purification by column chromatography with hexane/ethyl acetate (4:1) as the eluent were 65-95%.
For HmimOAc (bp=240 degC; 70 degC/1.2 mbar) and H2mpyrOAc (bp=255 degC; 90 degC/1.2 mbar) distillation of the product or ionic liquid from the reaction mixture was performed.
ILs were purified for recycling tests. After the filtration of post reaction mixture, and the extraction of the product with ethyl acetate (bmimBF4) or dibutyl ether (HmimOAc), ILs were concentrated, dried under vacuum (60 degC, 5 h) and reused.
A solution of Oxone(r) (0.63 g) in 0.7 g of IL was stirred for 5 h at 40 degC. After this time the content of KHSO5 was determined by iodometric titration.
Oxone(r) (4 g) was mixed with 5 g of IL and stirred for 1 h at room temperature. Next, CH2Cl2 was added to isolate the insoluble parts of Oxone(r) and to lower the viscosity. The mixture was filtered and the amount of KHSO5 in the filtrate was determined by iodometric titration.Fabrication of calcium phosphate fibres through electrospinning and sintering of hydroxyapatite nanoparticles

Abstract
Calcium phosphate (CaP) materials such as synthetic hydroxyapatite (HA, Ca10(PO4)6·(OH)2) and β-tricalcium phosphate (β-TCP, Ca3(PO4)2) are well-known for their potential in bone tissue engineering and drug delivery applications.
Processing such materials into submicrofibres might contribute to improve their biocompatibility.
This paper presents a new method for creating CaP submicrofibres through the electrospinning route.
A thermal treatment at sintering temperature (1100°C) was applied to electrospun polymer fibres filled with hydroxyapatite nanoparticles to cause aggregation of the nanoparticles and vaporise the polymer matrix.
The images taken by electron microscopy revealed that the treated samples maintained their submicrofibrous morphology.
Moreover, Fourier transform infrared spectroscopy, X-ray diffraction and X-ray photoelectron spectroscopy analysis confirmed that the resulting fibres are made of hydroxypatite and tricalcium phosphates.
Highlights
•
CaP submicrofibres can be produced by using a combination of electrospinning and sintering.
•
Electrospun fibres were produced from a PLGA solution highly concentrated with HA nanoparticles.
•
Heat treatments were applied to evaporate the polymer and induce sintering of the nanoparticles.
•
Hydroxypatite was identified to be the main material in the sintered scaffold.
•
Approach potentially applicable to make submicrofibres from a variety of ceramic material.

Introduction
Calcium phosphate (CaP) materials are known to support bone ingrowth and to promote bone integration [1].
They are also non-toxic, biodegradable and can easily adsorb bioactive molecules at their surface.
These characteristics make them useful for both tissue engineering (TE) and drug delivery applications [2,3].
Among CaP materials, synthetic hydroxyapatite (HA, Ca10PO46·(OH)2) and β-tricalcium phosphate (β-TCP, Ca3(PO4)2) are most widely used.
HA is the main form of calcium-phosphate crystal found in human bone.
Usually, the degradability of CaP materials varies according to the Ca/P ratio, with β-TCP being more soluble than HA [4].
Therefore, mixing CaP materials may help to control, to some extent, the degradation of the TE scaffold.
In particular, HA/β-TCP ceramic scaffolds have been shown to be more effective in bone repair than pure HA or pure β-TCP [5].
CaP solid bodies have been used for many applications but they usually face the issue of being too brittle.
The poor mechanical properties of CaP ceramic materials have severely obstructed their clinical applications [6].
Fibres and other types of particle are generally more successful for filling bone defects and helping to repair the damaged tissue.
Furthermore, they can be dispersed in polymer matrices for mechanical reinforcement purposes and for improved biocompatibility [7].
CaP fibres have been produced through several routes including spinning, extrusion, precipitation, sol-gel and pyrolysis, and electrospinning [8-13].
Electrospinning is a simple technology that allows the production of nano- and micro-fibres by applying a high voltage to a polymer solution.
It has been used to produce different types of ceramic fibres through the use of chemical precursors mixed to the polymer solution and by performing a thermal treatment on the resulting fibres [14,15].
However, the production of CaP fibres by electrospinning remains poorly investigated.
Wu et al. have created HA microfibres by electrospinning a polymer mixture containing Ca(NO3)2·4H2O, (C2H5O)3PO (precursors) and by performing a thermal treatment on the collected fibres at 600°C for 1h [13].
The generation of HA nanofibres has been investigated by Kim and Kim [16].
They have based their strategy on the use of sol-gel precursors of the apatites and by adjusting the concentration of the sols, they could vary the diameter of the fibres in a range of a few micrometres to hundreds of nanometres.
Calcination was performed at 700°C for 2h.
Also, Tadjiev et al. have used the same precursors as Wu and colleagues and heat treatments between 500 and 800°C to produce β-TCP ceramic nanofibres [17].
Sintering of pure HA particles is usually reported to occur above 1000°C.
The choice of the sintering temperature is important as it has an effect on the properties of the resulting sample.
Most investigators agree that pure HA (ratio CaP=1.67) is stable in an air and argon atmosphere at temperatures upto 1200°C [18-20].
However, decomposition of HA at temperatures as low as 800°C has been observed for calcium deficient HA samples [19].
This paper investigates a new approach to obtain CaP submicrofibres through the electrospinning route.
In a previous research article [7], we have demonstrated that HA nanoparticles can be incorporated into Poly(lactic-co-glycolic acid) (PLGA) fibres at high concentration (upto 50% in weight to volume ratio within the polymer solution).
Here, we report that applying a heat treatment at sintering temperatures to such PLGA-HA composite fibres forces the aggregations of the nanoparticles and vaporises the polymer matrix, leading to the creation of CaP submicrofibres.
Materials and methods
Preparation of electrospinning solutions: Polymer solutions were prepared by dissolving Poly(lactic-co-glycolic acid) (PLGA, ratio 75:25, Mw: 66-107kDa, Sigma-Aldrich Chemical Company Ltd., Dorset, UK) into 1,1,1,3,3,3-hexafluoroisopropanol (HFIP, Apollo Scientific Limited, Cheshire, UK) at a concentration of 15% (weight to volume ratio).
Deficient hydroxyapatite synthetic nanoparticles with an average particle diameter of 93nm (HA nanopowder, Sigma-Aldrich) was then added to the polymer solution at concentrations of 50% (weight to volume ratio).
Solutions were agitated at room temperature on a roller for at least 24h to allow for complete dissolution of the polymers.
Each solution was then homogenised on ice using a vibracell ultrasonicator (130W, 20kHz; Sonics Materials Inc., Newtown, USA) prior to electrospinning.
Preparation of calcium phosphate fibres: The PLGA-HA electrospun samples were folded to fit into an alumina ceramic crucible and the container was placed in the middle of a furnace (University of Oxford, Materials Department, Parks Road).
The heat treatment was applied in a 50kPa Argon atmosphere at 1100°C for 1h.
The furnace was brought to a temperature with a heating rate of 0.27°C/s and was cooled at a rate of 0.1°C/s.
The treated samples were collected next day and were removed for characterisation.
Scanning electron microscopy (SEM): Samples (PLGA-HA composites before and after treatment) were mounted on an aluminium stub using a carbon adhesive disk.
The materials were then coated with a 2.5nm layer of platinum using a Cressington 208 HR Sputter coater (Vortex Control Systems Inc., Texas, USA) and high resolution images of the scaffolds were taken using a JSM 840F scanning electron microscope (JEOL, Tokyo, Japan).
On each sample, three different areas were observed randomly at different magnifications.
Fourier transform infrared spectroscopy (FTIR): The surface chemistry of the treated samples was analysed by using a FTIR spectrometer (Bruker Optics Limited, Ettlingen, Germany).
Pure PLGA fibres, PLGA-HA composite fibres and pure HA nanopowders were also analysed as controls.
Each spectrum was acquired and averaged with a resolution of 4cm-1 by accumulation of 128 scans and the signal was measured in a range between 400 and 4000cm-1.
X-ray diffraction (XRD): Samples were prepared by grinding the electrospun materials into powder and spreading the powder onto a crystal of silicon covered with a layer of silicone grease.
Excess powder was removed and the crystal and powder were exposed to an X-ray beam.
X-ray diffraction measurements were carried out using a fully automated Siemens D5000 powder diffractometer, employing a Cu Kα radiation (λ=0.15406nm) and a secondary monochromator.
The samples were continuously spun during data collection and scanned using a step size of 0.05° 2θ over the range of 5°-75° 2θ, with a count time of 12s/step.
For comparison purposes, the X-ray diffraction of the HA nanopowder and of the PLGA-HA fibres were also recorded.
X-ray photoelectron spectroscopy (XPS): Samples were mounted on a stub using a double sided adhesive tab before being placed into the ultrahigh vacuum (2×10-7mPa) analysis chamber of the spectrometer.
XPS analysis was carried out using a VG Clam 4 MCD analyser system with X-ray radiation from the Mg Kα band (hν=1253.6eV).
Recording was performed with an energy of 100eV and a take-off angle of 90°.
The spectra were analysed using Microcal Origin 6.0 software and the assignment of the peaks was performed with reference to the UKSAF and NIST databases.
As controls, HA nanopowder and PLGA-HA fibres were also analysed.
Results
Sample morphology: The morphologies of PLGA-HA electrospun fibres before (left side) and after the treatment at 1100°C (right side) are shown in Fig. 1.
Interestingly, the treated samples (referred as "CaP fibres" in this paper) retain the submicrofibrous morphology of PLGA-HA composites.
Fig. 1(e) and (f) also reveals that the nanoparticles observed on PLGA-HA fibres have disappeared to leave a smooth and segmented surface in CaP fibres.
FTIR analysis: The FTIR spectra obtained from the different samples are indicated in Fig. 2.
The spectra obtained from pure PLGA fibres and from HA nanopowder serve as controls.
In the PLGA spectrum, the C3O characteristic bands in the region 1065-1280cm-1 can be seen.
On the other hand, the spectrum of HA nanopowder reveals the characteristic peak assigned to PO4-3: ν1 vibration mode at about 964cm-1, ν3 vibration mode at 1031cm-1 and 1091cm-1 (asymmetric).
The spectrum of untreated PLGA-HA sample displays both characteristics from PLGA and HA, as described in [7].
On the other hand, the spectrum of CaP fibres seems to have less characteristics of PLGA (CåO and C3O bands).
However, the aspect of the curve is slightly different from the one recorded for the HA nanopowder.
Indeed, weak absorption peaks can be detected at about 993, 1043 and 1067cm-1 near the ν3 vibration mode of PO4-3 (1031cm-1 and 1091cm-1).
Another small shoulder could be observed at about 950cm-1 near the ν1 vibration mode characteristic (964cm-1).
XPS analysis: The experimental binding energies of the O 1s, C 1s, P 2p, and Ca 2p electrons and proportions of the corresponding atomes are indicated in Table 1.
In PLGA-HA fibres, C 1s was found at 284.4eV, O 1s at 530.7eV, Ca 2p at 345.5eV, and P 2p at 130.2eV.
Carbon was the dominant element (51%) while only 2.37% of Ca and 2.95% of P were detected in the sample (ratio Ca/P=1.24).
In HA nanopowder, O was found to be the dominant element with 50.87%, while Ca and P reached values as high as 13.94% and 10.13% respectively (ratio Ca/P=1.37).
Ca 2p and P 2p were found at slightly higher binding energies: 347.5eV and 133.3eV, respectively.
CaP fibres showed very similar observation when compared to the pure nanopowder in terms of binding energies and percentages of elements.
Although the ratio O/C is slightly lower, 1.49 against 2.03 for the HA nanopowder, O was still the major element at 46.41%.
Ca and P reached values of 12.78% and 9.6%, respectively (ratio Ca/P=1.33).
XRD analysis: The XRD patterns of the samples are shown in Fig. 3.
The chemical identification was performed by comparing the experimental X-ray patterns to standards compiled by the International Centre for Diffraction Data (ICDD).
The analysis show that the diffractogram obtained for the PLGA-HA sample and HA nanopowder identified synthetic hydroxyapatite (Ca5(PO4)3OH) with a probability above 99% for both.
The pattern observed for CaP fibres identified hydroxyapatite with a probability of 81% with small amounts of whitlockite (Ca9(Mg,Fe++)(PO4)6·(PO3OH)) and β-TCP (Ca3(PO4)2) with probabilities of 14% and 11% respectively.
Discussions
In previous research, CaP fibres have been successfully produced by electrospinning through the combinative use of chemical precursors and heat treatments at temperature below 800°C [13-17].
The strategy presented in this paper differs in the way that the CaP material was directly incorporated into the electrospun composite fibres in the form of nanoparticles.
Furthermore, the electrospinning process was followed by the heat treatment at sintering temperatures, i.e. above 1000°C.
This simple way of creating CaP fibres avoids the use of chemical precursors, often toxic and expensive, and potentially allows the creation of nano- and micro-fibres from a wide variety of ceramic material.
Here, we successfully demonstrated the approach with PLGA as a sacrificial polymer.
However, cheaper materials such as polyvinylalcohol and polyvinylpyrrolidone could be used since these polymers have already been shown to be suitable for the preparation of ceramic fibres using a high temperature treatment [15].
Polymers having low melting points, such as polycaprolactone, were not able to maintain the fibrous structure during the thermal treatment (data not shown).
To enable the sintering process, particles must be densely packed.
Therefore fibres prepared from a solution containing highly concentrated HA nanoparticles (50% w/v) were selected for this work.
Moreover, the large amount of nanoparticles in the polymer matrix might have played a role in maintaining the fibrous morphology while the polymer was subsequently molten and vaporised.
The SEM images have revealed that the resulting CaP fibres acquire a segmented aspect, while the nanoparticles (seen at the surface of PLGA-HA composites) are not visible anymore (Fig. 1, right side).
These observations suggest that sintering has occurred.
In sintering processes, small individual structures merge together by diffusion to form bigger structures.
To confirm the chemical nature of the heat-treated samples, further characterisations involving FTIR, XPS and XRD were performed.
FTIR analysis confirmed that the polymer was burnt out and evaporated during the thermal treatment since no characteristic of PLGA was observed in the spectrum of CaP fibres.
Moreover, no band corresponding to carbonates was observed in the range 1400-1550cm-1, which suggests that carbonate ions were only present in trace quantities.
This may be due to the volatile nature of the carbonate group.
Also, the emerged peaks from the splitting of the ν1 and ν3 vibration modes are characteristic of tricalcium phosphate [21].
This suggests that the decomposition of HA started during the heat treatment.
Moreover, consultation of the literature indicates that the spectra observed for CaP fibres is typical for a mixture of HA and β-tricalcium phosphate [22-24].
This last observation is in agreement with the outcomes of the XRD analysis: HA was identified as the main phase of the CaP fibres, but β-tricalcium phosphates (β-TCP) and whitlockite (a Mg-containing β-TCP) were also identified.
The XPS data clearly indicates that the HA nanoparticles used in this experiment are deficient in calcium (Ca/P ratio=1.37).
This could explain why the HA decomposition occurs before 1200°C since deficient HA start their decomposition at temperatures lower than pure HA (ratio=1.67) [19].
In the literature, this decomposition is described as partial although the reason remains unknown.
The commonly accepted decomposition reaction for deficient HA isCa10(PO4)6(OH)2→3Ca3(PO4)2+CaO+H2O
The XPS analysis also reveals low calcium phosphate levels in the PLGA-HA samples, despite the fact that the fibres are filled with a large amount of nanoparticles.
Since XPS is a surface chemical analysis technique (with analysis depth less than 10nm), this suggests that the nanoparticles lying at the surface of the fibres were embedded in a thin layer of polymer.
The presence of carbon observed by XPS at the surface of the HA nanoparticles might result from atmospheric contamination by CO2 and volatile fatty acids.
At the surface of the CaP fibres, an additional reason for the presence of carbon might be the existence of residual atoms from the degradation-evaporation of the PLGA polymer.
This would explain the higher proportion of carbon when compared to the native HA nanopowder and why the ratio of oxygen to carbon is lower.
The slightly lower percentage of oxygen in CaP fibres, compared to the HA nanopowder, may result from the loss of OH groups (dehydroxylation) that occur at high temperature.
For pure HA, this reaction is known to occur at sintering temperatures below 1200°C with a conversion degree of 0.4 to 0.5 [25].
A simple way to present the reaction formula for dehydroxylation would beCa10(PO4)6(OH)2→Ca10(PO4)6O+H2O
The resulting oxyhydroxyapatite cannot be detected by XRD due to the high similarity of its crystalline structures with hydroxyapatite.
It could not be seen by FTIR either, since the band characteristic of 3OH was observed below 400cm-1 which is below the range explored in this study.
Systems containing both HA and TCP phases have been intensively investigated and have shown potential for bone tissue engineering [26,27].
β-TCP has exceptionally good tissue compatibility, allowing direct bonding between the regenerated and the native bone without intermediate connective tissue, and encouraging faster bone regeneration and resorbability than HA.
HA/TCP systems have been developed in order to regulate the resorption kinetics through varying the ratio HA/TCP.
For similar reasons, the chemistry of the CaP fibres is likely to induce an excellent response from bone tissue.
Measurements of mechanical robustness of the CaP fibres remain to be conducted, but initial inspection has indicated profound scaffold fragility compared to the PLGA-HA composites.
Although the polymer-based fibres are more flexible and less fragile, the absence of polymer matrix may be preferred for applications such as bone tissue engineering and drug delivery.
For PLGA in particular, the acidity of the polymer degradation product can cause tissue inflammation or deactivation of the drug.
If bone tissue engineering applications are considered, where robustness is required, CaP fibres will necessitate further improvement to their mechanical properties.
Alternatively, in their current condition, fibres may be more appropriate as coating or filling material for ceramic or metal implants, in order to enhance osteoconduction and facilitate the incorporation of the implant into the native bone tissue.
Conclusions
To conclude, the results presented in this paper have demonstrated that CaP fibres can be prepared by electrospinning a PLGA-HA composite solution and subsequently applying a thermal treatment at sintering temperature to the electrospun fibres.
The polymer phase was evaporated at the high temperatures while the nanoparticles were sintered, allowing the samples to retain their submicrofibrous morphology.
Hydroxyapatite was identified to be the main phase in the treated samples.
Tricalcium phosphates were also found, as a result of the dehydroxylation and decomposition of deficient HA at high temperature.
The chemistry of the new fibres holds great potential for bone tissue engineering.
However, more investigations are to be carried out, in particular to improve their mechanical properties, as these are currently insufficient for the fibres to be used as bone scaffold.
Nevertheless, this simple approach is promising for the creation of submicrofibres made of a wide variety of ceramic materials without the use of toxic and expensive chemical precursors.
Acknowledgements
The authors would like to thank EPSRC for supporting PA Mouthuy's Ph.D. study with a studentship.
Also we thank Mr Ian Lloyd for his assistance in the sample preparation.
Supporting information
Supplementary data associated with this article can be found in the online version at doi:10.1016/j.matlet.2013.04.110.
Supplementary materials
Fig. S1
XPS spectra obtained for (a) PLGA-HA fibres, (b) HA nanopowder, (c) CaP fibres.

Thermally induced formation of zinc oxide nanostructures with tailoring morphology during metal organic framework (MOF-5) carbonization process
Hydrochloric acid (36%) and N,N-dimethylformamide were purchased from Chempur (Poland). Zinc nitrate hexahydrate (Zn(NO3)2*6H2O), terephthalic acid (C6H4 (COOH)2) and triethylamine were bought from Sigma Aldrich.
The MOF-5 was prepared according to a previously reported procedure [26]. Briefly, 1.65 mmol of zinc nitrate hexahydrate, 0.89 mmol terephthalic acid and 3.25 mmol triethylamine (TEA) were dissolved in 2.34 mol N,N-dimethylformamide (DMF). The mixture was sonicated for 1 h. The solution obtained was transferred into an autoclave and stirred for 48 h at a temperature of 150 degC. The reaction vessel was then removed from the autoclave and allowed to cool to room temperature. Finally, the sample was vacuum-dried at 110 degC for about 2 h to remove the solvent.
Porous carbon was prepared by direct thermolysis of the previously obtained MOF-5. A ceramic boat containing the white MOF-5 powder was inserted into the centre of a tube furnace. The furnace was then heated from room temperature to the desired temperature (between 600 degC to 1000 degC), with a heating rate of 8 degC/min, The MOF-5 powder was maintained at this temperature for 2 h with an Ar flow of 100 ml/min. Cooling the MOF-5 back down to room temperature was followed by an investigation of the morphology of the samples obtained.Selective oxidation of p-cymene catalyzed by VPO catalyst: Process performance and kinetics studies
Three distinctive standard methods were used to prepare the VPO catalyst precursors phase, VOHPO4*0.5H2O [14]. In the first method, the precursor was prepared in an aqueous medium by dissolving V2O5 (6.0 g; 33 mmol) in aqueous HCl (35%, 79 mL) and refluxing for 2 h. The H3PO4 (85%; 8.9 g; 78 mmol) was added and the solution refluxed for further 2 h. The solution was subsequently evaporated to dryness and the resulting solid was refluxed in water (20 mL H2O g-1 solid) for 1 h. It was then filtered hot, washed with warm water, and dried in air (110 degC, 16 h). The precursor prepared by this method is denoted P1.In the second method, the precursor was prepared in an organic medium, by adding V2O5 (11.8 g; 66 mmol) to isobutanol (250 mL). H3PO4 (85%; 16.4 g; 39 mmol) was then introduced and the whole mixture was refluxed for 16 h. The light blue suspension was then separated from the organic solution by filtration and washed with isobutanol (200 mL) and ethanol (150 mL, 100%). The resulting solid was refluxed in water (9 mL g-1 solid), filtered hot, and dried in air (110 degC, 16 h). The precursor prepared by this method is denoted P2. The third method precursor was prepared via the dihydrate compound VOPO4*2H2O. A mixture V2O5 (11.8 g; 66 mmol) and H3PO4 (85%; 115.5 g; 106 mmol) were refluxed in water (24 mL g-1 solid) for 8 h. The resulting VOPO4*2H2O was recovered by filtration and washed with a little water. Some of the VOPO4*2H2O (4.0 g; 20 mmol) was subsequently refluxed with isobutanol (80 mL) for 21 h, and the resulting solid was recovered by filtration and dried in air (110 degC, 16 h). The precursor prepared by this method is denoted P3. The precursors (P1, P2 and P3) were activated under N2 flow in the tube furnace at 550 degC for 4 h. They are denoted C1, C2 and C3, respectively.Structural and magnetic properties of ferromagnetic nano-sized (Ni1-xCox)0.85Se prepared by simple hydrothermal method
(Ni1-xCox)0.85Se with 0.0<=x<=1 solid solutions were prepared by a simple hydrothermal route. An aqueous solution was prepared by dissolving the appropriate amount of NiCl2*6H2O or/and CoCl2*6H2O and Na2SeO3 in 70 ml distilled water. 8 ml hydrazine hydrate (N2H4.H2O, 99%) was added under constant stirring for 30 min. The mixture was transferred to a stainless Teflon-lined 100 mL autoclave, and heated at 140 degC for 12 h. After cooling to room temperature, the products were filtered off, washed with distilled water and absolute ethanol several times, and then dried in vacuum at 50 degC for 3 h.Magnetosphere-atmosphere coupling at Saturn: 1 - Response of thermosphere and ionosphere to steady state polar forcing

Highlights
► We self-consistently calculate Saturn magnetosphere-ionosphere-thermosphere coupling.
► Magnetospheric energy controls flow of mass and energy in Saturn's thermosphere.
► Thermospheric dynamics are key to understanding high latitude energy balance.
► H3+ observations combined with our simulations may constrain H2 (v⩾4) abundances.
Abstract
We present comprehensive calculations of the steady state response of Saturn's coupled thermosphere-ionosphere to forcing by solar radiation, magnetospheric energetic electron precipitation and high latitude electric fields caused by sub-corotation of magnetospheric plasma.
Significant additions to the physical processes calculated in our Saturn Thermosphere Ionosphere General Circulation Model (STIM-GCM) include the comprehensive and self-consistent treatment of neutral-ion dynamical coupling and the use of self-consistently calculated rates of plasma production from incident energetic electrons.
Our simulations successfully reproduce the observed high latitude temperatures as well as the latitudinal variations of ionospheric peak electron densities that have been observed by the Cassini Radio Science Subsystem experiment (RSS).
We find magnetospheric energy deposition to strongly control the flow of mass and energy in the high and mid-latitude thermosphere and thermospheric dynamics to play a crucial role in driving this flow, highlighting the importance of including dynamics in any high latitude energy balance studies on Saturn and other Gas Giants.
By relating observed H3+ column emissions and temperatures to the same quantities inferred from simulated atmosphere profiles we identify a potential method of better constraining the still unknown abundance of vibrationally excited H2 which strongly affects the H3+ densities.
Our calculations also suggest that local time variability in H3+ column emission flux may be largely driven by local time changes of H3+ densities rather than temperatures.
By exploring the parameter space of possible high latitude electric field strengths and incident energetic electron fluxes, we determine the response of thermospheric polar temperatures to a range of these magnetospheric forcing parameters, illustrating that 10keV electron fluxes of 0.1-1.2mWm-2 in combination with electric field strengths of 80-100mVm-1 produce H3+ emissions consistent with observations.
Our calculations highlight the importance of considering thermospheric temperatures as one of the constraints when examining the state of Saturn's magnetosphere and its coupling to the upper atmosphere.

Introduction
For the Gas Giants in our Solar System the coupling between magnetospheres and atmospheres is likely to play a key role for the energy and momentum balance of their thermospheres and ionospheres.
While the same can be said to be the case at polar latitudes on Earth, its global energy balance due to closer proximity to the Sun is most of the time dominated by solar heating.
Magnetospheric forcing on Earth is controlled by the interaction between the solar wind and magnetosphere via the Dungey cycle (Dungey, 1961), while on Jupiter the planet's rotation represents the primary generator of electric fields and driver of magnetospheric currents which ultimately lead to auroral emissions and enhanced ionospheric Pedersen and Hall currents (e.g., Clarke et al., 2004, Cowley et al., 2004, Bougher et al., 2005).
On Saturn, evidence from auroral observations indicates that planetary rotation and solar wind both play a role, though their exact relative importance is still subject of debate (Clarke et al., 2009).
As first described by Hill (1979), corotation of magnetospheric plasma with a Gas Giant Planet such as Jupiter and Saturn is ultimately ensured by transfer of angular momentum from the upper atmosphere to the magnetosphere via a system of field-aligned Birkeland currents.
In the magnetosphere the Birkeland current system is closed via radial currents in the equatorial plane which via j×B accelerations drive the plasma towards corotation.
In the ionosphere the Birkeland currents close predominantly via field-perpendicular Pedersen currents which exert westward (against the sense of planetary rotation) acceleration on the ionospheric plasma and, via ion-neutral collisions, onto thermospheric neutrals.
The upper atmosphere at auroral latitudes where this coupling occurs will thus corotate to a lesser extent with the planet which, in the rotating frame of the planet, is manifested via westward wind velocities in the thermosphere.
Furthermore, the Pedersen and Hall currents cause thermal heating, often referred to as Joule heating, due to the ionosphere's resistivity.
Using a radial profile of magnetospheric plasma velocities inferred from Voyager plasma observations and assuming fixed ionospheric conductances of 1mho, Cowley et al. (2004) calculated the associated field aligned currents and resulting ionospheric Joule heating rates of around 2.5TW per hemisphere, considerably larger than energy from the direct precipitation of electrons (globally ⩽0.06TW) and solar EUV heating (globally 0.15-0.27TW) (Müller-Wodarg et al., 2006).
Using simultaneous observations of fields and plasmas in Saturn's magnetosphere from Cassini and UV images from the Hubble Space Telescope (HST), Cowley et al. (2008) confirmed their earlier general results but revised the assumed conductances in the southern (summer-) hemisphere up from 1 to 4mho.
Signatures of magnetosphere-atmosphere coupling are the auroral emissions that have been studied on Saturn in the EUV and FUV (emitted by H, H2) and in the IR (emitted by H3+) (Kurth et al., 2009; Melin et al., 2011).
The EUV/FUV emissions are associated primarily with energetic electron precipitation at energies ranging from 5 to 30keV (Sandel et al., 1982; Gérard et al., 2004; Gustin et al., 2009; Lamy et al., 2010).
Galand et al. (2011) studied the response of Saturn's ionosphere to precipitation of hard (10keV) and soft (500eV) electrons using their suprathermal electron transport code.
They self-consistently calculated the ionisation rates and used these as input to the ionospheric model of Moore et al. (2010) to infer the resulting profiles of ion and electron densities.
Galand et al. (2011) calculated Pedersen and Hall conductances as a function of precipitating particle energy and energy flux, deriving a square-root dependency of the conductances to energy flux for hard electrons.
They also found the soft electrons to be important as a source of thermal electron heating but to have a minor influence upon the conductances.
The present study investigates magnetosphere-atmosphere coupling, specifically its effects on Saturn's polar thermosphere and ionosphere.
Our goal is to present a comprehensive assessment of the effects of magnetospheric currents on temperatures, dynamics and composition.
Using a global model of Saturn's coupled thermosphere and ionosphere (Moore et al., 2004, 2010; Müller-Wodarg et al., 2006; Galand et al., 2009, 2011), we self-consistently calculate for the first time the response of the coupled thermosphere-ionosphere system to a range of values for energetic particle precipitation flux and high latitude electric fields.
Through comparisons of our calculations with observed thermospheric temperatures, we define the ranges of magnetospheric parameters that are consistent with atmospheric observations, thereby presenting a framework for using the atmosphere as an additional constraint in quantitatively describing Saturn's coupled magnetosphere/atmosphere system.
Our study extends the work of Galand et al. (2011) in that it calculates the response of the neutral atmosphere to changing conductances, while their calculations had assumed a constant background neutral atmosphere.
Our calculations show that thermospheric dynamics are crucial in determining the thermal structure in the polar atmosphere, highlighting the limitation of any 1-D thermal balance calculation which cannot include horizontal and resulting vertical dynamics.
In Section 2 we introduce the model and provide in Section 2.5 an overview of the simulation input parameters.
Results for key physical quantities are presented in Section 3 alongside comparisons with observations.
We provide a broader discussion of our findings including the limitations of our approach in Section 4.
The STIM model
The main tool in this study is the Saturn Thermosphere Ionosphere Model (STIM), a General Circulation Model (GCM) that treats the global response of Saturn's upper atmosphere to solar and magnetospheric forcing.
Key physical quantities calculated by the code include global neutral temperatures, global densities of neutral and ion constituents, as well as neutral winds and ion drifts.
In Sections 2.1, 2.2, 2.3, 2.4 we describe key components of STIM along with recent updates.
We list the range of simulations presented in this study in Section 2.5.
Thermosphere-ionosphere GCM
Our simulations originate from two codes developed side-by-side but separately, namely, the Saturn Thermosphere GCM (Müller-Wodarg et al., 2006) and Saturn 1-D Ionosphere Model (Moore et al., 2004) which were subsequently fully coupled to form the Saturn Thermosphere Ionosphere Model (STIM).
The thermosphere component globally solves the non-linear Navier-Stokes equations of momentum, continuity and energy on a spherical pressure level grid.
The momentum equation includes terms such as pressure gradients, viscous drag, Coriolis acceleration, curvature accelerations and advection.
The energy equation includes all processes of internal energy redistribution, such as advection, adiabatic heating and cooling as well as molecular and turbulent conduction.
Solar EUV heating is calculated through explicit line-of-sight integration of solar irradiance attenuation (the Lambert-Beer Law), assuming solar spectra derived from the Thermosphere Ionosphere Mesosphere Energetics and Dynamics (TIMED)/Solar EUV Experiment (SEE) (Woods et al., 2005; Woods, 2008) and heating efficiencies of 50%, a value in agreement with estimates for Jupiter by Waite et al. (1983).
While we include direct solar EUV heating in our calculations, it has a negligible influence on the energy balance of Saturn's thermosphere, as shown earlier by Müller-Wodarg et al. (2006).
We show in Section 3 that the main importance of solar EUV radiation lies in its ionising role that leads to conductivities, Joule heating and ion drag which in turn affect the thermospheric energy budget and dynamics.
A new addition to the thermospheric energy equation is the inclusion of H3+ cooling, a process known to be important on Jupiter (Miller et al., 2006, 2010).
At thermospheric temperatures typically found on Saturn (320-500K, Nagy et al., 2009), we do not expect H3+ cooling to play an important role, but we included the process to be able to assess its importance for cases where polar magnetospheric heating raises temperatures above ∼500K.
We implemented globally the H3+ cooling rates of Miller et al. (2010) in the form of a parameterisation as a function of local thermospheric temperature and H3+ density.
The STIM GCM calculates the transport by winds and molecular and turbulent diffusion of key neutral species (H, H2, He, CH4, H2O), following the procedures outlined by Müller-Wodarg et al. (2006).
The global spherical grid has flexible resolution.
For simulations in this study we assumed spacing in latitude and longitude of 2° and 10°, respectively, and a vertical resolution of 0.4 scale heights.
Our time integration step was 5s and we ran the code for 500 Saturn rotations to reach steady state.
Fully coupled chemically and dynamically to the thermosphere is a global ionosphere model based largely on the 1-D model of Moore et al. (2004).
Neutral species undergo primary ionisation by solar EUV photons, assuming the solar spectra specified above.
We include secondary ionisation by suprathermal photoelectrons using the parameterisation of Moore et al. (2009).
The ions (H+, H2+, H3+, He+, CH3+, CH4+, CH5+, H2O+, H3O+) undergo reactions of charge exchange with neutral species and recombination with electrons, following the chemical scheme of Moore et al. (2004), with additional reactions for hydrocarbon ions CH3+, CH4+ and CH5+, as given by Moses and Bass (2000).
We assume Te=Ti=Tn, a reasonable approximation as the relevant chemistry is not strongly influenced by Te (Moore et al., 2008).
We calculate ion velocities resulting from accelerations by magnetospheric electric fields, collisions with neutral gas particles and field-aligned diffusion (Moore et al., 2004).
The ion continuity equation is solved considering photo- and particle ionisation, chemical sources and sinks as well as transport by winds and diffusion.
As shown by Moore et al. (2004), the ionosphere throughout the region studied here (near the main ionospheric peak) is largely in photochemical equilibrium, so dynamics have little influence on the ion distribution.
This was predicted from comparison of transport and chemical lifetimes by Moore et al. (2004) and with the fully coupled model used here we confirm their finding.
In particular, neutral winds are of little importance to the ion distribution.
This is different from what is found in other atmospheres including those of Earth, Venus and Titan.
Water and vibrationally excited H2
Two important components of the ionospheric photochemistry in STIM are the ion charge exchange reactions with ambient neutral water molecules and with vibrationally excited H2.
As shown by Moses and Bass (2000) and Moore et al. (2004), the dominant ion produced through solar ionisation in Saturn's ionosphere is H2+ which primarily results from solar radiation absorption by the dominant neutral species near the main ionospheric peak, H2 (Galand et al., 2009).
The H2+ produced is rapidly lost through charge exchange reactions with H2, forming H3+, a shorter lived ion (relative to H+) whose presence in the auroral regions of Saturn has been confirmed by ground-based observations (Stallard et al., 1999).
Another primary ion produced is H+ which as an atomic ion recombines very slowly with free electrons, making it potentially longer-lived than H3+.
As a result, H+ becomes a key ion alongside H3+ despite the H+ production rate near the ionospheric peak being lower by about an order of magnitude than that of H2+.
In the absence of any further chemical sink, H+ becomes the dominant ion on Saturn and due to its long lifetime barely varies with local time (Moore et al., 2004).
A pattern of no appreciable diurnal behaviour is in contradiction to Saturn Electrostatic Discharge (SED) measurements (Kaiser et al., 1984; Fischer et al., 2011) and the dawn/dusk asymmetries observed by the Cassini Radio Science Subsystem (RSS) experiment (Nagy et al., 2006; Kliore, 2009).
This dawn-dusk asymmetry suggests ionospheric recombination timescales of the dominant ion on Saturn's nightside to be of the order of a few hours, giving ions enough time to recombine on the nightside and their densities to be reduced in the dawn sector.
Two chemical processes have been investigated over the past decades which could effectively destroy H+ ions, thereby reducing its (and the ionosphere's) chemical lifetime, generating local time variations in Saturn's ionosphere.
These are the charge exchange reactions of H+ with water,(1)H++H2O→H2O++Hand with vibrationally excited H2,(2)H++H2(ν⩾4)→H2++HThe reaction rate of (1) assumed in STIM is given by kH2O=8.2×10-9cm3s-1 (Anicich, 1993).
Moore et al. (2006) presented a comparison of calculated ionospheric densities with low latitude Cassini RSS observations (Nagy et al., 2006) and concluded that the observed dawn-dusk asymmetry in the ionosphere at low latitudes was best reproduced by the model when imposing an external influx of neutral water molecules into the low- to mid-latitude upper atmosphere at a rate of (0.5-1.0)×107cm-2s-1.
In their more extensive recent study, Moore et al. (2010) obtained a best fit between latitudinal profiles of Total Electron Content (TEC) in model and data when imposing the water flux as a Gaussian profile centered on the equator with a peak value of 0.5×107cm-2s-1 and full width half maximum (FWHM) of 23.5° latitude.
Fig. 1 shows the influx of water that we assume as upper boundary condition in the present study, as specified in Moore et al. (2010).
Our model calculates the global transport of water molecules by diffusion and advection, and thereby their horizontal and vertical redistribution in the thermosphere.
In imposing a peak water influx at equatorial latitudes, rather than a latitudinally more uniform distribution, we follow the notion that the bulk of gaseous water in the saturnian system would originate from the plumes of Enceladus and impact Saturn's upper atmosphere as a neutral constituent, thereby being unaffected by the magnetosphere and concentrated in the equatorial plane (Moore et al., 2006, 2010).
Globally integrated, our assumed water influx amounts to 5×1026s-1.
Assuming a water source rate from Enceladus of 1×1028s-1 (Jurac and Richardson, 2007; Cassidy and Johnson, 2010), this implies that we assume 5% of the produced water being lost to Saturn's atmosphere, slightly less than the values of 10% and 7% obtained by Jurac and Richardson (2007) and Cassidy and Johnson (2010), respectively.
For reaction (2) above, as discussed by Moore et al. (2010) and Galand et al. (2011), the basic reaction rate of H+ with vibrationally excited H2 has recently been updated to a value of (0.6-1.3)×10-9cm3s-1 (Huestis, 2008).
However, a large uncertainty remains in the fractional abundance of H2(ν⩾4) required for the reaction to proceed.
Moore et al. (2010) defined an "effective" reaction rate (k1∗), the product of the rate k1 for reaction (2) and the volume mixing ratio, χ, of H2(ν⩾4) relative to H2:k1∗=k1·χ(H2(ν⩾4))[cm3s-1].
Thus the uncertainty in the population of vibrationally excited H2 manifests itself in the reaction rate k1∗ of reaction (2).
Moore et al. (2010), in the light of additional Cassini RSS observations, revisited their k1∗ rate and concluded that the best fit between model and observations was obtained when multiplying the original reaction rate of Moses and Bass (2000) by a factor of 0.125 which, with a revised average base reaction rate (from k1=2×10-9 to k1=1×10-9cm3s-1) (Huestis, 2008), corresponds effectively to a reduction of the assumed volume mixing ratio of H2(ν⩾4) by a factor of 4 with respect to that assumed by Moses and Bass (2000).
For a more detailed discussion see Moore et al. (2010) and Galand et al. (2011).
The auroral region, which is the focus of the present study, is subject to energetic electron precipitation from Saturn's magnetosphere.
We expect such precipitation to enhance the population of vibrationally excited H2.
As a result, we have assumed a H2(ν⩾4) abundance of twice the value assumed by Moses and Bass (2000).
This approach was also followed by Galand et al. (2011).
We have adopted this value globally, even though H2(ν⩾4) abundances are expected to be lower at non-auroral latitudes.
Tests with STIM that we carried out for this study have revealed that such variations of H2(ν⩾4) at low and mid latitudes have little influence on the overall response of the thermosphere-ionosphere system to auroral forcing.
Ion drag and Joule heating
Key new additions to the thermospheric component of STIM with respect to that of Müller-Wodarg et al. (2006) are the inclusion of dynamical (momentum) coupling between the thermospheric neutrals and ionospheric ions and self-consistent calculations of Joule heating.
In the absence of an external electric field, ions are constrained in their motion by the magnetic field.
The neutral gases have collisional interactions with ions leading to a viscous-type force damping the motion of the neutral gases relative to that of the ions.
When an external electric field is present, the ions are accelerated and the same collisional interaction leads to an acceleration of the neutral gases in the direction of ion motion.
This latter interaction becomes important at auroral latitudes where an electric field is present.
The ion drag term can, in general, be expressed as(3)ani=-νniu-vwhere ani denotes the acceleration due to neutral-ion collisions in the atmosphere, νni is the neutral-ion collision frequency and u, v are the neutral and ion velocities, respectively.
In our model we implement the ion drag term in a different form, following the procedure used by Fuller-Rowell and Rees (1981), whereby the ion drag term is instead expressed as a function of the current density J in the ionosphere:(4)ani=-νniu-v=1ρJ×Bwhere B denotes the ambient magnetic field in Saturn's ionosphere (Davis and Smith, 1990), and ρ is the atmospheric neutral mass density.
In the simulations presented here, we enforced hemispheric symmetry in the magnetic field.
We calculate the current density J by using a generalisation of Ohm's law(5)J=σ̲·E+u×Bwhere σ̲ denotes the 3×3 conductivity tensor, E is an externally applied electric field (or internal polarisation field) and u×B represents the dynamo field.
Thus, (E+u×B) is the electric field in the neutral rest frame.
Following Rishbeth and Garriott (1969), we assume the concept of layer conductivities, whereby the conducting layer is assumed to have a limited vertical extent and may thus instead be expressed as a 2×2 tensor in the horizontal (latitudinal, zonal), given by(6)σ̲=σP/sin2(I)σH/sin(I)-σH/sin(I)σPHere, σP and σH denote the Pedersen and Hall conductivities, respectively, and I is the dip angle of the magnetic field B.
We calculate σP and σH self-consistently in the model at every grid point.
Combining the 2-D version of (5) with (6) yields expressions for the latitudinal (jθ) and longitudinal (jϕ) components of the current density as(7)jθ=σPsin2(I)Eθ+uϕBr-σHsin(I)-Eϕ+uθBrand(8)jϕ=σPEϕ-uθBr-σHsin(I)Eθ+uϕBrwhere Eθ and Eϕ denote meridional and zonal components of the electric convection or polarisation field, uθ and uϕ are the meridional and zonal neutral wind components and Br is the radial magnetic field.
With Eq.
(4) we obtain for the meridional and zonal ion drag acceleration terms the expressions(9)ani,θ=1ρjϕ·Brand(10)ani,ϕ=-1ρjθ·Brwhich are added to the neutral wind momentum equation of Müller-Wodarg et al. (2006).
The above implementation is consistent with that commonly used by General Circulation Models for Earth, such as the Coupled Thermosphere Ionosphere Model (CTIM) by Fuller-Rowell et al. (1996).
While the above treatment assumes the layer conductivity concept, which neglects vertical currents, we have in a test version of STIM also implemented the ion drag term in its more generalised form using the full 3×3 conductivity tensor and found almost identical results.
In the interest of simplicity and computing speed we have thus retained the 2×2 treatment in our model.
When currents flow in the ionosphere, an environment which is not perfectly conducting, resistive heating occurs, a process often referred to as Joule heating.
Following the treatment of Fuller-Rowell and Rees (1981), we express the rate of Joule heating per unit mass using the relation(11)qJoule=1ρJ·E=1ρjθEθ+jϕEϕNote that the electrical current J in the Joule heating term (Eq.
(5)) includes the effect of neutral winds.
Physically this means that the above expression for Joule heating consists of two components, the thermal heating of the atmosphere by electrical currents and the change of kinetic energy of the atmospheric gases which results from the momentum change due to ion drag (Eqs.
(9) and (10)).
Sometimes this latter component of heating is referred to as "ion drag heating".
While the thermal heating by currents, qJoule∗, can only be a positive quantity, the ion drag heating, qJoule, can also attain negative values, implying loss of kinetic energy of the neutral atmosphere (Vasyliũnas and Song, 2005).
The Joule heating expression (Eq.
(11)) is added to the neutral gas energy equation of Müller-Wodarg et al. (2006).
The ion drag and Joule heating terms are thus calculated self-consistently in STIM, assuming a given external electric field E and auroral electron precipitation.
This electric field originates from the departure of regions in Saturn's magnetosphere from corotation due to plasma production from internal sources.
Thus E represents in our calculations a key parameter determining the coupling between magnetosphere and ionosphere.
In a fully two-way coupled ionosphere-magnetosphere model, the value of E would change in response to atmospheric conditions, but we currently do not include this feedback in our model and define a fixed value of E based on calculations of Cowley et al. (2004).
Auroral electron precipitation
At polar latitudes, Saturn is known to possess auroral ovals which have been observed in the UV (Judge et al., 1980; Clarke et al., 1981; Gustin et al., 2009; Lamy et al., 2009), IR (Geballe et al., 1993; Stallard et al., 1999) and at visible wavelengths, as reviewed by Kurth et al. (2009).
They are signatures of magnetosphere-ionosphere-thermosphere coupling processes, such as precipitation of energetic electrons and ions into the upper atmosphere, yielding ionisation, excitation, dissociation and heating.
Particle ionisation processes exceed solar primary and secondary ionisation in the auroral regions.
Ionisation at auroral latitudes due to precipitating suprathermal electrons thus plays a key role not only locally, but more globally due to the currents that can then flow, which in turn substantially affect the global energy balance.
To account for auroral particle ionisation processes, we calculate ionisation rates from suprathermal magnetospheric electrons using the electron transport model of Galand et al. (2011).
Populations with a mean energy of 10keV have been identified at Saturn in Voyager Ultraviolet Spectrometer (UVS), Hubble Space Telescope (HST), Cassini Plasma Spectrometer (CAPS) and Cassini Ultraviolet Imaging Spectrograph (UVIS) observations (Sandel et al., 1982; Gérard et al., 2004; Grodent et al., 2010).
While we have the option of specifying any electron population in our model, we have chosen for this study to focus on 10keV electrons.
Additionally, we can independently specify the latitudinal distribution as well as any local time variations of the energy flux.
The production rate resulting from the incident electron population is proportional to the assumed energy flux.
Simulation settings
In simulating the response of Saturn's coupled thermosphere-ionosphere system to magnetospheric forcing, we varied two key parameters in the model, namely, the energy flux of precipitating auroral 10keV electrons and the auroral electric field strength.
Table 1 provides a summary of all simulations, which will hereafter be referred to by their run codes (R1-R19).
We have run all simulations for solar minimum conditions (May 15, 2008), identical to the fluxes used by Galand et al. (2009, 2011).
Simulations R1-R18 assume equinox conditions, while R19 is identical to R15 but for southern hemisphere summer conditions.
We find however the overall seasonal variations in Saturn's upper atmosphere outside of the auroral regions to be of secondary importance only.
All simulations were run to steady state for 500 Saturn rotations.
While the ionosphere reaches steady state conditions considerably earlier, the thermosphere is characterised by long thermal time scales, thus requiring long run times before a steady state is reached.
Even so, we note that no evidence is available to determine whether or not Saturn's upper atmosphere is in thermal equilibrium.
Fig. 2 shows the azimuthal (equatorward) electric field strength that we applied in all simulations.
The (co-)latitude variations are consistent with calculations by Cowley et al. (2004) but we chose to vary the peak electric field strength from a maximum value equal to that of Cowley et al. (2004) (E⩽95mVm-1, solid line) to scalings of 0.9 and 0.8 times their value (E⩽85mVm-1 (dotted) and E⩽76mVm-1 (dashed), respectively).
In applying these scalings we explore the sensitivity of the atmosphere to uncertainties and any variation in the electric field strength.
The field is applied symmetrically in both hemispheres (pointing southward in the northern hemisphere and northward in the southern hemisphere) and assumed independent of local time and longitude.
The black box in Fig. 2 indicates the location of maximum precipitating energetic electron flux assumed in this study.
It coincides with the location of sudden change in the degree of corotation.
This shear may contribute towards the acceleration of the particles into the atmosphere (Cowley et al., 2004).
The electric field strength mapped into the polar upper atmosphere is effectively a measure of the degree of corotation of plasma in Saturn's magnetosphere.
A lower electric field strength implies a higher degree of corotation for any given value of conductance.
While not a free parameter per se, enough uncertainties in the observed degree of plasma corotation in Saturn's magnetosphere (Stallard et al., 2004) and in the modelling of associated electric fields justify investigating the atmosphere response to variations of E within ≈20%.
In reality the electric field will be more complex, including a zonal field component as well as longitude, latitude and temporal variations, but this is a reasonable first attempt.
The ionospheric plasma densities, and thereby Pedersen and Hall conductivities in the auroral region, are primarily controlled by the second parameter we vary, the incident electron energy flux.
We assume a single population of electrons (10keV), though in reality other energies are also present.
We assume five different levels of electron energy flux which we allow to vary with local time.
Local time-averages of the fluxes we assumed are listed in Table 1 as 0.07, 0.12, 0.17, 0.22, 0.62 and 1.24mWm-2.
The black line in Fig. 3 shows the local time variation of incident electron energy flux that we assume for the representative case of simulation R15 (see Table 1) at the latitude of maximum incident flux (78° in both hemispheres, see also black marker in Fig. 2).
Fluxes in Fig. 3 vary from 0.03mWm-2 at midnight to 1.3mWm-2 at 08:00h Solar Local Time (SLT).
These local time variations are consistent with those inferred from Hubble Space Telescope (HST) auroral observations in the UV analysed by Lamy et al. (2009), scaled to our assumed average incident flux of 0.62mWm-2 in R15 and to different averages for other simulations, as listed in Table 1.
The local time dependence of the incident electron flux is combined in the model with a latitudinal Gaussian weighting function centered around 78° latitude, assuming a FWHM of 1.4°.
In response to particle precipitation and associated ion production rates, the self-consistently calculated ionospheric plasma densities are locally enhanced, generating an increase in Pedersen and Hall conductances as well as thermal Joule heating.
The red curve of Fig. 3 shows the resulting Pedersen conductances which range from 5mho at 00:40h SLT to 16.7mho at 08:40h SLT, with an average of 11.5mho.
Conductances at Saturn are considerably larger than those at Jupiter due to the weaker magnetic field at Saturn.
Note the 40min SLT (corresponding to ∼17min real time) delay in local time between the maximum in precipitation and that in conductances.
This delay, identified also by Galand et al. (2011) at similar magnitude, is associated with photochemical lifetimes in the ionosphere.
The purely thermal component of Joule heating, qJoule∗ (shown as height-integrated quantity by the dashed blue line in Fig. 3), responds simultaneously to changes in conductance, with a similar delay to the precipitation flux.
However, as discussed in Section 2.3 the actual heating rate, qJoule, in the atmosphere due to Pedersen and Hall currents needs to take into account the neutral wind velocities as well and is shown in Fig. 3 as solid blue line.
Values range from 13.9mWm-2 at 00:00h SLT to 72mWm-2 at 07:20h SLT.
As a result of westward neutral winds the maximum in Joule heating thus interestingly occurs before the maximum in electron precipitation.
This highlights the importance of considering neutral winds when calculating auroral energy deposition rates.
We furthermore note in the figure that height-integrated thermal Joule heating exceeds the height-integrated total Joule heating at all local times, implying that energy is transferred to the thermosphere throughout the day.
The difference between the solid and dashed blue curves is larger during local times of enhanced ion production.
Following Eq.
(11) with Eqs.
(7) and (8) it can be shown that under the assumption of Eϕ=0 (as is the case in our simulations), the sign of the expression [σPuϕ-σHuθsin(I)] determines whether Joule heating is enhanced (positive sign) or reduced (negative sign) due to neutral winds.
While we find the sign of this term to become positive in parts of the bottomside ionosphere, it is negative throughout the ionospheric peak region and above.
In a height-integrated sense, thus, energy is at that particular latitude transferred to the thermosphere.
At other latitudes (not shown) the energy flow locally becomes opposite to that.
Simulation results
Simulations R15 and R19 serve as representative cases for average levels of magnetospheric forcing under equinox and solstice conditions, respectively.
Comparisons with ionosphere and thermosphere observations are used to validate these simulations.
In Section 3.5 we explore the sensitivity of Saturn's upper atmosphere to changes in magnetosphere forcing by analysing the results of all simulations listed in Table 1.
Ionosphere
Vertical profiles of noontime ionospheric plasma densities are shown in Fig. 4 for the case of R15.
The left panel shows profiles in the region of maximum electron precipitation (78°) while the right panel shows densities at the sub-solar point (latitude 0°).
Black lines denote the total electron density, blue lines are H+ and red lines H3+ densities.
Not shown individually are profiles of other ions calculated in the model, namely H2+, CH3+, CH4+, CH5+, H2O+ and H3O+.
The hydrocarbon densities populate the bottomside ionosphere, accounting for most of the electron density below around 1000km altitude.
Calculated electron densities are about a factor of 10 larger in the auroral region than at the equator.
Furthermore, the principal ion at the equator is H3+ while in the auroral region it is H+.
This difference is primarily due to differences in neutral composition, specifically the presence of water at equatorial latitudes.
As shown in Fig. 1, we assume a water influx over the equator and ignore water chemistry poleward of around 40° latitude.
H2O is particularly effective in removing H+ from the ionosphere via the charge-exchange reaction given in Eq.
(1) which generates an ionosphere richer in molecular ions and depleted in H+.
Ionisation rates for 10keV electrons peak in the lower ionosphere near 800km above the 1bar level (Galand et al., 2011), explaining the bottomside secondary maximum in electron densities in the left panel.
Fig. 5 shows the peak electron densities in Saturn's ionosphere as a function of latitude.
The "plus" and "star" symbols are values observed by the Cassini RSS experiment for dusk and dawn conditions, respectively (Nagy et al., 2006; Kliore, 2009).
Blue and red lines are calculated peak electron densities from simulations R15 (equinox) and R19 (southern hemisphere summer), respectively, for dusk (solid lines) and dawn (dashed) conditions.
The RSS observations were made between 2005 and 2008 when Saturn was transitioning from southern hemisphere summer to near-vernal equinox conditions.
Our calculated values for equinox (blue) and solstice (red) capture the range of observed values, thus validating our simulations and furthermore indicating that the observed trends between 2005 and 2008 can be accounted for by changes in solar ionisation.
At low latitudes our calculations reproduce well the observed differences between dusk and dawn densities.
This validates our calculated equatorial ion composition shown in Fig. 4 (right panel), as the dominance of H3+ there generates sufficiently short ion recombination times to produce the observed dawn-dusk asymmetry.
The range of observed peak electron densities is well captured by our calculations.
To-date radio occultations have not yet observed Saturn's ionosphere within the auroral oval.
For clarity, modeled auroral electron density values are not fully captured with the chosen axis range in Fig. 5.
Calculated peak densities at latitude 78° at dusk/dawn (06:00/18:00 SLT) are 2.2×105cm-3/2.0×105cm-3 for the equinox simulation (R15, blue) and for solstice (R19, red) they are 2.4×105cm-3/2.2×105cm-3 in the summer hemisphere (78°S) and 2.2×105cm-3/2.0×105cm-3 in the winter hemisphere (78°N).
Thus, seasonal differences in solar ionisation in the auroral region near the 06:00/18:00 SLT sectors amount to no more than around 10% of the local plasma density.
Despite the longer chemical lifetimes of H+ relative to H3+, chemistry still dominates over dynamics.
As calculated by Moore et al. (2004), the overall chemical lifetime of Saturn's ionosphere below 2500km is τC⩽10-2s.
Meridional wind speeds in the auroral region (discussed in Section 3.3) are below 200ms-1, giving an approximate transport time scale of τu≈103s, considerably longer than τC.
Diffusive time scales for ions are around 105s near the ionospheric peak.
Thus the ionosphere of Saturn near the peak is approximately in photochemical equilibrium.
Furthermore, the large inclination angles of the B field at high latitudes imply primarily vertical redistribution of plasma by horizontal neutral winds.
Thus, horizontal thermospheric winds are ineffective in redistributing plasma densities to regions outside of the regions of particle precipitation, giving rise to the sharp boundaries between regions with and without precipitation seen in Fig. 5.
Thermosphere temperatures
Diurnally-averaged thermospheric temperatures, as calculated in simulation R15, are presented in Fig. 6 for the southern hemisphere (with those in the northern hemisphere being identical).
We find daily variations of polar temperatures to be less than 6K and thus virtually negligible, despite the strong diurnal variation of electron precipitation and thereby Joule heating (Fig. 3).
The reasons for this are the long thermal time scales in Saturn's upper atmosphere combined with the fast planetary rotation rate.
This justifies discussing diurnally-averaged quantities hereafter.
To-date a single polar exospheric temperature value for Saturn has been published from UV stellar occultations and yielded an exospheric temperature of 418±54K at 82°S (Vervack and Moses, 2012).
Temperatures poleward of 75° latitude above the 10-5mbar level in our calculations reach between 350 and 500K, consistent with the value of Vervack and Moses (2012) and the range of observed auroral H3+ temperatures on Saturn of (380-420)±70K (Melin et al., 2007).
Thermospheric temperatures poleward of 80° decrease slightly (⩽50K) when moving to higher levels in the atmosphere above 10-5mbar.
This decrease, as will be shown later, is associated with adiabatic cooling due to atmospheric expansion there.
Thus we find the thermosphere to be approximately isothermal above 10-5mbar to within ∼50K, implying that observed H3+ temperatures (Stallard et al., 1999; Melin et al., 2007) are almost the same as exospheric temperatures in polar regions on Saturn.
The same is suggested by the similarity of exospheric temperatures inferred from Voyager UVS observations at 82°S of 418±54K (Vervack and Moses, 2012) to temperatures inferred from ground-based observations of H3+ IR emissions of 400±50K (Melin et al., 2007), even though these observations were not made at the same time.
Our calculated polar temperatures in R15 thus agree well with observations.
At lower latitudes our calculations do not capture observed values well.
Fig. 6 shows that exospheric temperatures decrease from around 450K near the pole to around 180K near the equator.
Voyager 2 UVS occultations of δ-Sco suggested an exospheric temperature of 420±30K near 29.5°N (Smith et al., 1983), while a recent reanalysis of Voyager UVS data inferred a value of 488±14K (Vervack and Moses, 2012).
These and other observations suggest low and mid-latitude exospheric temperatures on Saturn to be of the order of 450K, roughly twice the value shown in Fig. 6.
Our model is presently unable to reproduce observed low and mid-latitude exospheric temperatures on Saturn, illustrating that magnetospheric energy is not being transported from the polar to the equatorial regions.
This is related to Saturn's fast rotation rate and the sub-corotation of the auroral thermosphere, which ultimately generates a meridional wind transporting energy from equator to pole in the deep atmosphere, thus cooling down the equatorial regions (Smith et al., 2007).
However, since this study is concerned with polar temperatures only we will defer discussion of the equatorial temperature problem to future investigations.
Dynamics and composition
Auroral forcing directly controls polar thermospheric temperatures through the effects of Joule heating (see Section 3.4).
Additionally, the associated ion drag and pressure gradients have a profound influence on thermospheric winds, which in turn also control the energy balance and thus temperatures.
Fig. 7 shows vertical profiles of diurnally-averaged meridional (left panel), zonal (middle panel) and vertical winds (right panel).
Local time variations in all wind components above the 10-5mbar level are below 1%, making the display of diurnally-averaged quantities there plausible.
In the deeper atmosphere at and below 10-5mbar however the ion-neutral momentum coupling is more efficient and causes considerable local time variations in neutral wind velocities.
There the displayed diurnal averages do not fully capture the wind behaviour, which we will discuss separately below.
Fig. 7 displays thermospheric wind velocities in the southern hemisphere for a near-auroral latitude of 82°S (solid line), high mid-latitude of 60°S (dashed) and low mid-latitude of 40°S (dotted).
Velocities are defined as positive southward (here, poleward), eastward and upward.
In the region poleward of the auroral oval (solid line) meridional winds near 10-5mbar are directed poleward, away from the region of Joule heating.
At levels above 4×10-7mbar they reverse direction and blow away from the pole, towards the equator.
Equatorward winds in the upper thermosphere persist towards mid-latitudes as well (dashed and dotted lines), but decreasing from around 200ms-1 near the pole to around 40ms-1 at mid-latitudes and zero at the equator (not shown).
The wind pattern is symmetric in both hemispheres and thus indicates a global meridional circulation cell driven at polar latitudes and consisting of a large pole-to-equator circulation in the upper thermosphere followed by a return flow at lower levels.
The polar forcing via ion drag generates strong westward (sub-corotating) winds at peak velocities of around 1300ms-1 near 82° latitude and ∼1600ms-1 near 78° (not shown).
In order to relate zonal wind velocities near the ionospheric peak to the degree of corotation of the upper atmosphere, Fig. 8 shows latitudinal profiles of the atmospheric angular velocity relative to Saturn's rotational velocity, ω/ΩS.
The solid black line displays the magnetospheric plasma angular velocity of Cowley et al. (2004) from which the electric field used in our simulations (Fig. 2) was derived.
The solid blue line is the atmosphere's diurnally-averaged angular velocity near the ionospheric peak.
The magnetospheric sub-corotation via electric fields mapped into the upper atmosphere is related to sub-corotation of the upper atmosphere with ω/ΩS≈0.45 near 78° latitude in simulation R15 (high precipitation; solid blue line in Fig. 8).
The magnitude of ω/ΩS is affected by the conductivity of the ionosphere.
In simulation R6 (low precipitation, dashed line), the peak incident electron flux is around 20% the value of R15, resulting in maximum Pedersen conductances of 1.4mho in R6 (versus 11.2mho in R15) and ω/ΩS≈0.60 in R6.
Thus, the lower conductances in R6 lead to a lesser degree of sub-corotation in the atmosphere.
At reduced conductances angular momentum is less efficiently transferred from the upper atmosphere to the magnetosphere.
The right panel of Fig. 7 shows vertical divergence winds in Saturn's upper atmosphere.
These are wind velocities generated by the divergence of horizontal winds.
They represent the motion of atmospheric gases relative to levels of fixed pressure (rather than simple expansion/contraction of the atmosphere) (Rishbeth and Müller-Wodarg, 1999).
In our shown simulation (R15) upwelling occurs above the ionospheric peak (near 10-5mbar) and downwelling below.
The vertical divergence wind generates composition changes in the atmosphere relative to pressure levels, which are presented in Fig. 9 for simulation R15.
Solid lines represent mole fractions of neutral gases at 78° and dashed curves are mole fractions over the equator.
The high latitude upwelling identified in Fig. 7 enhances mole fractions of heavier gases (He, blue) at a given pressure level and reduces those of lighter gases (H, black).
We see wind-induced composition changes only above the 10-5mbar pressure level and not below since eddy mixing is the dominant process transporting gases below the homopause (near 10-4mbar in our model) and vertical gradients of mixing ratios are small there.
Hence the auroral CH4 profile (green) is identical to that at the equator.
We note that H2O is present only over the equator since we specified a topside influx of water which peaked over the equator (Fig. 1), so densities at auroral latitudes are negligible.
Not shown in Fig. 9 is the dominant gas (H2) which is given by 1-∑iXi,Xi being the mole fractions of the gases shown in the figure.
H2 mole fractions are close to 1 and, being the principal gas throughout the domain examined, are little affected by vertical motion in the atmosphere.
One important aspect of thermospheric dynamics is the overall transport of gases which they induce.
To examine this, Fig. 10 displays the neutral gas mass flux in Saturn's upper atmosphere which results from neutral wind transport of gases in simulation R15.
The figure displays height- and longitude-integrated mass fluxes from meridional winds (solid line), zonal winds (dotted) and vertical divergence winds (dashed).
Mass fluxes particularly emphasise the importance of dynamics in the lower thermosphere (below the 10-5mbar pressure level) where wind velocities are smaller (see Fig. 7) but mass densities considerably larger than in the upper thermosphere.
Considerable meridional transport occurs in the auroral region, transporting material away from the sub-auroral thermosphere (76-78°) primarily into the polar cap region (poleward of 78°) and to a smaller extent equatorward as well (solid line in Fig. 10).
Vertical transport ensures continuity throughout, supplying mass from the deeper atmosphere to the 76-78° latitude region and transporting material downward in the polar cap area (dashed line in Fig. 10).
Note that the downward wind velocities seen in the right panel of Fig. 7 are the dominant cause of this mass flux in the polar cap, by far offsetting the upwelling that is seen at higher levels where the atmospheric densities are considerably lower.
Similarly, the meridional wind velocities in Fig. 7 (left panel) near the ionospheric peak are responsible for the bulk of meridional mass transport, rather than the high-altitude winds.
Zonal mass fluxes (dotted line in Fig. 10) are negligible (despite the larger zonal wind velocities) since zonal mass density gradients are negligible.
Energy balance
We now examine the thermospheric energy balance in the auroral region.
Fig. 11 shows diurnally-averaged energy terms at (78°S) from simulation R15.
Solid lines denote energy sources and dashed lines are energy sinks.
The dominant energy source is total Joule heating (green) which includes the contribution from thermospheric neutral winds according to Eq.
(11), illustrated also in Fig. 3.
As expected for Saturn, and the polar regions in particular, solar EUV heating (black) plays only a minor role.
Vertical molecular conduction (blue) acts mostly as an energy sink in the upper thermosphere, conducting the energy down into the lower thermosphere (below around 10-4mbar) where it is deposited and represents a key energy source.
Horizontal advection (red) provides the main energy sink in the region of peak heating, due to meridional winds transporting the energy equatorward.
In the upper thermosphere energy is transported from the hotter polar region towards the equator, so advection acts as an energy source near 78°.
Vertical upward winds provide a further key energy sink in the region via adiabatic cooling (magenta) and vertical advection (cyan).
Cooling by H3+ IR emissions (grey) plays a minor role on Saturn, unlike what is found on Jupiter (Miller et al., 2010; Bougher et al., 2005; Achilleos et al., 1998).
Our calculations illustrate that dynamics play a key role in controlling the energy balance on Saturn, particularly in the auroral region.
The mass flux of Fig. 10 can be regarded as representing the bulk energy flow in the atmosphere and thus ultimately also helps to understand the thermal structure (Fig. 6), including the cold equatorial temperatures.
As can be inferred from Fig. 10, auroral (magnetospheric) energy is transported by meridional winds primarily into the polar cap region, explaining the temperature maximum there (Fig. 6).
Equatorward energy transport is negligible despite the upper thermosphere pole-to-equator winds (left panel of Fig. 7) since those occur in a region where the atmospheric density is considerably lower and hence energetically insignificant.
Sensitivity to magnetospheric forcing parameters
Having focused so far on simulations for specific high latitude magnetospheric forcing conditions, we now explore the parameter space of possible electric field and particle precipitation fluxes to examine the atmospheric sensitivity to magnetospheric forcing.
Diurnally-averaged temperatures at the peak ionospheric density level (10-5mbar) and latitude 78° from simulations R1-R18 (Table 1) are shown in the upper panel of Fig. 12 as a function of 10keV electron energy flux and peak electric field strength.
As discussed in Section 3.2 (and shown for R15 in Fig. 6) the temperatures may be regarded as representing to within ±50K exospheric and H3+ temperatures.
While the values are based on equinox simulations, we found seasonal differences to be insignificant, generating temperature changes of ⩽10K.
The bottom panel of Fig. 12 shows as a function of 10keV electron energy flux and peak electric field strength the column emission rates of H3+ calculated from the vertical profiles of H3+ densities and temperatures of simulations R1-R18.
High latitude temperatures in Saturn's upper atmosphere published until recently had values below ∼460K (Melin et al., 2007; Vervack and Moses, 2012), but Melin et al. (2011) and Stallard et al. (2012) have shown that H3+ emission may be brighter than previously indicated, and temperatures hotter.
Using high resolution Cassini Visual and Infrared Mapping Spectrometer (VIMS) images, Melin et al. (2011) inferred temperatures of a segment of the auroral oval of 440±50K.
Even higher temperatures in Saturn's auroral oval of (563-624)±30K were derived from Cassini VIMS observations by Stallard et al. (2012), so auroral temperatures on Saturn up to around 650K are within the observed range.
The thick red line in the upper panel Fig. 12 highlights the 650K contour line and thus roughly separates values of polar temperatures that have been observed on Saturn (T⩽650K) from those that as yet have not been observed (T>650K).
The general trend we find in our simulations is that polar temperatures increase with electric field strength and electron energy flux.
At a given energy flux of 1.2mWm-2 the temperatures increase from 450K to 850K (by a factor of ∼1.9) when increasing the electric field strength from 80mVm-1 to 100mVm-1.
At the lower energy flux of 0.2mWm-2 the temperature changes from 450K to 550K, or by a factor of ∼1.2 for the same change in electric field.
Thus, temperatures are less responsive to electric field variations when ionospheric conductivities (at lower energy fluxes) are smaller.
A wider implication of this finding is that Saturn's thermosphere responds less efficiently to magnetospheric input bursts when it is less ionised and more efficiently when in a more ionised state, either due to enhanced electron energy fluxes or due to enhanced solar EUV ionisation (at solar maximum).
For the case of magnetic storms, therefore, Saturn's upper atmosphere responds stronger to variations in magnetic field if they were preceded by enhancements in precipitating electron fluxes.
A further finding from the upper panel of Fig. 12 relates to possible restrictions on combinations of electric field strength and 10keV electron energy flux.
The bottom left half of the figure (below the thick red line) represents a range of observed temperatures on Saturn (400-650K) and thus of "allowed" combinations of electric field strength and particle flux.
In contrast, combinations of these two magnetospheric forcing parameters that result in temperatures in the top right part of the figure (above the red line) need to be treated with caution as they produce temperatures in excess of observations.
A magnetospheric electric field of ∼100mVm-1 mapped into the ionosphere would in combination with at 10keV electron flux of 1mWm-2 generate thermosphere temperatures of ∼800K, well in excess of observed values.
This combination of values cannot thus occur for extended periods on Saturn.
Most temperature constraints on Saturn's polar thermosphere derive from the analysis of H3+ emissions, so for a more direct comparison of our simulations we have calculated the total wavelength integrated column emission rates of H3+ which are shown in the bottom panel of Fig. 12.
We find emission rates to range from (0.1-3.0)×10-5Wm-2sr-1, which is well within the range of observed emission rates.
The implication of this is further discussed in Section 4.
Finally, it is of interest to relate H3+ emissions in Saturn's auroral region to column temperature and H3+ density.
Fig. 13 displays these quantities as a function of local time for latitude 78°S for the case of simulation R15.
Temperature (black) varies by 5K (or ∼1%) while H3+ column density changes from around (1-15)×1015m-2, a factor of 15 variation.
The IR column emission (red) changes with local time from a minimum near midnight of 0.1×10-6Wm-2sr-1 to a maximum emission near 08:00h SLT of 14×10-6Wm-2sr-1, a factor of 140 variation.
We see from this that variations in IR emissions in our simulations are driven primarily by changes in H3+ abundance and to a lesser extent by temperature changes.
The peaks of all quantities in Fig. 13 near 08:00 SLT are related to the maximum in 10keV electron energy flux which occurs near 08:00 SLT (black line in Fig. 3) which leads to maximum H3+ densities there as well.
As the particle ionisation source decreases towards later hours, fast dissociative recombination of H3+ leads to an almost immediate decrease of its densities as well.
The IR brightness may thus be directly related to the incident particle energy flux or any other major ionisation source.
Discussion and conclusions
Our simulations over a range of magnetospheric forcing parameters and seasons successfully reproduce observed ionospheric densities and high latitude temperatures.
Analysis of the simulations gives a basic understanding of the processes that control the dynamics and energy balance in Saturn's high latitude coupled thermosphere and ionosphere.
We have seen that magnetospheric forcing is responsible for the bulk of energy and mass transport in the atmosphere, driving bulk atmospheric internal mass, momentum and energy redistribution.
Joule heating is a major direct contributor to the energy balance, but internal redistribution of this energy by dynamics is crucial as well.
Thermospheric winds driven by ion drag and Joule-heating induced pressure gradients play a key role in determining the high latitude energy balance and, more far-reaching, in controlling the global distribution - or lack thereof - of magnetospheric energy deposited at high latitudes.
Under the range of conditions examined in this study the general pattern is consistently that of polar energy being "trapped" at high latitudes and not propagating equatorward.
This behaviour results from poleward mass and energy transport in the lower thermosphere, a response previously reported by Smith et al. (2007) with identical conclusions.
We find the sub-corotation of the high latitude thermosphere, which results from magnetospheric plasma sub-corotation and associated electric fields, to be a relatively localised phenomenon which does not extend equatorward of around 65° latitude, beyond which the upper atmosphere is in near co-rotation with the planet.
At reduced atmospheric conductances we find angular momentum to be less efficiently transferred from the upper atmosphere to the magnetosphere, thus directly relating precipitating particle mean energy and energy flux to the efficiency of the magnetosphere-atmosphere dynamical coupling processes.
Our results are directly dependent upon the assumed magnetospheric plasma velocity profile (and high latitude electric field), so any future revisions of our assumed profile (Fig. 8, black line) in the context of Cassini plasma observations in Saturn's magnetosphere will similarly affect our results in terms of wind velocities, degree of atmospheric co-rotation and thermal structure, moderately shifting with latitude the basic interaction patterns that we identified in our calculations.
Our simulations demonstrate that dynamical coupling to Saturn's ionosphere via ion drag (Eq.
(3)) critically controls the pattern of thermospheric winds at high latitudes.
Radio science observations of Saturn's ionosphere over the past decades (Atreya et al., 1984; Nagy et al., 2006; Kliore, 2009) have revealed a high degree of variability.
Our simulations successfully reproduce the overall latitudinal trend of peak electron densities (Fig. 5), suggesting that the overall neutral-ion collisional coupling calculations are likely to be realistic in our model as well, but in looking at a steady state situation we have not considered the effects of a variable ionosphere.
Such observed variations in electron density may have an influence on thermospheric dynamics as well, particularly near the ionospheric peak where we found horizontal winds to vary greatly with local time, responding directly to auroral forcing.
A forthcoming second part of this study will examine variability in Saturn's thermosphere-ionosphere system.
Despite strong thermospheric winds at high latitudes we find photochemical equilibrium to hold remarkably well throughout, reducing any role of thermospheric horizontal winds in redistributing ionisation and giving rise to sharp boundaries in ion densities between the auroral and non-auroral regions.
Such sharp boundaries may in practice affect the propagation of radio waves through the atmosphere, which may be of relevance during radio occultation measurements at auroral latitudes.
To-date no radio science observations have probed the vertical structure of the auroral ionosphere, so our simulations can only be validated there using available H3+ IR observations.
In our calculations the dominant ion at the ionospheric peak varies with latitude.
At mid and high latitudes including the auroral region H+ is the principal ion, while at low latitudes it is H3+ - a consequence of our assumed influx of H2O there.
The shorter chemical lifetimes of H3+ give rise to dawn-dusk asymmetries which have been observed near the equator and are captured remarkably well in our calculations, supporting the notion of an influx of H2O, most likely from Enceladus.
As a result of the dominance of H+ away from the equator (outside of the region of H2O influx), chemical lifetimes there increase, thus reducing the chemical sinks and leading to a build-up of ionisation, despite the higher zenith angles at mid latitudes and reduced solar photo-ionisation.
The increase of peak ion density away from the equator is entirely consistent with observations by Cassini RSS (Nagy et al., 2006; Kliore, 2009; Moore et al., 2010).
At auroral latitudes the main peak in ion production is due to particle ionisation from incident electrons (Galand et al., 2011).
It should be noted, as discussed in Section 2.2, that the H3+ densities for any given ionisation rate however depend also on the largely unknown abundance of vibrationally excited H2.
We assumed twice the value of Moses and Bass (2000), but how realistic is this choice? If the incident electron energy and energy flux are known (from UV auroral observations), it may be possible to constrain the abundance of H2(ν⩾4) by comparing observed H3+ emission rates and inferred temperature values to the same quantities calculated from simulated atmospheric temperature and H3+ profiles.
We found temperatures for large electric field strengths and energy fluxes (upper panel of Fig. 12) to partly exceed the observed values, while at the same time the inferred H3+ emission rates (bottom panel of Fig. 12) were entirely within observed ranges.
This suggests either that our calculations underestimate emission rates or that observed temperatures should in fact be larger for the observed emission rates.
The more likely of these options is that our calculations underestimate the ionospheric H3+ column densities, which itself may be due to us assuming too low abundances of H2(ν⩾4).
Our calculations thus highlight the potential of constraining H2(ν⩾4) abundances in the auroral regions via analysis of H3+ emissions.
Despite direct solar EUV heating of Saturn's upper atmosphere representing a minor energy source only, it is however important to note that solar EUV and shorter wavelength radiation is responsible for the majority of ionisation, and thus conductivity, outside of the narrow band of high latitude electron precipitation.
This, in turn, may control thermospheric temperatures.
As shown in Fig. 2, the region of magnetospheric electric field is considerably wider than the electron precipitation region, so the Joule heating region extends over a much wider region as well.
Therefore, solar radiation does affect high latitude temperatures by means of its role as source of ionisation.
This causes hemispheric differences in high latitude temperatures at solstice.
In our solstice simulation (R19) we find exospheric temperatures averaged from 74°S to 90°S (the summer polar region) of 490K, while averaging over the same latitude band in the northern (winter) hemisphere gives a value of 430K.
This difference of 60K is a direct result of enhanced ionisation in the summer hemisphere, leading to stronger Joule heating there.
We expect solar cycle variations of high latitude temperatures to lie within the same approximate range, stronger in the summer hemisphere than winter hemisphere, where solar ionisation is considerably weaker.
The solar ionisation-induced hemispheric differences in atmospheric conductivity should similarly affect the magnetosphere, highlighting an interesting Sun-atmosphere-magnetosphere coupling chain that deserves more thorough examination in future studies.
Our calculations assumed fixed electrical fields at high latitudes and we did not change these in response to changing conditions in the atmosphere.
In principle, enhanced conductivity would lead to more efficient transport of angular momentum from atmosphere to magnetosphere, thus reducing the departure from co-rotation there and the generated electric field which maps into the upper atmosphere.
By keeping the electric field constant we assume a continuous supply of new material into Saturn's magnetosphere which is ionised and maintains a continuous lag from co-rotation.
This aspect of coupling from atmosphere to magnetosphere is not considered in our model.
Smith and Aylward (2008) developed a simple model of Saturn's coupled thermosphere-ionosphere-magnetosphere which considered the feedback from atmosphere to magnetosphere, but assumed a constant ionosphere which did not change in response to thermospheric and magnetospheric conditions.
The effects of the feedback on the magnetosphere did however have little influence on the atmosphere behaviour, which is the focus of our study here.
Future developments, though, should ideally focus on an upper atmosphere model such as STIM which considers the full feedback to the magnetosphere as well, thereby providing the possibility of using additional observational constraints from the magnetosphere to validate the calculations.
According to the simulations of our study magnetospheric energy cannot explain the observed thermospheric temperatures at low and mid latitudes.
While the "energy crisis" is not the focus of this study, this result emphasises the need to consider thermospheric winds when examining the energy balance, rendering problematic the use of 1-D models which by nature cannot account for winds.
In exploring the parameter space of two magnetospheric forcing parameters, electric field strength and incident particle flux, we have demonstrated that the upper atmosphere observations (in particular, H3+ IR emissions) need to be considered when examining Saturn's magnetosphere.
Sub-corotation of plasma in the magnetosphere will affect atmospheric temperatures and dynamics, and a multi-instrument analysis is necessary to ensure that any magnetospheric observations are consistent with those of the atmosphere.
In applying STIM to an examination of this coupling we have shown that multi-dimensional time-dependent models of the coupled thermosphere-ionosphere-magnetosphere are a powerful and important tool in understanding the exchange of energy and momentum between the regions and in ultimately understanding the global energy balance of Gas Giants within and beyond our Solar System.
Acknowledgments
This work was supported, in part, by a grant from the NASA Planetary Atmosphere Program (PATM) to Boston University.
During part of this work I.M.-W. was funded by a Royal Society University Research Fellowship.
M.G. and I.M.-W. were partially supported by a UK Science and Technology Facilities Council (STFC) grant to Imperial College London.
We would like to thank the International Space Science Institute (ISSI) Bern for their kind hospitality and support of the International Team on Saturn Aeronomy (166), which helped advance through very valuable discussions amongst the Team Members many topics raised in this study.
The H3+ cooling code has been developed as part of the European Modelling and Data Analysis Facility (EMDAF) under construction by Europlanet RI, the European Union funded network of planetary scientists (see http://europlanet.projects.phys.ucl.ac.uk/europlanetjra3).
The authors would like to thank both reviewers of this paper for their constructive and helpful comments.

Magnetosphere-atmosphere coupling at Saturn: 1 - Response of thermosphere and ionosphere to steady state polar forcing

Highlights
► We self-consistently calculate Saturn magnetosphere-ionosphere-thermosphere coupling.
► Magnetospheric energy controls flow of mass and energy in Saturn's thermosphere.
► Thermospheric dynamics are key to understanding high latitude energy balance.
► H3+ observations combined with our simulations may constrain H2 (v⩾4) abundances.
Abstract
We present comprehensive calculations of the steady state response of Saturn's coupled thermosphere-ionosphere to forcing by solar radiation, magnetospheric energetic electron precipitation and high latitude electric fields caused by sub-corotation of magnetospheric plasma.
Significant additions to the physical processes calculated in our Saturn Thermosphere Ionosphere General Circulation Model (STIM-GCM) include the comprehensive and self-consistent treatment of neutral-ion dynamical coupling and the use of self-consistently calculated rates of plasma production from incident energetic electrons.
Our simulations successfully reproduce the observed high latitude temperatures as well as the latitudinal variations of ionospheric peak electron densities that have been observed by the Cassini Radio Science Subsystem experiment (RSS).
We find magnetospheric energy deposition to strongly control the flow of mass and energy in the high and mid-latitude thermosphere and thermospheric dynamics to play a crucial role in driving this flow, highlighting the importance of including dynamics in any high latitude energy balance studies on Saturn and other Gas Giants.
By relating observed H3+ column emissions and temperatures to the same quantities inferred from simulated atmosphere profiles we identify a potential method of better constraining the still unknown abundance of vibrationally excited H2 which strongly affects the H3+ densities.
Our calculations also suggest that local time variability in H3+ column emission flux may be largely driven by local time changes of H3+ densities rather than temperatures.
By exploring the parameter space of possible high latitude electric field strengths and incident energetic electron fluxes, we determine the response of thermospheric polar temperatures to a range of these magnetospheric forcing parameters, illustrating that 10keV electron fluxes of 0.1-1.2mWm-2 in combination with electric field strengths of 80-100mVm-1 produce H3+ emissions consistent with observations.
Our calculations highlight the importance of considering thermospheric temperatures as one of the constraints when examining the state of Saturn's magnetosphere and its coupling to the upper atmosphere.

Introduction
For the Gas Giants in our Solar System the coupling between magnetospheres and atmospheres is likely to play a key role for the energy and momentum balance of their thermospheres and ionospheres.
While the same can be said to be the case at polar latitudes on Earth, its global energy balance due to closer proximity to the Sun is most of the time dominated by solar heating.
Magnetospheric forcing on Earth is controlled by the interaction between the solar wind and magnetosphere via the Dungey cycle (Dungey, 1961), while on Jupiter the planet's rotation represents the primary generator of electric fields and driver of magnetospheric currents which ultimately lead to auroral emissions and enhanced ionospheric Pedersen and Hall currents (e.g., Clarke et al., 2004, Cowley et al., 2004, Bougher et al., 2005).
On Saturn, evidence from auroral observations indicates that planetary rotation and solar wind both play a role, though their exact relative importance is still subject of debate (Clarke et al., 2009).
As first described by Hill (1979), corotation of magnetospheric plasma with a Gas Giant Planet such as Jupiter and Saturn is ultimately ensured by transfer of angular momentum from the upper atmosphere to the magnetosphere via a system of field-aligned Birkeland currents.
In the magnetosphere the Birkeland current system is closed via radial currents in the equatorial plane which via j×B accelerations drive the plasma towards corotation.
In the ionosphere the Birkeland currents close predominantly via field-perpendicular Pedersen currents which exert westward (against the sense of planetary rotation) acceleration on the ionospheric plasma and, via ion-neutral collisions, onto thermospheric neutrals.
The upper atmosphere at auroral latitudes where this coupling occurs will thus corotate to a lesser extent with the planet which, in the rotating frame of the planet, is manifested via westward wind velocities in the thermosphere.
Furthermore, the Pedersen and Hall currents cause thermal heating, often referred to as Joule heating, due to the ionosphere's resistivity.
Using a radial profile of magnetospheric plasma velocities inferred from Voyager plasma observations and assuming fixed ionospheric conductances of 1mho, Cowley et al. (2004) calculated the associated field aligned currents and resulting ionospheric Joule heating rates of around 2.5TW per hemisphere, considerably larger than energy from the direct precipitation of electrons (globally ⩽0.06TW) and solar EUV heating (globally 0.15-0.27TW) (Müller-Wodarg et al., 2006).
Using simultaneous observations of fields and plasmas in Saturn's magnetosphere from Cassini and UV images from the Hubble Space Telescope (HST), Cowley et al. (2008) confirmed their earlier general results but revised the assumed conductances in the southern (summer-) hemisphere up from 1 to 4mho.
Signatures of magnetosphere-atmosphere coupling are the auroral emissions that have been studied on Saturn in the EUV and FUV (emitted by H, H2) and in the IR (emitted by H3+) (Kurth et al., 2009; Melin et al., 2011).
The EUV/FUV emissions are associated primarily with energetic electron precipitation at energies ranging from 5 to 30keV (Sandel et al., 1982; Gérard et al., 2004; Gustin et al., 2009; Lamy et al., 2010).
Galand et al. (2011) studied the response of Saturn's ionosphere to precipitation of hard (10keV) and soft (500eV) electrons using their suprathermal electron transport code.
They self-consistently calculated the ionisation rates and used these as input to the ionospheric model of Moore et al. (2010) to infer the resulting profiles of ion and electron densities.
Galand et al. (2011) calculated Pedersen and Hall conductances as a function of precipitating particle energy and energy flux, deriving a square-root dependency of the conductances to energy flux for hard electrons.
They also found the soft electrons to be important as a source of thermal electron heating but to have a minor influence upon the conductances.
The present study investigates magnetosphere-atmosphere coupling, specifically its effects on Saturn's polar thermosphere and ionosphere.
Our goal is to present a comprehensive assessment of the effects of magnetospheric currents on temperatures, dynamics and composition.
Using a global model of Saturn's coupled thermosphere and ionosphere (Moore et al., 2004, 2010; Müller-Wodarg et al., 2006; Galand et al., 2009, 2011), we self-consistently calculate for the first time the response of the coupled thermosphere-ionosphere system to a range of values for energetic particle precipitation flux and high latitude electric fields.
Through comparisons of our calculations with observed thermospheric temperatures, we define the ranges of magnetospheric parameters that are consistent with atmospheric observations, thereby presenting a framework for using the atmosphere as an additional constraint in quantitatively describing Saturn's coupled magnetosphere/atmosphere system.
Our study extends the work of Galand et al. (2011) in that it calculates the response of the neutral atmosphere to changing conductances, while their calculations had assumed a constant background neutral atmosphere.
Our calculations show that thermospheric dynamics are crucial in determining the thermal structure in the polar atmosphere, highlighting the limitation of any 1-D thermal balance calculation which cannot include horizontal and resulting vertical dynamics.
In Section 2 we introduce the model and provide in Section 2.5 an overview of the simulation input parameters.
Results for key physical quantities are presented in Section 3 alongside comparisons with observations.
We provide a broader discussion of our findings including the limitations of our approach in Section 4.
The STIM model
The main tool in this study is the Saturn Thermosphere Ionosphere Model (STIM), a General Circulation Model (GCM) that treats the global response of Saturn's upper atmosphere to solar and magnetospheric forcing.
Key physical quantities calculated by the code include global neutral temperatures, global densities of neutral and ion constituents, as well as neutral winds and ion drifts.
In Sections 2.1, 2.2, 2.3, 2.4 we describe key components of STIM along with recent updates.
We list the range of simulations presented in this study in Section 2.5.
Thermosphere-ionosphere GCM
Our simulations originate from two codes developed side-by-side but separately, namely, the Saturn Thermosphere GCM (Müller-Wodarg et al., 2006) and Saturn 1-D Ionosphere Model (Moore et al., 2004) which were subsequently fully coupled to form the Saturn Thermosphere Ionosphere Model (STIM).
The thermosphere component globally solves the non-linear Navier-Stokes equations of momentum, continuity and energy on a spherical pressure level grid.
The momentum equation includes terms such as pressure gradients, viscous drag, Coriolis acceleration, curvature accelerations and advection.
The energy equation includes all processes of internal energy redistribution, such as advection, adiabatic heating and cooling as well as molecular and turbulent conduction.
Solar EUV heating is calculated through explicit line-of-sight integration of solar irradiance attenuation (the Lambert-Beer Law), assuming solar spectra derived from the Thermosphere Ionosphere Mesosphere Energetics and Dynamics (TIMED)/Solar EUV Experiment (SEE) (Woods et al., 2005; Woods, 2008) and heating efficiencies of 50%, a value in agreement with estimates for Jupiter by Waite et al. (1983).
While we include direct solar EUV heating in our calculations, it has a negligible influence on the energy balance of Saturn's thermosphere, as shown earlier by Müller-Wodarg et al. (2006).
We show in Section 3 that the main importance of solar EUV radiation lies in its ionising role that leads to conductivities, Joule heating and ion drag which in turn affect the thermospheric energy budget and dynamics.
A new addition to the thermospheric energy equation is the inclusion of H3+ cooling, a process known to be important on Jupiter (Miller et al., 2006, 2010).
At thermospheric temperatures typically found on Saturn (320-500K, Nagy et al., 2009), we do not expect H3+ cooling to play an important role, but we included the process to be able to assess its importance for cases where polar magnetospheric heating raises temperatures above ∼500K.
We implemented globally the H3+ cooling rates of Miller et al. (2010) in the form of a parameterisation as a function of local thermospheric temperature and H3+ density.
The STIM GCM calculates the transport by winds and molecular and turbulent diffusion of key neutral species (H, H2, He, CH4, H2O), following the procedures outlined by Müller-Wodarg et al. (2006).
The global spherical grid has flexible resolution.
For simulations in this study we assumed spacing in latitude and longitude of 2° and 10°, respectively, and a vertical resolution of 0.4 scale heights.
Our time integration step was 5s and we ran the code for 500 Saturn rotations to reach steady state.
Fully coupled chemically and dynamically to the thermosphere is a global ionosphere model based largely on the 1-D model of Moore et al. (2004).
Neutral species undergo primary ionisation by solar EUV photons, assuming the solar spectra specified above.
We include secondary ionisation by suprathermal photoelectrons using the parameterisation of Moore et al. (2009).
The ions (H+, H2+, H3+, He+, CH3+, CH4+, CH5+, H2O+, H3O+) undergo reactions of charge exchange with neutral species and recombination with electrons, following the chemical scheme of Moore et al. (2004), with additional reactions for hydrocarbon ions CH3+, CH4+ and CH5+, as given by Moses and Bass (2000).
We assume Te=Ti=Tn, a reasonable approximation as the relevant chemistry is not strongly influenced by Te (Moore et al., 2008).
We calculate ion velocities resulting from accelerations by magnetospheric electric fields, collisions with neutral gas particles and field-aligned diffusion (Moore et al., 2004).
The ion continuity equation is solved considering photo- and particle ionisation, chemical sources and sinks as well as transport by winds and diffusion.
As shown by Moore et al. (2004), the ionosphere throughout the region studied here (near the main ionospheric peak) is largely in photochemical equilibrium, so dynamics have little influence on the ion distribution.
This was predicted from comparison of transport and chemical lifetimes by Moore et al. (2004) and with the fully coupled model used here we confirm their finding.
In particular, neutral winds are of little importance to the ion distribution.
This is different from what is found in other atmospheres including those of Earth, Venus and Titan.
Water and vibrationally excited H2
Two important components of the ionospheric photochemistry in STIM are the ion charge exchange reactions with ambient neutral water molecules and with vibrationally excited H2.
As shown by Moses and Bass (2000) and Moore et al. (2004), the dominant ion produced through solar ionisation in Saturn's ionosphere is H2+ which primarily results from solar radiation absorption by the dominant neutral species near the main ionospheric peak, H2 (Galand et al., 2009).
The H2+ produced is rapidly lost through charge exchange reactions with H2, forming H3+, a shorter lived ion (relative to H+) whose presence in the auroral regions of Saturn has been confirmed by ground-based observations (Stallard et al., 1999).
Another primary ion produced is H+ which as an atomic ion recombines very slowly with free electrons, making it potentially longer-lived than H3+.
As a result, H+ becomes a key ion alongside H3+ despite the H+ production rate near the ionospheric peak being lower by about an order of magnitude than that of H2+.
In the absence of any further chemical sink, H+ becomes the dominant ion on Saturn and due to its long lifetime barely varies with local time (Moore et al., 2004).
A pattern of no appreciable diurnal behaviour is in contradiction to Saturn Electrostatic Discharge (SED) measurements (Kaiser et al., 1984; Fischer et al., 2011) and the dawn/dusk asymmetries observed by the Cassini Radio Science Subsystem (RSS) experiment (Nagy et al., 2006; Kliore, 2009).
This dawn-dusk asymmetry suggests ionospheric recombination timescales of the dominant ion on Saturn's nightside to be of the order of a few hours, giving ions enough time to recombine on the nightside and their densities to be reduced in the dawn sector.
Two chemical processes have been investigated over the past decades which could effectively destroy H+ ions, thereby reducing its (and the ionosphere's) chemical lifetime, generating local time variations in Saturn's ionosphere.
These are the charge exchange reactions of H+ with water,(1)H++H2O→H2O++Hand with vibrationally excited H2,(2)H++H2(ν⩾4)→H2++HThe reaction rate of (1) assumed in STIM is given by kH2O=8.2×10-9cm3s-1 (Anicich, 1993).
Moore et al. (2006) presented a comparison of calculated ionospheric densities with low latitude Cassini RSS observations (Nagy et al., 2006) and concluded that the observed dawn-dusk asymmetry in the ionosphere at low latitudes was best reproduced by the model when imposing an external influx of neutral water molecules into the low- to mid-latitude upper atmosphere at a rate of (0.5-1.0)×107cm-2s-1.
In their more extensive recent study, Moore et al. (2010) obtained a best fit between latitudinal profiles of Total Electron Content (TEC) in model and data when imposing the water flux as a Gaussian profile centered on the equator with a peak value of 0.5×107cm-2s-1 and full width half maximum (FWHM) of 23.5° latitude.
Fig. 1 shows the influx of water that we assume as upper boundary condition in the present study, as specified in Moore et al. (2010).
Our model calculates the global transport of water molecules by diffusion and advection, and thereby their horizontal and vertical redistribution in the thermosphere.
In imposing a peak water influx at equatorial latitudes, rather than a latitudinally more uniform distribution, we follow the notion that the bulk of gaseous water in the saturnian system would originate from the plumes of Enceladus and impact Saturn's upper atmosphere as a neutral constituent, thereby being unaffected by the magnetosphere and concentrated in the equatorial plane (Moore et al., 2006, 2010).
Globally integrated, our assumed water influx amounts to 5×1026s-1.
Assuming a water source rate from Enceladus of 1×1028s-1 (Jurac and Richardson, 2007; Cassidy and Johnson, 2010), this implies that we assume 5% of the produced water being lost to Saturn's atmosphere, slightly less than the values of 10% and 7% obtained by Jurac and Richardson (2007) and Cassidy and Johnson (2010), respectively.
For reaction (2) above, as discussed by Moore et al. (2010) and Galand et al. (2011), the basic reaction rate of H+ with vibrationally excited H2 has recently been updated to a value of (0.6-1.3)×10-9cm3s-1 (Huestis, 2008).
However, a large uncertainty remains in the fractional abundance of H2(ν⩾4) required for the reaction to proceed.
Moore et al. (2010) defined an "effective" reaction rate (k1∗), the product of the rate k1 for reaction (2) and the volume mixing ratio, χ, of H2(ν⩾4) relative to H2:k1∗=k1·χ(H2(ν⩾4))[cm3s-1].
Thus the uncertainty in the population of vibrationally excited H2 manifests itself in the reaction rate k1∗ of reaction (2).
Moore et al. (2010), in the light of additional Cassini RSS observations, revisited their k1∗ rate and concluded that the best fit between model and observations was obtained when multiplying the original reaction rate of Moses and Bass (2000) by a factor of 0.125 which, with a revised average base reaction rate (from k1=2×10-9 to k1=1×10-9cm3s-1) (Huestis, 2008), corresponds effectively to a reduction of the assumed volume mixing ratio of H2(ν⩾4) by a factor of 4 with respect to that assumed by Moses and Bass (2000).
For a more detailed discussion see Moore et al. (2010) and Galand et al. (2011).
The auroral region, which is the focus of the present study, is subject to energetic electron precipitation from Saturn's magnetosphere.
We expect such precipitation to enhance the population of vibrationally excited H2.
As a result, we have assumed a H2(ν⩾4) abundance of twice the value assumed by Moses and Bass (2000).
This approach was also followed by Galand et al. (2011).
We have adopted this value globally, even though H2(ν⩾4) abundances are expected to be lower at non-auroral latitudes.
Tests with STIM that we carried out for this study have revealed that such variations of H2(ν⩾4) at low and mid latitudes have little influence on the overall response of the thermosphere-ionosphere system to auroral forcing.
Ion drag and Joule heating
Key new additions to the thermospheric component of STIM with respect to that of Müller-Wodarg et al. (2006) are the inclusion of dynamical (momentum) coupling between the thermospheric neutrals and ionospheric ions and self-consistent calculations of Joule heating.
In the absence of an external electric field, ions are constrained in their motion by the magnetic field.
The neutral gases have collisional interactions with ions leading to a viscous-type force damping the motion of the neutral gases relative to that of the ions.
When an external electric field is present, the ions are accelerated and the same collisional interaction leads to an acceleration of the neutral gases in the direction of ion motion.
This latter interaction becomes important at auroral latitudes where an electric field is present.
The ion drag term can, in general, be expressed as(3)ani=-νniu-vwhere ani denotes the acceleration due to neutral-ion collisions in the atmosphere, νni is the neutral-ion collision frequency and u, v are the neutral and ion velocities, respectively.
In our model we implement the ion drag term in a different form, following the procedure used by Fuller-Rowell and Rees (1981), whereby the ion drag term is instead expressed as a function of the current density J in the ionosphere:(4)ani=-νniu-v=1ρJ×Bwhere B denotes the ambient magnetic field in Saturn's ionosphere (Davis and Smith, 1990), and ρ is the atmospheric neutral mass density.
In the simulations presented here, we enforced hemispheric symmetry in the magnetic field.
We calculate the current density J by using a generalisation of Ohm's law(5)J=σ̲·E+u×Bwhere σ̲ denotes the 3×3 conductivity tensor, E is an externally applied electric field (or internal polarisation field) and u×B represents the dynamo field.
Thus, (E+u×B) is the electric field in the neutral rest frame.
Following Rishbeth and Garriott (1969), we assume the concept of layer conductivities, whereby the conducting layer is assumed to have a limited vertical extent and may thus instead be expressed as a 2×2 tensor in the horizontal (latitudinal, zonal), given by(6)σ̲=σP/sin2(I)σH/sin(I)-σH/sin(I)σPHere, σP and σH denote the Pedersen and Hall conductivities, respectively, and I is the dip angle of the magnetic field B.
We calculate σP and σH self-consistently in the model at every grid point.
Combining the 2-D version of (5) with (6) yields expressions for the latitudinal (jθ) and longitudinal (jϕ) components of the current density as(7)jθ=σPsin2(I)Eθ+uϕBr-σHsin(I)-Eϕ+uθBrand(8)jϕ=σPEϕ-uθBr-σHsin(I)Eθ+uϕBrwhere Eθ and Eϕ denote meridional and zonal components of the electric convection or polarisation field, uθ and uϕ are the meridional and zonal neutral wind components and Br is the radial magnetic field.
With Eq.
(4) we obtain for the meridional and zonal ion drag acceleration terms the expressions(9)ani,θ=1ρjϕ·Brand(10)ani,ϕ=-1ρjθ·Brwhich are added to the neutral wind momentum equation of Müller-Wodarg et al. (2006).
The above implementation is consistent with that commonly used by General Circulation Models for Earth, such as the Coupled Thermosphere Ionosphere Model (CTIM) by Fuller-Rowell et al. (1996).
While the above treatment assumes the layer conductivity concept, which neglects vertical currents, we have in a test version of STIM also implemented the ion drag term in its more generalised form using the full 3×3 conductivity tensor and found almost identical results.
In the interest of simplicity and computing speed we have thus retained the 2×2 treatment in our model.
When currents flow in the ionosphere, an environment which is not perfectly conducting, resistive heating occurs, a process often referred to as Joule heating.
Following the treatment of Fuller-Rowell and Rees (1981), we express the rate of Joule heating per unit mass using the relation(11)qJoule=1ρJ·E=1ρjθEθ+jϕEϕNote that the electrical current J in the Joule heating term (Eq.
(5)) includes the effect of neutral winds.
Physically this means that the above expression for Joule heating consists of two components, the thermal heating of the atmosphere by electrical currents and the change of kinetic energy of the atmospheric gases which results from the momentum change due to ion drag (Eqs.
(9) and (10)).
Sometimes this latter component of heating is referred to as "ion drag heating".
While the thermal heating by currents, qJoule∗, can only be a positive quantity, the ion drag heating, qJoule, can also attain negative values, implying loss of kinetic energy of the neutral atmosphere (Vasyliũnas and Song, 2005).
The Joule heating expression (Eq.
(11)) is added to the neutral gas energy equation of Müller-Wodarg et al. (2006).
The ion drag and Joule heating terms are thus calculated self-consistently in STIM, assuming a given external electric field E and auroral electron precipitation.
This electric field originates from the departure of regions in Saturn's magnetosphere from corotation due to plasma production from internal sources.
Thus E represents in our calculations a key parameter determining the coupling between magnetosphere and ionosphere.
In a fully two-way coupled ionosphere-magnetosphere model, the value of E would change in response to atmospheric conditions, but we currently do not include this feedback in our model and define a fixed value of E based on calculations of Cowley et al. (2004).
Auroral electron precipitation
At polar latitudes, Saturn is known to possess auroral ovals which have been observed in the UV (Judge et al., 1980; Clarke et al., 1981; Gustin et al., 2009; Lamy et al., 2009), IR (Geballe et al., 1993; Stallard et al., 1999) and at visible wavelengths, as reviewed by Kurth et al. (2009).
They are signatures of magnetosphere-ionosphere-thermosphere coupling processes, such as precipitation of energetic electrons and ions into the upper atmosphere, yielding ionisation, excitation, dissociation and heating.
Particle ionisation processes exceed solar primary and secondary ionisation in the auroral regions.
Ionisation at auroral latitudes due to precipitating suprathermal electrons thus plays a key role not only locally, but more globally due to the currents that can then flow, which in turn substantially affect the global energy balance.
To account for auroral particle ionisation processes, we calculate ionisation rates from suprathermal magnetospheric electrons using the electron transport model of Galand et al. (2011).
Populations with a mean energy of 10keV have been identified at Saturn in Voyager Ultraviolet Spectrometer (UVS), Hubble Space Telescope (HST), Cassini Plasma Spectrometer (CAPS) and Cassini Ultraviolet Imaging Spectrograph (UVIS) observations (Sandel et al., 1982; Gérard et al., 2004; Grodent et al., 2010).
While we have the option of specifying any electron population in our model, we have chosen for this study to focus on 10keV electrons.
Additionally, we can independently specify the latitudinal distribution as well as any local time variations of the energy flux.
The production rate resulting from the incident electron population is proportional to the assumed energy flux.
Simulation settings
In simulating the response of Saturn's coupled thermosphere-ionosphere system to magnetospheric forcing, we varied two key parameters in the model, namely, the energy flux of precipitating auroral 10keV electrons and the auroral electric field strength.
Table 1 provides a summary of all simulations, which will hereafter be referred to by their run codes (R1-R19).
We have run all simulations for solar minimum conditions (May 15, 2008), identical to the fluxes used by Galand et al. (2009, 2011).
Simulations R1-R18 assume equinox conditions, while R19 is identical to R15 but for southern hemisphere summer conditions.
We find however the overall seasonal variations in Saturn's upper atmosphere outside of the auroral regions to be of secondary importance only.
All simulations were run to steady state for 500 Saturn rotations.
While the ionosphere reaches steady state conditions considerably earlier, the thermosphere is characterised by long thermal time scales, thus requiring long run times before a steady state is reached.
Even so, we note that no evidence is available to determine whether or not Saturn's upper atmosphere is in thermal equilibrium.
Fig. 2 shows the azimuthal (equatorward) electric field strength that we applied in all simulations.
The (co-)latitude variations are consistent with calculations by Cowley et al. (2004) but we chose to vary the peak electric field strength from a maximum value equal to that of Cowley et al. (2004) (E⩽95mVm-1, solid line) to scalings of 0.9 and 0.8 times their value (E⩽85mVm-1 (dotted) and E⩽76mVm-1 (dashed), respectively).
In applying these scalings we explore the sensitivity of the atmosphere to uncertainties and any variation in the electric field strength.
The field is applied symmetrically in both hemispheres (pointing southward in the northern hemisphere and northward in the southern hemisphere) and assumed independent of local time and longitude.
The black box in Fig. 2 indicates the location of maximum precipitating energetic electron flux assumed in this study.
It coincides with the location of sudden change in the degree of corotation.
This shear may contribute towards the acceleration of the particles into the atmosphere (Cowley et al., 2004).
The electric field strength mapped into the polar upper atmosphere is effectively a measure of the degree of corotation of plasma in Saturn's magnetosphere.
A lower electric field strength implies a higher degree of corotation for any given value of conductance.
While not a free parameter per se, enough uncertainties in the observed degree of plasma corotation in Saturn's magnetosphere (Stallard et al., 2004) and in the modelling of associated electric fields justify investigating the atmosphere response to variations of E within ≈20%.
In reality the electric field will be more complex, including a zonal field component as well as longitude, latitude and temporal variations, but this is a reasonable first attempt.
The ionospheric plasma densities, and thereby Pedersen and Hall conductivities in the auroral region, are primarily controlled by the second parameter we vary, the incident electron energy flux.
We assume a single population of electrons (10keV), though in reality other energies are also present.
We assume five different levels of electron energy flux which we allow to vary with local time.
Local time-averages of the fluxes we assumed are listed in Table 1 as 0.07, 0.12, 0.17, 0.22, 0.62 and 1.24mWm-2.
The black line in Fig. 3 shows the local time variation of incident electron energy flux that we assume for the representative case of simulation R15 (see Table 1) at the latitude of maximum incident flux (78° in both hemispheres, see also black marker in Fig. 2).
Fluxes in Fig. 3 vary from 0.03mWm-2 at midnight to 1.3mWm-2 at 08:00h Solar Local Time (SLT).
These local time variations are consistent with those inferred from Hubble Space Telescope (HST) auroral observations in the UV analysed by Lamy et al. (2009), scaled to our assumed average incident flux of 0.62mWm-2 in R15 and to different averages for other simulations, as listed in Table 1.
The local time dependence of the incident electron flux is combined in the model with a latitudinal Gaussian weighting function centered around 78° latitude, assuming a FWHM of 1.4°.
In response to particle precipitation and associated ion production rates, the self-consistently calculated ionospheric plasma densities are locally enhanced, generating an increase in Pedersen and Hall conductances as well as thermal Joule heating.
The red curve of Fig. 3 shows the resulting Pedersen conductances which range from 5mho at 00:40h SLT to 16.7mho at 08:40h SLT, with an average of 11.5mho.
Conductances at Saturn are considerably larger than those at Jupiter due to the weaker magnetic field at Saturn.
Note the 40min SLT (corresponding to ∼17min real time) delay in local time between the maximum in precipitation and that in conductances.
This delay, identified also by Galand et al. (2011) at similar magnitude, is associated with photochemical lifetimes in the ionosphere.
The purely thermal component of Joule heating, qJoule∗ (shown as height-integrated quantity by the dashed blue line in Fig. 3), responds simultaneously to changes in conductance, with a similar delay to the precipitation flux.
However, as discussed in Section 2.3 the actual heating rate, qJoule, in the atmosphere due to Pedersen and Hall currents needs to take into account the neutral wind velocities as well and is shown in Fig. 3 as solid blue line.
Values range from 13.9mWm-2 at 00:00h SLT to 72mWm-2 at 07:20h SLT.
As a result of westward neutral winds the maximum in Joule heating thus interestingly occurs before the maximum in electron precipitation.
This highlights the importance of considering neutral winds when calculating auroral energy deposition rates.
We furthermore note in the figure that height-integrated thermal Joule heating exceeds the height-integrated total Joule heating at all local times, implying that energy is transferred to the thermosphere throughout the day.
The difference between the solid and dashed blue curves is larger during local times of enhanced ion production.
Following Eq.
(11) with Eqs.
(7) and (8) it can be shown that under the assumption of Eϕ=0 (as is the case in our simulations), the sign of the expression [σPuϕ-σHuθsin(I)] determines whether Joule heating is enhanced (positive sign) or reduced (negative sign) due to neutral winds.
While we find the sign of this term to become positive in parts of the bottomside ionosphere, it is negative throughout the ionospheric peak region and above.
In a height-integrated sense, thus, energy is at that particular latitude transferred to the thermosphere.
At other latitudes (not shown) the energy flow locally becomes opposite to that.
Simulation results
Simulations R15 and R19 serve as representative cases for average levels of magnetospheric forcing under equinox and solstice conditions, respectively.
Comparisons with ionosphere and thermosphere observations are used to validate these simulations.
In Section 3.5 we explore the sensitivity of Saturn's upper atmosphere to changes in magnetosphere forcing by analysing the results of all simulations listed in Table 1.
Ionosphere
Vertical profiles of noontime ionospheric plasma densities are shown in Fig. 4 for the case of R15.
The left panel shows profiles in the region of maximum electron precipitation (78°) while the right panel shows densities at the sub-solar point (latitude 0°).
Black lines denote the total electron density, blue lines are H+ and red lines H3+ densities.
Not shown individually are profiles of other ions calculated in the model, namely H2+, CH3+, CH4+, CH5+, H2O+ and H3O+.
The hydrocarbon densities populate the bottomside ionosphere, accounting for most of the electron density below around 1000km altitude.
Calculated electron densities are about a factor of 10 larger in the auroral region than at the equator.
Furthermore, the principal ion at the equator is H3+ while in the auroral region it is H+.
This difference is primarily due to differences in neutral composition, specifically the presence of water at equatorial latitudes.
As shown in Fig. 1, we assume a water influx over the equator and ignore water chemistry poleward of around 40° latitude.
H2O is particularly effective in removing H+ from the ionosphere via the charge-exchange reaction given in Eq.
(1) which generates an ionosphere richer in molecular ions and depleted in H+.
Ionisation rates for 10keV electrons peak in the lower ionosphere near 800km above the 1bar level (Galand et al., 2011), explaining the bottomside secondary maximum in electron densities in the left panel.
Fig. 5 shows the peak electron densities in Saturn's ionosphere as a function of latitude.
The "plus" and "star" symbols are values observed by the Cassini RSS experiment for dusk and dawn conditions, respectively (Nagy et al., 2006; Kliore, 2009).
Blue and red lines are calculated peak electron densities from simulations R15 (equinox) and R19 (southern hemisphere summer), respectively, for dusk (solid lines) and dawn (dashed) conditions.
The RSS observations were made between 2005 and 2008 when Saturn was transitioning from southern hemisphere summer to near-vernal equinox conditions.
Our calculated values for equinox (blue) and solstice (red) capture the range of observed values, thus validating our simulations and furthermore indicating that the observed trends between 2005 and 2008 can be accounted for by changes in solar ionisation.
At low latitudes our calculations reproduce well the observed differences between dusk and dawn densities.
This validates our calculated equatorial ion composition shown in Fig. 4 (right panel), as the dominance of H3+ there generates sufficiently short ion recombination times to produce the observed dawn-dusk asymmetry.
The range of observed peak electron densities is well captured by our calculations.
To-date radio occultations have not yet observed Saturn's ionosphere within the auroral oval.
For clarity, modeled auroral electron density values are not fully captured with the chosen axis range in Fig. 5.
Calculated peak densities at latitude 78° at dusk/dawn (06:00/18:00 SLT) are 2.2×105cm-3/2.0×105cm-3 for the equinox simulation (R15, blue) and for solstice (R19, red) they are 2.4×105cm-3/2.2×105cm-3 in the summer hemisphere (78°S) and 2.2×105cm-3/2.0×105cm-3 in the winter hemisphere (78°N).
Thus, seasonal differences in solar ionisation in the auroral region near the 06:00/18:00 SLT sectors amount to no more than around 10% of the local plasma density.
Despite the longer chemical lifetimes of H+ relative to H3+, chemistry still dominates over dynamics.
As calculated by Moore et al. (2004), the overall chemical lifetime of Saturn's ionosphere below 2500km is τC⩽10-2s.
Meridional wind speeds in the auroral region (discussed in Section 3.3) are below 200ms-1, giving an approximate transport time scale of τu≈103s, considerably longer than τC.
Diffusive time scales for ions are around 105s near the ionospheric peak.
Thus the ionosphere of Saturn near the peak is approximately in photochemical equilibrium.
Furthermore, the large inclination angles of the B field at high latitudes imply primarily vertical redistribution of plasma by horizontal neutral winds.
Thus, horizontal thermospheric winds are ineffective in redistributing plasma densities to regions outside of the regions of particle precipitation, giving rise to the sharp boundaries between regions with and without precipitation seen in Fig. 5.
Thermosphere temperatures
Diurnally-averaged thermospheric temperatures, as calculated in simulation R15, are presented in Fig. 6 for the southern hemisphere (with those in the northern hemisphere being identical).
We find daily variations of polar temperatures to be less than 6K and thus virtually negligible, despite the strong diurnal variation of electron precipitation and thereby Joule heating (Fig. 3).
The reasons for this are the long thermal time scales in Saturn's upper atmosphere combined with the fast planetary rotation rate.
This justifies discussing diurnally-averaged quantities hereafter.
To-date a single polar exospheric temperature value for Saturn has been published from UV stellar occultations and yielded an exospheric temperature of 418±54K at 82°S (Vervack and Moses, 2012).
Temperatures poleward of 75° latitude above the 10-5mbar level in our calculations reach between 350 and 500K, consistent with the value of Vervack and Moses (2012) and the range of observed auroral H3+ temperatures on Saturn of (380-420)±70K (Melin et al., 2007).
Thermospheric temperatures poleward of 80° decrease slightly (⩽50K) when moving to higher levels in the atmosphere above 10-5mbar.
This decrease, as will be shown later, is associated with adiabatic cooling due to atmospheric expansion there.
Thus we find the thermosphere to be approximately isothermal above 10-5mbar to within ∼50K, implying that observed H3+ temperatures (Stallard et al., 1999; Melin et al., 2007) are almost the same as exospheric temperatures in polar regions on Saturn.
The same is suggested by the similarity of exospheric temperatures inferred from Voyager UVS observations at 82°S of 418±54K (Vervack and Moses, 2012) to temperatures inferred from ground-based observations of H3+ IR emissions of 400±50K (Melin et al., 2007), even though these observations were not made at the same time.
Our calculated polar temperatures in R15 thus agree well with observations.
At lower latitudes our calculations do not capture observed values well.
Fig. 6 shows that exospheric temperatures decrease from around 450K near the pole to around 180K near the equator.
Voyager 2 UVS occultations of δ-Sco suggested an exospheric temperature of 420±30K near 29.5°N (Smith et al., 1983), while a recent reanalysis of Voyager UVS data inferred a value of 488±14K (Vervack and Moses, 2012).
These and other observations suggest low and mid-latitude exospheric temperatures on Saturn to be of the order of 450K, roughly twice the value shown in Fig. 6.
Our model is presently unable to reproduce observed low and mid-latitude exospheric temperatures on Saturn, illustrating that magnetospheric energy is not being transported from the polar to the equatorial regions.
This is related to Saturn's fast rotation rate and the sub-corotation of the auroral thermosphere, which ultimately generates a meridional wind transporting energy from equator to pole in the deep atmosphere, thus cooling down the equatorial regions (Smith et al., 2007).
However, since this study is concerned with polar temperatures only we will defer discussion of the equatorial temperature problem to future investigations.
Dynamics and composition
Auroral forcing directly controls polar thermospheric temperatures through the effects of Joule heating (see Section 3.4).
Additionally, the associated ion drag and pressure gradients have a profound influence on thermospheric winds, which in turn also control the energy balance and thus temperatures.
Fig. 7 shows vertical profiles of diurnally-averaged meridional (left panel), zonal (middle panel) and vertical winds (right panel).
Local time variations in all wind components above the 10-5mbar level are below 1%, making the display of diurnally-averaged quantities there plausible.
In the deeper atmosphere at and below 10-5mbar however the ion-neutral momentum coupling is more efficient and causes considerable local time variations in neutral wind velocities.
There the displayed diurnal averages do not fully capture the wind behaviour, which we will discuss separately below.
Fig. 7 displays thermospheric wind velocities in the southern hemisphere for a near-auroral latitude of 82°S (solid line), high mid-latitude of 60°S (dashed) and low mid-latitude of 40°S (dotted).
Velocities are defined as positive southward (here, poleward), eastward and upward.
In the region poleward of the auroral oval (solid line) meridional winds near 10-5mbar are directed poleward, away from the region of Joule heating.
At levels above 4×10-7mbar they reverse direction and blow away from the pole, towards the equator.
Equatorward winds in the upper thermosphere persist towards mid-latitudes as well (dashed and dotted lines), but decreasing from around 200ms-1 near the pole to around 40ms-1 at mid-latitudes and zero at the equator (not shown).
The wind pattern is symmetric in both hemispheres and thus indicates a global meridional circulation cell driven at polar latitudes and consisting of a large pole-to-equator circulation in the upper thermosphere followed by a return flow at lower levels.
The polar forcing via ion drag generates strong westward (sub-corotating) winds at peak velocities of around 1300ms-1 near 82° latitude and ∼1600ms-1 near 78° (not shown).
In order to relate zonal wind velocities near the ionospheric peak to the degree of corotation of the upper atmosphere, Fig. 8 shows latitudinal profiles of the atmospheric angular velocity relative to Saturn's rotational velocity, ω/ΩS.
The solid black line displays the magnetospheric plasma angular velocity of Cowley et al. (2004) from which the electric field used in our simulations (Fig. 2) was derived.
The solid blue line is the atmosphere's diurnally-averaged angular velocity near the ionospheric peak.
The magnetospheric sub-corotation via electric fields mapped into the upper atmosphere is related to sub-corotation of the upper atmosphere with ω/ΩS≈0.45 near 78° latitude in simulation R15 (high precipitation; solid blue line in Fig. 8).
The magnitude of ω/ΩS is affected by the conductivity of the ionosphere.
In simulation R6 (low precipitation, dashed line), the peak incident electron flux is around 20% the value of R15, resulting in maximum Pedersen conductances of 1.4mho in R6 (versus 11.2mho in R15) and ω/ΩS≈0.60 in R6.
Thus, the lower conductances in R6 lead to a lesser degree of sub-corotation in the atmosphere.
At reduced conductances angular momentum is less efficiently transferred from the upper atmosphere to the magnetosphere.
The right panel of Fig. 7 shows vertical divergence winds in Saturn's upper atmosphere.
These are wind velocities generated by the divergence of horizontal winds.
They represent the motion of atmospheric gases relative to levels of fixed pressure (rather than simple expansion/contraction of the atmosphere) (Rishbeth and Müller-Wodarg, 1999).
In our shown simulation (R15) upwelling occurs above the ionospheric peak (near 10-5mbar) and downwelling below.
The vertical divergence wind generates composition changes in the atmosphere relative to pressure levels, which are presented in Fig. 9 for simulation R15.
Solid lines represent mole fractions of neutral gases at 78° and dashed curves are mole fractions over the equator.
The high latitude upwelling identified in Fig. 7 enhances mole fractions of heavier gases (He, blue) at a given pressure level and reduces those of lighter gases (H, black).
We see wind-induced composition changes only above the 10-5mbar pressure level and not below since eddy mixing is the dominant process transporting gases below the homopause (near 10-4mbar in our model) and vertical gradients of mixing ratios are small there.
Hence the auroral CH4 profile (green) is identical to that at the equator.
We note that H2O is present only over the equator since we specified a topside influx of water which peaked over the equator (Fig. 1), so densities at auroral latitudes are negligible.
Not shown in Fig. 9 is the dominant gas (H2) which is given by 1-∑iXi,Xi being the mole fractions of the gases shown in the figure.
H2 mole fractions are close to 1 and, being the principal gas throughout the domain examined, are little affected by vertical motion in the atmosphere.
One important aspect of thermospheric dynamics is the overall transport of gases which they induce.
To examine this, Fig. 10 displays the neutral gas mass flux in Saturn's upper atmosphere which results from neutral wind transport of gases in simulation R15.
The figure displays height- and longitude-integrated mass fluxes from meridional winds (solid line), zonal winds (dotted) and vertical divergence winds (dashed).
Mass fluxes particularly emphasise the importance of dynamics in the lower thermosphere (below the 10-5mbar pressure level) where wind velocities are smaller (see Fig. 7) but mass densities considerably larger than in the upper thermosphere.
Considerable meridional transport occurs in the auroral region, transporting material away from the sub-auroral thermosphere (76-78°) primarily into the polar cap region (poleward of 78°) and to a smaller extent equatorward as well (solid line in Fig. 10).
Vertical transport ensures continuity throughout, supplying mass from the deeper atmosphere to the 76-78° latitude region and transporting material downward in the polar cap area (dashed line in Fig. 10).
Note that the downward wind velocities seen in the right panel of Fig. 7 are the dominant cause of this mass flux in the polar cap, by far offsetting the upwelling that is seen at higher levels where the atmospheric densities are considerably lower.
Similarly, the meridional wind velocities in Fig. 7 (left panel) near the ionospheric peak are responsible for the bulk of meridional mass transport, rather than the high-altitude winds.
Zonal mass fluxes (dotted line in Fig. 10) are negligible (despite the larger zonal wind velocities) since zonal mass density gradients are negligible.
Energy balance
We now examine the thermospheric energy balance in the auroral region.
Fig. 11 shows diurnally-averaged energy terms at (78°S) from simulation R15.
Solid lines denote energy sources and dashed lines are energy sinks.
The dominant energy source is total Joule heating (green) which includes the contribution from thermospheric neutral winds according to Eq.
(11), illustrated also in Fig. 3.
As expected for Saturn, and the polar regions in particular, solar EUV heating (black) plays only a minor role.
Vertical molecular conduction (blue) acts mostly as an energy sink in the upper thermosphere, conducting the energy down into the lower thermosphere (below around 10-4mbar) where it is deposited and represents a key energy source.
Horizontal advection (red) provides the main energy sink in the region of peak heating, due to meridional winds transporting the energy equatorward.
In the upper thermosphere energy is transported from the hotter polar region towards the equator, so advection acts as an energy source near 78°.
Vertical upward winds provide a further key energy sink in the region via adiabatic cooling (magenta) and vertical advection (cyan).
Cooling by H3+ IR emissions (grey) plays a minor role on Saturn, unlike what is found on Jupiter (Miller et al., 2010; Bougher et al., 2005; Achilleos et al., 1998).
Our calculations illustrate that dynamics play a key role in controlling the energy balance on Saturn, particularly in the auroral region.
The mass flux of Fig. 10 can be regarded as representing the bulk energy flow in the atmosphere and thus ultimately also helps to understand the thermal structure (Fig. 6), including the cold equatorial temperatures.
As can be inferred from Fig. 10, auroral (magnetospheric) energy is transported by meridional winds primarily into the polar cap region, explaining the temperature maximum there (Fig. 6).
Equatorward energy transport is negligible despite the upper thermosphere pole-to-equator winds (left panel of Fig. 7) since those occur in a region where the atmospheric density is considerably lower and hence energetically insignificant.
Sensitivity to magnetospheric forcing parameters
Having focused so far on simulations for specific high latitude magnetospheric forcing conditions, we now explore the parameter space of possible electric field and particle precipitation fluxes to examine the atmospheric sensitivity to magnetospheric forcing.
Diurnally-averaged temperatures at the peak ionospheric density level (10-5mbar) and latitude 78° from simulations R1-R18 (Table 1) are shown in the upper panel of Fig. 12 as a function of 10keV electron energy flux and peak electric field strength.
As discussed in Section 3.2 (and shown for R15 in Fig. 6) the temperatures may be regarded as representing to within ±50K exospheric and H3+ temperatures.
While the values are based on equinox simulations, we found seasonal differences to be insignificant, generating temperature changes of ⩽10K.
The bottom panel of Fig. 12 shows as a function of 10keV electron energy flux and peak electric field strength the column emission rates of H3+ calculated from the vertical profiles of H3+ densities and temperatures of simulations R1-R18.
The general trend we find in our simulations is that polar temperatures increase with electric field strength and electron energy flux.
At a given energy flux of 1.2mWm-2 the temperatures increase from 450K to 850K (by a factor of ∼1.9) when increasing the electric field strength from 80mVm-1 to 100mVm-1.
At the lower energy flux of 0.2mWm-2 the temperature changes from 450K to 550K, or by a factor of ∼1.2 for the same change in electric field.
Thus, temperatures are less responsive to electric field variations when ionospheric conductivities (at lower energy fluxes) are smaller.
A wider implication of this finding is that Saturn's thermosphere responds less efficiently to magnetospheric input bursts when it is less ionised and more efficiently when in a more ionised state, either due to enhanced electron energy fluxes or due to enhanced solar EUV ionisation (at solar maximum).
For the case of magnetic storms, therefore, Saturn's upper atmosphere responds stronger to variations in magnetic field if they were preceded by enhancements in precipitating electron fluxes.
A further finding from the upper panel of Fig. 12 relates to possible restrictions on combinations of electric field strength and 10keV electron energy flux.
The bottom left half of the figure (below the thick red line) represents a range of observed temperatures on Saturn (400-650K) and thus of "allowed" combinations of electric field strength and particle flux.
In contrast, combinations of these two magnetospheric forcing parameters that result in temperatures in the top right part of the figure (above the red line) need to be treated with caution as they produce temperatures in excess of observations.
A magnetospheric electric field of ∼100mVm-1 mapped into the ionosphere would in combination with at 10keV electron flux of 1mWm-2 generate thermosphere temperatures of ∼800K, well in excess of observed values.
This combination of values cannot thus occur for extended periods on Saturn.
Most temperature constraints on Saturn's polar thermosphere derive from the analysis of H3+ emissions, so for a more direct comparison of our simulations we have calculated the total wavelength integrated column emission rates of H3+ which are shown in the bottom panel of Fig. 12.
We find emission rates to range from (0.1-3.0)×10-5Wm-2sr-1, which is well within the range of observed emission rates.
The implication of this is further discussed in Section 4.
Finally, it is of interest to relate H3+ emissions in Saturn's auroral region to column temperature and H3+ density.
Fig. 13 displays these quantities as a function of local time for latitude 78°S for the case of simulation R15.
Temperature (black) varies by 5K (or ∼1%) while H3+ column density changes from around (1-15)×1015m-2, a factor of 15 variation.
The IR column emission (red) changes with local time from a minimum near midnight of 0.1×10-6Wm-2sr-1 to a maximum emission near 08:00h SLT of 14×10-6Wm-2sr-1, a factor of 140 variation.
We see from this that variations in IR emissions in our simulations are driven primarily by changes in H3+ abundance and to a lesser extent by temperature changes.
The peaks of all quantities in Fig. 13 near 08:00 SLT are related to the maximum in 10keV electron energy flux which occurs near 08:00 SLT (black line in Fig. 3) which leads to maximum H3+ densities there as well.
As the particle ionisation source decreases towards later hours, fast dissociative recombination of H3+ leads to an almost immediate decrease of its densities as well.
The IR brightness may thus be directly related to the incident particle energy flux or any other major ionisation source.
Discussion and conclusions
Our simulations over a range of magnetospheric forcing parameters and seasons successfully reproduce observed ionospheric densities and high latitude temperatures.
Analysis of the simulations gives a basic understanding of the processes that control the dynamics and energy balance in Saturn's high latitude coupled thermosphere and ionosphere.
We have seen that magnetospheric forcing is responsible for the bulk of energy and mass transport in the atmosphere, driving bulk atmospheric internal mass, momentum and energy redistribution.
Joule heating is a major direct contributor to the energy balance, but internal redistribution of this energy by dynamics is crucial as well.
Thermospheric winds driven by ion drag and Joule-heating induced pressure gradients play a key role in determining the high latitude energy balance and, more far-reaching, in controlling the global distribution - or lack thereof - of magnetospheric energy deposited at high latitudes.
Under the range of conditions examined in this study the general pattern is consistently that of polar energy being "trapped" at high latitudes and not propagating equatorward.
This behaviour results from poleward mass and energy transport in the lower thermosphere, a response previously reported by Smith et al. (2007) with identical conclusions.
We find the sub-corotation of the high latitude thermosphere, which results from magnetospheric plasma sub-corotation and associated electric fields, to be a relatively localised phenomenon which does not extend equatorward of around 65° latitude, beyond which the upper atmosphere is in near co-rotation with the planet.
At reduced atmospheric conductances we find angular momentum to be less efficiently transferred from the upper atmosphere to the magnetosphere, thus directly relating precipitating particle mean energy and energy flux to the efficiency of the magnetosphere-atmosphere dynamical coupling processes.
Our results are directly dependent upon the assumed magnetospheric plasma velocity profile (and high latitude electric field), so any future revisions of our assumed profile (Fig. 8, black line) in the context of Cassini plasma observations in Saturn's magnetosphere will similarly affect our results in terms of wind velocities, degree of atmospheric co-rotation and thermal structure, moderately shifting with latitude the basic interaction patterns that we identified in our calculations.
Our simulations demonstrate that dynamical coupling to Saturn's ionosphere via ion drag (Eq.
(3)) critically controls the pattern of thermospheric winds at high latitudes.
Radio science observations of Saturn's ionosphere over the past decades (Atreya et al., 1984; Nagy et al., 2006; Kliore, 2009) have revealed a high degree of variability.
Our simulations successfully reproduce the overall latitudinal trend of peak electron densities (Fig. 5), suggesting that the overall neutral-ion collisional coupling calculations are likely to be realistic in our model as well, but in looking at a steady state situation we have not considered the effects of a variable ionosphere.
Such observed variations in electron density may have an influence on thermospheric dynamics as well, particularly near the ionospheric peak where we found horizontal winds to vary greatly with local time, responding directly to auroral forcing.
A forthcoming second part of this study will examine variability in Saturn's thermosphere-ionosphere system.
Despite strong thermospheric winds at high latitudes we find photochemical equilibrium to hold remarkably well throughout, reducing any role of thermospheric horizontal winds in redistributing ionisation and giving rise to sharp boundaries in ion densities between the auroral and non-auroral regions.
Such sharp boundaries may in practice affect the propagation of radio waves through the atmosphere, which may be of relevance during radio occultation measurements at auroral latitudes.
To-date no radio science observations have probed the vertical structure of the auroral ionosphere, so our simulations can only be validated there using available H3+ IR observations.
In our calculations the dominant ion at the ionospheric peak varies with latitude.
At mid and high latitudes including the auroral region H+ is the principal ion, while at low latitudes it is H3+ - a consequence of our assumed influx of H2O there.
The shorter chemical lifetimes of H3+ give rise to dawn-dusk asymmetries which have been observed near the equator and are captured remarkably well in our calculations, supporting the notion of an influx of H2O, most likely from Enceladus.
As a result of the dominance of H+ away from the equator (outside of the region of H2O influx), chemical lifetimes there increase, thus reducing the chemical sinks and leading to a build-up of ionisation, despite the higher zenith angles at mid latitudes and reduced solar photo-ionisation.
The increase of peak ion density away from the equator is entirely consistent with observations by Cassini RSS (Nagy et al., 2006; Kliore, 2009; Moore et al., 2010).
At auroral latitudes the main peak in ion production is due to particle ionisation from incident electrons (Galand et al., 2011).
It should be noted, as discussed in Section 2.2, that the H3+ densities for any given ionisation rate however depend also on the largely unknown abundance of vibrationally excited H2.
We assumed twice the value of Moses and Bass (2000), but how realistic is this choice? If the incident electron energy and energy flux are known (from UV auroral observations), it may be possible to constrain the abundance of H2(ν⩾4) by comparing observed H3+ emission rates and inferred temperature values to the same quantities calculated from simulated atmospheric temperature and H3+ profiles.
We found temperatures for large electric field strengths and energy fluxes (upper panel of Fig. 12) to partly exceed the observed values, while at the same time the inferred H3+ emission rates (bottom panel of Fig. 12) were entirely within observed ranges.
This suggests either that our calculations underestimate emission rates or that observed temperatures should in fact be larger for the observed emission rates.
The more likely of these options is that our calculations underestimate the ionospheric H3+ column densities, which itself may be due to us assuming too low abundances of H2(ν⩾4).
Our calculations thus highlight the potential of constraining H2(ν⩾4) abundances in the auroral regions via analysis of H3+ emissions.
Despite direct solar EUV heating of Saturn's upper atmosphere representing a minor energy source only, it is however important to note that solar EUV and shorter wavelength radiation is responsible for the majority of ionisation, and thus conductivity, outside of the narrow band of high latitude electron precipitation.
This, in turn, may control thermospheric temperatures.
As shown in Fig. 2, the region of magnetospheric electric field is considerably wider than the electron precipitation region, so the Joule heating region extends over a much wider region as well.
Therefore, solar radiation does affect high latitude temperatures by means of its role as source of ionisation.
This causes hemispheric differences in high latitude temperatures at solstice.
In our solstice simulation (R19) we find exospheric temperatures averaged from 74°S to 90°S (the summer polar region) of 490K, while averaging over the same latitude band in the northern (winter) hemisphere gives a value of 430K.
This difference of 60K is a direct result of enhanced ionisation in the summer hemisphere, leading to stronger Joule heating there.
We expect solar cycle variations of high latitude temperatures to lie within the same approximate range, stronger in the summer hemisphere than winter hemisphere, where solar ionisation is considerably weaker.
The solar ionisation-induced hemispheric differences in atmospheric conductivity should similarly affect the magnetosphere, highlighting an interesting Sun-atmosphere-magnetosphere coupling chain that deserves more thorough examination in future studies.
Our calculations assumed fixed electrical fields at high latitudes and we did not change these in response to changing conditions in the atmosphere.
In principle, enhanced conductivity would lead to more efficient transport of angular momentum from atmosphere to magnetosphere, thus reducing the departure from co-rotation there and the generated electric field which maps into the upper atmosphere.
By keeping the electric field constant we assume a continuous supply of new material into Saturn's magnetosphere which is ionised and maintains a continuous lag from co-rotation.
This aspect of coupling from atmosphere to magnetosphere is not considered in our model.
Smith and Aylward (2008) developed a simple model of Saturn's coupled thermosphere-ionosphere-magnetosphere which considered the feedback from atmosphere to magnetosphere, but assumed a constant ionosphere which did not change in response to thermospheric and magnetospheric conditions.
The effects of the feedback on the magnetosphere did however have little influence on the atmosphere behaviour, which is the focus of our study here.
Future developments, though, should ideally focus on an upper atmosphere model such as STIM which considers the full feedback to the magnetosphere as well, thereby providing the possibility of using additional observational constraints from the magnetosphere to validate the calculations.
According to the simulations of our study magnetospheric energy cannot explain the observed thermospheric temperatures at low and mid latitudes.
While the "energy crisis" is not the focus of this study, this result emphasises the need to consider thermospheric winds when examining the energy balance, rendering problematic the use of 1-D models which by nature cannot account for winds.
In exploring the parameter space of two magnetospheric forcing parameters, electric field strength and incident particle flux, we have demonstrated that the upper atmosphere observations (in particular, H3+ IR emissions) need to be considered when examining Saturn's magnetosphere.
Sub-corotation of plasma in the magnetosphere will affect atmospheric temperatures and dynamics, and a multi-instrument analysis is necessary to ensure that any magnetospheric observations are consistent with those of the atmosphere.
In applying STIM to an examination of this coupling we have shown that multi-dimensional time-dependent models of the coupled thermosphere-ionosphere-magnetosphere are a powerful and important tool in understanding the exchange of energy and momentum between the regions and in ultimately understanding the global energy balance of Gas Giants within and beyond our Solar System.
Acknowledgments
This work was supported, in part, by a grant from the NASA Planetary Atmosphere Program (PATM) to Boston University.
During part of this work I.M.-W. was funded by a Royal Society University Research Fellowship.
M.G. and I.M.-W. were partially supported by a UK Science and Technology Facilities Council (STFC) grant to Imperial College London.
We would like to thank the International Space Science Institute (ISSI) Bern for their kind hospitality and support of the International Team on Saturn Aeronomy (166), which helped advance through very valuable discussions amongst the Team Members many topics raised in this study.
The H3+ cooling code has been developed as part of the European Modelling and Data Analysis Facility (EMDAF) under construction by Europlanet RI, the European Union funded network of planetary scientists (see http://europlanet.projects.phys.ucl.ac.uk/europlanetjra3).
The authors would like to thank both reviewers of this paper for their constructive and helpful comments.

Valence and oxide impurities in MoS2 and WS2 dramatically change their electrocatalytic activity towards proton reduction
Bulk molybdenum(IV) sulfide powder (<2 μm, 99%), molybdenum(IV) oxide powder (99%), molybdenum(VI) oxide powder (99.97%), tungsten(IV) oxide powder (-100 mesh, 99.99%), and tungsten(VI) oxide powder (99.9%) were purchased from Sigma-Aldrich, Singapore while bulk tungsten(IV) sulfide (99.8%) and molybdenum(VI) sulfide dehydrate were obtained from Alfa Aesar (Germany). Ammonium heptamolybdate tetrahydrate (99.5%), sodium tungstate dihydrate (99.5%), hydrochloric acid (35%), acetone (99.9%) and carbon disulfide (99.99%) were obtained from Penta, Czech Republic. Selenium (99.5%) and aluminium (99.7%) were obtained from STREM, Germany. H2S (99.5%) and argon (99.999%) were obtained from SIAD, Czech Republic.
Synthesis was performed by acidic decomposition of ammonium tetrathiotungstate. 25 g of sodium tungstate dihydrate was dissolved in 250 mL of water. Subsequently, 100 mL of 1 M hydrochloric acid was slowly added to the solution. The tungstic acid that was formed was purified by repeated decantation and centrifugation. Thereafter, the tungstic acid was separated by suction filtration, washed with water and dried in a vacuum oven at 50 degC for 48 hours. Following that, 10 g of tungstic acid was dissolved in 100 mL of concentrated ammonia and filtered using a 450 nm nylon membrane. The solution was then bubbled with H2S gas for 10 hours (about 200 mL min-1). After which, the red crystals of ammonium tetrathiotungstate formed were separated by suction filtration and washed with cold water. To 5 g of (NH4)2WS4 in 5 wt% of ammonia solution, 1 M solution of hydrochloric acid was slowly added to the mixture with vigorous stirring and under an argon atmosphere. WS3 formed was then separated by suction filtration and washed with water followed by acetone. WS3 was subsequently dried under vacuum at 50 degC for 24 hours. Later, WS3 was dispersed in carbon disulfide and repeatedly decanted to remove excess of sulphur. Finally, WS3 was separated by suction filtration, washed with carbon disulfide and dried in a vacuum oven at 50 degC for 48 hours.Sink or source-The potential of coffee agroforestry systems to sequester atmospheric CO2 into soil organic carbon

Highlights
•
SOC stocks decreased by 12.4% in Costa Rica and 0.13% in Nicaragua after establishment of coffee AFS.
•
SOC stocks increased in the top 10cm of soil; greater reduction occurred at 20-40cm.
•
Organic management caused a greater increase in 0-10cm SOC but did not influence reduction at depth.
•
Shade type effects on SOC were smaller; no significant difference between shaded and unshaded coffee.
•
SOC stocks tend to converge on a level determined by site environment during establishment.
Abstract
Current carbon accounting methodologies often assume interactions between above-ground and below-ground carbon, without considering effects of land management.
We used data from two long-term coffee agroforestry experiments in Costa Rica and Nicaragua to assess the effect on total soil organic carbon (SOC) stocks of (i) organic versus conventional management, (ii) higher versus moderate agronomic inputs, (iii) tree shade types.
During the first nine years of coffee establishment total 0-40cm depth SOC stocks decreased by 12.4% in Costa Rica and 0.13% in Nicaragua.
Change in SOC differed consistently amongst soil layers: at 0-10cm SOC stocks increased by 2.14 and 1.26MgCha-1 in Costa Rica and Nicaragua respectively; however much greater reduction occurred at 20-40cm (9.65 and 2.85MgCha-1 respectively).
Organic management caused a greater increase in 0-10cm SOC but did not influence its reduction at depth.
Effects of shade type were smaller, though heavily pruned legume shade trees produced a greater increase in 0-10cm SOC than unpruned timber trees.
No significant differences in SOC stocks were found between shaded and unshaded systems at any depth and SOC was poorly correlated with above-ground biomass stocks highlighting poor validity of "expansion factors" currently used to estimate SOC.
SOC stock changes were significantly negatively correlated with initial SOC stock per plot, providing evidence that during establishment of these woody-plant-dominated agricultural systems SOC stocks tend to converge towards a new equilibrium as a function of the change in the quantity and distribution of organic inputs.
Therefore it cannot be assumed that tree-based agricultural systems necessarily lead to increases in soil C stocks.
While high inputs of organic fertiliser/tree pruning mulch increased surface-layer SOC stocks, this did not affect stocks in deeper soil, where decreases generally exceeded any gains in surface soil.
Therefore site- and system-specific sampling is essential to draw meaningful conclusions for climate change mitigation strategies.

Introduction
Soils are the greatest terrestrial C stock and hold an estimated 1462-1548Pg of organic C to 1m depth (Batjes, 1996).
However, surface soils (0-30cm depth), which store almost half of soil organic carbon (SOC) and up to three times the C stored above-ground in vegetation, are considered to be the most vulnerable to loss as CO2 emissions due to climatic and land-management change, highlighting a major threat to climate regulation (Powlson et al., 2011a).
At the same time, it has been widely recognised that practices which maintain SOC stocks are important in ensuring the sustainability of soil functions (Lal, 2004; Nair et al., 2009a; Powlson et al., 2011a).
Identifying how different agricultural management practices or changes in land-use create SOC sinks (accumulating additional C), act as C sources (emitting C) or maintain stocks at current levels is imperative in identifying effective strategies for land-based climate change mitigation.
Agriculture that is established on land depleted in SOC will have potential to sequester C.
However, some practices such as addition of organic matter that may increase SOC can also increase N2O emissions.
In addition, it is not always clear how farm annual GHG flux may be altered by change in SOC stock, as this tends to occur slowly and with an uncertain trajectory.
Therefore, assessment of how best to achieve climate change mitigation through agriculture needs to consider both short-term changes in GHG emissions from soil and longer-term changes in SOC stocks (Lal, 2004; Smith et al., 2008).
To get a whole-system perspective this should be combined with assessment of changes in other C pools, such as above- and below-ground biomass and litter (e.g. of shade trees or crops such as coffee).
Agroforestry systems (AFS) have been recognised for their potential to sequester large amounts of C above ground (and in some cases below ground into SOC) (Albrecht and Kandji, 2003; Nair et al., 2009a; Soto-Pinto et al., 2010; Verchot et al., 2007).
Nair et al. (2009a) have suggested an area of more than 1000million (M) ha globally to be currently managed under AFS, including silvopastoral systems, with 630Mha more estimated to be suitable for conversion of unproductive croplands and grasslands to AFS (IPCC, 2000).
This suggests a great potential for further above- and below-ground C sequestration.
It is commonly believed that AFS enhance SOC stocks compared with tree-less annual crop systems (Nair et al., 2009b).
However, much of this evidence is based on changes in the SOC of surface soils and little has been published on the effects of trees on stocks deeper in the soil.
Understanding of the soil processes involved is still limited, making it difficult to predict accurately changes in SOC over time (Nair et al., 2009b).
Much evidence of increases in SOC stocks after changes in agricultural management is based on extrapolation from rates of C sequestration by growing plants using weak evidence about the processes by which this might influence SOC stocks (which can be positive or negative (Sanderman and Jeffrey, 2010)).
As a result of the complexity of assessing long-term SOC change, it had until recently been largely excluded from carbon accounting within land-based projects for international carbon markets, which tended to focus only on above-ground C as it is relatively easy to measure and model (IPCC, 2006).
Recently, SOC has been included as a C pool within respected accounting methodologies, e.g.
in four out of the seven used for small-scale afforestation and reforestation under the CDM pool (UNFCCC, 2011).
However, all except one use a default value of an increase in SOC of 0.5MgCha-1year-1 for a C accounting period of 20 years following afforestation or reforestation of land.
Similarly, the UNFCCC (2011) methodology specifies accounting by means of an assessment tool which is based on climatic default values that only allow for an increase in SOC, with a maximum value of 0.8MgCha-1year-1.
Although initial losses of SOC through site preparation are recognised, the potential for reduction of SOC due to tree establishment is not accounted for.
Use of these default values is rarely replaced by monitoring of actual changes in SOC stocks ex post, which might, in fact, reveal longer-term decreases in SOC (Bashkin and Binkley, 1998).
Coffee production systems occupy over 10 million ha globally (FAO, 2011) so their design and management have potentially major importance for land-based C flux and storage.
The aim of this study was to advance understanding of the extent to which producing coffee with shade trees (coffee agroforestry systems - CAFS) change SOC stocks and whether this provides a viable climate change mitigation strategy.
Major variables in CAFS as implemented by farmers in Central America that we hypothesised would affect SOC stocks are: (i) the use of shade trees versus full-sun, (ii) amongst shade trees the use of timber species (unpruned, therefore predominantly providing only a litter input above-ground) versus nitrogen-fixing species that are frequently and heavily pruned; (iii) conventional chemical fertilisation versus organic fertilisation; (iv) the level at which these inputs are applied.
By using experimental comparison of these specific variations amongst types of CAFS, this study sought to improve our understanding of the C cycle, the effects of coffee shade management on sequestration of C in soil relative to that in above-ground biomass, and the extent to which SOC should be taken into account in coffee-farm C projects considering the relative merits of alternative land-use C-accounting methods.
The specific objectives were to investigate (a) how the addition and management of trees in agricultural systems change total SOC stocks through the soil profile and (b) how agronomic management affects SOC stocks in comparison with the effects of the trees.
We evaluated these by assessing the differences in SOC firstly between shaded and un-shaded (full sun) coffee farming systems, and the effect that tree pruning has within shaded systems, and secondly between conventional and organic management, each with different input levels.
Methods and materials
Site description
The research was conducted at two field sites in Costa Rica and Nicaragua, chosen to represent low altitude coffee growing regions, both managed by the 'Centro Agronómico Tropical de Investigación y Enseñanza' (CATIE).
Experiments were established in both sites at the end of 2000.
The Costa Rica site was located in Turrialba (9° 53′ 44″ N, 83° 40′ 7″ W) at 685m above sea level.
The climate is humid tropical with no marked dry season: annual precipitation is 2600mmyear-1 and mean annual temperature is 22°C (Haggar et al., 2011).
The soils have been classified as Inceptisols (Typic Endoaquepts) and Ultisols (Typic Endoaquults) under the USDA Soil Taxonomy classification system (Soil Survey Staff, 1999) and a water table that fluctuated up to 50cm depth (prior to drainage of the site at the time of establishing the experiment).
The former land-use was sugar cane cultivation.
The cultivar Coffea arabica L.
'Caturra' was then planted in 2000.
The Nicaragua site was located in Masatepe (11° 53′ 54″ N, 86° 08′ 56″ W) at 455m above sea level.
The climate is semi-dry tropical with a distinct rainy season between May and November: mean annual rainfall is 1386mm and mean annual temperature is 24°C (Haggar et al., 2011).
The soils have been classified as Andisols (Humic Durustands) or Andosols (Humic Haplustands) under the USDA Soil Taxonomy classification system (Soil Survey Staff, 1999).
The former land-use was long-established shaded coffee.
The cultivar C. arabica L.
'Pacas' was then planted in 2001.
At the Costa Rican site Ultisols were present in two of the three experimental blocks and are distinguished by the accumulation of clay in the B-horizon.
Inceptisols were present in the third experimental block; they are distinguished by an absence of clay.
High cation-exchange capacity (>30cmol(+)kg-1) was common throughout the site.
The soils of the Nicaraguan site were commonly associated with low bulk densities, high amorphous mineral content, high retention of phosphorus, high organic matter content and high water retention.
A particular feature of the soils in this region is the presence of a material locally known as 'talpetate'.
This is a horizon of indurated volcanic tuff, which occurs between 15cm and 1m depth and can pose difficulties for agriculture due to its durability and the associated difficulties of water flow and root penetration.
For the experiment, all of the existing coffee plants were uprooted and removed and the shade trees were felled and all trunk and branch material removed.
Remaining leaf and fine branch material and root systems of the shade trees were left on-site to decompose.
Experimental design
The experiments were set up to study the ecological basis of efficiency in coffee production.
A main aim was to compare organic and conventional coffee production systems under various types of shade.
The main-plot treatments (on average 0.4ha) at each site were full sun (not agroforestry) and agroforestry with four different individual species or species combinations of shade tree (Table 1) and were allocated at random.
The four treatments applied to subplots (with a size of 0.06ha in Nicaragua, 0.08ha in Costa Rica) were coffee management systems combining the two different types (conventional and organic), each with two different levels of nutrient and pest management inputs (intensive and moderate) (Table 2) and were allocated at random.
The design was a randomised block with three blocks per site (1.8ha each in Costa Rica and 1ha in Nicaragua), each containing one replicate of each main-plot/subplot treatment combination; not all subplot treatments were represented within main-plot treatments as some combinations are not representative of real farming systems (e.g. full sun with organic management, Table 1).
Shade trees were planted in 2000 at a density of 416 and 667treesperha-1 in Costa Rica and Nicaragua respectively but have since been progressively thinned and managed to achieve a uniform shade level (Table 1).
Tree management
The tree management regime varied according to species; at both sites timber tree shade was primarily managed through periodic thinning of trees to reduce tree density (Table 1).
Across all four management treatments trunks and major branches of thinned and pruned timber trees were removed from the plots whereas leaf and small branch material was left.
Trees of two of the leguminous species, Erythrina poeppigiana in Costa Rica and Inga laurina in Nicaragua, were pruned both for the management of shade level and to provide input of organic matter (rich in N) input to the soil.
In Costa Rica, in the conventional intensive (CI) subplot treatments with E. poeppigiana, the trees were pruned at a height of 1.8-2.0m with the removal of all branches above this height (pollarding).
This practice is frequently found in conventional high-intensity coffee agroforestry systems in Costa Rica.
In the other three subplot treatments, however, E. poeppigiana trees were managed according to the recommendations of Muschler (2001) with pruning at a height of around 4m and a minimum of three branches left for partial shade cover.
In Nicaragua, I. laurina was pruned to create a homogeneous canopy cover of approximately 40%, through annual pruning of branches at any height.
Coffee bushes were pruned according to standard coffee agronomic practice, to the same level across all treatments, and all the pruned material was also left in the plots.
Estimation of soil organic carbon stocks
Soil was sampled in August to October 2001 and in February and March 2010 (10 years after the start of the experiment).
The soil sampling design was systematic using a 7.6cm diameter metal auger with each sample divided into three depths (0-10, 10-20, and 20-40cm).
In each subplot samples were taken at three different positions relative to shade trees within two different coffee rows: (a) within 1m of the shade tree stem, (b) half way between two shade trees within the same coffee row, and (c) half way between sampling points (a) and (b).
For each of these positions three samples were taken at different distances from the coffee row: (i) within the coffee row, (ii) between adjacent coffee rows, and (iii) half way between positions (i) and (ii).
Separately for each of the three depths, all 18 of the samples collected in each subplot were thoroughly mixed and then a single composite sample was taken for analysis of C content.
The composite samples for each depth for each subplot in 2001 and 2010 were air dried on the same day as collection from the field.
They were then ground and sieved through a 2-mm sieve to remove larger pieces of root material and the stone fraction.
To measure bulk density, in each site a separate undisturbed core of soil 5cm diameter and 5cm deep was collected in 2010 in the centre of each of the subplots for each of the three designated sampling depths and oven dried to constant dry mass at 105°C, sieved to separate the fine fraction from the stones (>2mm), and then both fractions were weighed (calculation given in Supplementary Information).
In 2001 bulk density was measured only in the Costa Rican site, in each subplot at the 0-10cm soil depth.
Differences in 0-10cm depth soil bulk density between 2001 and 2010 across the experiment in Costa Rica (where drainage had been installed and land-use had been changed from sugar cane to shaded coffee) were small (0.84 and 0.86g/cm3 respectively) and were shown by a t-test to be far from significant (p=0.41).
In the Nicaraguan site no drainage had been installed and the land-use (shaded coffee) was not changed at the initiation of the experiment.
Therefore, we extrapolated from the Costa Rican result to assume a similar lack of change in soil bulk density from 2001 to 2010 in the Nicaraguan experiment.
Changes in soil bulk density associated with land use change and agricultural practice are found to be much greater in surface than deeper soil levels (Wen-Jie et al., 2011).
Therefore, we made our calculations of C stock per area for both years were made using the 2010 bulk density data (Table S1) collected in both countries separately at each of the three soil depths; thus the reported changes in stocks are proportional to the changes in measured C concentration.
The soil samples from the two countries were analysed for bulk density at the Universidad Nacional Agraria (UNA) in Managua, Nicaragua and at the Soil Laboratories of the Centro Agronómico Tropical de Investigación y Enseñanza (CATIE) in Turrialba, Costa Rica.
All soil samples were analysed at the latter laboratory for C content using a Thermo Finnegan combustion analyser.
Statistical analysis
To test the effect of main-plot shade and subplot coffee management treatments on the changes in SOC stocks 2001-2010 we fitted separate linear mixed effects models for each country using R (R Development Core Team, 2012) with the lme4 package (Bates et al., 2012).
Main-plot/subplot treatment combinations were fitted as a factor with 15 levels for each country.
Results were assessed using the Akaike Information Criterion (AIC) (Burnham and Anderson, 1998), and the model presenting the smallest AIC selected.
This analysis was carried out on the measured SOC stocks between the three sampled depths (0-10, 10-20 and 20-40cm) and depth was included as a term in the model as the different depths are not independent.
To elucidate specific treatment effects an ANOVA was carried out on changes in SOC stock for the main-plot/subplot combinations for each depth and country separately using INFOSTAT (InfoStat, 2004).
Specific contrasts within the ANOVA were developed based on shaded versus non-shaded main-plot treatments, heavily versus lightly pruned treatments, organic versus conventional subplot treatments and a contrast between the two intensities of subplot treatment.
Bivariate correlation analyses using Pearson's correlation coefficient for parametric data and Kendall's tau correlation for non-parametric data were carried out (separately for each depth) between all combinations of SOC stocks, SOC stock changes, above-ground biomass C stocks, pruning inputs and organic fertiliser inputs.
These correlation tests were carried out separately for each country with each individual subplot as a replicate using SPSS (vers.
19).
Statistical significance is judged as p<0.05 unless otherwise stated in the text.
The results are presented graphically as SOC stocks in MgCha-1 because this is the form that is of most relevance for carbon accounting, and for assessing the net impact of treatments on ecosystem carbon storage and thus their potential for climate change mitigation.
Results
Changes in soil organic carbon (SOC) stocks
Overall, during the first nine years of coffee establishment total 0-40cm depth SOC stocks decreased by an average of 12.4% in Costa Rica and 0.13% in Nicaragua.
The best fitting mixed effects model for predicting changes in total SOC stocks for both the experiment in Costa Rica and that in Nicaragua is based on subplot treatments (management type), depths, and the initial C content as fixed effects with random slope effects of the replicate blocks and of main-plot treatments nested within the replicate blocks (the AIC values of this model for Costa Rica and for Nicaragua were respectively 47.2 and 327.8), although for Nicaragua a model based on main-plot treatments instead of subplot treatments was equally as good (AIC=326.3).
Effects of the individual main-plot and subplot treatments and of soil depth are presented below.
The inclusion of initial SOC concentrations led to a considerable improvement in the models' prediction: in those subplots with a higher initial SOC concentration there was a greater subsequent reduction in concentration (Costa Rica) or smaller increase (Nicaragua) during the experiments (this result is also addressed below in more detail).
There was a difference between the experiments in the two countries in the effects of main-plot (shade) and subplot (coffee management) treatments on total SOC stocks (MgCha-1).
In Costa Rica the ANOVA showed significant (p<0.01) overall effects of both on the change in SOC stock at 0-10cm depth over the 9-year period.
However, in deeper soil only the shade treatment effect remained significant and there was an additional significant (p<0.01) effect of initial C concentration at the 20-40cm depth.
In contrast, in Nicaragua the ANOVA showed no significant effects of main-plot treatment or subplot treatment or of initial C concentration at any soil depth.
Differences between pruned and un-pruned shade tree systems
The ANOVA contrast of the main-plot full-sun treatment versus all the shaded treatments as a group showed no significant differences in change of SOC stocks at each depth in each country.
However, in Costa Rica the pruned-legume (E, ET) shade treatments showed significantly different changes in SOC stock compared with the un-pruned shade systems (C, T, FS), at each of the three sampling depths (treatment codes are defined in Table 1).
Across the treatments there were differences in trend of SOC stocks amongst the soil depths in both countries.
In Costa Rica, for every shade type there was an increase in SOC at 0-10cm (average 2.14MgCha-1 or 8.5%), a decrease at 10-20cm (average -2.48MgCha-1 or 11.4%) and a large decrease at 20-40cm (average 9.65MgCha-1 or 28.6%) (Fig. 1 and Table S2).
At 0-10cm the greatest increase was for the two pruned shade types (E, ET) (which (by chance) had lower initial average SOC stocks at the start of the experiment), whereas at both 10-20 and 20-40cm depth the E shade type showed the greatest decreases.
Therefore, over the whole 0-40cm soil depth there was a similar mean decrease in SOC stock between the two pruned and two un-pruned shade types (9.9 and 9.7MgCha-1 respectively); thus the average SOC stock increased across all treatments by 8.5% in the top 10cm of soil and decreased by 21.8% in the 10-40cm depth.
In contrast, the surrounding fields in which sugar cane cultivation had continued over the study period lost on average 11% of SOC in the top 10cm of soil but gained around 42% (from 47.3 to 67.3MgCha-1) in the 10-40cm depth.
In Nicaragua, similar to the results in Costa Rica, in the top 10cm of soil there was an increase in mean SOC stock for every shade treatments (average 1.26MgCha-1 or 2.8%) (Fig. 2 and Table S2).
However, in contrast to Costa Rica, at 10-20cm depth every shade type showed an increase in mean SOC stock (average 1.38MgCha-1 or 3.8%).
At 20-40cm depth, the same as Costa Rica, across shade treatments average SOC stock generally decreased (by -2.85MgCha-1 or 4.6%), however this trend was only shown in four out of the five shade treatments.
Over the whole 0-40cm soil depth there was a decrease in SOC stock during the experiment for three and an increase in two of the shade treatments.
Therefore, across all the shade treatments there was an overall average decrease in SOC stock in both countries, but it was much smaller in Nicaragua (0.13%) than in Costa Rica (12.4%).
Changes in soil organic carbon (SOC) stocks with management type (conventional versus organic)
When the mixed effects model is restricted to the 0-10cm soil layer, the results for the best fitting models in both Costa Rica and Nicaragua include the coffee management (subplot) treatments and the initial C concentration as fixed effects with random slope effects of the replicate blocks and of main-plot treatments nested within the replicate blocks; AICs were 18.2 and 97.7 respectively (compared with 33.0 and 105.6 for models including main-plot treatment and subplot treatment as fixed effects and 37.0 and 99.8 for models based on main-plot treatments only).
The contrasts within the ANOVA for 0-10cm soil depth SOC stock changes for Costa Rica further support the findings of the mixed effect models, showing a significantly greater increase in SOC stock in the organic than the conventional management treatments (p=0.0001) (Fig. 3).
The difference between management treatments is likely to be due to the application of organic fertilisers (at up to 11.25Mgha-1year-1), as no significant differences were found between these subplot treatments for total inputs of above-ground biomass to the soil in the form of senescent leaf litter and pruned material (p=0.24).
Further, there was a positive correlation between the mass of organic fertiliser inputs and changes in 0-10cm depth SOC (r2=0.18, p<0.01).
Both conventional and organic managements showed a consistent decline in SOC stocks at the two lower soil depths with no significant between-treatment differences (Fig. 3).
Changes in total 0-40cm depth SOC stock showed no significant correlations with either pruning or organic fertiliser inputs.
Relationships between above-ground biomass and soil organic carbon stocks
In Costa Rica there was a highly significant (r2=0.53, p<0.001) negative correlation between SOC stocks in 2001 and the change in SOC stocks between 2001 and 2010 (Fig. 5a), however there was no significant correlation between above-ground C (AGC) stocks and SOC stocks in 2010 (Fig. 5b).
In contrast, in Nicaragua there was a weaker, though still significant, negative correlation (r2=0.17, p<0.01) between SOC stocks in 2001 and the change in SOC stocks between 2001 and 2010 but a highly significant (r2=0.2, p<0.01) positive correlation between AGC and SOC stocks in 2010 (Fig. 5).
Changes between 2001 and 2010 in 0-10cm depth SOC stock were not significantly correlated with 2010 AGC in either country (r2=0.01, p=0.29; r2=0.01, p=0.25 for Costa Rica and Nicaragua respectively).
However, increases in the 0-10cm depth SOC stock were significantly positively correlated with the quantity of organic inputs in every form except for pruning inputs in Nicaragua which were non-significant (fertiliser: r2=0.18, p<0.01; r2=0.07, p<0.05; prunings: r2=0.08, p<0.05; r2=0.001, p=0.31 for Costa Rica and Nicaragua respectively; litter r2=0.07, p<0.05 for Costa Rica only as not measured in Nicaragua).
The strongest correlation was with organic fertiliser inputs in both countries.
Discussion
Do trees help to sequester more C in soil?
It is important to understand the effects on SOC of change in land use systems or agricultural practices when assessing their potential environmental impact.
It is widely acknowledged that shifting from natural to managed ecosystems, such as arable cropping, results in a loss of SOC (Powlson et al., 2011b).
In the present study, the plots with initially higher SOC stocks tended to have greater SOC losses (or smaller gains) during the observed period of coffee system establishment, notwithstanding the major difference between them in shade tree and coffee management treatments (Figs.
4 and 5).
This indicates that these systems, with biomass dominated by woody plants and limited soil disturbance after crop establishment, are in a transition towards a new equilibrium between inputs of organic matter and SOC stocks.
Specifically, in Costa Rica the change in land-use from long-term arable sugar cane agriculture to an agroforestry system with perennial coffee and shade trees does not lead to an increase in SOC stocks over the first nine years, which is contrary to the widely held expectation (Powlson et al., 2011b).
In fact, we found a nine-year decrease in SOC stocks over 0-40cm depth by an average, across all shade types, of 9.99MgCha-1 (12.4%) in Costa Rica, whereas in Nicaragua (where the long-term land use before the experiment had been the same as afterwards, shaded coffee) there was a much smaller decrease in average 0-40cm depth SOC stock of 0.2MgCha-1 (0.14%).
The direction of change in SOC stocks varied with soil depth in a similar way between the two countries.
In both countries there was an increase in 0-10cm depth SOC stocks which was positively correlated with the input mass of organic fertiliser (and in Costa Rica of prunings and litter too).
This shows that, although their long-term development is influenced by soil type, climate, management and the SOC-storage capacity of the soil (Fließbach et al., 2007), SOC stocks in the surface do also depend on the quantity of above-ground organic matter inputs (Carter et al., 2002; Parton et al., 1996).
This is further supported by the significant differences in SOC stock changes between the treatments with pruned and un-pruned trees in Costa Rica, though all treatments showed a huge contrast in trends of SOC stock with soil depth between an increase at 0-10cm and a decrease at 20-40cm.
Despite the great variation in above-ground biomass between the shade treatments (between an average of 9.1MgCha-1 for full sun, 22.6MgCha-1 for pruned leguminous shade systems and 115.8MgCha-1 for unpruned timber shade systems (Noponen et al., 2013)), there were no significant differences in SOC stock changes between the shaded and full-sun systems at any depth.
As the above-ground biomass was entirely represented by trees and coffee bushes planted at the start of the experiment, the 2010 biomass standing stock directly corresponds to biomass growth rate.
There was a difference between the two experiments in the relationship between above-ground biomass and SOC stocks.
In Nicaragua SOC stocks were correlated with above-ground biomass C stocks (though r2 was only 0.20) but there was no such correlation in Costa Rica.
This lack of universality in relationships between above-ground biomass and soil carbon stocks indicates the potential for introduction of a large error into calculations of total ecosystem C stocks when they include estimates of SOC stocks based simply on an assumed linear correlation with above-ground biomass as is commonly used in some of the small-scale afforestation and reforestation C accounting methodologies described in the introduction (UNFCCC, 2011).
Therefore, it is just as essential that soil be adequately sampled and SOC measured directly, as it is for an adequate inventory of above-ground biomass.
Do tree-based systems sequester more C in deeper soil layers?
In both the Nicaraguan and Costa Rican experiments during the first nine years of coffee and tree establishment, SOC stocks in 20-40cm depth soil generally decreased (and this also occurred in 10-20cm depth soil in Costa Rica, giving an average loss over 10-40cm of 12.1MgCha-1).
The stocks of SOC in deeper soil are generally considered to be more stable than in the surface layer, reacting more slowly to changes in the land-use system (Jenkinson and Coleman, 2008).
There are strong limitations to the rate of incorporation of organic material from the soil surface into deeper soil layers, where SOC stocks are predominantly controlled by mechanisms mediated by root systems (both direct inputs of organic matter through root turnover, exudation, mycorrhizas and herbivory, and indirect effects, e.g.
due to the effect of the root sink on soil water relations).
In the Costa Rican experiment reduction in average SOC stocks in 10-40cm depth soil occurred in all shade and management treatments.
This SOC decomposition might have been stimulated by an increase in aeration which could in turn have accelerated the effect of labile C from root systems priming the soil microbes to accelerate their depletion of existing SOC stocks (Richter et al., 2007, 1999; Dunne and Leopold, 1999).
Such aeration could have been due to greater transpiration of coffee bushes/trees compared with the previous annual crop of sugar cane and/or to the drainage carried out as part of the site preparation for the experiment, although the redox zone in the soil profiles would suggest that the previous high water level was below 50cm (Haggar, unpublished data).
In the Nicaraguan experiment the previous land use had been coffee with shade trees and no drainage was carried out, and its reduction in average SOC stock in deeper soil had been much less (only 2.85MgCha-1 at 20-40cm soil depth).
Nonetheless, a reduction did occur in all four management treatments and four out of the five shade treatments, therefore (on balance) the present study does provide some evidence of the generality of this phenomenon to the development phase of coffee systems after replanting and during the rapid early growth during shade tree establishment.
It cannot just be attributed to the particular conditions at the Costa Rican site.
A similar result was found in a long-term forest re-establishment experiment in South Carolina where, over the 50 years of loblolly pine establishment after previous arable land use under cotton, SOC stocks increased in the surface soil but decreased in the soil deeper than 35cm (Richter et al., 2007).
In order to compare SOC stock changes between coffee cultivation and the previous land use at the site in Costa Rica (sugar cane cultivation), SOC was also monitored in the surrounding fields, which continued to be used to grow sugar cane without additional drainage.
SOC stocks in the sugar cane fields showed an opposite trend to that in the experiment at each depth: decreasing by 11% in the 0-10cm depth soil, but increasing greatly at 10-40cm (by 42%), giving an overall increase of 16.0MgCha-1 (19%) over the nine year period.
Here, fields are annually fertilised primarily with N-based fertilisers, burned before harvest and periodically tilled before replanting (the latter is likely to be a major factor in the loss of SOC from the surface soil).
Similar results have been found by other studies where the long-term cultivation of sugar cane that is burnt before harvesting resulted in a decrease in SOC stocks at 0-10cm depth (Galdos et al., 2009) and an increase in SOC stocks at 20-40cm to levels near those of natural forest (Silva et al., 2007).
Grass species such as sugar cane are known to input carbon into deeper soil layers quicker than some tree species (Bashkin and Binkley, 1998).
Changes in SOC in an experiment in Hawaii in which land formerly under sugar cane cultivation was afforested with a fast growing eucalyptus plantation showed remarkably similar results.
Measured using stable isotope ratios to examine changes in soil organic C derived from cane (SOC4) and eucalyptus (SOC3), 10-13 years after establishment SOC in the top 10cm had increased by 11.5Mgha-1 in the eucalyptus plantation but decreased by 10.1Mgha-1 in the 10-55cm depth soil (Bashkin and Binkley, 1998).
These losses in deeper soil were indicated by losses of SOC4 derived from sugar cane being much greater than the gains of SOC3 in this layer attributed to the growth of the eucalyptus.
Similarly, in the present study's experiment in Nicaragua, although the prior land-use was a coffee agroforestry system, the accumulation of organic matter inputs to the soil was disrupted by its clearance and the subsequent re-establishment of new coffee and shade trees.
As a result, the levels of organic matter input of the previous system will have only been reached after several years of the experiment.
In addition, the penetration of roots into the deeper soil, and thereby the deposition of C at that depth (which showed the greatest decrease in SOC stocks) would have been delayed during the establishment of the new trees and coffee bushes.
Thus, although tree-based systems might have a greater potential to sequester C into more stable stocks in deeper soil than some treeless systems (Haile et al., 2010), this is strongly influenced by other site- and land use change-specific variables.
Organic versus conventional management
The results of the present study showed that coffee production systems under organic management increased SOC stocks in the top 10cm of soil more than did conventional production systems in Costa Rica (with a highly significant ANOVA test result), but not in Nicaragua.
However, evidence for the generality of this result was provided by the more powerful mixed effects model which showed that management system had a greater effect on changes in 0-10cm depth SOC concentration than did shade type in both countries.
The mixed effects model applied to all three soil depths also showed that management system was an important factor (as well as depth itself) influencing changes in SOC concentration in both countries.
In the last decade much attention has centred on the management of SOC and its potential for climate change mitigation through increased C sequestration into soils.
Proponents of organic systems have often claimed that they sequester more C into the soil than do conventional systems (Freibauer et al., 2004; Scialabba and Müller-Lindenlauf, 2010).
Recent studies (Sanderman and Jeffrey, 2010; Powlson et al., 2011a,b), however, have warned of the shortcomings of many field trial results and of current C-accounting methodologies that can over-estimate the net sequestration of C into soil.
The term sequestration is often used simply to describe an increase in SOC stocks over time following a change in land-use system or practice.
Powlson et al. (2011b), however, argue that these changes only contribute to climate change mitigation if they do actually result in a net additional transfer of C from atmospheric CO2 to soil or vegetation, which is not necessarily the case.
At the centre of this argument lies the issue exemplified by the question of how the fate of added organic C material would have differed were it to have an alternative use.
For example, management practices that increase SOC through application of manure and other organic materials such as crop residues or prunings are often only a transfer of C from one terrestrial pool to another (Powlson et al., 2011b).
If alternative uses would have stored the C for longer (e.g. in solid wood products or through conversion of the organic material to biochar) or would have substituted for fossil fuel emissions (e.g. from domestic cooking) then they may have had a more positive effect on climate change mitigation.
Assessments restricted to the soil itself show that changes in land management which increase SOC stocks could still have a detrimental net climate change impact by increasing emissions of non-CO2 greenhouse gases (GHG) such as CH4 and N2O which have much higher global warming potentials (25 and 298 times respectively over 100 years) compared with CO2.
For example, Noponen et al. (2012) estimated that the coffee management systems of the present experiments produced non-CO2 GHG emissions from soil ranging between 0.66 and 2.24MgCO2eha-1year-1 for the conventional and 0.55 and 2.02MgCO2eha-1year-1 for the organic treatments.
Especially in the organic systems, which have additional organic matter inputs in the form of manures and coffee pulp, the climate change mitigation potential of the gains in SOC stock in the 0-10cm depth soil equate to an average of 1.45MgCO2eha-1year-1 in Costa Rica and 0.88MgCO2eha-1year-1 in Nicaragua, both of which lie well within the range of estimated non-CO2 GHG emissions from soil resulting from the inputs of organic matter.
Therefore, the organic management may lead to no net mitigation of global warming via the soil and may even cause net GHG emissions.
This calculation, however, does not include the GHG emissions associated with the transport of the organic material, or consider which of the emissions would also occur if the organic material is subject to alternative uses or fates, while analysis of conventional coffee management also needs to include the emissions associated with the production and transport of the agrochemicals that are used (Powlson et al., 2011b).
Incorporation of some of the chicken manure and coffee pulp applied to coffee farms into the soil might result in lower GHG emissions than their decomposition in open air, should the soil have a capacity to absorb some of the CH4 and N2O emissions, which would be a priority for future research.
Through their increase in the SOC content of upper soil layers, organic amendments can improve physical soil properties that are beneficial for crop production (Powlson et al., 2011a).
This improvement in soil growing conditions might achieve equivalent yields to those obtained with higher applied nutrient contents in inorganic fertiliser, thereby reducing the net GHG emissions (especially of N2O) of the farming operation.
Increased biomass growth rates of perennial crops and shade trees resulting from improved soil properties will further contribute to a real reduction of atmospheric CO2 concentration while the biomass remains intact.
The existing condition of the soil is also an important consideration.
The results of the present study show that where agroforestry systems are established on soils more depleted in SOC concentration they provide a greater potential for climate change mitigation through higher SOC stocks, at least until a new equilibrium in SOC concentration is reached (Johnston et al., 2009).
Therefore, despite the detailed measurement of these experiments, covering many aspects of C stocks and GHG emissions, it remains difficult to answer the question of the extent to which organic management is more favourable to mitigating global warming compared with conventional management, such is the complexity of processes involved.
The diversity of net changes in SOC stocks amongst treatments found in the present study in Nicaragua and in Costa Rica illustrates the complexity of predicting which changes in existing coffee production systems will have a net positive or negative impact, especially where they involve soil-disturbing agronomic operations.
The long timescale for changes in SOC stocks to become manifest also presents a challenge for the evidence.
There was an overall mean decrease of 0-40cm depth SOC stocks in eight of the ten shade treatments over the nine years after system establishment.
However, positive effects of shade tree growth might be realised over the longer term.
As already reported in many previous studies (Bashkin and Binkley, 1998; Binkley et al., 2004; Poulton et al., 2003; Resh et al., 2002; Richter et al., 1999) the benefits of shade trees in terms of sequestration of C into above-ground biomass are already apparent.
For Costa Rica there was a range of mean sequestration rates per shade treatment of 3.3-12.9MgCha-1year-1 (Noponen et al., 2013), more than five times the rates of loss of 0-40cm depth SOC (with a range of 0.65-1.54MgCha-1year-1 per shade treatment) reported in the present paper.
In Nicaragua, mean above-ground C sequestration rates ranged between 1.73 and 2.70MgCha-1year-1 per shade treatment (Noponen unpublished data) which are again higher than the loss or gain of 0-40cm depth SOC reported here (ranging from 0.44MgCha-1year-1 loss to 1.58MgCha-1year-1 gain).
Conclusion
It is commonly assumed that increasing above-ground C stocks by planting trees or perennial crops will result in an automatic proportional increase in SOC.
The results of this nine-year study highlight, however, that this is not always the case and that, on the contrary, overall SOC might even decrease.
Such a result should not be surprising given the multitude of factors influencing changes in SOC stock.
Overall the results of this study show that the C stock changes down to 40cm soil depth were greatly outweighed by the C gains in the above-ground biomass.
While loss of SOC below 40cm depth probably also occurred, it is improbable that it matched the increases in above-ground biomass.
This further emphasises the importance both of conservation of tree biomass in established forest and agroforestry systems and of avoiding practices that reduce stocks of SOC.
Land use decisions designed to take into account impacts on climate change mitigation should be based on analyses that include all of the major components.
For example, assessment of alternative agricultural soil management systems that change SOC stocks should take into account not only C sequestration in the soil, but also emissions of all GHGs, impacts on biomass growth rate of all system components and impacts on crop yield (with its potential effect on future farmer management decisions).
Acknowledgements
We thank CATIE for providing and managing the study sites; Mirna Barrios, Elias de Melo, Luis Romero, Elvin Navarette and Ledis Navarette for their great efforts in helping to collect the data for this study; James Gibbons for his advice on the statistical analysis; Davey Jones for his advice on the soil analysis, and last but not least Gareth Edwards-Jones whose sheer brilliance is greatly missed.
This research was funded by a UK Economic & Social Research Council/Natural Environment Research Council studentship award and partial fieldwork grants by CAFNET and the Coalbourn Trust to MRAN.
Supplementary data
Supplementary data associated with this article can be found, in the online version, at http://dx.doi.org/10.1016/j.agee.2013.04.012.
Supplementary data


CLAIMS
1. Pulverulent compound of the formula Li<sub>a</sub>Ni<sub>b</sub>Ml<sub>c</sub>M2<sub>d</sub>(O)<sub>2</sub>(SO<sub>4</sub>),, in which Ml denotes at least one element selected from the group consisting of Fe, Co, Cr, Mg, Zn, Cu and/or mixtures thereof, M2 denotes at least one element selected from the group consisting of Mn, Al, B, Ca, Sr, Ba, Si and/or mixtures thereof, and 0.95 < a < 1.1, 0.3 < b < 0.83, 0.1 < c < 0.5, 0.03 < d < 0.5 and 0.001 < x < 0.03, characterized in that the secondary particles have a compressive strength of at least 100 MPa.2. Pulverulent compound according to Claim 1, characterized in that it has a compressive strength of at least 200 MPa.3. Pulverulent compound according to Claim 1, characterized in that it has a compressive strength of at least 300 MPa.4. Pulverulent compound according to at least one of Claims 1 to 3, characterized by a porosity of up to 0.01 cm3/g measured according to ASTM D 4222.5. Pulverulent compound according to at least one of Claims 1 to 3, characterized by a porosity of up to 0.008 cm3/g measured according to ASTM D 4222.6. Pulverulent compound according to at least one of Claims 1 to 3, characterized by a porosity of up to 0.006 cmVg measured according to ASTM D 4222.7. Pulverulent compound according to at least one of Claims 1 to 6, characterized in that the secondary particles have a spheroidal shape.8. Pulverulent compound according to Claim 7, characterized in that the secondary particles thereof have a shape factor greater than 0.8.9. Pulverulent compound according to Claim 7, characterized in that the secondary particles thereof have a shape factor greater than 0.9.10. Pulverulent compound according to at least one of Claims 1 to 9, characterized in that the DlO value, measured according to ASTM B 822, after compression of the material at a pressure of 200 MPa changes by not more than 0.5 μm compared with the starting material.11. Pulverulent compound according to at least one of Claims 1 to 9, characterized in that the D90 value, measured according to ASTM B 822, after compression of the material at a pressure of 200 MPa changes by not more than 1 μm compared with the starting material.12. Pulverulent compound according to at least one of Claims 1 to 11, characterized in that the normalized width of the particle size distribution, measured according to the Formula (1) D90 -DlO D50 in which D denotes the diameter of the secondary particles, is less than 1.4.13. Pulverulent compound according to at least one of Claims 1 to 11, characterized in that the normalized width of the particle size distribution, measured according to the Formula (1) D90 -DlO 1) D50 in which D denotes the diameter of the secondary particles, is less than 1.2.14. Pulverulent compound according to at least one of Claims 1 to 13, characterized in that it has a compressed density of at least 3.2 g/cm3at a compression pressure of 200 MPa.15. Pulverulent compound according to at least one of Claims 1 to 14, characterized in that it has a tapped density measured according to ASTM B 527, of at least 2.2 g/cm3.16. Pulverulent compound according to at least one of Claims 1 to 14, characterized in that it has a tapped density measured according to ASTM B 527, of at least 2.4 g/cm3.17. Process for the preparation of the pulverulent compound according to at least one of Claims 1-16, comprising the following steps: a. provision of a co-precipitated nickel-containing precursor having a porosity of less than 0.05 cm<sup>3</sup>/g, measured according to ASTM D 4222, b. mixing the precursor according to a) with a lithium- containing component, c. calcination of the mixture with multistage heating to temperatures of 1000<sup>0</sup>C with the use of a Cθ<sub>2</sub>~free (< 0.5 ppm of CO<sub>2</sub>), oxygen-containing carrier gas and production of a pulverulent product, d. deagglomeration of the powder by means of ultrasound and sieving of the deagglomerated powder.18. Process according to Claim 17, characterized in that the nickel-containing component is a mixed oxide, mixed hydroxide, mixed oxyhydroxide, partially oxidized mixed hydroxide, partially oxidized mixed hydroxysulphate of the metals Ni, Co, Mn, Al, Fe, Cr, Mg, Zr, B, Zn, Cu, Ca, Sr, Ba and/or mixtures thereof.19. Process according to Claim 17, characterized in that the lithium-containing component is lithium carbonate, lithium hydroxide, lithium hydroxide monohydrate, lithium oxide, lithium nitrate and/or mixtures thereof.20. Process according to at least one of Claims 17 to 19, characterized in that the calcination of the precursor mixture is effected at a temperature of 200- 4000C for 2-10 hours in the first stage, at 500-7000C for 2-10 hours in the second stage and at 700-10000C for 2-20 hours in the third stage.21. Process according to at least one of Claims 17 to 19, characterized in that the calcination of the precursor mixture is effected at a temperature of 250- 3500C for 2-10 hours in the first stage, at 550-650°C for 2-10 hours in the second stage and at 725-9750C for 2-20 hours in the third stage.22. Process according to at least one of Claims 17 to 19, characterized in that the calcination of the precursor mixture is effected at a temperature of 250- 435°C for 4-8 hours in the first stage, at 550-650°C for 4-8 hours in the second stage and at 725-9750C for 5-15 hours in the third stage.23. Process according to at least one of Claims 17 to 22, characterized in that the carrier gas contains 20 to 100% by volume of oxygen.24. Process according to at least one of Claims 17 to 22, characterized in that the carrier gas contains 40 to 100% by volume of oxygen.25. Process according to at least one of Claims 17 to
24. characterized in that the reaction of the nickel- containing precursor takes place with retention of the shape of the secondary particles and/or particle size distribution .26. Pulverulent compounds obtainable according to at least one of Claims 17 to 25.27. Use of the pulverulent compound according to at least one of Claims 1 to 26 as electrode material in lithium secondary batteries.High surface area, mesoporous carbon for low-polarization, catalyst-free lithium oxygen battery

The mesoporous carbon was prepared, according to previous paper [37], by mixing 48.0 g of F127 (PEO106PPO70PEO106, MW 12600, Aldrich) and 15.0 g of HCl (0.2 M) in 120 g of ethanol and stirring for 1 h at 40 degC, until the formation of a transparent solution. Following, the formed solution has been added by 31.2 g of tetraethyl orthosilicate (TEOS, Acros) and 77.3 g of resol-ethanolic-20% mix (prepared by polymerizing phenol and formaldehyde, Aldrich) [37], stirred for 5 h and heated at 40 degC to remove the ethanol and at 100 degC to thermo-polymerize the resol. The dry components where then annealed at 900 degC for 2 h under argon flow, using a heating rate of 1 degC min- 1 until 600 degC and 5 degC min- 1 until 900 degC. Finally, mesoporous carbon was obtained after removing SiO2 using a 50 wt.% HF solution for 24 h.
Oxidation of reduced polycrystalline Nb-doped SrTiO3: Characterization of surface islands

Nb-doped (5 mol%) polycrystalline SrTiO3 was prepared from highly pure SrCO3, TiO2, and Nb2O5 powders (99.999%, Aldrich) by the mixed oxide route. The mixed powder was calcined at 1100 degC under air for 1 h and isostatically pressed into a pellet under 680 MPa. The pellet was sintered under reducing atmosphere (5% H2 + 95% Ar) at 1360 degC for 15 h. The reduced polycrystalline SrTi0.95Nb0.05O3 exhibits a deep dark gray color indicating high electron conductivity, i.e. electron compensation of the dopant.

A novel FeAs anode material for lithium ion battery

FeAs was prepared using Fe powder and arsenic bulk by solid state reaction. The starting materials were mixed in a mole ratio of 1:1, and then transferred into a quartz tube. After evacuation, the quartz tube was sealed by an oxyhydrogen flame. The tube was then put into a muffle furnace, heated to 800 degC at 5 degC min-1, and calcined at 800 degC for 10 h.
Synthesis of cage-like LiFePO4/C microspheres for high performance lithium ion batteries

All chemicals were purchased from Sinopharm Chemical Reagent Co., Ltd. and used without further purification. In a typical synthesis, 0.015 mol of Li2SO4, 0.01 mol of Fe(NO3)3*9H2O and 0.005 mol of P2O5 were added in a Teflon vessel together with 40 mL ethylene glycol (EG) and 20 mL deionized water. After vigorous stirring for 0.5 h, the vessel was sealed in a stainless-steel autoclave, then placed inside an oven and heated at 180 degC for 24 h. Afterward, the precipitates were collected by centrifugation and dried at 80 degC for 72 h in vacuum atmosphere. The obtained product was calcined at 600 degC under flowing Ar/H2 atmosphere (100 mL min-1, 95 vol. % Ar and 5 vol. % H2) for 10 h, and for the carbon coating, the gas was then switched to C2H4/Ar (20 mL min-1, 50 vol. % C2H4 and 50 vol. % Ar) for 15 min. To investigate the formation process of the cage-like LPCs, time-dependent experiments were carefully conducted. And the experimental parameters, such as the phosphate sources and the solvent, were also varied during the synthesis to study their effects on the shape of the final product. Commercial LiFePO4/C sample (particle size around 2-3 μm, carbon content about 5.2%) was purchased from Tianjin Sterlan-Energy Ltd., China.
Low-temperature fabrication of an HfO2 passivation layer for amorphous indium-gallium-zinc oxide thin film transistors using a solution process

To fabricate the HfO2 passivation layer, the HfO2 precursor solution (0.1 M) was prepared by dissolving hafnium (IV) chloride (HfCl4; Aldrich, 98%) in DI water. For the Y2O3 passivation layer, the Y2O3 precursor solution was made using yttrium (III) chloride hexahydrate (YCl3*6H2O; Aldrich, 99.9%) using the same molar ratio and solvent as the HfO2 precursor solution. For the PMMA passivation layer, the PMMA precursor solution was synthesized by dissolving 40 mg/mL of PMMA ([CH2C(CH3)(CO2CH3)]n; Aldrich; Mw ca.15,000) in butyl acetate (CH3COO(CH2)3CH3; Sigma-Aldrich, 99%). All solutions were stirred for 1 h at room temperature and aged for 24 h. The precursor solutions were then spin-coated onto the fabricated a-IGZO TFTs at 3,000 rpm for 30 s, and annealed at 100 to 250 degC in air for 1 h.


1. A carbonate precursor compound for the manufacture of lithium metal (M) oxide powder useful as an active positive electrode material in lithium ion batteries, M comprising 20 to 90 mol% Ni, 10 to 70 mol% Mn and 10 to 40 mol% Co, the precursor further comprising sodium and sulfur impurities, wherein the molar ratio of sodium to sulfur (Na/S) is 0.4<Na/S<2, and wherein the sodium (Na) is expressed in weight%<Sub>Weight (D)</Sub>) And sulfur (S)<Sub>Weight (D)</Sub>) Sum of contents (2. multidot. Na)<Sub>Weight (D)</Sub>)+S<Sub>Weight (D)</Sub>More than 0.4 wt% and less than 1.6 wt%.2. The carbonate precursor compound of claim 1, having the general formula MCO3Wherein M is NixMnyCozAvAnd A is a dopant, wherein x is more than or equal to 0.20 and less than or equal to 0.90, y is more than or equal to 0.10 and less than or equal to 0.67, z is more than or equal to 0.10 and less than or equal to 0.40, v is less than or equal to 0.05, and x + y + z + v is 1.3. The carbonate precursor compound of claim 2, wherein a is any one or more of Mg, Al, Ti, Zr, Ca, Ce, Cr, Nb, Sn, Zn, and B.4. A carbonate precursor compound for the manufacture of lithium metal (M) oxide powder useful as active positive electrode material in lithium ion batteries having the general formula MCO<Sub>3</Sub>Wherein M is Ni<Sub>x</Sub>Mn<Sub>y</Sub>Co<Sub>z</Sub>A<Sub>v</Sub>A is a dopant, wherein x is more than or equal to 0.10<Y is more than or equal to 0.30, 0.55 and less than or equal to 0.80, and 0<z ≦ 0.30, v ≦ 0.05, and x + y + z + v ≦ 1, the precursor further comprising sodium and sulfur impurities, wherein the molar ratio of sodium to sulfur (Na/S) is 0.4<Na/S<2。5. The carbonate precursor compound of claim 1, wherein the sodium content is between 0.1 and 0.7 wt% and the sulfur content is between 0.2 and 0.9 wt%.6. A lithium metal oxide powder for use in a positive electrode material in a rechargeable battery having the general formula Li1+aM1-aO2Wherein M is NixMnyCozAvA is a dopant, where-0.05. ltoreq. a.ltoreq.0.25, 0.20. ltoreq. x.ltoreq.0.90, 0.10. ltoreq. y.ltoreq.0.67, and 0.10. ltoreq. z.ltoreq.0.40, v.ltoreq.0.05, and x + y + z + v.ltoreq.1, the powder having a particle size distribution of 10 μm. ltoreq.D 50. ltoreq.20 μm, a specific surface area of 0.9. ltoreq.
BET 5 in m<Sup>2</Sup>Expressed in terms of/g, said powder further comprising sodium and sulphur impurities, wherein said sodium (Na) is expressed in weight%<Sub>Weight (D)</Sub>) And sulfur (S)<Sub>Weight (D)</Sub>) ComprisesSum of the amounts (2 Na)<Sub>Weight (D)</Sub>)+S<Sub>Weight (D)</Sub>More than 0.4 wt% and less than 1.6 wt%, and wherein the molar ratio of sodium to sulfur (Na/S) is 0.4<Na/S<2。7. The lithium metal oxide powder of claim 6, comprising secondary LiNaSO4And (4) phase(s).8. The lithium metal oxide powder of claim 7, wherein the secondary LiNaSO4The relative weight of the phases is at least 0.5 wt%, as determined by rietvell (Rietveld) analysis of the XRD pattern of the powder.9. The lithium metal oxide powder of claim 6, wherein:
0.4<Na/S<1, and the powder further comprises Na<Sub>2</Sub>SO<Sub>4</Sub>(ii) a Or
1<Na/S<2 and the powder further comprises Li<Sub>2</Sub>SO<Sub>4</Sub>。10. The lithium metal oxide powder of claim 6, wherein A is any one or more of Mg, Al, Ti, Zr, Ca, Ce, Cr, Nb, Sn, Zn, and B.11. A lithium metal oxide powder for use in a positive electrode material in a rechargeable battery having the general formula Li<Sub>1+a</Sub>M<Sub>1-a</Sub>O<Sub>2</Sub>Wherein M is Ni<Sub>x</Sub>Mn<Sub>y</Sub>Co<Sub>z</Sub>A<Sub>v</Sub>A is a dopant, wherein a is more than or equal to 0.10 and less than or equal to 0.25, and x is more than or equal to 0.10 and less than or equal to 0.25<Y is more than or equal to 0.30, 0.55 and less than or equal to 0.80, and 0<z 0.30, v 0.05 and x + y + z + v 1, said powder having a particle size distribution of 10 [ mu ] m 20 [ mu ] m D50, 0.9 BET 5 in m<Sup>2</Sup>Expressed in terms of/g, said powder further comprising sodium and sulphur impurities, wherein said sodium (Na) is expressed in weight%<Sub>Weight (D)</Sub>) And sulfur (S)<Sub>Weight (D)</Sub>) Sum of contents (2. multidot. Na)<Sub>Weight (D)</Sub>)+S<Sub>Weight (D)</Sub>More than 0.4% by weightAnd less than 1.6 wt%, and wherein the molar ratio of sodium to sulfur (Na/S) is 0.4<Na/S<2。12. A method for preparing the carbonate precursor compound of claim 2, comprising the steps of:
-providing a feed solution comprising Ni ions, Mn ions, Co ions and an A source, wherein the Ni ions, Mn ions, Co ions and A ions are present in a water-soluble sulfate compound,
-providing an ionic solution comprising a carbonate solution and Na ions,
wherein said CO<Sub>3</Sub>/SO<Sub>4</Sub>The ratio is selected so as to obtain a ratio of 0.4<Na/S<2, and said sodium (Na) in weight%<Sub>Weight (D)</Sub>) And sulfur (S)<Sub>Weight (D)</Sub>) The content yields a sum (2 Na) of more than 0.4 wt% and less than 1.6 wt%<Sub>Weight (D)</Sub>)+S<Sub>Weight (D)</Sub>，
-providing a slurry comprising seed crystals containing M' ions, wherein M ═ Nix’Mny’Coz’A’n’A ' is a dopant, and x ' is not less than 0 and not more than 1, y ' is not less than 0 and not more than 1, z ' is not less than 0 and not more than 1, n ' is not less than 0 and not more than 1, and x ' + y ' + z ' + n ' + 1,
-mixing the feed solution, the ionic solution and the slurry in a reactor, thereby obtaining a reactive liquid mixture,
-precipitating a carbonate salt on the seed crystals in the reaction liquid mixture, thereby obtaining a reacted liquid mixture and the carbonate salt precursor, and
-separating the carbonate precursor from the reacted liquid mixture.13. The method of claim 12, wherein the M 'ion is present in a water-insoluble compound that is M' CO3、M’(OH)2M 'oxide and M' OOH.14. According to claim 12The process of (1), wherein the molar ratio (M ') of the metal content in the seed slurry to the metal content in the feed solution'Seed crystal/MFeeding of the feedstock) Between 0.001 and 0.1, and wherein the carbonate precursor has a median particle size of M'Seed crystal/MFeeding of the feedstockAnd (4) determining the ratio.15. The method of claim 12, wherein a and a' are any one or more of Mg, Al, Ti, Zr, Ca, Ce, Cr, Nb, Sn, Zn, and B.16. The method of claim 12, wherein NH in the reactor3The concentration is lower than 5.0 g/L.17. The method of claim 12, wherein M ═ M'.18. The method of claim 12, wherein the ionic solution further comprises either or both of a hydroxide solution and a bicarbonate solution, and OH/CO3Ratio, or OH/HCO3The ratio, or both, is less than 1/10.19. The method of claim 12, wherein the seed crystals have a median particle diameter D50 of between 0.1 and 3 μ ι η.Surface waves on Saturn's magnetopause

Abstract
Waves on the surface of a planetary magnetopause promote energy transport into the magnetosphere, representing an important aspect of solar wind-magnetosphere coupling.
At Saturn's magnetopause it has been proposed that growth of the Kelvin-Helmholtz (K-H) instability produces greater wave activity on the dawn side of the surface than on the dusk side.
We test this hypothesis using data taken by the Cassini spacecraft during crossings of Saturn's magnetopause.
Surface orientation perturbations are primarily controlled by the local magnetospheric magnetic field orientation, and are generally greater at dusk than at dawn.
53% of all crossings were part of a sequence of regular oscillations arising in consecutive surface normals that is strong evidence for tailward propagating surface waves, with no detectable local time asymmetry in this phenomenon.
We estimate the dominant wave period to be ∼5h at dawn and ∼3h at dusk.
The role played by the magnetospheric magnetic field, tailward wave propagation, and the dawn-dusk difference in wave period suggests that K-H instability is a major wave driving mechanism.
Using linear K-H theory we estimate the dominant wavelength to be ∼10 Saturn radii (RS) and amplitude to be ∼1 RS at both dawn and dusk, giving propagation speeds of ∼30 and ∼50kms-1 at dawn and dusk, respectively.
The lack of the hypothesized dawn-dusk asymmetry in wave activity demonstrates that we need to revise our understanding of the growth of the K-H instability at Saturn's magnetopause, which will have implications for the study of other planetary magnetospheres.
Highlights
► Surface waves on a planetary magnetopause transport energy into the magnetosphere.
► Differences in the level of wave activity across Saturn's magnetopause has been predicted.
► Kelvin-Helmholtz waves often form on Saturn's magnetopause surface.
► There is no evidence for the predicted difference across the surface.
► We need to revise our understanding of surface waves on Saturn's magnetopause.

Introduction
A planetary magnetosphere results from the interaction between the solar wind and a magnetized planet, producing a cavity around the planet that effectively shields it from the solar wind flow.
The boundary of a magnetosphere is referred to as its magnetopause, and is often the site of transport of solar wind energy into the system.
Waves on the surface of a planetary magnetopause are one of the processes that promotes this energy transport.
Our understanding of this phenomenon is mainly based on spacecraft data taken at the boundary of Earth's magnetosphere, which is understandably the most-observed planetary magnetopause (see the review by de Keyser et al. (2005) and references therein).
Substantial evidence for surface waves on Earth's magnetopause has been reported (e.g. Aubry et al., 1971) and the driving mechanisms responsible for these waves include solar wind pressure pulses (Song et al., 1988; Sibeck, 1990), growth of the Kelvin-Helmholtz (K-H) instability (Owen et al., 2004; Foullon et al., 2008; Eriksson et al., 2009), and magnetic reconnection (Song et al., 1988).
Resolving the wave driving mechanism is often difficult, but is important because of the insight it provides into the nature of the transport of momentum and energy between the solar wind and the magnetosphere.
Saturn's magnetosphere is significantly different from Earth's in many respects (see the reviews by Gombosi et al. (2009) and Mitchell et al. (2009)).
In terms of the phenomenon of magnetopause surface waves, Saturn's magnetosphere is a natural laboratory in which we can examine one of the most widely studied wave driving mechanisms: The growth of the K-H instability.
This instability can grow at an interface between two fluids (e.g. a planetary magnetopause (Dungey, 1955)), particularly under high flow shear conditions.
A seed perturbation of a K-H unstable interface will grow with time, rather than be suppressed, leading to surface waves in the linear phase of the instability, and complex boundary vortices in the subsequent nonlinear phase.
Growth of the K-H instability at Earth's magnetopause has been the subject of much research attention, with spacecraft data revealing evidence for K-H vortices associated with plasma mixing and local magnetic reconnection (Fairfield et al., 2000; Nikutowski et al., 2002; Hasegawa et al., 2004, 2006, 2009; Nykyri et al., 2006; Nishino et al., 2011).
One of the principal differences between the magnetospheres of Earth and Saturn is that plasma in Saturn's magnetosphere circulates in the sense of planetary rotation throughout the dayside magnetosphere (Thomsen et al., 2010), whereas at Earth such plasma motion is confined to the inner magnetosphere.
Consequently, although it is subject to large temporal and spatial variability, the shear between the flow of magnetosheath solar wind plasma and the flow of magnetospheric plasma is significantly greater on the dawn side of Saturn's magnetopause than on the dusk side.
This variation in the flow shear across Saturn's magnetopause with local time is expected to lead to a generally K-H unstable boundary on the dawn side and a generally K-H stable boundary on the dusk side, producing greater boundary perturbations (waves and vortices) at dawn than at dusk (Galopeau et al., 1995).
This plausible expectation has featured in theories of magnetosphere-ionosphere coupling at Saturn (Galopeau et al., 1995; Sittler et al., 2006).
However, this prediction of a local time asymmetry in the K-H instability of Saturn's magnetopause has not been previously tested.
Two initial studies of the orientation of the magnetopause surface, each based on data returned by the Cassini Saturn orbiter during magnetopause crossings that occurred in an interval of a few days, have revealed evidence for wave activity on both the dawn side and on the dusk side (Masters et al., 2009; Cutler et al., 2011).
In both these cases the K-H instability was found to be a plausible driver of the identified waves, with both sets of waves propagating tailward.
We note that although this tailward propagation was correctly concluded by Masters et al. (2009) in their analysis of the dawn side waves the presented analysis results suggest sunward propagation instead, due to a discrepancy between the data and the text.
Another region of interest concerning the predicted dawn-dusk asymmetry in magnetopause K-H instability is Saturn's Low-Latitude Boundary Layer (LLBL).
This quasi-permanent layer lies adjacent to, and planetward of, the magnetopause at low latitudes, resulting from the transport of solar wind plasma into the magnetosphere (McAndrews, 2007; Masters et al., 2011a, 2011b).
If present, a strong dawn-dusk asymmetry in magnetopause K-H instability could produce a related asymmetry in the properties (e.g. thickness) of the LLBL, since K-H instability can facilitate the cross-magnetopause plasma transport that creates the layer (see the review by Sibeck (1999) and references therein).
However, a recent examination of Saturn's LLBL was not able to detect any clear dawn-dusk difference in the thickness of the layer (Masters et al., 2011a), further questioning whether the hypothesized dawn-dusk asymmetry in magnetopause K-H instability is present.
In addition, Saturn's LLBL represents an intermediate plasma regime that will affect the K-H stability of the magnetopause, as well as creating an additional interface that could also become unstable: The inner (planetward) edge of the LLBL.
Since plasma flow in Saturn's LLBL is poorly understood at present it is not possible to accurately predict how the presence of the layer affects the hypothesized magnetopause K-H instability asymmetry; however, a Cassini encounter with a K-H vortex has been identified on the inner edge of the LLBL (Masters et al., 2010), implying that this interface may be typically more K-H unstable than the magnetopause (e.g. Ogilvie and Fitzenreiter, 1989).
As well as these observational studies, magnetohydrodynamic simulations of Saturn's magnetosphere have been carried out in which waves and vortices form on the magnetopause (Fukazawa et al., 2007a, b; Walker et al., 2011).
The perturbations of the magnetopause surface in these simulations are consistent with a K-H instability driving mechanism, and an important result of these studies is that simulated waves and vortices can form on both the dawn and dusk sides, with the orientation of the interplanetary magnetic field being an important controlling factor.
Recent hybrid simulations have drawn attention to the importance of the growth of K-H instability at Saturn's magnetopause for mass transfer across the boundary, which can influence magnetospheric dynamics (Delamere et al., 2011).
The sum of these Cassini-era studies of Saturn's magnetopause suggest that we need to test the prediction of a dawn-dusk asymmetry in the K-H instability of Saturn's magnetopause using spacecraft observations.
Additional magnetopause studies based on Cassini observations have confirmed an aspect of Saturn's magnetopause dynamics that must also be considered when investigating boundary dynamics.
The mysterious, persistent modulation of Saturn's magnetospheric environment at approximately the period of planetary rotation (∼10.75h, see the review by Mitchell et al. (2009)) leads to a quasi-periodic modulation of the total pressure adjacent to the magnetopause in Saturn's outer magnetosphere at a similar period (hereafter referred to as the "magnetospheric period").
This leads to an oscillation of the magnetopause with an estimated typical amplitude of ∼1.2 Saturn radii (RS; 1 RS=60,268km) (Clarke et al., 2006, 2010).
This effect has been interpreted as a displacement of the boundary on a large-scale (with respect to the scale of the dayside magnetopause), which is not expected to produce perturbations of the surface orientation as great as those reported by recent surface wave case studies (Masters et al., 2009; Cutler et al., 2011).
In this paper we test the hypothesized dawn-dusk asymmetry in the K-H instability of Saturn's magnetopause for the first time by assessing the extent of surface wave activity using data taken by the Cassini spacecraft during 520 magnetopause crossings.
Although we find evidence for significant perturbations of the magnetopause surface orientation and surface wave activity, and that growth of the K-H instability is a major driver of these surface perturbations, we find no clear evidence for the hypothesized dawn-dusk asymmetry.
Observations
The coordinate system used throughout this study is the Kronocentric Solar Magnetospheric (KSM) system, which is Saturn-centered with the positive x-axis pointing toward the Sun.
The z-axis is chosen such that the xz plane contains Saturn's magnetic dipole axis, with the positive z-axis pointing toward the North.
The y-axis completes the orthogonal set, with the positive y-axis pointing toward dusk.
To examine the occurrence of surface waves on Saturn's magnetopause we analyzed data taken during 520 crossings of the boundary made by the Cassini spacecraft between 28 June 2004 and 23 July 2007.
This period was chosen as it provides us with 260 crossings on the dawn side of the magnetopause and 260 crossings on the dusk side.
The positions of these crossings are shown in Fig. 1.
The crossings took place between magnetic latitudes of -38° and 52°, and between Saturn Local Times (SLT) of 03:25 and 17:37.
This distribution of crossings largely confines our assessment of surface waves to the region sunward of the terminator.
The scatter of the crossing positions is a result of the highly variable position of the magnetopause (Kanani et al., 2010).
Many of the crossings clearly occurred on the same orbital pass due to magnetopause motion at speeds greater than that of the spacecraft.
The time duration between crossings on a particular pass was highly variable, with some as short as ∼20min and others as long as a few days.
This variability arises from the superposition of different influences on the boundary position, for example, solar wind dynamic pressure changes, magnetospheric period oscillation, and wave activity.
The basis of this study of surface waves is the calculation of the orientation of Saturn's magnetopause surface for each appropriate Cassini magnetopause crossing.
This calculation is based on magnetic field data acquired by the fluxgate magnetometer sensor of the Cassini dual-technique magnetometer (Dougherty et al., 2004) during the crossing.
To provide confirmation of the timing of each magnetopause crossing we also used electron data taken by the ELectron Spectrometer (ELS) of the Cassini plasma spectrometer, which detects electrons between 0.5eV and 26keV (Young et al., 2004).
Electron moments were calculated using background-subtracted data from anode 5, assuming an isotropic distribution in the spacecraft frame (Lewis et al., 2008).
Since ELS data were only used to confirm magnetopause crossings we present only electron moments in this paper, to complement the magnetic field data.
Fig. 2 shows magnetic field and electron data taken during a Cassini magnetopause crossing on 1 May 2005.
The spacecraft began the interval in the magnetosphere where the magnetic field strength was ∼5nT and the local electron population was characterized by a number density of ∼0.006cm-3 and at a temperature of ∼100eV.
From the beginning of the interval until ∼08:18 Universal Time (UT) the spacecraft gradually entered a region of similar field characteristics, but where the electron density increased and the electron temperature decreased.
We identify this region as the LLBL (Masters et al., 2011a).
Between ∼08:18 and ∼08:22 UT the field orientation changed and the field strength decreased.
The angle between the field vectors measured at the start and end of this interval is ∼139°.
The interval was also associated with a strong increase in the electron density and a decrease in the electron temperature with time.
We identify this interval (shaded dark-gray in Fig. 2) as the magnetopause current layer (MPCL).
From ∼08:22 UT until the end of the interval the spacecraft was in the magnetosheath, where the field remained weak but steady and the electron environment remained relatively dense and cold.
The determination of the orientation of Saturn's magnetopause surface at the time of each crossing was dependent on the identification of shaded intervals analogous to those shown in Fig. 2.
In all 520 cases we attempted to identify the interval when the spacecraft crossed the MPCL, and intervals immediately either side of this MPCL interval (shaded light-gray in the example shown in Fig. 2).
At some crossings the identification of the MPCL interval was ambiguous, particularly in cases where there was a low magnetic shear between the magnetospheric and magnetosheath magnetic fields.
We did not consider such crossings further, reducing our total crossing set to 477 (233 at dawn and 244 at dusk).
Determining magnetopause surface orientation
In this paper we use two methods to determine the orientation of Saturn's magnetopause, both of which are based on magnetic field data taken near and during a traversal of the MPCL.
These surface normals can be compared to the normal predicted by the most recent model of Saturn's large-scale magnetopause surface that was constructed by Kanani et al. (2010).
To obtain this predicted normal for each crossing the model was scaled to intersect the position of the crossing, and the normal to the model surface at this position was calculated.
Throughout this paper we refer to such normals as "model normals".
A model normal is effectively the nominal orientation of the magnetopause surface at a specific location, providing us with a reference to use when assessing the perturbation of the boundary orientation.
All model normals, and all normals determined using the two approaches described below, were defined to point out into magnetosheath, away from Saturn.
The first normal determination method is based on the average magnetic fields measured in few-minute intervals either side of an MPCL traversal (Fig. 2, intervals shaded light-gray).
If the boundary is a Tangential Discontinuity (TD-magnetic field is parallel to the plane of the boundary surface on both sides) then the vector product of these two average fields gives the normal direction.
To a first approximation a planetary magnetopause is a TD that separates the interplanetary and planetary magnetic fields, albeit with a finite thickness, making Tangential Discontinuity Analysis (TDA) a viable method for determining the surface orientation.
Throughout this paper we refer to the normals arising from this technique as "TDA normals".
The second normal determination method is based on all the magnetic field measurements made during an MPCL traversal (Fig. 2, interval shaded dark-gray).
Minimum Variance Analysis (MVA) (Sonnerup and Scheible, 1998) is widely employed in space plasma physics.
It is an analysis technique applied to a set of vectors that determines the directions of minimum, intermediate, and maximum variance of the observed vector field.
These directions form an orthogonal set and each direction is associated with an eigenvalue, with the smallest and largest eigenvalues respectively associated with the minimum and maximum variance directions.
The greater the ratio between eigenvalues the better defined a variance direction is.
For example, the higher the ratio of intermediate to minimum eigenvalue the better defined the minimum variance direction is.
The minimum variance direction resulting from the application of MVA to the set of magnetic field vectors measured during an MPCL traversal is a measure of the orientation of a magnetopause boundary, often favored in single spacecraft studies (e.g. Lepping and Burlaga, 1979).
Assuming that the boundary is a perfect TD, the component of the field in the direction normal to the actual magnetopause should equal 0 and remain constant throughout the crossing of the MPCL, and this direction is detected by MVA as the minimum variance direction.
Throughout this paper we refer to such normals as "MVA normals".
We refer the reader to Lepping and Behannon (1980) and Knetter et al. (2004) for a detailed discussion of both the TDA and MVA methods applied to interplanetary magnetic field discontinuities.
Figs. 3 and 4 show data taken during two different sets of consecutive Cassini magnetopause crossings.
These example crossing sets are used here to introduce the important quantities associated with the normal determination at each crossing, and to illustrate the possible variability of these values.
In Section 5 these crossing sets will be re-visited in the context of identifying evidence for surface wave activity.
Five crossings occurred during the interval shown in Fig. 3 and six occurred during the interval shown in Fig. 4.
In both figures panels (c) and (d) present information related to the normal determination methods described in this section.
The ratio of intermediate to minimum MVA eigenvalue (λ2/λ3) was generally above 10 for the crossings in both sets, although some crossings were associated with lower values.
The mean normal field component divided by the mean field strength during the MPCL transition (|Bn|/|B|) is based on the MVA normal, and assesses to what degree the boundary may be approximated as a TD (for a perfect TD this value should be 0).
All of these |Bn|/|B| values are below 0.3, which is generally consistent with a TD (Lepping and Behannon, 1980; Knetter et al., 2004), and also suggests TDA is an appropriate method for determining the surface normal.
The angles between the MVA and TDA normals are all below 30°, revealing a good agreement between these two different methods, and also giving a measure of the angular uncertainty associated with the normals resulting from either approach.
Lastly, the angles between the MVA and model normals are variable and sometimes as high as 60° for these two sets of crossings, suggesting that strong perturbations of the surface orientation can occur.
Fig. 5 shows histograms of parameters resulting from our assessment of the orientation of Saturn's magnetopause for all 477 crossings.
The magnetic shear across the boundary (given by the angular difference between magnetic fields measured immediately before and after each MPCL traversal) was variable, and often below 90°.
Higher shears are preferable for both the MVA and TDA normal determination methods (Knetter et al., 2004).
A large level of variability was also associated with the intermediate to minimum eigenvalue ratio from MVA (median value: 6.9).
At some of the crossings the minimum variance direction was poorly defined, particularly in the cases of a ratio approaching 1.
The |Bn|/|B| value during each MPCL transition was predominantly below 0.3 (95%), with a significant fraction of the values below 0.1 (73%).
This suggests that Saturn's magnetopause was generally well described by a TD for the crossings in our set.
High values of the normal field component should be analyzed in a future study to assess whether they are examples of an "open" magnetopause resulting from magnetic reconnection.
The angle between the MVA and TDA normals was predominantly below 30° (91%), confirming the typically good agreement between these different approaches to determining the normal.
These results concerning the orientation of Saturn's magnetopause surface suggest that both the MVA and TDA methods generally capture the true surface normal to within an angular uncertainty of roughly 30°, but in some cases the validity of the normals is questionable due to a low magnetic shear and/or a low MVA eigenvalue ratio.
We note that alternative methods for determining the angular uncertainty in these normals (e.g. Sonnerup and Scheible, 1998) result in errors that are generally comparable to our estimate of 30°.
Furthermore, the conclusions we draw in Sections 4 and 5 are based on statistics, and the identification of a systematic oscillation of consecutive surface normals by angles predominantly in excess of 30°.
We conclude that a more sophisticated error analysis is not necessary here, given the aims of the present study.
Since we rely on the number of crossings in our data set to reveal the global picture of waves on Saturn's magnetopause surface, we do not omit any crossings on the basis of an inferred low level of confidence in the associated normal.
Our conclusions are not affected by omitting crossings below a reasonable threshold of magnetic shear (e.g. 30°) or MVA eigenvalue ratio (e.g. 5) in order to isolate the most reliable normals.
In the following sections we use MVA normals, with the exception of one part of our analysis where we use TDA normals for reasons discussed in Section 4.
However, none of the conclusions we draw from our results are sensitive to the choice of normal determination method, consistent with the generally good agreement between the normals produced by these different methods (see Fig. 5).
Perturbations of the magnetopause surface
In this section we examine the nature of perturbations of Saturn's magnetopause surface orientation, given by the difference between measured surface normals and predicted surface normals from a large-scale model of magnetopause morphology (Kanani et al., 2010).
We refer to the angle between the MVA and model normal for each crossing as the "perturbation angle".
Fig. 6 shows the variation of the perturbation angle with SLT, and histograms of this angle for all dawn side crossings and all dusk side crossings.
There is a clear local time asymmetry in the perturbation angles.
At dawn the perturbation angle is typically between 10 and 50°, whereas at dusk the distribution is broader with more angles above 50°.
These results show that strong perturbations of the surface orientation can occur, and that they are typically greater at dusk than at dawn.
This is the opposite of the hypothesis based on K-H instability that was discussed in Section 1, which predicts greater perturbation angles at dawn than at dusk.
Note that the bins of perturbation angle used in Fig. 6 do not cover equal solid angles, producing an apparent deficit of crossings in the 0 to 10° bin (best agreement with the model normal).
To extend this assessment of surface orientation perturbations we examined the dependence of the perturbation direction on the magnetic field orientation measured immediately either side of the boundary.
However, the MVA normals may be influenced by the local magnetospheric magnetic field orientation as a by-product of the MVA analysis technique.
The local magnetospheric field is generally stronger than the local magnetosheath field (e.g. Masters et al., 2011a), thus when MVA was applied to each MPCL interval the maximum variance direction was often well-defined (median maximum-to-intermediate eigenvalue ratio of 13.7, compared to median intermediate-to-minimum eigenvalue ratio of 6.9), and was generally close to the direction of the local magnetospheric field (mean angle between maximum variance direction and local magnetospheric magnetic field: 33°).
As a result the minimum variance direction may have been constrained to lie in a plane that is perpendicular to the local magnetospheric magnetic field due to the analysis technique.
However, this does not affect the TDA normals, and the good agreement between the two normal determination methods (see Fig. 5) suggests that this is not a major issue concerning the MVA normals.
Nonetheless, in this detailed assessment of the role of the local magnetic field orientations we use TDA normals to avoid the issue entirely.
Like all our conclusions, those we draw from these results are not sensitive to the choice of MVA or TDA normals.
Note that this is the only part of our analysis (in this section or Section 5) where TDA normals were preferred to MVA normals.
Fig. 7 shows a representation of the orientation of all the normals organized by the model normal and the local magnetic field direction (magnetosheath or magnetosphere) for each crossing.
In the case of each adjacent magnetic field all the TDA normals have been plotted from a common origin, with all model normals in alignment.
Each TDA normal was then rotated about the common model normal direction so that the adjacent magnetic field vector in question points vertically upwards when viewed in the model normal-perpendicular plane.
It is evident that the model normal represents the typical, un-perturbed orientation of the boundary.
To quantify this, the average TDA normal (in this coordinate system) agrees with the model normal to within 7°.
However, Fig. 7 also reveals that the orientation of the magnetospheric magnetic field clearly influences the surface orientation.
The measured surface normal is generally confined to lie in a plane containing the model normal and the direction perpendicular to the magnetospheric magnetic field.
The orientation of the magnetosheath field appears to have a similar influence, but it is clearly weaker.
There is no local time asymmetry in this effect, and it is not the result of a bias introduced by poorly determined normals (indicated by relatively large angles between MVA and TDA normals, see Fig. 7).
We interpret this role played by the local magnetic field as a consequence of the stabilizing influence of magnetic tension forces.
These tension forces act to resist boundary perturbations in the field-parallel direction, but are unable to resist field-perpendicular perturbations.
This interpretation also explains the difference between the magnetosheath and magnetospheric field influences; the magnetospheric field is generally stronger than the magnetosheath field (see examples in Figs.
3 and 4), leading to greater magnetospheric field tension forces and a greater influence exerted by this magnetic field regime.
We note that predominantly field-perpendicular boundary normal perturbations are consistent with perturbations resulting from the growth of the K-H instability, on the basis of the same argument related to magnetic tension forces given above (e.g. Southwood, 1968).
Surface wave analysis
Identification of oscillation of consecutive normals
To investigate the occurrence and properties of surface waves we identified the crossings that were most likely to have been associated with wave activity.
This was done on the basis of the behavior of consecutive MVA normals within each set of crossings.
Although waves may have been present on the surface for the duration of a set of crossings, other drivers of motion of the boundary (e.g. the magnetospheric period oscillation (Clarke et al., 2006, 2010)) can play a dominant role in determining when spacecraft crossings occurred, each of which provides a "snapshot" of the surface orientation.
However, if surface wave activity played a sufficiently important role in controlling when the spacecraft crossed the boundary then this should be revealed by the oscillation of consecutive normals about the model normal, and the timing of crossings associated with such normal oscillations can be used to infer the typical wave properties.
Fig. 8 illustrates why an oscillation of consecutive normals is consistent with wave activity.
The quantitative approach we used to identify crossings that were part of such an oscillating set of boundary normals is the same as the approach that was employed by Masters et al. (2009) and Cutler et al. (2011), which is also illustrated in Fig. 8.
If a set of normals oscillate in a particular direction then they form a "fan-like" distribution when plotted using a common origin (using KSM coordinates).
Our motivation was to define a coordinate system (unique to each set of normals) where two of the axes define the approximate plane of this "fan", which could then be used to identify which crossings were most likely to have been associated with wave activity.
We hereafter refer to this coordinate system as "surface wave coordinates".
The "n" axis of surface wave coordinates is the average of the normals for the set of crossings being considered, which was generally in agreement with the average model normal to within 20°.
MVA was then applied to all the normals in a particular set of crossings (these normals also resulted from MVA, but applied to magnetic field vectors measured in Saturn's MPCL).
If a "fan" of normals exists, which was often apparent based on a visual inspection, then the maximum variance direction resulting from the application of MVA to the set of normal vectors should lie approximately in the plane of this "fan".
The plane containing this maximum variance direction and the "n" axis is roughly the required "fan" plane.
The maximum variance direction was flipped if necessary to ensure that it had a positive sunward component, and it was rotated in this plane to ensure that it was perpendicular to "n" (the rotation was less than 10° in all cases).
The resulting vector is the "b" axis.
The "a" axis completes the right-handed, orthogonal set.
The maximum variance direction used to define "b" in this coordinate system was generally well defined, with a typical ratio of maximum to intermediate eigenvalue of ∼8.
As expected based on the results presented in Section 4, the "nb" plane was generally close to containing the direction perpendicular to the local magnetospheric magnetic field (typical rotation of the "nb" plane about the "n" axis required to make it perpendicular to the local magnetospheric field vector: <30°).
Fig. 8a shows how the angles α and β are defined.
The perturbation of a normal in the "na" plane is given by α, and the perturbation in the "nb" plane is given by β.
If the normals for a set of crossings were part of an oscillating pattern caused by waves (with a greater spatial scale in the perpendicular rather than parallel direction to wave propagation) we would expect the magnitude of the α angles to be generally less than that of the β angles, and the β angles to oscillate between positive and negative values between crossings, with a difference between consecutive β angles of greater than the approximate angular uncertainty associated with the normals (∼30°, see Section 3).
The sets of crossings shown in Figs.
3 and 4 are respective examples of sets where there is not, and is, evidence of an oscillation of consecutive surface normals.
Panel (e) of both figures shows the α and β angle for each crossing.
For the set shown in Fig. 3 the β angles are typically larger than the α angles (as expected given how surface wave coordinates are defined) but there is no clear positive-negative oscillation of β.
However, for the set of crossings shown in Fig. 4 the β angles are not only typically higher than the α angles, but also the β angles clearly follow a positive-negative oscillation.
The first crossing in Fig. 4 was not identified as part of the oscillating set because in this case β was less than α, and the normal was poorly defined, as indicated by the eigenvalue ratio shown in panel (c).
This analysis was carried out for each set of crossings in our total data set of 477.
A set of crossings was typically defined as the crossings made during a particular inbound/outbound pass of a spacecraft orbit, although in some orbits more than one set was defined on a single pass on the basis of distinct sets of crossing locations.
Sets containing less than 4 consecutive crossings were not included in this surface wave analysis, since any conclusions about normal oscillation that are drawn from so few crossings are inconclusive.
This led to the identification of all the crossings that were part of a pattern of oscillating normals that is strong evidence for surface wave activity, hereafter referred to as crossings associated with waves.
Of the 412 crossings in all the sets that were included in this surface wave analysis 53% were associated with waves.
These comprise 46% of the included dawn side crossings and 58% of the included dusk side crossings.
This lack of a significant dawn-dusk asymmetry is further evidence against the hypothesis that the growth of the K-H instability at Saturn's magnetopause leads to greater wave activity at dawn than at dusk (see Section 1).
Determination of wave properties
The crossings associated with waves provide information about the nature of wave activity on Saturn's magnetopause.
Since the Cassini spacecraft moves at a typical speed of ∼4kms-1 in the vicinity of Saturn's magnetopause and estimated speeds of waves on the surfaces of planetary magnetopauses suggest wave speeds of order 100kms-1 (e.g. Lepping and Burlaga, 1979), these crossings can shed light on the direction of wave propagation.
Panels (b) and (c) of Fig. 8 illustrate how this information is extracted.
For a sunward propagating wave the normal for an inbound crossing should be tilted sunward, and vice versa for outbound crossings; whereas for a tailward propagating wave the normal for an inbound crossing should be tilted antisunward, and vice versa for outbound crossings.
These surface waves could plausibly have been caused by the growth of the K-H instability.
In Section 4 we showed that the dominant direction of perturbations of the boundary normal, including those normals for crossings clearly associated with waves, is related to the direction of the local magnetospheric field vector, and we highlighted that this magnetic field influence is consistent with K-H instability theory (e.g. Southwood, 1968).
In addition, since evolving seed perturbations of a K-H unstable boundary are stationary in the center-of-mass frame we expect the tailward flow of dense magnetosheath solar wind plasma (relative to magnetospheric plasma) to largely control the motion of the center-of-mass frame with respect to the rest frame of the planet, consistent with the observed predominantly tailward wave propagation.
We note that the crossings associated with waves that suggest sunward wave propagation are among the closest crossings to the subsolar region, where the magnetosheath flow speed is slower and the motion of the center-of-mass frame with respect to the planet is more likely to deviate from tailward.
Other possible drivers to consider are magnetospheric period oscillations, magnetic reconnection at the magnetopause, and solar wind pressure fluctuations.
Magnetospheric period oscillations are not expected to lead to strong perturbations of the surface normal, and would produce sunward propagation at dawn and tailward at dusk (Clarke et al., 2010), which is not observed.
Limited evidence for reconnection at Saturn's magnetopause has been identified to date (McAndrews et al., 2008), and the relatively small magnetic field components presented in Fig. 5 also do not suggest widespread reconnection.
However, solar wind pressure fluctuations as a wave driving mechanism is consistent with both tailward propagation and the absence of a clear dawn-dusk difference in the level of surface wave activity (Ridley et al., 2006; Samsonov et al., 2006; Safránková et al., 2007).
Based on these considerations the identified surface waves on Saturn's magnetopause are most likely a superposition of waves driven by solar wind pressure fluctuations and waves driven by the growth of the K-H instability.
The task of extracting surface wave properties such as wave speed from a set of crossings of a planetary magnetopause is non-trivial, and in previous work a range of approaches have been used (e.g. Lepping and Burlaga, 1979; Foullon et al., 2008; Boardsen et al., 2010; Sundberg et al., 2010a, 2011).
In the case of Saturn's magnetopause it appears that relatively large solar wind dynamic pressure changes (Kanani et al., 2010), the magnetospheric period oscillation (Clarke et al., 2006, 2010), and surface wave activity (Masters et al., 2009; Cutler et al., 2011) are the major drivers of boundary dynamics, and the superposition of these effects will need to be carefully considered in future studies of specific sets of magnetopause encounters.
Furthermore, Masters et al. (2009) showed that multiple surface waveforms are possible.
However, since this study uses multiple sets of Cassini magnetopause crossings to carry out a global assessment of surface wave activity on Saturn's magnetopause we can use our larger total crossing set to infer the typical properties of the dominant surface waveform.
After isolating the crossings associated with waves we extracted all the excursion durations, defined as the time between consecutive crossings.
Histograms of these durations for both the wave-associated crossings on the dawn side and on the dusk side are shown in Fig. 10.
The large range of these durations could be due to differences in the wave properties between crossing sets, multiple surface waveforms, the range of possible spacecraft trajectories with respect to the waves, the influence of other effects (e.g. the magnetospheric period oscillation), or a combination of these.
Despite the large range of durations the median of each distribution gives us a dominant excursion duration of ∼2.6h at dawn and ∼1.6h at dusk, which we identify as half the period of the dominant surface waveform, giving a typical wave period of ∼5h at dawn and ∼3h at dusk.
Although our approach of isolating the crossings associated with clear wave activity does not rigorously remove the effect of magnetospheric period oscillation of the boundary, we note that these periods are clearly shorter than that of the oscillation (∼10.75h).
Furthermore, these periods are in reasonable agreement with the results of both the case study of surface wave activity on a particular dawn pass carried out by Masters et al. (2009) and the simulations of Saturn's magnetosphere carried out by Walker et al. (2011), which both suggested a dominant waveform with a period of order hours.
Although the relative importance of solar wind pressure fluctuations and growth of the K-H instability for generating magnetopause surface waves is unclear, the difference between the inferred dominant wave periods at dawn and dusk provides evidence that growth of the K-H instability is a major wave driving mechanism.
The center-of-mass argument described earlier in this section suggests that the oppositely directed magnetosheath and magnetospheric plasma flows at dawn should lead to a slower motion of the center-of-mass frame with respect to the planet than at dusk, producing a slower wave speed at dawn that could lead to a longer wave period compared to dusk.
This dawn-dusk difference in the period of the dominant waveform cannot be clearly explained by a solar wind pressure fluctuation driving mechanism alone, and demonstrates that the influence of growth of the K-H instability can be statistically resolved despite the possible presence of surface waves driven by this other mechanism (i.e. if there is a dawn-dusk asymmetry in growth of the K-H instability at Saturn's magnetopause the levels of surface wave activity calculated here should reflect this).
In the following we assume that the dominant wave periods correspond to K-H surface waves, allowing us to estimate further wave properties.
Linear K-H instability theory relates the wave vector (k) to the thickness of the LLBL (d) by kd∼1 (Miura and Pritchett, 1982), and this relationship is supported by simulations of waves on Saturn's magnetopause (Walker et al., 2011).
Using this relation and the median thickness of Saturn's LLBL calculated by Masters et al. (2011a) (∼2 RS) leads to a typical dominant wavelength of ∼10 RS at both dawn and dusk, since Masters et al. (2011a) found no evidence for a dawn-dusk asymmetry in the LLBL thickness.
Combining this wavelength with the dominant dawn and dusk periods leads to typical wave speeds of ∼30kms-1 at dawn, and ∼50kms-1 at dusk.
We note that the wave speed is likely to vary considerably across the dayside magnetopause, as flow conditions either side of the boundary change.
The nightside magnetopause is better sampled at dawn than at dusk by our set of crossings, but the removal of crossings in order to give a better balance between the dawn-dusk coverage of the nightside boundary does not have a significant effect on our results.
If we assume a sinusoidal waveform, and that the mean value of the wave angle β (see Section 5.1) for the crossings associated with waves represents the typical steepness of the waveform at the point of 0 amplitude (the position of the unperturbed surface), then we can estimate the typical wave amplitude as ∼0.6 RS at dawn and ∼0.9 RS at dusk.
Both the inferred wavelength of ∼10 RS and these amplitudes of order 1 RS are comparable to those calculated by Lepping et al. (1981) in their case study of surface waves on Saturn's magnetopause based on Voyager observations.
Separating the sunward and antisunward tilted normals for the crossings associated with waves allows us to examine wave steepening.
The mean β angle in these two subsets is ∼32 and ∼34°, respectively, therefore our approach and data set suggests that strong wave steepening is not widespread on the sampled region of Saturn's magnetopause.
Although there may be a latitude dependence on the level of surface wave activity and wave properties, the results presented in Section 4 and in the present section do not reveal such a dependence.
This may be due to the limited range of latitudes covered by the positions of the set of crossings used in this study.
Summary
In this paper we have used data taken by the Cassini spacecraft during 520 crossings of Saturn's magnetopause to examine the nature of wave activity on the boundary.
We have shown that strong perturbations of the boundary orientation can occur, the nature of which is primarily controlled by the orientation of the local magnetospheric magnetic field.
53% of the crossings were part of a sequence of regular oscillations arising in consecutive surface normals that is strong evidence for tailward propagating surface waves.
We estimated the typical period of the waves as ∼5h at dawn and ∼3h at dusk.
These results suggest that growth of the K-H instability is a major driver of these boundary normal perturbations and surface waves.
Using linear K-H theory, we also estimate the dominant wavelength to be ∼10 Saturn radii (RS) and amplitude to be ∼1 RS at both dawn and dusk, giving propagation speeds of ∼30 and ∼50kms-1 at dawn and dusk, respectively.
The main objective of this analysis of waves on Saturn's magnetopause was to test the prediction of a greater level of wave activity on the dawn side of the surface than on the dusk side, due to an assumed asymmetry in the nature of the growth of the K-H instability at the interface.
We find no evidence of either a significantly greater level of perturbations of the surface orientation or wave activity on the dawn side than on the dusk side.
In fact, for the sample of crossings analyzed, perturbations of the boundary orientation are generally greater at dusk than at dawn.
We note that the absence of a dawn-dusk asymmetry in wave activity reported here is consistent with studies based on Cassini data that identified initial evidence for waves on both the dawn and dusk side magnetopause (Masters et al., 2009; Cutler et al., 2011), and a study that has demonstrated that there is no clear local time asymmetry in the properties of Saturn's low-latitude boundary layer that could be attributed to a dawn-dusk asymmetry in the growth of the K-H instability at the magnetopause (Masters et al., 2011a).
This lack of observational support for a strong dawn-dusk asymmetry in surface waves on Saturn's magnetopause related to the K-H instability means we need to revise our understanding of this topic, particularly since this predicted asymmetry has been discussed in the context of magnetosphere-ionosphere coupling at Saturn (Galopeau et al., 1995; Sittler et al., 2006).
The hypothesized asymmetry was based on the clear differences in the flow shear across the boundary caused by the corotating sense of outer magnetospheric plasma motion that has been confirmed by Cassini (Thomsen et al., 2010), which makes the lack of observational support for a strong local time asymmetry surprising.
The K-H stability of a space plasma boundary is not only affected by flow conditions but also the plasma densities and local magnetic fields; thus it is likely that for the Saturnian magnetopause one or both of these other factors plays a more important role than we expected.
Furthermore, as discussed in Section 1, the presence of the LLBL and its associated plasma flow may have a strong effect on the growth of the K-H instability at Saturn's magnetopause.
To make further progress in understanding this topic there are four main directions for future research.
The first of these is a detailed examination of the dynamics of Saturn's magnetopause with the aim of assessing the relative importance of all the drivers of boundary motion, which would shed more light on the properties of surface waves.
The second is a search for Cassini encounters with K-H vortices, which would require a detailed analysis of Cassini ion data to infer the distinctive flow patterns associated with these complex structures.
The third is a detailed examination of the nature of plasma flow in Saturn's LLBL, and an assessment of how this plasma regime will affects the flow shear across the magnetopause.
The fourth is an assessment of the K-H stability of Saturn's magnetopause using Cassini measurements of all local plasma and magnetic field conditions.
Such a study would resolve the open issue concerning the K-H instability at Saturn's magnetopause that has been highlighted by the present study, and which also has implications for other corotation-dominated magnetospheres.
Acknowledgments
AM acknowledges useful discussions with S.W.H.
Cowley and H.
Hasegawa.
We acknowledge the support of the MAG and CAPS data processing/distribution staff, and L.K.
Gilbert and G.R.
Lewis for ELS data processing.
We acknowledge the use of the list of Cassini magnetopause crossings compiled by H.J.
McAndrews and S.J.
Kanani.
This work was supported by UK STFC through rolling grants to MSSL/UCL and Imperial College London.

Low-temperature CO oxidation on CuO/CeO 2 catalysts: the significant effect of copper precursor and calcination temperature

All chemicals were of analytical grade from Sinopharm Chemical Reagent Co., Ltd (China) and used without further purification. CuO/CeO2 catalysts were prepared by impregnating a commercial CeO2 support (SBET = 11.1 m2 g-1) with an aqueous solution of Cu(CH3COO)2, Cu(NO3)2, CuCl2, or CuSO4 to obtain a Cu loading of 10 wt.%. The CeO2 support was pre-treated at 120 degC for 4 h before use to remove the impurity adsorbed on the surface. After being impregnated quiescently at room temperature (RT) for 24 h, the sample was dried at 90 degC for 24 h, followed by calcination in static air at 500 or 800 degC for 4 h. For comparison, CuO was prepared by thermal decomposition of Cu(NO3)2 at 500 degC for 4 h. The prepared catalysts were denoted as CuO/CeO2(X)-Y, where X and Y refer to precursor (A for acetate, N for nitrate, C for chloride, and S for sulfate) and calcination temperature (500 or 800 degC), respectively.
Synthesis and electrochemical properties of LiNi0.8Co0.2O2 nanopowders for lithium ion battery applications
Nitrates of lithium, cobalt and nickel are utilized to synthesize LiNi0.8Co0.2O2 cathode material through sol-gel technique. Various synthesis parameters such as calcination time and temperature as well as chelating agent are studied to determine the optimized condition for material processing. Using TG/DTA techniques, the optimized calcination temperatures are selected. Different characterization techniques such as ICP, XRD and TEM are employed to characterize the chemical composition, crystal structure, size and morphology of the powders. Micron and nano-sized powders are produced using citric/oxalic and TEA as chelating agent, respectively. Selected powders are used as cathode material to assemble batteries. Charge-discharge testing of these batteries show that the highest discharge capacity is 173 mAh g-1 at a constant current of 0.1 mA cm-2, between 3.0 and 4.2 V. This is obtained in a battery assembled with the nanopowder produced by TEA as chelating agent.
The synthesis of LiNi0.8Co0.2O2 powders was carried out by sol-gel method using materials listed in Table 1. The stoichiometric values of lithium, nickel and cobalt nitrate salts in a cationic ratio of Li:Ni:Co equal to 1:0.8:0.2 were dissolved in distilled water. Three chelating agents including triethanolamine (TEA), citric acid, oxalic acid are used in this investigation. These agents were used in a molar ratio of 1 with respect to the total metal cations by dissolving in distilled water. The solution was then added to the nitrate solution and was heated under constant stirring at 80 degC for 1 h. The resulting sol was heated at 100 degC to form the gel which was dried subsequently. To remove the moisture and achieve the dried mass, the obtained gel was successively dried at 100, 120, 150, 200 degC for 2 h per each step and 5 h at 240 degC. The thermal behavior of LiNi0.8Co0.2O2 powders was determined by thermogravimetric/differential thermal analysis (TG/DTA) conducted on the dried gel at a heating rate of 5 degC min-1 up to 1100 degC in the air using a Perkin Elmer analyzer. The tests were conducted in the air using alumina pans. The initial sample weights for TEA, oxalic and citric were 460, 250 and 290 mg, respectively. The dried mass was heated at 500, 600, 700, 800 and 900 degC for varying durations and slowly cooled to room temperature. Inductively coupled plasma (ICP) analysis was performed using an ARL 3410 unit to confirm the stoichiometry of lithium, nickel and cobalt in the synthesized powders. The resulting powders had molar ratios of Li:Ni:Co in the order of 1:0.8:0.2, respectively.
